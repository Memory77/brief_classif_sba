{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 437765, number of negative: 111204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 548969, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.797431 -> initscore=1.370316\n",
      "[LightGBM] [Info] Start training from score 1.370316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.89      0.84      0.87     27801\n",
      "       P I F       0.96      0.97      0.97    109442\n",
      "\n",
      "    accuracy                           0.95    137243\n",
      "   macro avg       0.93      0.91      0.92    137243\n",
      "weighted avg       0.95      0.95      0.95    137243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', lgb.LGBMClassifier())\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Effectuer la recherche sur la grille\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Meilleurs hyperparamètres trouvés\u001b[39;00m\n\u001b[1;32m     86\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:890\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1307\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:914\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[0;32m--> 914\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:823\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    811\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    812\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    813\u001b[0m             delayed(func)(\n\u001b[1;32m    814\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m         )\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1307\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:543\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    541\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    548\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    549\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1064\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:1023\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1022\u001b[0m }\n\u001b[0;32m-> 1023\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:239\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[1;32m    236\u001b[0m             Xi[\u001b[38;5;241m~\u001b[39mvalid_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# We use check_unknown=False, since _check_unknown was\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# already called above.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     X_int[:, i] \u001b[38;5;241m=\u001b[39m _encode(Xi, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], check_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns_with_unknown:\n\u001b[1;32m    241\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    242\u001b[0m         (\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories in columns \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    248\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist()\n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', lgb.LGBMClassifier())])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__num_leaves': [20, 30],\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__min_child_samples': [20, 30],\n",
    "    'classifier__subsample': [0.6, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 1.0],\n",
    "}\n",
    "\n",
    "# rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Prédire les étiquettes sur l'ensemble de test\n",
    "# y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Créer une instance de GridSearchCV\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Effectuer la recherche sur la grille\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres trouvés\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Meilleur score obtenu sur l'ensemble d'entraînement\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Utiliser le meilleur modèle pour prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres:\", best_params)\n",
    "print(\"Meilleur score sur l'ensemble d'entraînement:\", best_score)\n",
    "print(\"Rapport de classification:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gbdt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'np.random.Generator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'split'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      LightGBM classifier.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Construct a gradient boosting model.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "boosting_type : str, optional (default='gbdt')\n",
      "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "    'rf', Random Forest.\n",
      "num_leaves : int, optional (default=31)\n",
      "    Maximum tree leaves for base learners.\n",
      "max_depth : int, optional (default=-1)\n",
      "    Maximum tree depth for base learners, <=0 means no limit.\n",
      "learning_rate : float, optional (default=0.1)\n",
      "    Boosting learning rate.\n",
      "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "    in training using ``reset_parameter`` callback.\n",
      "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "n_estimators : int, optional (default=100)\n",
      "    Number of boosted trees to fit.\n",
      "subsample_for_bin : int, optional (default=200000)\n",
      "    Number of samples for constructing bins.\n",
      "objective : str, callable or None, optional (default=None)\n",
      "    Specify the learning task and the corresponding learning objective or\n",
      "    a custom objective function to be used (see note below).\n",
      "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "    Weights associated with classes in the form ``{class_label: weight}``.\n",
      "    Use this parameter only for multi-class classification task;\n",
      "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "    You may want to consider performing probability calibration\n",
      "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "    If None, all classes are supposed to have weight one.\n",
      "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "    if ``sample_weight`` is specified.\n",
      "min_split_gain : float, optional (default=0.)\n",
      "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "min_child_weight : float, optional (default=1e-3)\n",
      "    Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
      "min_child_samples : int, optional (default=20)\n",
      "    Minimum number of data needed in a child (leaf).\n",
      "subsample : float, optional (default=1.)\n",
      "    Subsample ratio of the training instance.\n",
      "subsample_freq : int, optional (default=0)\n",
      "    Frequency of subsample, <=0 means no enable.\n",
      "colsample_bytree : float, optional (default=1.)\n",
      "    Subsample ratio of columns when constructing each tree.\n",
      "reg_alpha : float, optional (default=0.)\n",
      "    L1 regularization term on weights.\n",
      "reg_lambda : float, optional (default=0.)\n",
      "    L2 regularization term on weights.\n",
      "random_state : int, RandomState object or None, optional (default=None)\n",
      "    Random number seed.\n",
      "    If int, this number is used to seed the C++ code.\n",
      "    If RandomState or Generator object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "    If None, default seeds in C++ code are used.\n",
      "n_jobs : int or None, optional (default=None)\n",
      "    Number of parallel threads to use for training (can be changed at prediction time by\n",
      "    passing it as an extra keyword argument).\n",
      "\n",
      "    For better performance, it is recommended to set this to the number of physical cores\n",
      "    in the CPU.\n",
      "\n",
      "    Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
      "    scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
      "    threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
      "    to using the number of physical cores in the system (its correct detection requires\n",
      "    either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
      "\n",
      "    .. versionchanged:: 4.0.0\n",
      "\n",
      "importance_type : str, optional (default='split')\n",
      "    The type of feature importance to be filled into ``feature_importances_``.\n",
      "    If 'split', result contains numbers of times the feature is used in a model.\n",
      "    If 'gain', result contains total gains of splits which use the feature.\n",
      "**kwargs\n",
      "    Other parameters for the model.\n",
      "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "\n",
      "Note\n",
      "----\n",
      "A custom objective function can be provided for the ``objective`` parameter.\n",
      "In this case, it should have the signature\n",
      "``objective(y_true, y_pred) -> grad, hess``,\n",
      "``objective(y_true, y_pred, weight) -> grad, hess``\n",
      "or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
      "\n",
      "    y_true : numpy 1-D array of shape = [n_samples]\n",
      "        The target values.\n",
      "    y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "        The predicted values.\n",
      "        Predicted values are returned before any transformation,\n",
      "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
      "    weight : numpy 1-D array of shape = [n_samples]\n",
      "        The weight of samples. Weights should be non-negative.\n",
      "    group : numpy 1-D array\n",
      "        Group/query data.\n",
      "        Only used in the learning-to-rank task.\n",
      "        sum(group) = n_samples.\n",
      "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "    grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "        The value of the first order derivative (gradient) of the loss\n",
      "        with respect to the elements of y_pred for each sample point.\n",
      "    hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "        The value of the second order derivative (Hessian) of the loss\n",
      "        with respect to the elements of y_pred for each sample point.\n",
      "\n",
      "For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
      "and grad and hess should be returned in the same format.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/lightgbm/sklearn.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     DaskLGBMClassifier"
     ]
    }
   ],
   "source": [
    "lgb.LGBMClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.163904\n",
      "0:\tlearn: 0.5133075\ttotal: 133ms\tremaining: 2m 12s\n",
      "1:\tlearn: 0.4105912\ttotal: 253ms\tremaining: 2m 6s\n",
      "2:\tlearn: 0.3491368\ttotal: 361ms\tremaining: 1m 59s\n",
      "3:\tlearn: 0.3135722\ttotal: 488ms\tremaining: 2m 1s\n",
      "4:\tlearn: 0.2853642\ttotal: 612ms\tremaining: 2m 1s\n",
      "5:\tlearn: 0.2609955\ttotal: 732ms\tremaining: 2m 1s\n",
      "6:\tlearn: 0.2431557\ttotal: 857ms\tremaining: 2m 1s\n",
      "7:\tlearn: 0.2328903\ttotal: 957ms\tremaining: 1m 58s\n",
      "8:\tlearn: 0.2258369\ttotal: 1.06s\tremaining: 1m 57s\n",
      "9:\tlearn: 0.2197614\ttotal: 1.19s\tremaining: 1m 57s\n",
      "10:\tlearn: 0.2139799\ttotal: 1.31s\tremaining: 1m 58s\n",
      "11:\tlearn: 0.2076089\ttotal: 1.43s\tremaining: 1m 58s\n",
      "12:\tlearn: 0.2041217\ttotal: 1.54s\tremaining: 1m 57s\n",
      "13:\tlearn: 0.2011700\ttotal: 1.65s\tremaining: 1m 56s\n",
      "14:\tlearn: 0.1985280\ttotal: 1.76s\tremaining: 1m 55s\n",
      "15:\tlearn: 0.1944670\ttotal: 1.88s\tremaining: 1m 55s\n",
      "16:\tlearn: 0.1911154\ttotal: 2.01s\tremaining: 1m 56s\n",
      "17:\tlearn: 0.1889839\ttotal: 2.12s\tremaining: 1m 55s\n",
      "18:\tlearn: 0.1865988\ttotal: 2.23s\tremaining: 1m 55s\n",
      "19:\tlearn: 0.1848886\ttotal: 2.35s\tremaining: 1m 55s\n",
      "20:\tlearn: 0.1830137\ttotal: 2.49s\tremaining: 1m 55s\n",
      "21:\tlearn: 0.1813309\ttotal: 2.6s\tremaining: 1m 55s\n",
      "22:\tlearn: 0.1800141\ttotal: 2.71s\tremaining: 1m 55s\n",
      "23:\tlearn: 0.1773158\ttotal: 2.82s\tremaining: 1m 54s\n",
      "24:\tlearn: 0.1758628\ttotal: 2.95s\tremaining: 1m 54s\n",
      "25:\tlearn: 0.1747780\ttotal: 3.08s\tremaining: 1m 55s\n",
      "26:\tlearn: 0.1733057\ttotal: 3.2s\tremaining: 1m 55s\n",
      "27:\tlearn: 0.1724486\ttotal: 3.32s\tremaining: 1m 55s\n",
      "28:\tlearn: 0.1706810\ttotal: 3.43s\tremaining: 1m 54s\n",
      "29:\tlearn: 0.1696458\ttotal: 3.55s\tremaining: 1m 54s\n",
      "30:\tlearn: 0.1688176\ttotal: 3.67s\tremaining: 1m 54s\n",
      "31:\tlearn: 0.1679482\ttotal: 3.79s\tremaining: 1m 54s\n",
      "32:\tlearn: 0.1668786\ttotal: 3.9s\tremaining: 1m 54s\n",
      "33:\tlearn: 0.1661693\ttotal: 4.02s\tremaining: 1m 54s\n",
      "34:\tlearn: 0.1652166\ttotal: 4.14s\tremaining: 1m 54s\n",
      "35:\tlearn: 0.1645101\ttotal: 4.26s\tremaining: 1m 53s\n",
      "36:\tlearn: 0.1631914\ttotal: 4.37s\tremaining: 1m 53s\n",
      "37:\tlearn: 0.1626316\ttotal: 4.48s\tremaining: 1m 53s\n",
      "38:\tlearn: 0.1616258\ttotal: 4.59s\tremaining: 1m 53s\n",
      "39:\tlearn: 0.1609100\ttotal: 4.71s\tremaining: 1m 53s\n",
      "40:\tlearn: 0.1602982\ttotal: 4.83s\tremaining: 1m 52s\n",
      "41:\tlearn: 0.1595055\ttotal: 4.95s\tremaining: 1m 52s\n",
      "42:\tlearn: 0.1581369\ttotal: 5.06s\tremaining: 1m 52s\n",
      "43:\tlearn: 0.1576114\ttotal: 5.17s\tremaining: 1m 52s\n",
      "44:\tlearn: 0.1571364\ttotal: 5.29s\tremaining: 1m 52s\n",
      "45:\tlearn: 0.1566349\ttotal: 5.4s\tremaining: 1m 52s\n",
      "46:\tlearn: 0.1560390\ttotal: 5.52s\tremaining: 1m 51s\n",
      "47:\tlearn: 0.1554296\ttotal: 5.63s\tremaining: 1m 51s\n",
      "48:\tlearn: 0.1549824\ttotal: 5.75s\tremaining: 1m 51s\n",
      "49:\tlearn: 0.1546806\ttotal: 5.85s\tremaining: 1m 51s\n",
      "50:\tlearn: 0.1539480\ttotal: 5.98s\tremaining: 1m 51s\n",
      "51:\tlearn: 0.1533332\ttotal: 6.09s\tremaining: 1m 51s\n",
      "52:\tlearn: 0.1528745\ttotal: 6.2s\tremaining: 1m 50s\n",
      "53:\tlearn: 0.1524664\ttotal: 6.31s\tremaining: 1m 50s\n",
      "54:\tlearn: 0.1521309\ttotal: 6.53s\tremaining: 1m 52s\n",
      "55:\tlearn: 0.1516760\ttotal: 6.66s\tremaining: 1m 52s\n",
      "56:\tlearn: 0.1511918\ttotal: 6.77s\tremaining: 1m 52s\n",
      "57:\tlearn: 0.1506964\ttotal: 6.88s\tremaining: 1m 51s\n",
      "58:\tlearn: 0.1503142\ttotal: 6.99s\tremaining: 1m 51s\n",
      "59:\tlearn: 0.1500804\ttotal: 7.1s\tremaining: 1m 51s\n",
      "60:\tlearn: 0.1497068\ttotal: 7.21s\tremaining: 1m 51s\n",
      "61:\tlearn: 0.1493490\ttotal: 7.32s\tremaining: 1m 50s\n",
      "62:\tlearn: 0.1489953\ttotal: 7.44s\tremaining: 1m 50s\n",
      "63:\tlearn: 0.1483547\ttotal: 7.56s\tremaining: 1m 50s\n",
      "64:\tlearn: 0.1480856\ttotal: 7.68s\tremaining: 1m 50s\n",
      "65:\tlearn: 0.1477322\ttotal: 7.79s\tremaining: 1m 50s\n",
      "66:\tlearn: 0.1471626\ttotal: 7.91s\tremaining: 1m 50s\n",
      "67:\tlearn: 0.1468113\ttotal: 8.03s\tremaining: 1m 50s\n",
      "68:\tlearn: 0.1465684\ttotal: 8.15s\tremaining: 1m 49s\n",
      "69:\tlearn: 0.1461284\ttotal: 8.26s\tremaining: 1m 49s\n",
      "70:\tlearn: 0.1457951\ttotal: 8.38s\tremaining: 1m 49s\n",
      "71:\tlearn: 0.1454287\ttotal: 8.49s\tremaining: 1m 49s\n",
      "72:\tlearn: 0.1450957\ttotal: 8.61s\tremaining: 1m 49s\n",
      "73:\tlearn: 0.1447429\ttotal: 8.71s\tremaining: 1m 49s\n",
      "74:\tlearn: 0.1445330\ttotal: 8.82s\tremaining: 1m 48s\n",
      "75:\tlearn: 0.1442521\ttotal: 8.93s\tremaining: 1m 48s\n",
      "76:\tlearn: 0.1439433\ttotal: 9.04s\tremaining: 1m 48s\n",
      "77:\tlearn: 0.1436972\ttotal: 9.15s\tremaining: 1m 48s\n",
      "78:\tlearn: 0.1433928\ttotal: 9.27s\tremaining: 1m 48s\n",
      "79:\tlearn: 0.1430298\ttotal: 9.37s\tremaining: 1m 47s\n",
      "80:\tlearn: 0.1427762\ttotal: 9.49s\tremaining: 1m 47s\n",
      "81:\tlearn: 0.1425715\ttotal: 9.6s\tremaining: 1m 47s\n",
      "82:\tlearn: 0.1422116\ttotal: 9.71s\tremaining: 1m 47s\n",
      "83:\tlearn: 0.1419423\ttotal: 9.82s\tremaining: 1m 47s\n",
      "84:\tlearn: 0.1416328\ttotal: 9.93s\tremaining: 1m 46s\n",
      "85:\tlearn: 0.1413018\ttotal: 10.1s\tremaining: 1m 46s\n",
      "86:\tlearn: 0.1409400\ttotal: 10.2s\tremaining: 1m 46s\n",
      "87:\tlearn: 0.1406539\ttotal: 10.3s\tremaining: 1m 46s\n",
      "88:\tlearn: 0.1404359\ttotal: 10.4s\tremaining: 1m 46s\n",
      "89:\tlearn: 0.1401606\ttotal: 10.5s\tremaining: 1m 46s\n",
      "90:\tlearn: 0.1399762\ttotal: 10.6s\tremaining: 1m 46s\n",
      "91:\tlearn: 0.1397028\ttotal: 10.7s\tremaining: 1m 45s\n",
      "92:\tlearn: 0.1394923\ttotal: 10.8s\tremaining: 1m 45s\n",
      "93:\tlearn: 0.1393145\ttotal: 10.9s\tremaining: 1m 45s\n",
      "94:\tlearn: 0.1390141\ttotal: 11s\tremaining: 1m 45s\n",
      "95:\tlearn: 0.1386463\ttotal: 11.2s\tremaining: 1m 45s\n",
      "96:\tlearn: 0.1385194\ttotal: 11.3s\tremaining: 1m 44s\n",
      "97:\tlearn: 0.1381555\ttotal: 11.4s\tremaining: 1m 44s\n",
      "98:\tlearn: 0.1379345\ttotal: 11.5s\tremaining: 1m 44s\n",
      "99:\tlearn: 0.1377025\ttotal: 11.6s\tremaining: 1m 44s\n",
      "100:\tlearn: 0.1375426\ttotal: 11.7s\tremaining: 1m 44s\n",
      "101:\tlearn: 0.1373099\ttotal: 11.8s\tremaining: 1m 43s\n",
      "102:\tlearn: 0.1370324\ttotal: 11.9s\tremaining: 1m 43s\n",
      "103:\tlearn: 0.1367817\ttotal: 12s\tremaining: 1m 43s\n",
      "104:\tlearn: 0.1363139\ttotal: 12.2s\tremaining: 1m 43s\n",
      "105:\tlearn: 0.1359756\ttotal: 12.3s\tremaining: 1m 43s\n",
      "106:\tlearn: 0.1358082\ttotal: 12.4s\tremaining: 1m 43s\n",
      "107:\tlearn: 0.1356046\ttotal: 12.5s\tremaining: 1m 43s\n",
      "108:\tlearn: 0.1354732\ttotal: 12.6s\tremaining: 1m 42s\n",
      "109:\tlearn: 0.1352382\ttotal: 12.7s\tremaining: 1m 42s\n",
      "110:\tlearn: 0.1350351\ttotal: 12.8s\tremaining: 1m 42s\n",
      "111:\tlearn: 0.1348896\ttotal: 12.9s\tremaining: 1m 42s\n",
      "112:\tlearn: 0.1347157\ttotal: 13s\tremaining: 1m 42s\n",
      "113:\tlearn: 0.1345918\ttotal: 13.1s\tremaining: 1m 42s\n",
      "114:\tlearn: 0.1343610\ttotal: 13.3s\tremaining: 1m 42s\n",
      "115:\tlearn: 0.1341562\ttotal: 13.4s\tremaining: 1m 41s\n",
      "116:\tlearn: 0.1340317\ttotal: 13.5s\tremaining: 1m 41s\n",
      "117:\tlearn: 0.1338646\ttotal: 13.6s\tremaining: 1m 41s\n",
      "118:\tlearn: 0.1337146\ttotal: 13.7s\tremaining: 1m 41s\n",
      "119:\tlearn: 0.1334719\ttotal: 13.8s\tremaining: 1m 41s\n",
      "120:\tlearn: 0.1333480\ttotal: 13.9s\tremaining: 1m 41s\n",
      "121:\tlearn: 0.1330940\ttotal: 14s\tremaining: 1m 41s\n",
      "122:\tlearn: 0.1328448\ttotal: 14.2s\tremaining: 1m 40s\n",
      "123:\tlearn: 0.1326977\ttotal: 14.3s\tremaining: 1m 40s\n",
      "124:\tlearn: 0.1325118\ttotal: 14.4s\tremaining: 1m 40s\n",
      "125:\tlearn: 0.1323107\ttotal: 14.5s\tremaining: 1m 40s\n",
      "126:\tlearn: 0.1320056\ttotal: 14.6s\tremaining: 1m 40s\n",
      "127:\tlearn: 0.1318350\ttotal: 14.7s\tremaining: 1m 40s\n",
      "128:\tlearn: 0.1316352\ttotal: 14.8s\tremaining: 1m 40s\n",
      "129:\tlearn: 0.1314644\ttotal: 15s\tremaining: 1m 40s\n",
      "130:\tlearn: 0.1312548\ttotal: 15.1s\tremaining: 1m 39s\n",
      "131:\tlearn: 0.1311208\ttotal: 15.2s\tremaining: 1m 39s\n",
      "132:\tlearn: 0.1308947\ttotal: 15.3s\tremaining: 1m 39s\n",
      "133:\tlearn: 0.1307360\ttotal: 15.4s\tremaining: 1m 39s\n",
      "134:\tlearn: 0.1305837\ttotal: 15.5s\tremaining: 1m 39s\n",
      "135:\tlearn: 0.1304544\ttotal: 15.6s\tremaining: 1m 39s\n",
      "136:\tlearn: 0.1303163\ttotal: 15.7s\tremaining: 1m 39s\n",
      "137:\tlearn: 0.1300877\ttotal: 15.9s\tremaining: 1m 39s\n",
      "138:\tlearn: 0.1299515\ttotal: 16s\tremaining: 1m 38s\n",
      "139:\tlearn: 0.1297764\ttotal: 16.1s\tremaining: 1m 38s\n",
      "140:\tlearn: 0.1296765\ttotal: 16.2s\tremaining: 1m 38s\n",
      "141:\tlearn: 0.1295566\ttotal: 16.3s\tremaining: 1m 38s\n",
      "142:\tlearn: 0.1293414\ttotal: 16.4s\tremaining: 1m 38s\n",
      "143:\tlearn: 0.1291914\ttotal: 16.5s\tremaining: 1m 38s\n",
      "144:\tlearn: 0.1290562\ttotal: 16.6s\tremaining: 1m 38s\n",
      "145:\tlearn: 0.1288912\ttotal: 16.7s\tremaining: 1m 37s\n",
      "146:\tlearn: 0.1287479\ttotal: 16.9s\tremaining: 1m 37s\n",
      "147:\tlearn: 0.1286063\ttotal: 17.1s\tremaining: 1m 38s\n",
      "148:\tlearn: 0.1285115\ttotal: 17.2s\tremaining: 1m 38s\n",
      "149:\tlearn: 0.1284296\ttotal: 17.3s\tremaining: 1m 38s\n",
      "150:\tlearn: 0.1283034\ttotal: 17.4s\tremaining: 1m 38s\n",
      "151:\tlearn: 0.1281235\ttotal: 17.6s\tremaining: 1m 37s\n",
      "152:\tlearn: 0.1279027\ttotal: 17.7s\tremaining: 1m 37s\n",
      "153:\tlearn: 0.1277333\ttotal: 17.8s\tremaining: 1m 37s\n",
      "154:\tlearn: 0.1274803\ttotal: 17.9s\tremaining: 1m 37s\n",
      "155:\tlearn: 0.1273281\ttotal: 18s\tremaining: 1m 37s\n",
      "156:\tlearn: 0.1271808\ttotal: 18.1s\tremaining: 1m 37s\n",
      "157:\tlearn: 0.1270541\ttotal: 18.2s\tremaining: 1m 37s\n",
      "158:\tlearn: 0.1268544\ttotal: 18.3s\tremaining: 1m 37s\n",
      "159:\tlearn: 0.1267563\ttotal: 18.5s\tremaining: 1m 36s\n",
      "160:\tlearn: 0.1266249\ttotal: 18.6s\tremaining: 1m 36s\n",
      "161:\tlearn: 0.1265364\ttotal: 18.7s\tremaining: 1m 36s\n",
      "162:\tlearn: 0.1263823\ttotal: 18.8s\tremaining: 1m 36s\n",
      "163:\tlearn: 0.1262838\ttotal: 18.9s\tremaining: 1m 36s\n",
      "164:\tlearn: 0.1261647\ttotal: 19s\tremaining: 1m 36s\n",
      "165:\tlearn: 0.1260577\ttotal: 19.1s\tremaining: 1m 36s\n",
      "166:\tlearn: 0.1258919\ttotal: 19.2s\tremaining: 1m 35s\n",
      "167:\tlearn: 0.1257768\ttotal: 19.4s\tremaining: 1m 35s\n",
      "168:\tlearn: 0.1256650\ttotal: 19.5s\tremaining: 1m 35s\n",
      "169:\tlearn: 0.1255694\ttotal: 19.6s\tremaining: 1m 35s\n",
      "170:\tlearn: 0.1254617\ttotal: 19.7s\tremaining: 1m 35s\n",
      "171:\tlearn: 0.1253538\ttotal: 19.8s\tremaining: 1m 35s\n",
      "172:\tlearn: 0.1252045\ttotal: 19.9s\tremaining: 1m 35s\n",
      "173:\tlearn: 0.1250303\ttotal: 20s\tremaining: 1m 35s\n",
      "174:\tlearn: 0.1249362\ttotal: 20.1s\tremaining: 1m 34s\n",
      "175:\tlearn: 0.1248058\ttotal: 20.2s\tremaining: 1m 34s\n",
      "176:\tlearn: 0.1246902\ttotal: 20.4s\tremaining: 1m 34s\n",
      "177:\tlearn: 0.1246143\ttotal: 20.5s\tremaining: 1m 34s\n",
      "178:\tlearn: 0.1245124\ttotal: 20.6s\tremaining: 1m 34s\n",
      "179:\tlearn: 0.1243833\ttotal: 20.7s\tremaining: 1m 34s\n",
      "180:\tlearn: 0.1242653\ttotal: 20.8s\tremaining: 1m 34s\n",
      "181:\tlearn: 0.1241444\ttotal: 20.9s\tremaining: 1m 34s\n",
      "182:\tlearn: 0.1239848\ttotal: 21s\tremaining: 1m 33s\n",
      "183:\tlearn: 0.1237757\ttotal: 21.2s\tremaining: 1m 33s\n",
      "184:\tlearn: 0.1237084\ttotal: 21.3s\tremaining: 1m 33s\n",
      "185:\tlearn: 0.1235393\ttotal: 21.4s\tremaining: 1m 33s\n",
      "186:\tlearn: 0.1234708\ttotal: 21.5s\tremaining: 1m 33s\n",
      "187:\tlearn: 0.1233041\ttotal: 21.6s\tremaining: 1m 33s\n",
      "188:\tlearn: 0.1232040\ttotal: 21.7s\tremaining: 1m 33s\n",
      "189:\tlearn: 0.1231466\ttotal: 21.8s\tremaining: 1m 32s\n",
      "190:\tlearn: 0.1230257\ttotal: 21.9s\tremaining: 1m 32s\n",
      "191:\tlearn: 0.1229408\ttotal: 22s\tremaining: 1m 32s\n",
      "192:\tlearn: 0.1228256\ttotal: 22.1s\tremaining: 1m 32s\n",
      "193:\tlearn: 0.1227153\ttotal: 22.3s\tremaining: 1m 32s\n",
      "194:\tlearn: 0.1226127\ttotal: 22.4s\tremaining: 1m 32s\n",
      "195:\tlearn: 0.1225331\ttotal: 22.5s\tremaining: 1m 32s\n",
      "196:\tlearn: 0.1224462\ttotal: 22.6s\tremaining: 1m 32s\n",
      "197:\tlearn: 0.1223710\ttotal: 22.7s\tremaining: 1m 31s\n",
      "198:\tlearn: 0.1222829\ttotal: 22.8s\tremaining: 1m 31s\n",
      "199:\tlearn: 0.1222196\ttotal: 22.9s\tremaining: 1m 31s\n",
      "200:\tlearn: 0.1221243\ttotal: 23s\tremaining: 1m 31s\n",
      "201:\tlearn: 0.1220321\ttotal: 23.2s\tremaining: 1m 31s\n",
      "202:\tlearn: 0.1219594\ttotal: 23.3s\tremaining: 1m 31s\n",
      "203:\tlearn: 0.1218687\ttotal: 23.4s\tremaining: 1m 31s\n",
      "204:\tlearn: 0.1217812\ttotal: 23.5s\tremaining: 1m 31s\n",
      "205:\tlearn: 0.1216567\ttotal: 23.6s\tremaining: 1m 30s\n",
      "206:\tlearn: 0.1215594\ttotal: 23.7s\tremaining: 1m 30s\n",
      "207:\tlearn: 0.1214802\ttotal: 23.8s\tremaining: 1m 30s\n",
      "208:\tlearn: 0.1213875\ttotal: 23.9s\tremaining: 1m 30s\n",
      "209:\tlearn: 0.1213309\ttotal: 24s\tremaining: 1m 30s\n",
      "210:\tlearn: 0.1212334\ttotal: 24.2s\tremaining: 1m 30s\n",
      "211:\tlearn: 0.1211200\ttotal: 24.3s\tremaining: 1m 30s\n",
      "212:\tlearn: 0.1209781\ttotal: 24.4s\tremaining: 1m 30s\n",
      "213:\tlearn: 0.1208681\ttotal: 24.5s\tremaining: 1m 30s\n",
      "214:\tlearn: 0.1207434\ttotal: 24.6s\tremaining: 1m 29s\n",
      "215:\tlearn: 0.1206788\ttotal: 24.7s\tremaining: 1m 29s\n",
      "216:\tlearn: 0.1205854\ttotal: 24.9s\tremaining: 1m 29s\n",
      "217:\tlearn: 0.1204586\ttotal: 25s\tremaining: 1m 29s\n",
      "218:\tlearn: 0.1203515\ttotal: 25.1s\tremaining: 1m 29s\n",
      "219:\tlearn: 0.1202720\ttotal: 25.2s\tremaining: 1m 29s\n",
      "220:\tlearn: 0.1201863\ttotal: 25.3s\tremaining: 1m 29s\n",
      "221:\tlearn: 0.1201002\ttotal: 25.4s\tremaining: 1m 29s\n",
      "222:\tlearn: 0.1199851\ttotal: 25.5s\tremaining: 1m 28s\n",
      "223:\tlearn: 0.1199015\ttotal: 25.6s\tremaining: 1m 28s\n",
      "224:\tlearn: 0.1197874\ttotal: 25.8s\tremaining: 1m 28s\n",
      "225:\tlearn: 0.1197104\ttotal: 25.9s\tremaining: 1m 28s\n",
      "226:\tlearn: 0.1195866\ttotal: 26s\tremaining: 1m 28s\n",
      "227:\tlearn: 0.1195302\ttotal: 26.1s\tremaining: 1m 28s\n",
      "228:\tlearn: 0.1194663\ttotal: 26.2s\tremaining: 1m 28s\n",
      "229:\tlearn: 0.1193890\ttotal: 26.3s\tremaining: 1m 28s\n",
      "230:\tlearn: 0.1192589\ttotal: 26.4s\tremaining: 1m 27s\n",
      "231:\tlearn: 0.1191519\ttotal: 26.5s\tremaining: 1m 27s\n",
      "232:\tlearn: 0.1190783\ttotal: 26.7s\tremaining: 1m 27s\n",
      "233:\tlearn: 0.1189882\ttotal: 26.8s\tremaining: 1m 27s\n",
      "234:\tlearn: 0.1189373\ttotal: 26.9s\tremaining: 1m 27s\n",
      "235:\tlearn: 0.1188184\ttotal: 27s\tremaining: 1m 27s\n",
      "236:\tlearn: 0.1187615\ttotal: 27.1s\tremaining: 1m 27s\n",
      "237:\tlearn: 0.1186542\ttotal: 27.2s\tremaining: 1m 27s\n",
      "238:\tlearn: 0.1185858\ttotal: 27.3s\tremaining: 1m 26s\n",
      "239:\tlearn: 0.1185084\ttotal: 27.4s\tremaining: 1m 26s\n",
      "240:\tlearn: 0.1183863\ttotal: 27.5s\tremaining: 1m 26s\n",
      "241:\tlearn: 0.1183176\ttotal: 27.6s\tremaining: 1m 26s\n",
      "242:\tlearn: 0.1182383\ttotal: 27.8s\tremaining: 1m 26s\n",
      "243:\tlearn: 0.1181757\ttotal: 27.9s\tremaining: 1m 26s\n",
      "244:\tlearn: 0.1180777\ttotal: 28.1s\tremaining: 1m 26s\n",
      "245:\tlearn: 0.1179578\ttotal: 28.2s\tremaining: 1m 26s\n",
      "246:\tlearn: 0.1178907\ttotal: 28.3s\tremaining: 1m 26s\n",
      "247:\tlearn: 0.1178208\ttotal: 28.4s\tremaining: 1m 26s\n",
      "248:\tlearn: 0.1177338\ttotal: 28.5s\tremaining: 1m 26s\n",
      "249:\tlearn: 0.1176480\ttotal: 28.7s\tremaining: 1m 25s\n",
      "250:\tlearn: 0.1175371\ttotal: 28.8s\tremaining: 1m 25s\n",
      "251:\tlearn: 0.1174275\ttotal: 28.9s\tremaining: 1m 25s\n",
      "252:\tlearn: 0.1173555\ttotal: 29s\tremaining: 1m 25s\n",
      "253:\tlearn: 0.1172917\ttotal: 29.1s\tremaining: 1m 25s\n",
      "254:\tlearn: 0.1172253\ttotal: 29.2s\tremaining: 1m 25s\n",
      "255:\tlearn: 0.1171823\ttotal: 29.3s\tremaining: 1m 25s\n",
      "256:\tlearn: 0.1170868\ttotal: 29.4s\tremaining: 1m 25s\n",
      "257:\tlearn: 0.1170257\ttotal: 29.5s\tremaining: 1m 24s\n",
      "258:\tlearn: 0.1169676\ttotal: 29.6s\tremaining: 1m 24s\n",
      "259:\tlearn: 0.1169160\ttotal: 29.8s\tremaining: 1m 24s\n",
      "260:\tlearn: 0.1168230\ttotal: 29.9s\tremaining: 1m 24s\n",
      "261:\tlearn: 0.1167436\ttotal: 30s\tremaining: 1m 24s\n",
      "262:\tlearn: 0.1166934\ttotal: 30.1s\tremaining: 1m 24s\n",
      "263:\tlearn: 0.1166043\ttotal: 30.2s\tremaining: 1m 24s\n",
      "264:\tlearn: 0.1165534\ttotal: 30.3s\tremaining: 1m 24s\n",
      "265:\tlearn: 0.1164630\ttotal: 30.4s\tremaining: 1m 23s\n",
      "266:\tlearn: 0.1164093\ttotal: 30.5s\tremaining: 1m 23s\n",
      "267:\tlearn: 0.1163832\ttotal: 30.6s\tremaining: 1m 23s\n",
      "268:\tlearn: 0.1162775\ttotal: 30.8s\tremaining: 1m 23s\n",
      "269:\tlearn: 0.1162097\ttotal: 30.9s\tremaining: 1m 23s\n",
      "270:\tlearn: 0.1161047\ttotal: 31s\tremaining: 1m 23s\n",
      "271:\tlearn: 0.1160374\ttotal: 31.1s\tremaining: 1m 23s\n",
      "272:\tlearn: 0.1159059\ttotal: 31.2s\tremaining: 1m 23s\n",
      "273:\tlearn: 0.1157560\ttotal: 31.3s\tremaining: 1m 22s\n",
      "274:\tlearn: 0.1156773\ttotal: 31.4s\tremaining: 1m 22s\n",
      "275:\tlearn: 0.1155896\ttotal: 31.5s\tremaining: 1m 22s\n",
      "276:\tlearn: 0.1155061\ttotal: 31.7s\tremaining: 1m 22s\n",
      "277:\tlearn: 0.1154273\ttotal: 31.8s\tremaining: 1m 22s\n",
      "278:\tlearn: 0.1153558\ttotal: 31.9s\tremaining: 1m 22s\n",
      "279:\tlearn: 0.1152546\ttotal: 32s\tremaining: 1m 22s\n",
      "280:\tlearn: 0.1151662\ttotal: 32.1s\tremaining: 1m 22s\n",
      "281:\tlearn: 0.1150964\ttotal: 32.2s\tremaining: 1m 22s\n",
      "282:\tlearn: 0.1150047\ttotal: 32.3s\tremaining: 1m 21s\n",
      "283:\tlearn: 0.1149627\ttotal: 32.4s\tremaining: 1m 21s\n",
      "284:\tlearn: 0.1149163\ttotal: 32.5s\tremaining: 1m 21s\n",
      "285:\tlearn: 0.1148226\ttotal: 32.7s\tremaining: 1m 21s\n",
      "286:\tlearn: 0.1147484\ttotal: 32.8s\tremaining: 1m 21s\n",
      "287:\tlearn: 0.1146876\ttotal: 32.9s\tremaining: 1m 21s\n",
      "288:\tlearn: 0.1146073\ttotal: 33s\tremaining: 1m 21s\n",
      "289:\tlearn: 0.1145656\ttotal: 33.1s\tremaining: 1m 21s\n",
      "290:\tlearn: 0.1144964\ttotal: 33.2s\tremaining: 1m 20s\n",
      "291:\tlearn: 0.1144363\ttotal: 33.3s\tremaining: 1m 20s\n",
      "292:\tlearn: 0.1143560\ttotal: 33.4s\tremaining: 1m 20s\n",
      "293:\tlearn: 0.1142566\ttotal: 33.6s\tremaining: 1m 20s\n",
      "294:\tlearn: 0.1141785\ttotal: 33.7s\tremaining: 1m 20s\n",
      "295:\tlearn: 0.1141135\ttotal: 33.8s\tremaining: 1m 20s\n",
      "296:\tlearn: 0.1140512\ttotal: 33.9s\tremaining: 1m 20s\n",
      "297:\tlearn: 0.1139785\ttotal: 34s\tremaining: 1m 20s\n",
      "298:\tlearn: 0.1138997\ttotal: 34.1s\tremaining: 1m 20s\n",
      "299:\tlearn: 0.1138316\ttotal: 34.3s\tremaining: 1m 19s\n",
      "300:\tlearn: 0.1137792\ttotal: 34.4s\tremaining: 1m 19s\n",
      "301:\tlearn: 0.1136962\ttotal: 34.5s\tremaining: 1m 19s\n",
      "302:\tlearn: 0.1136141\ttotal: 34.6s\tremaining: 1m 19s\n",
      "303:\tlearn: 0.1134951\ttotal: 34.7s\tremaining: 1m 19s\n",
      "304:\tlearn: 0.1134068\ttotal: 34.8s\tremaining: 1m 19s\n",
      "305:\tlearn: 0.1133336\ttotal: 34.9s\tremaining: 1m 19s\n",
      "306:\tlearn: 0.1132716\ttotal: 35s\tremaining: 1m 19s\n",
      "307:\tlearn: 0.1132012\ttotal: 35.2s\tremaining: 1m 18s\n",
      "308:\tlearn: 0.1130892\ttotal: 35.3s\tremaining: 1m 18s\n",
      "309:\tlearn: 0.1130169\ttotal: 35.4s\tremaining: 1m 18s\n",
      "310:\tlearn: 0.1129298\ttotal: 35.5s\tremaining: 1m 18s\n",
      "311:\tlearn: 0.1128848\ttotal: 35.6s\tremaining: 1m 18s\n",
      "312:\tlearn: 0.1128355\ttotal: 35.7s\tremaining: 1m 18s\n",
      "313:\tlearn: 0.1127570\ttotal: 35.8s\tremaining: 1m 18s\n",
      "314:\tlearn: 0.1126780\ttotal: 35.9s\tremaining: 1m 18s\n",
      "315:\tlearn: 0.1126177\ttotal: 36s\tremaining: 1m 18s\n",
      "316:\tlearn: 0.1125582\ttotal: 36.1s\tremaining: 1m 17s\n",
      "317:\tlearn: 0.1124894\ttotal: 36.3s\tremaining: 1m 17s\n",
      "318:\tlearn: 0.1123976\ttotal: 36.4s\tremaining: 1m 17s\n",
      "319:\tlearn: 0.1123231\ttotal: 36.5s\tremaining: 1m 17s\n",
      "320:\tlearn: 0.1122500\ttotal: 36.6s\tremaining: 1m 17s\n",
      "321:\tlearn: 0.1121563\ttotal: 36.7s\tremaining: 1m 17s\n",
      "322:\tlearn: 0.1120872\ttotal: 36.8s\tremaining: 1m 17s\n",
      "323:\tlearn: 0.1120121\ttotal: 36.9s\tremaining: 1m 17s\n",
      "324:\tlearn: 0.1119477\ttotal: 37s\tremaining: 1m 16s\n",
      "325:\tlearn: 0.1118979\ttotal: 37.2s\tremaining: 1m 16s\n",
      "326:\tlearn: 0.1118037\ttotal: 37.3s\tremaining: 1m 16s\n",
      "327:\tlearn: 0.1117318\ttotal: 37.4s\tremaining: 1m 16s\n",
      "328:\tlearn: 0.1116708\ttotal: 37.5s\tremaining: 1m 16s\n",
      "329:\tlearn: 0.1116294\ttotal: 37.6s\tremaining: 1m 16s\n",
      "330:\tlearn: 0.1115353\ttotal: 37.7s\tremaining: 1m 16s\n",
      "331:\tlearn: 0.1114730\ttotal: 37.9s\tremaining: 1m 16s\n",
      "332:\tlearn: 0.1114151\ttotal: 38s\tremaining: 1m 16s\n",
      "333:\tlearn: 0.1113067\ttotal: 38.1s\tremaining: 1m 16s\n",
      "334:\tlearn: 0.1112568\ttotal: 38.2s\tremaining: 1m 15s\n",
      "335:\tlearn: 0.1112050\ttotal: 38.3s\tremaining: 1m 15s\n",
      "336:\tlearn: 0.1111245\ttotal: 38.5s\tremaining: 1m 15s\n",
      "337:\tlearn: 0.1110727\ttotal: 38.6s\tremaining: 1m 15s\n",
      "338:\tlearn: 0.1109983\ttotal: 38.7s\tremaining: 1m 15s\n",
      "339:\tlearn: 0.1109371\ttotal: 38.9s\tremaining: 1m 15s\n",
      "340:\tlearn: 0.1108428\ttotal: 39.1s\tremaining: 1m 15s\n",
      "341:\tlearn: 0.1107651\ttotal: 39.2s\tremaining: 1m 15s\n",
      "342:\tlearn: 0.1107384\ttotal: 39.3s\tremaining: 1m 15s\n",
      "343:\tlearn: 0.1106468\ttotal: 39.4s\tremaining: 1m 15s\n",
      "344:\tlearn: 0.1105929\ttotal: 39.5s\tremaining: 1m 15s\n",
      "345:\tlearn: 0.1105574\ttotal: 39.6s\tremaining: 1m 14s\n",
      "346:\tlearn: 0.1104915\ttotal: 39.7s\tremaining: 1m 14s\n",
      "347:\tlearn: 0.1104337\ttotal: 39.9s\tremaining: 1m 14s\n",
      "348:\tlearn: 0.1103802\ttotal: 40s\tremaining: 1m 14s\n",
      "349:\tlearn: 0.1103251\ttotal: 40.1s\tremaining: 1m 14s\n",
      "350:\tlearn: 0.1102657\ttotal: 40.2s\tremaining: 1m 14s\n",
      "351:\tlearn: 0.1101935\ttotal: 40.3s\tremaining: 1m 14s\n",
      "352:\tlearn: 0.1101144\ttotal: 40.4s\tremaining: 1m 14s\n",
      "353:\tlearn: 0.1100385\ttotal: 40.6s\tremaining: 1m 14s\n",
      "354:\tlearn: 0.1099307\ttotal: 40.7s\tremaining: 1m 13s\n",
      "355:\tlearn: 0.1098691\ttotal: 40.8s\tremaining: 1m 13s\n",
      "356:\tlearn: 0.1098259\ttotal: 40.9s\tremaining: 1m 13s\n",
      "357:\tlearn: 0.1097944\ttotal: 41s\tremaining: 1m 13s\n",
      "358:\tlearn: 0.1097308\ttotal: 41.1s\tremaining: 1m 13s\n",
      "359:\tlearn: 0.1096885\ttotal: 41.3s\tremaining: 1m 13s\n",
      "360:\tlearn: 0.1096230\ttotal: 41.4s\tremaining: 1m 13s\n",
      "361:\tlearn: 0.1095831\ttotal: 41.5s\tremaining: 1m 13s\n",
      "362:\tlearn: 0.1095509\ttotal: 41.6s\tremaining: 1m 13s\n",
      "363:\tlearn: 0.1094813\ttotal: 41.7s\tremaining: 1m 12s\n",
      "364:\tlearn: 0.1094304\ttotal: 41.8s\tremaining: 1m 12s\n",
      "365:\tlearn: 0.1093819\ttotal: 42s\tremaining: 1m 12s\n",
      "366:\tlearn: 0.1093040\ttotal: 42.1s\tremaining: 1m 12s\n",
      "367:\tlearn: 0.1091945\ttotal: 42.2s\tremaining: 1m 12s\n",
      "368:\tlearn: 0.1091464\ttotal: 42.3s\tremaining: 1m 12s\n",
      "369:\tlearn: 0.1090631\ttotal: 42.4s\tremaining: 1m 12s\n",
      "370:\tlearn: 0.1090049\ttotal: 42.5s\tremaining: 1m 12s\n",
      "371:\tlearn: 0.1089466\ttotal: 42.6s\tremaining: 1m 11s\n",
      "372:\tlearn: 0.1088727\ttotal: 42.8s\tremaining: 1m 11s\n",
      "373:\tlearn: 0.1088068\ttotal: 42.9s\tremaining: 1m 11s\n",
      "374:\tlearn: 0.1087733\ttotal: 43s\tremaining: 1m 11s\n",
      "375:\tlearn: 0.1087046\ttotal: 43.1s\tremaining: 1m 11s\n",
      "376:\tlearn: 0.1086383\ttotal: 43.2s\tremaining: 1m 11s\n",
      "377:\tlearn: 0.1085757\ttotal: 43.3s\tremaining: 1m 11s\n",
      "378:\tlearn: 0.1085026\ttotal: 43.4s\tremaining: 1m 11s\n",
      "379:\tlearn: 0.1084106\ttotal: 43.6s\tremaining: 1m 11s\n",
      "380:\tlearn: 0.1083722\ttotal: 43.7s\tremaining: 1m 10s\n",
      "381:\tlearn: 0.1083405\ttotal: 43.8s\tremaining: 1m 10s\n",
      "382:\tlearn: 0.1082769\ttotal: 43.9s\tremaining: 1m 10s\n",
      "383:\tlearn: 0.1082364\ttotal: 44s\tremaining: 1m 10s\n",
      "384:\tlearn: 0.1081874\ttotal: 44.1s\tremaining: 1m 10s\n",
      "385:\tlearn: 0.1081201\ttotal: 44.2s\tremaining: 1m 10s\n",
      "386:\tlearn: 0.1080762\ttotal: 44.3s\tremaining: 1m 10s\n",
      "387:\tlearn: 0.1079727\ttotal: 44.5s\tremaining: 1m 10s\n",
      "388:\tlearn: 0.1078969\ttotal: 44.6s\tremaining: 1m 10s\n",
      "389:\tlearn: 0.1078670\ttotal: 44.7s\tremaining: 1m 9s\n",
      "390:\tlearn: 0.1078011\ttotal: 44.8s\tremaining: 1m 9s\n",
      "391:\tlearn: 0.1077730\ttotal: 44.9s\tremaining: 1m 9s\n",
      "392:\tlearn: 0.1077226\ttotal: 45s\tremaining: 1m 9s\n",
      "393:\tlearn: 0.1076171\ttotal: 45.1s\tremaining: 1m 9s\n",
      "394:\tlearn: 0.1075551\ttotal: 45.3s\tremaining: 1m 9s\n",
      "395:\tlearn: 0.1074725\ttotal: 45.4s\tremaining: 1m 9s\n",
      "396:\tlearn: 0.1074249\ttotal: 45.5s\tremaining: 1m 9s\n",
      "397:\tlearn: 0.1073596\ttotal: 45.6s\tremaining: 1m 8s\n",
      "398:\tlearn: 0.1072756\ttotal: 45.7s\tremaining: 1m 8s\n",
      "399:\tlearn: 0.1072056\ttotal: 45.8s\tremaining: 1m 8s\n",
      "400:\tlearn: 0.1071614\ttotal: 45.9s\tremaining: 1m 8s\n",
      "401:\tlearn: 0.1071199\ttotal: 46s\tremaining: 1m 8s\n",
      "402:\tlearn: 0.1070545\ttotal: 46.1s\tremaining: 1m 8s\n",
      "403:\tlearn: 0.1069997\ttotal: 46.3s\tremaining: 1m 8s\n",
      "404:\tlearn: 0.1069163\ttotal: 46.4s\tremaining: 1m 8s\n",
      "405:\tlearn: 0.1068801\ttotal: 46.5s\tremaining: 1m 8s\n",
      "406:\tlearn: 0.1068111\ttotal: 46.6s\tremaining: 1m 7s\n",
      "407:\tlearn: 0.1067132\ttotal: 46.7s\tremaining: 1m 7s\n",
      "408:\tlearn: 0.1066594\ttotal: 46.8s\tremaining: 1m 7s\n",
      "409:\tlearn: 0.1066419\ttotal: 46.9s\tremaining: 1m 7s\n",
      "410:\tlearn: 0.1066036\ttotal: 47.1s\tremaining: 1m 7s\n",
      "411:\tlearn: 0.1065619\ttotal: 47.2s\tremaining: 1m 7s\n",
      "412:\tlearn: 0.1065025\ttotal: 47.3s\tremaining: 1m 7s\n",
      "413:\tlearn: 0.1064636\ttotal: 47.4s\tremaining: 1m 7s\n",
      "414:\tlearn: 0.1064205\ttotal: 47.5s\tremaining: 1m 6s\n",
      "415:\tlearn: 0.1063731\ttotal: 47.6s\tremaining: 1m 6s\n",
      "416:\tlearn: 0.1063097\ttotal: 47.7s\tremaining: 1m 6s\n",
      "417:\tlearn: 0.1063009\ttotal: 47.8s\tremaining: 1m 6s\n",
      "418:\tlearn: 0.1062666\ttotal: 47.9s\tremaining: 1m 6s\n",
      "419:\tlearn: 0.1061825\ttotal: 48.1s\tremaining: 1m 6s\n",
      "420:\tlearn: 0.1061217\ttotal: 48.2s\tremaining: 1m 6s\n",
      "421:\tlearn: 0.1060793\ttotal: 48.3s\tremaining: 1m 6s\n",
      "422:\tlearn: 0.1059999\ttotal: 48.4s\tremaining: 1m 6s\n",
      "423:\tlearn: 0.1059462\ttotal: 48.5s\tremaining: 1m 5s\n",
      "424:\tlearn: 0.1058989\ttotal: 48.7s\tremaining: 1m 5s\n",
      "425:\tlearn: 0.1058372\ttotal: 48.9s\tremaining: 1m 5s\n",
      "426:\tlearn: 0.1058032\ttotal: 49s\tremaining: 1m 5s\n",
      "427:\tlearn: 0.1057713\ttotal: 49.1s\tremaining: 1m 5s\n",
      "428:\tlearn: 0.1056650\ttotal: 49.2s\tremaining: 1m 5s\n",
      "429:\tlearn: 0.1055995\ttotal: 49.3s\tremaining: 1m 5s\n",
      "430:\tlearn: 0.1055571\ttotal: 49.4s\tremaining: 1m 5s\n",
      "431:\tlearn: 0.1054861\ttotal: 49.5s\tremaining: 1m 5s\n",
      "432:\tlearn: 0.1054525\ttotal: 49.6s\tremaining: 1m 5s\n",
      "433:\tlearn: 0.1054078\ttotal: 49.8s\tremaining: 1m 4s\n",
      "434:\tlearn: 0.1053670\ttotal: 49.9s\tremaining: 1m 4s\n",
      "435:\tlearn: 0.1053224\ttotal: 50s\tremaining: 1m 4s\n",
      "436:\tlearn: 0.1052693\ttotal: 50.1s\tremaining: 1m 4s\n",
      "437:\tlearn: 0.1052120\ttotal: 50.2s\tremaining: 1m 4s\n",
      "438:\tlearn: 0.1051666\ttotal: 50.3s\tremaining: 1m 4s\n",
      "439:\tlearn: 0.1051166\ttotal: 50.4s\tremaining: 1m 4s\n",
      "440:\tlearn: 0.1050460\ttotal: 50.6s\tremaining: 1m 4s\n",
      "441:\tlearn: 0.1050014\ttotal: 50.7s\tremaining: 1m 3s\n",
      "442:\tlearn: 0.1049393\ttotal: 50.8s\tremaining: 1m 3s\n",
      "443:\tlearn: 0.1048759\ttotal: 50.9s\tremaining: 1m 3s\n",
      "444:\tlearn: 0.1047972\ttotal: 51s\tremaining: 1m 3s\n",
      "445:\tlearn: 0.1047204\ttotal: 51.1s\tremaining: 1m 3s\n",
      "446:\tlearn: 0.1046656\ttotal: 51.2s\tremaining: 1m 3s\n",
      "447:\tlearn: 0.1046185\ttotal: 51.3s\tremaining: 1m 3s\n",
      "448:\tlearn: 0.1045917\ttotal: 51.5s\tremaining: 1m 3s\n",
      "449:\tlearn: 0.1045261\ttotal: 51.6s\tremaining: 1m 3s\n",
      "450:\tlearn: 0.1044777\ttotal: 51.7s\tremaining: 1m 2s\n",
      "451:\tlearn: 0.1044252\ttotal: 51.8s\tremaining: 1m 2s\n",
      "452:\tlearn: 0.1043943\ttotal: 51.9s\tremaining: 1m 2s\n",
      "453:\tlearn: 0.1043665\ttotal: 52s\tremaining: 1m 2s\n",
      "454:\tlearn: 0.1042948\ttotal: 52.1s\tremaining: 1m 2s\n",
      "455:\tlearn: 0.1042632\ttotal: 52.2s\tremaining: 1m 2s\n",
      "456:\tlearn: 0.1042133\ttotal: 52.3s\tremaining: 1m 2s\n",
      "457:\tlearn: 0.1041712\ttotal: 52.5s\tremaining: 1m 2s\n",
      "458:\tlearn: 0.1041357\ttotal: 52.6s\tremaining: 1m 2s\n",
      "459:\tlearn: 0.1040345\ttotal: 52.7s\tremaining: 1m 1s\n",
      "460:\tlearn: 0.1039575\ttotal: 52.8s\tremaining: 1m 1s\n",
      "461:\tlearn: 0.1039068\ttotal: 53s\tremaining: 1m 1s\n",
      "462:\tlearn: 0.1038674\ttotal: 53.1s\tremaining: 1m 1s\n",
      "463:\tlearn: 0.1038124\ttotal: 53.2s\tremaining: 1m 1s\n",
      "464:\tlearn: 0.1037632\ttotal: 53.3s\tremaining: 1m 1s\n",
      "465:\tlearn: 0.1037022\ttotal: 53.4s\tremaining: 1m 1s\n",
      "466:\tlearn: 0.1036743\ttotal: 53.5s\tremaining: 1m 1s\n",
      "467:\tlearn: 0.1036347\ttotal: 53.6s\tremaining: 1m\n",
      "468:\tlearn: 0.1035815\ttotal: 53.7s\tremaining: 1m\n",
      "469:\tlearn: 0.1035146\ttotal: 53.8s\tremaining: 1m\n",
      "470:\tlearn: 0.1034825\ttotal: 54s\tremaining: 1m\n",
      "471:\tlearn: 0.1034385\ttotal: 54.1s\tremaining: 1m\n",
      "472:\tlearn: 0.1033714\ttotal: 54.2s\tremaining: 1m\n",
      "473:\tlearn: 0.1033334\ttotal: 54.3s\tremaining: 1m\n",
      "474:\tlearn: 0.1032815\ttotal: 54.4s\tremaining: 1m\n",
      "475:\tlearn: 0.1032390\ttotal: 54.5s\tremaining: 1m\n",
      "476:\tlearn: 0.1031867\ttotal: 54.6s\tremaining: 59.9s\n",
      "477:\tlearn: 0.1031578\ttotal: 54.7s\tremaining: 59.8s\n",
      "478:\tlearn: 0.1031290\ttotal: 54.9s\tremaining: 59.7s\n",
      "479:\tlearn: 0.1030872\ttotal: 55s\tremaining: 59.5s\n",
      "480:\tlearn: 0.1030422\ttotal: 55.1s\tremaining: 59.4s\n",
      "481:\tlearn: 0.1029394\ttotal: 55.2s\tremaining: 59.3s\n",
      "482:\tlearn: 0.1028941\ttotal: 55.3s\tremaining: 59.2s\n",
      "483:\tlearn: 0.1028367\ttotal: 55.4s\tremaining: 59.1s\n",
      "484:\tlearn: 0.1027821\ttotal: 55.5s\tremaining: 59s\n",
      "485:\tlearn: 0.1027070\ttotal: 55.6s\tremaining: 58.9s\n",
      "486:\tlearn: 0.1026482\ttotal: 55.8s\tremaining: 58.7s\n",
      "487:\tlearn: 0.1025947\ttotal: 55.9s\tremaining: 58.6s\n",
      "488:\tlearn: 0.1025456\ttotal: 56s\tremaining: 58.5s\n",
      "489:\tlearn: 0.1025038\ttotal: 56.1s\tremaining: 58.4s\n",
      "490:\tlearn: 0.1024604\ttotal: 56.2s\tremaining: 58.3s\n",
      "491:\tlearn: 0.1023983\ttotal: 56.3s\tremaining: 58.2s\n",
      "492:\tlearn: 0.1023509\ttotal: 56.4s\tremaining: 58s\n",
      "493:\tlearn: 0.1022734\ttotal: 56.6s\tremaining: 57.9s\n",
      "494:\tlearn: 0.1022304\ttotal: 56.7s\tremaining: 57.8s\n",
      "495:\tlearn: 0.1021871\ttotal: 56.8s\tremaining: 57.7s\n",
      "496:\tlearn: 0.1021174\ttotal: 56.9s\tremaining: 57.6s\n",
      "497:\tlearn: 0.1020814\ttotal: 57s\tremaining: 57.5s\n",
      "498:\tlearn: 0.1020276\ttotal: 57.1s\tremaining: 57.3s\n",
      "499:\tlearn: 0.1019855\ttotal: 57.2s\tremaining: 57.2s\n",
      "500:\tlearn: 0.1019519\ttotal: 57.3s\tremaining: 57.1s\n",
      "501:\tlearn: 0.1019109\ttotal: 57.5s\tremaining: 57s\n",
      "502:\tlearn: 0.1018657\ttotal: 57.6s\tremaining: 56.9s\n",
      "503:\tlearn: 0.1018300\ttotal: 57.8s\tremaining: 56.9s\n",
      "504:\tlearn: 0.1017823\ttotal: 57.9s\tremaining: 56.8s\n",
      "505:\tlearn: 0.1017346\ttotal: 58s\tremaining: 56.7s\n",
      "506:\tlearn: 0.1016696\ttotal: 58.2s\tremaining: 56.5s\n",
      "507:\tlearn: 0.1016302\ttotal: 58.3s\tremaining: 56.4s\n",
      "508:\tlearn: 0.1015641\ttotal: 58.4s\tremaining: 56.3s\n",
      "509:\tlearn: 0.1015262\ttotal: 58.5s\tremaining: 56.2s\n",
      "510:\tlearn: 0.1014895\ttotal: 58.6s\tremaining: 56.1s\n",
      "511:\tlearn: 0.1014714\ttotal: 58.7s\tremaining: 56s\n",
      "512:\tlearn: 0.1014085\ttotal: 58.8s\tremaining: 55.8s\n",
      "513:\tlearn: 0.1013621\ttotal: 58.9s\tremaining: 55.7s\n",
      "514:\tlearn: 0.1013349\ttotal: 59s\tremaining: 55.6s\n",
      "515:\tlearn: 0.1012545\ttotal: 59.2s\tremaining: 55.5s\n",
      "516:\tlearn: 0.1012304\ttotal: 59.3s\tremaining: 55.4s\n",
      "517:\tlearn: 0.1011892\ttotal: 59.4s\tremaining: 55.3s\n",
      "518:\tlearn: 0.1011439\ttotal: 59.5s\tremaining: 55.1s\n",
      "519:\tlearn: 0.1010931\ttotal: 59.6s\tremaining: 55s\n",
      "520:\tlearn: 0.1010861\ttotal: 59.7s\tremaining: 54.9s\n",
      "521:\tlearn: 0.1010576\ttotal: 59.8s\tremaining: 54.8s\n",
      "522:\tlearn: 0.1010132\ttotal: 59.9s\tremaining: 54.7s\n",
      "523:\tlearn: 0.1009832\ttotal: 1m\tremaining: 54.5s\n",
      "524:\tlearn: 0.1009303\ttotal: 1m\tremaining: 54.4s\n",
      "525:\tlearn: 0.1008763\ttotal: 1m\tremaining: 54.3s\n",
      "526:\tlearn: 0.1008551\ttotal: 1m\tremaining: 54.2s\n",
      "527:\tlearn: 0.1007882\ttotal: 1m\tremaining: 54.1s\n",
      "528:\tlearn: 0.1007474\ttotal: 1m\tremaining: 54s\n",
      "529:\tlearn: 0.1007173\ttotal: 1m\tremaining: 53.8s\n",
      "530:\tlearn: 0.1006735\ttotal: 1m\tremaining: 53.7s\n",
      "531:\tlearn: 0.1006360\ttotal: 1m\tremaining: 53.6s\n",
      "532:\tlearn: 0.1005941\ttotal: 1m 1s\tremaining: 53.5s\n",
      "533:\tlearn: 0.1005283\ttotal: 1m 1s\tremaining: 53.4s\n",
      "534:\tlearn: 0.1004567\ttotal: 1m 1s\tremaining: 53.3s\n",
      "535:\tlearn: 0.1004374\ttotal: 1m 1s\tremaining: 53.1s\n",
      "536:\tlearn: 0.1003755\ttotal: 1m 1s\tremaining: 53s\n",
      "537:\tlearn: 0.1003431\ttotal: 1m 1s\tremaining: 52.9s\n",
      "538:\tlearn: 0.1002755\ttotal: 1m 1s\tremaining: 52.8s\n",
      "539:\tlearn: 0.1002387\ttotal: 1m 1s\tremaining: 52.7s\n",
      "540:\tlearn: 0.1001825\ttotal: 1m 1s\tremaining: 52.6s\n",
      "541:\tlearn: 0.1001404\ttotal: 1m 2s\tremaining: 52.4s\n",
      "542:\tlearn: 0.1001178\ttotal: 1m 2s\tremaining: 52.3s\n",
      "543:\tlearn: 0.1000481\ttotal: 1m 2s\tremaining: 52.2s\n",
      "544:\tlearn: 0.1000193\ttotal: 1m 2s\tremaining: 52.1s\n",
      "545:\tlearn: 0.0999851\ttotal: 1m 2s\tremaining: 52s\n",
      "546:\tlearn: 0.0999611\ttotal: 1m 2s\tremaining: 51.9s\n",
      "547:\tlearn: 0.0999397\ttotal: 1m 2s\tremaining: 51.7s\n",
      "548:\tlearn: 0.0998755\ttotal: 1m 2s\tremaining: 51.6s\n",
      "549:\tlearn: 0.0998469\ttotal: 1m 2s\tremaining: 51.5s\n",
      "550:\tlearn: 0.0998266\ttotal: 1m 3s\tremaining: 51.4s\n",
      "551:\tlearn: 0.0997561\ttotal: 1m 3s\tremaining: 51.3s\n",
      "552:\tlearn: 0.0996790\ttotal: 1m 3s\tremaining: 51.2s\n",
      "553:\tlearn: 0.0996131\ttotal: 1m 3s\tremaining: 51.1s\n",
      "554:\tlearn: 0.0995755\ttotal: 1m 3s\tremaining: 50.9s\n",
      "555:\tlearn: 0.0995039\ttotal: 1m 3s\tremaining: 50.8s\n",
      "556:\tlearn: 0.0994851\ttotal: 1m 3s\tremaining: 50.7s\n",
      "557:\tlearn: 0.0994093\ttotal: 1m 3s\tremaining: 50.6s\n",
      "558:\tlearn: 0.0993555\ttotal: 1m 3s\tremaining: 50.5s\n",
      "559:\tlearn: 0.0993154\ttotal: 1m 4s\tremaining: 50.4s\n",
      "560:\tlearn: 0.0992566\ttotal: 1m 4s\tremaining: 50.3s\n",
      "561:\tlearn: 0.0991577\ttotal: 1m 4s\tremaining: 50.1s\n",
      "562:\tlearn: 0.0991251\ttotal: 1m 4s\tremaining: 50s\n",
      "563:\tlearn: 0.0990860\ttotal: 1m 4s\tremaining: 49.9s\n",
      "564:\tlearn: 0.0990370\ttotal: 1m 4s\tremaining: 49.8s\n",
      "565:\tlearn: 0.0989954\ttotal: 1m 4s\tremaining: 49.7s\n",
      "566:\tlearn: 0.0989659\ttotal: 1m 4s\tremaining: 49.6s\n",
      "567:\tlearn: 0.0989261\ttotal: 1m 5s\tremaining: 49.5s\n",
      "568:\tlearn: 0.0988911\ttotal: 1m 5s\tremaining: 49.3s\n",
      "569:\tlearn: 0.0988603\ttotal: 1m 5s\tremaining: 49.2s\n",
      "570:\tlearn: 0.0988153\ttotal: 1m 5s\tremaining: 49.1s\n",
      "571:\tlearn: 0.0987752\ttotal: 1m 5s\tremaining: 49s\n",
      "572:\tlearn: 0.0987276\ttotal: 1m 5s\tremaining: 48.9s\n",
      "573:\tlearn: 0.0986547\ttotal: 1m 5s\tremaining: 48.8s\n",
      "574:\tlearn: 0.0986012\ttotal: 1m 5s\tremaining: 48.6s\n",
      "575:\tlearn: 0.0985282\ttotal: 1m 5s\tremaining: 48.5s\n",
      "576:\tlearn: 0.0984738\ttotal: 1m 6s\tremaining: 48.4s\n",
      "577:\tlearn: 0.0984536\ttotal: 1m 6s\tremaining: 48.3s\n",
      "578:\tlearn: 0.0984122\ttotal: 1m 6s\tremaining: 48.2s\n",
      "579:\tlearn: 0.0983750\ttotal: 1m 6s\tremaining: 48.1s\n",
      "580:\tlearn: 0.0983066\ttotal: 1m 6s\tremaining: 48s\n",
      "581:\tlearn: 0.0982850\ttotal: 1m 6s\tremaining: 47.9s\n",
      "582:\tlearn: 0.0982191\ttotal: 1m 6s\tremaining: 47.7s\n",
      "583:\tlearn: 0.0982073\ttotal: 1m 6s\tremaining: 47.6s\n",
      "584:\tlearn: 0.0981619\ttotal: 1m 6s\tremaining: 47.5s\n",
      "585:\tlearn: 0.0981428\ttotal: 1m 7s\tremaining: 47.4s\n",
      "586:\tlearn: 0.0981105\ttotal: 1m 7s\tremaining: 47.3s\n",
      "587:\tlearn: 0.0980771\ttotal: 1m 7s\tremaining: 47.2s\n",
      "588:\tlearn: 0.0980391\ttotal: 1m 7s\tremaining: 47s\n",
      "589:\tlearn: 0.0979619\ttotal: 1m 7s\tremaining: 46.9s\n",
      "590:\tlearn: 0.0979174\ttotal: 1m 7s\tremaining: 46.8s\n",
      "591:\tlearn: 0.0978715\ttotal: 1m 7s\tremaining: 46.7s\n",
      "592:\tlearn: 0.0978398\ttotal: 1m 7s\tremaining: 46.6s\n",
      "593:\tlearn: 0.0978155\ttotal: 1m 7s\tremaining: 46.5s\n",
      "594:\tlearn: 0.0977843\ttotal: 1m 8s\tremaining: 46.4s\n",
      "595:\tlearn: 0.0977316\ttotal: 1m 8s\tremaining: 46.2s\n",
      "596:\tlearn: 0.0977035\ttotal: 1m 8s\tremaining: 46.1s\n",
      "597:\tlearn: 0.0976847\ttotal: 1m 8s\tremaining: 46s\n",
      "598:\tlearn: 0.0976256\ttotal: 1m 8s\tremaining: 46s\n",
      "599:\tlearn: 0.0975760\ttotal: 1m 8s\tremaining: 45.9s\n",
      "600:\tlearn: 0.0975286\ttotal: 1m 8s\tremaining: 45.8s\n",
      "601:\tlearn: 0.0974511\ttotal: 1m 9s\tremaining: 45.7s\n",
      "602:\tlearn: 0.0974141\ttotal: 1m 9s\tremaining: 45.6s\n",
      "603:\tlearn: 0.0973926\ttotal: 1m 9s\tremaining: 45.4s\n",
      "604:\tlearn: 0.0973146\ttotal: 1m 9s\tremaining: 45.3s\n",
      "605:\tlearn: 0.0972702\ttotal: 1m 9s\tremaining: 45.2s\n",
      "606:\tlearn: 0.0972173\ttotal: 1m 9s\tremaining: 45.1s\n",
      "607:\tlearn: 0.0971770\ttotal: 1m 9s\tremaining: 45s\n",
      "608:\tlearn: 0.0971231\ttotal: 1m 9s\tremaining: 44.9s\n",
      "609:\tlearn: 0.0970974\ttotal: 1m 10s\tremaining: 44.8s\n",
      "610:\tlearn: 0.0970682\ttotal: 1m 10s\tremaining: 44.6s\n",
      "611:\tlearn: 0.0970358\ttotal: 1m 10s\tremaining: 44.5s\n",
      "612:\tlearn: 0.0969810\ttotal: 1m 10s\tremaining: 44.4s\n",
      "613:\tlearn: 0.0969377\ttotal: 1m 10s\tremaining: 44.3s\n",
      "614:\tlearn: 0.0969009\ttotal: 1m 10s\tremaining: 44.2s\n",
      "615:\tlearn: 0.0968656\ttotal: 1m 10s\tremaining: 44.1s\n",
      "616:\tlearn: 0.0967944\ttotal: 1m 10s\tremaining: 44s\n",
      "617:\tlearn: 0.0967617\ttotal: 1m 10s\tremaining: 43.9s\n",
      "618:\tlearn: 0.0967176\ttotal: 1m 11s\tremaining: 43.8s\n",
      "619:\tlearn: 0.0966648\ttotal: 1m 11s\tremaining: 43.6s\n",
      "620:\tlearn: 0.0966008\ttotal: 1m 11s\tremaining: 43.5s\n",
      "621:\tlearn: 0.0965640\ttotal: 1m 11s\tremaining: 43.4s\n",
      "622:\tlearn: 0.0965062\ttotal: 1m 11s\tremaining: 43.3s\n",
      "623:\tlearn: 0.0964655\ttotal: 1m 11s\tremaining: 43.2s\n",
      "624:\tlearn: 0.0964104\ttotal: 1m 11s\tremaining: 43.1s\n",
      "625:\tlearn: 0.0963609\ttotal: 1m 11s\tremaining: 43s\n",
      "626:\tlearn: 0.0963137\ttotal: 1m 12s\tremaining: 42.9s\n",
      "627:\tlearn: 0.0962807\ttotal: 1m 12s\tremaining: 42.7s\n",
      "628:\tlearn: 0.0962606\ttotal: 1m 12s\tremaining: 42.6s\n",
      "629:\tlearn: 0.0961905\ttotal: 1m 12s\tremaining: 42.5s\n",
      "630:\tlearn: 0.0960964\ttotal: 1m 12s\tremaining: 42.4s\n",
      "631:\tlearn: 0.0960729\ttotal: 1m 12s\tremaining: 42.3s\n",
      "632:\tlearn: 0.0960524\ttotal: 1m 12s\tremaining: 42.2s\n",
      "633:\tlearn: 0.0960132\ttotal: 1m 12s\tremaining: 42s\n",
      "634:\tlearn: 0.0959578\ttotal: 1m 12s\tremaining: 41.9s\n",
      "635:\tlearn: 0.0959119\ttotal: 1m 13s\tremaining: 41.8s\n",
      "636:\tlearn: 0.0959023\ttotal: 1m 13s\tremaining: 41.7s\n",
      "637:\tlearn: 0.0958632\ttotal: 1m 13s\tremaining: 41.6s\n",
      "638:\tlearn: 0.0958081\ttotal: 1m 13s\tremaining: 41.5s\n",
      "639:\tlearn: 0.0957746\ttotal: 1m 13s\tremaining: 41.3s\n",
      "640:\tlearn: 0.0957520\ttotal: 1m 13s\tremaining: 41.2s\n",
      "641:\tlearn: 0.0956806\ttotal: 1m 13s\tremaining: 41.1s\n",
      "642:\tlearn: 0.0956262\ttotal: 1m 13s\tremaining: 41s\n",
      "643:\tlearn: 0.0955822\ttotal: 1m 13s\tremaining: 40.9s\n",
      "644:\tlearn: 0.0955340\ttotal: 1m 14s\tremaining: 40.8s\n",
      "645:\tlearn: 0.0954833\ttotal: 1m 14s\tremaining: 40.6s\n",
      "646:\tlearn: 0.0954431\ttotal: 1m 14s\tremaining: 40.5s\n",
      "647:\tlearn: 0.0953988\ttotal: 1m 14s\tremaining: 40.4s\n",
      "648:\tlearn: 0.0953768\ttotal: 1m 14s\tremaining: 40.3s\n",
      "649:\tlearn: 0.0953438\ttotal: 1m 14s\tremaining: 40.2s\n",
      "650:\tlearn: 0.0952880\ttotal: 1m 14s\tremaining: 40.1s\n",
      "651:\tlearn: 0.0952552\ttotal: 1m 14s\tremaining: 40s\n",
      "652:\tlearn: 0.0952213\ttotal: 1m 14s\tremaining: 39.8s\n",
      "653:\tlearn: 0.0951735\ttotal: 1m 15s\tremaining: 39.7s\n",
      "654:\tlearn: 0.0951590\ttotal: 1m 15s\tremaining: 39.6s\n",
      "655:\tlearn: 0.0951262\ttotal: 1m 15s\tremaining: 39.5s\n",
      "656:\tlearn: 0.0951062\ttotal: 1m 15s\tremaining: 39.4s\n",
      "657:\tlearn: 0.0950429\ttotal: 1m 15s\tremaining: 39.3s\n",
      "658:\tlearn: 0.0950184\ttotal: 1m 15s\tremaining: 39.1s\n",
      "659:\tlearn: 0.0949776\ttotal: 1m 15s\tremaining: 39s\n",
      "660:\tlearn: 0.0949505\ttotal: 1m 15s\tremaining: 38.9s\n",
      "661:\tlearn: 0.0949273\ttotal: 1m 15s\tremaining: 38.8s\n",
      "662:\tlearn: 0.0948991\ttotal: 1m 16s\tremaining: 38.7s\n",
      "663:\tlearn: 0.0948556\ttotal: 1m 16s\tremaining: 38.6s\n",
      "664:\tlearn: 0.0948156\ttotal: 1m 16s\tremaining: 38.4s\n",
      "665:\tlearn: 0.0947844\ttotal: 1m 16s\tremaining: 38.3s\n",
      "666:\tlearn: 0.0947250\ttotal: 1m 16s\tremaining: 38.2s\n",
      "667:\tlearn: 0.0946977\ttotal: 1m 16s\tremaining: 38.1s\n",
      "668:\tlearn: 0.0946731\ttotal: 1m 16s\tremaining: 38s\n",
      "669:\tlearn: 0.0946277\ttotal: 1m 16s\tremaining: 37.9s\n",
      "670:\tlearn: 0.0945832\ttotal: 1m 16s\tremaining: 37.7s\n",
      "671:\tlearn: 0.0945601\ttotal: 1m 17s\tremaining: 37.6s\n",
      "672:\tlearn: 0.0945265\ttotal: 1m 17s\tremaining: 37.5s\n",
      "673:\tlearn: 0.0944997\ttotal: 1m 17s\tremaining: 37.4s\n",
      "674:\tlearn: 0.0944301\ttotal: 1m 17s\tremaining: 37.3s\n",
      "675:\tlearn: 0.0943926\ttotal: 1m 17s\tremaining: 37.2s\n",
      "676:\tlearn: 0.0942979\ttotal: 1m 17s\tremaining: 37s\n",
      "677:\tlearn: 0.0942631\ttotal: 1m 17s\tremaining: 36.9s\n",
      "678:\tlearn: 0.0942215\ttotal: 1m 17s\tremaining: 36.8s\n",
      "679:\tlearn: 0.0941802\ttotal: 1m 18s\tremaining: 36.7s\n",
      "680:\tlearn: 0.0941444\ttotal: 1m 18s\tremaining: 36.6s\n",
      "681:\tlearn: 0.0941005\ttotal: 1m 18s\tremaining: 36.5s\n",
      "682:\tlearn: 0.0940595\ttotal: 1m 18s\tremaining: 36.4s\n",
      "683:\tlearn: 0.0940209\ttotal: 1m 18s\tremaining: 36.3s\n",
      "684:\tlearn: 0.0939875\ttotal: 1m 18s\tremaining: 36.2s\n",
      "685:\tlearn: 0.0939481\ttotal: 1m 18s\tremaining: 36.1s\n",
      "686:\tlearn: 0.0939100\ttotal: 1m 18s\tremaining: 36s\n",
      "687:\tlearn: 0.0938749\ttotal: 1m 19s\tremaining: 35.8s\n",
      "688:\tlearn: 0.0938078\ttotal: 1m 19s\tremaining: 35.7s\n",
      "689:\tlearn: 0.0937905\ttotal: 1m 19s\tremaining: 35.6s\n",
      "690:\tlearn: 0.0937276\ttotal: 1m 19s\tremaining: 35.5s\n",
      "691:\tlearn: 0.0936817\ttotal: 1m 19s\tremaining: 35.4s\n",
      "692:\tlearn: 0.0936459\ttotal: 1m 19s\tremaining: 35.3s\n",
      "693:\tlearn: 0.0935752\ttotal: 1m 19s\tremaining: 35.2s\n",
      "694:\tlearn: 0.0935538\ttotal: 1m 19s\tremaining: 35s\n",
      "695:\tlearn: 0.0935203\ttotal: 1m 19s\tremaining: 34.9s\n",
      "696:\tlearn: 0.0934603\ttotal: 1m 20s\tremaining: 34.8s\n",
      "697:\tlearn: 0.0934525\ttotal: 1m 20s\tremaining: 34.7s\n",
      "698:\tlearn: 0.0934161\ttotal: 1m 20s\tremaining: 34.6s\n",
      "699:\tlearn: 0.0933660\ttotal: 1m 20s\tremaining: 34.5s\n",
      "700:\tlearn: 0.0933082\ttotal: 1m 20s\tremaining: 34.3s\n",
      "701:\tlearn: 0.0932707\ttotal: 1m 20s\tremaining: 34.2s\n",
      "702:\tlearn: 0.0932517\ttotal: 1m 20s\tremaining: 34.1s\n",
      "703:\tlearn: 0.0932148\ttotal: 1m 20s\tremaining: 34s\n",
      "704:\tlearn: 0.0931809\ttotal: 1m 20s\tremaining: 33.9s\n",
      "705:\tlearn: 0.0931538\ttotal: 1m 21s\tremaining: 33.8s\n",
      "706:\tlearn: 0.0930429\ttotal: 1m 21s\tremaining: 33.6s\n",
      "707:\tlearn: 0.0929979\ttotal: 1m 21s\tremaining: 33.5s\n",
      "708:\tlearn: 0.0929730\ttotal: 1m 21s\tremaining: 33.4s\n",
      "709:\tlearn: 0.0929407\ttotal: 1m 21s\tremaining: 33.3s\n",
      "710:\tlearn: 0.0929012\ttotal: 1m 21s\tremaining: 33.2s\n",
      "711:\tlearn: 0.0928736\ttotal: 1m 21s\tremaining: 33.1s\n",
      "712:\tlearn: 0.0928447\ttotal: 1m 21s\tremaining: 33s\n",
      "713:\tlearn: 0.0928011\ttotal: 1m 22s\tremaining: 32.9s\n",
      "714:\tlearn: 0.0927334\ttotal: 1m 22s\tremaining: 32.7s\n",
      "715:\tlearn: 0.0926927\ttotal: 1m 22s\tremaining: 32.6s\n",
      "716:\tlearn: 0.0926437\ttotal: 1m 22s\tremaining: 32.5s\n",
      "717:\tlearn: 0.0926235\ttotal: 1m 22s\tremaining: 32.4s\n",
      "718:\tlearn: 0.0926031\ttotal: 1m 22s\tremaining: 32.3s\n",
      "719:\tlearn: 0.0925602\ttotal: 1m 22s\tremaining: 32.2s\n",
      "720:\tlearn: 0.0925228\ttotal: 1m 22s\tremaining: 32.1s\n",
      "721:\tlearn: 0.0924820\ttotal: 1m 22s\tremaining: 31.9s\n",
      "722:\tlearn: 0.0924598\ttotal: 1m 23s\tremaining: 31.8s\n",
      "723:\tlearn: 0.0924095\ttotal: 1m 23s\tremaining: 31.7s\n",
      "724:\tlearn: 0.0923437\ttotal: 1m 23s\tremaining: 31.6s\n",
      "725:\tlearn: 0.0923268\ttotal: 1m 23s\tremaining: 31.5s\n",
      "726:\tlearn: 0.0922867\ttotal: 1m 23s\tremaining: 31.4s\n",
      "727:\tlearn: 0.0922643\ttotal: 1m 23s\tremaining: 31.3s\n",
      "728:\tlearn: 0.0922383\ttotal: 1m 23s\tremaining: 31.1s\n",
      "729:\tlearn: 0.0922086\ttotal: 1m 23s\tremaining: 31s\n",
      "730:\tlearn: 0.0921503\ttotal: 1m 24s\tremaining: 30.9s\n",
      "731:\tlearn: 0.0920572\ttotal: 1m 24s\tremaining: 30.8s\n",
      "732:\tlearn: 0.0920104\ttotal: 1m 24s\tremaining: 30.7s\n",
      "733:\tlearn: 0.0919441\ttotal: 1m 24s\tremaining: 30.6s\n",
      "734:\tlearn: 0.0918548\ttotal: 1m 24s\tremaining: 30.5s\n",
      "735:\tlearn: 0.0918004\ttotal: 1m 24s\tremaining: 30.4s\n",
      "736:\tlearn: 0.0917830\ttotal: 1m 24s\tremaining: 30.2s\n",
      "737:\tlearn: 0.0917510\ttotal: 1m 24s\tremaining: 30.1s\n",
      "738:\tlearn: 0.0917022\ttotal: 1m 24s\tremaining: 30s\n",
      "739:\tlearn: 0.0916589\ttotal: 1m 25s\tremaining: 29.9s\n",
      "740:\tlearn: 0.0916356\ttotal: 1m 25s\tremaining: 29.8s\n",
      "741:\tlearn: 0.0916131\ttotal: 1m 25s\tremaining: 29.7s\n",
      "742:\tlearn: 0.0915845\ttotal: 1m 25s\tremaining: 29.5s\n",
      "743:\tlearn: 0.0915344\ttotal: 1m 25s\tremaining: 29.4s\n",
      "744:\tlearn: 0.0914671\ttotal: 1m 25s\tremaining: 29.3s\n",
      "745:\tlearn: 0.0914339\ttotal: 1m 25s\tremaining: 29.2s\n",
      "746:\tlearn: 0.0914143\ttotal: 1m 25s\tremaining: 29.1s\n",
      "747:\tlearn: 0.0913837\ttotal: 1m 25s\tremaining: 29s\n",
      "748:\tlearn: 0.0913424\ttotal: 1m 26s\tremaining: 28.9s\n",
      "749:\tlearn: 0.0912938\ttotal: 1m 26s\tremaining: 28.7s\n",
      "750:\tlearn: 0.0912637\ttotal: 1m 26s\tremaining: 28.6s\n",
      "751:\tlearn: 0.0912427\ttotal: 1m 26s\tremaining: 28.5s\n",
      "752:\tlearn: 0.0912020\ttotal: 1m 26s\tremaining: 28.4s\n",
      "753:\tlearn: 0.0911834\ttotal: 1m 26s\tremaining: 28.3s\n",
      "754:\tlearn: 0.0911470\ttotal: 1m 26s\tremaining: 28.2s\n",
      "755:\tlearn: 0.0911071\ttotal: 1m 26s\tremaining: 28.1s\n",
      "756:\tlearn: 0.0910604\ttotal: 1m 27s\tremaining: 28s\n",
      "757:\tlearn: 0.0910092\ttotal: 1m 27s\tremaining: 27.8s\n",
      "758:\tlearn: 0.0909657\ttotal: 1m 27s\tremaining: 27.7s\n",
      "759:\tlearn: 0.0909457\ttotal: 1m 27s\tremaining: 27.6s\n",
      "760:\tlearn: 0.0909035\ttotal: 1m 27s\tremaining: 27.5s\n",
      "761:\tlearn: 0.0908760\ttotal: 1m 27s\tremaining: 27.4s\n",
      "762:\tlearn: 0.0908319\ttotal: 1m 27s\tremaining: 27.3s\n",
      "763:\tlearn: 0.0908008\ttotal: 1m 28s\tremaining: 27.2s\n",
      "764:\tlearn: 0.0907348\ttotal: 1m 28s\tremaining: 27.1s\n",
      "765:\tlearn: 0.0906836\ttotal: 1m 28s\tremaining: 27s\n",
      "766:\tlearn: 0.0906379\ttotal: 1m 28s\tremaining: 26.9s\n",
      "767:\tlearn: 0.0906140\ttotal: 1m 28s\tremaining: 26.8s\n",
      "768:\tlearn: 0.0905905\ttotal: 1m 28s\tremaining: 26.6s\n",
      "769:\tlearn: 0.0905501\ttotal: 1m 28s\tremaining: 26.5s\n",
      "770:\tlearn: 0.0905108\ttotal: 1m 28s\tremaining: 26.4s\n",
      "771:\tlearn: 0.0904488\ttotal: 1m 29s\tremaining: 26.3s\n",
      "772:\tlearn: 0.0904414\ttotal: 1m 29s\tremaining: 26.2s\n",
      "773:\tlearn: 0.0903878\ttotal: 1m 29s\tremaining: 26.1s\n",
      "774:\tlearn: 0.0903401\ttotal: 1m 29s\tremaining: 26s\n",
      "775:\tlearn: 0.0903131\ttotal: 1m 29s\tremaining: 25.8s\n",
      "776:\tlearn: 0.0902524\ttotal: 1m 29s\tremaining: 25.7s\n",
      "777:\tlearn: 0.0902090\ttotal: 1m 29s\tremaining: 25.6s\n",
      "778:\tlearn: 0.0901910\ttotal: 1m 29s\tremaining: 25.5s\n",
      "779:\tlearn: 0.0901318\ttotal: 1m 30s\tremaining: 25.4s\n",
      "780:\tlearn: 0.0900985\ttotal: 1m 30s\tremaining: 25.3s\n",
      "781:\tlearn: 0.0900478\ttotal: 1m 30s\tremaining: 25.2s\n",
      "782:\tlearn: 0.0900104\ttotal: 1m 30s\tremaining: 25.1s\n",
      "783:\tlearn: 0.0899918\ttotal: 1m 30s\tremaining: 24.9s\n",
      "784:\tlearn: 0.0899684\ttotal: 1m 30s\tremaining: 24.8s\n",
      "785:\tlearn: 0.0899147\ttotal: 1m 30s\tremaining: 24.7s\n",
      "786:\tlearn: 0.0898787\ttotal: 1m 30s\tremaining: 24.6s\n",
      "787:\tlearn: 0.0898334\ttotal: 1m 30s\tremaining: 24.5s\n",
      "788:\tlearn: 0.0898234\ttotal: 1m 31s\tremaining: 24.4s\n",
      "789:\tlearn: 0.0897901\ttotal: 1m 31s\tremaining: 24.2s\n",
      "790:\tlearn: 0.0897655\ttotal: 1m 31s\tremaining: 24.1s\n",
      "791:\tlearn: 0.0897252\ttotal: 1m 31s\tremaining: 24s\n",
      "792:\tlearn: 0.0896960\ttotal: 1m 31s\tremaining: 23.9s\n",
      "793:\tlearn: 0.0896704\ttotal: 1m 31s\tremaining: 23.8s\n",
      "794:\tlearn: 0.0896349\ttotal: 1m 31s\tremaining: 23.7s\n",
      "795:\tlearn: 0.0895981\ttotal: 1m 31s\tremaining: 23.5s\n",
      "796:\tlearn: 0.0895598\ttotal: 1m 31s\tremaining: 23.4s\n",
      "797:\tlearn: 0.0895109\ttotal: 1m 32s\tremaining: 23.3s\n",
      "798:\tlearn: 0.0894639\ttotal: 1m 32s\tremaining: 23.2s\n",
      "799:\tlearn: 0.0894126\ttotal: 1m 32s\tremaining: 23.1s\n",
      "800:\tlearn: 0.0893805\ttotal: 1m 32s\tremaining: 23s\n",
      "801:\tlearn: 0.0893235\ttotal: 1m 32s\tremaining: 22.9s\n",
      "802:\tlearn: 0.0893011\ttotal: 1m 32s\tremaining: 22.7s\n",
      "803:\tlearn: 0.0892856\ttotal: 1m 32s\tremaining: 22.6s\n",
      "804:\tlearn: 0.0892694\ttotal: 1m 32s\tremaining: 22.5s\n",
      "805:\tlearn: 0.0892559\ttotal: 1m 33s\tremaining: 22.4s\n",
      "806:\tlearn: 0.0892130\ttotal: 1m 33s\tremaining: 22.3s\n",
      "807:\tlearn: 0.0891931\ttotal: 1m 33s\tremaining: 22.2s\n",
      "808:\tlearn: 0.0891533\ttotal: 1m 33s\tremaining: 22s\n",
      "809:\tlearn: 0.0891296\ttotal: 1m 33s\tremaining: 21.9s\n",
      "810:\tlearn: 0.0890879\ttotal: 1m 33s\tremaining: 21.8s\n",
      "811:\tlearn: 0.0890473\ttotal: 1m 33s\tremaining: 21.7s\n",
      "812:\tlearn: 0.0890000\ttotal: 1m 33s\tremaining: 21.6s\n",
      "813:\tlearn: 0.0889428\ttotal: 1m 33s\tremaining: 21.5s\n",
      "814:\tlearn: 0.0889180\ttotal: 1m 34s\tremaining: 21.4s\n",
      "815:\tlearn: 0.0889048\ttotal: 1m 34s\tremaining: 21.2s\n",
      "816:\tlearn: 0.0888623\ttotal: 1m 34s\tremaining: 21.1s\n",
      "817:\tlearn: 0.0887777\ttotal: 1m 34s\tremaining: 21s\n",
      "818:\tlearn: 0.0887507\ttotal: 1m 34s\tremaining: 20.9s\n",
      "819:\tlearn: 0.0887064\ttotal: 1m 34s\tremaining: 20.8s\n",
      "820:\tlearn: 0.0886526\ttotal: 1m 34s\tremaining: 20.7s\n",
      "821:\tlearn: 0.0886495\ttotal: 1m 34s\tremaining: 20.5s\n",
      "822:\tlearn: 0.0886399\ttotal: 1m 34s\tremaining: 20.4s\n",
      "823:\tlearn: 0.0886153\ttotal: 1m 35s\tremaining: 20.3s\n",
      "824:\tlearn: 0.0885591\ttotal: 1m 35s\tremaining: 20.2s\n",
      "825:\tlearn: 0.0884904\ttotal: 1m 35s\tremaining: 20.1s\n",
      "826:\tlearn: 0.0884548\ttotal: 1m 35s\tremaining: 20s\n",
      "827:\tlearn: 0.0884095\ttotal: 1m 35s\tremaining: 19.9s\n",
      "828:\tlearn: 0.0883945\ttotal: 1m 35s\tremaining: 19.7s\n",
      "829:\tlearn: 0.0883727\ttotal: 1m 35s\tremaining: 19.6s\n",
      "830:\tlearn: 0.0883347\ttotal: 1m 35s\tremaining: 19.5s\n",
      "831:\tlearn: 0.0883091\ttotal: 1m 36s\tremaining: 19.4s\n",
      "832:\tlearn: 0.0882762\ttotal: 1m 36s\tremaining: 19.3s\n",
      "833:\tlearn: 0.0882310\ttotal: 1m 36s\tremaining: 19.2s\n",
      "834:\tlearn: 0.0882171\ttotal: 1m 36s\tremaining: 19s\n",
      "835:\tlearn: 0.0881752\ttotal: 1m 36s\tremaining: 18.9s\n",
      "836:\tlearn: 0.0881326\ttotal: 1m 36s\tremaining: 18.8s\n",
      "837:\tlearn: 0.0880966\ttotal: 1m 36s\tremaining: 18.7s\n",
      "838:\tlearn: 0.0880584\ttotal: 1m 36s\tremaining: 18.6s\n",
      "839:\tlearn: 0.0880383\ttotal: 1m 36s\tremaining: 18.5s\n",
      "840:\tlearn: 0.0879889\ttotal: 1m 37s\tremaining: 18.4s\n",
      "841:\tlearn: 0.0879502\ttotal: 1m 37s\tremaining: 18.2s\n",
      "842:\tlearn: 0.0879268\ttotal: 1m 37s\tremaining: 18.1s\n",
      "843:\tlearn: 0.0879079\ttotal: 1m 37s\tremaining: 18s\n",
      "844:\tlearn: 0.0878791\ttotal: 1m 37s\tremaining: 17.9s\n",
      "845:\tlearn: 0.0877935\ttotal: 1m 37s\tremaining: 17.8s\n",
      "846:\tlearn: 0.0877713\ttotal: 1m 37s\tremaining: 17.7s\n",
      "847:\tlearn: 0.0877488\ttotal: 1m 38s\tremaining: 17.6s\n",
      "848:\tlearn: 0.0877155\ttotal: 1m 38s\tremaining: 17.5s\n",
      "849:\tlearn: 0.0876959\ttotal: 1m 38s\tremaining: 17.3s\n",
      "850:\tlearn: 0.0876654\ttotal: 1m 38s\tremaining: 17.2s\n",
      "851:\tlearn: 0.0876413\ttotal: 1m 38s\tremaining: 17.1s\n",
      "852:\tlearn: 0.0875990\ttotal: 1m 38s\tremaining: 17s\n",
      "853:\tlearn: 0.0875526\ttotal: 1m 38s\tremaining: 16.9s\n",
      "854:\tlearn: 0.0875218\ttotal: 1m 38s\tremaining: 16.8s\n",
      "855:\tlearn: 0.0874978\ttotal: 1m 38s\tremaining: 16.6s\n",
      "856:\tlearn: 0.0874628\ttotal: 1m 39s\tremaining: 16.5s\n",
      "857:\tlearn: 0.0874348\ttotal: 1m 39s\tremaining: 16.4s\n",
      "858:\tlearn: 0.0874316\ttotal: 1m 39s\tremaining: 16.3s\n",
      "859:\tlearn: 0.0874254\ttotal: 1m 39s\tremaining: 16.2s\n",
      "860:\tlearn: 0.0873878\ttotal: 1m 39s\tremaining: 16.1s\n",
      "861:\tlearn: 0.0873435\ttotal: 1m 39s\tremaining: 15.9s\n",
      "862:\tlearn: 0.0872704\ttotal: 1m 39s\tremaining: 15.8s\n",
      "863:\tlearn: 0.0872502\ttotal: 1m 39s\tremaining: 15.7s\n",
      "864:\tlearn: 0.0872202\ttotal: 1m 39s\tremaining: 15.6s\n",
      "865:\tlearn: 0.0871486\ttotal: 1m 40s\tremaining: 15.5s\n",
      "866:\tlearn: 0.0871270\ttotal: 1m 40s\tremaining: 15.4s\n",
      "867:\tlearn: 0.0871222\ttotal: 1m 40s\tremaining: 15.3s\n",
      "868:\tlearn: 0.0870832\ttotal: 1m 40s\tremaining: 15.1s\n",
      "869:\tlearn: 0.0870116\ttotal: 1m 40s\tremaining: 15s\n",
      "870:\tlearn: 0.0869899\ttotal: 1m 40s\tremaining: 14.9s\n",
      "871:\tlearn: 0.0869392\ttotal: 1m 40s\tremaining: 14.8s\n",
      "872:\tlearn: 0.0869133\ttotal: 1m 40s\tremaining: 14.7s\n",
      "873:\tlearn: 0.0868720\ttotal: 1m 41s\tremaining: 14.6s\n",
      "874:\tlearn: 0.0868139\ttotal: 1m 41s\tremaining: 14.4s\n",
      "875:\tlearn: 0.0867627\ttotal: 1m 41s\tremaining: 14.3s\n",
      "876:\tlearn: 0.0867159\ttotal: 1m 41s\tremaining: 14.2s\n",
      "877:\tlearn: 0.0866858\ttotal: 1m 41s\tremaining: 14.1s\n",
      "878:\tlearn: 0.0866515\ttotal: 1m 41s\tremaining: 14s\n",
      "879:\tlearn: 0.0865988\ttotal: 1m 41s\tremaining: 13.9s\n",
      "880:\tlearn: 0.0865765\ttotal: 1m 41s\tremaining: 13.8s\n",
      "881:\tlearn: 0.0865539\ttotal: 1m 41s\tremaining: 13.6s\n",
      "882:\tlearn: 0.0865180\ttotal: 1m 42s\tremaining: 13.5s\n",
      "883:\tlearn: 0.0864921\ttotal: 1m 42s\tremaining: 13.4s\n",
      "884:\tlearn: 0.0864682\ttotal: 1m 42s\tremaining: 13.3s\n",
      "885:\tlearn: 0.0864395\ttotal: 1m 42s\tremaining: 13.2s\n",
      "886:\tlearn: 0.0864135\ttotal: 1m 42s\tremaining: 13.1s\n",
      "887:\tlearn: 0.0863857\ttotal: 1m 42s\tremaining: 12.9s\n",
      "888:\tlearn: 0.0863542\ttotal: 1m 42s\tremaining: 12.8s\n",
      "889:\tlearn: 0.0863271\ttotal: 1m 42s\tremaining: 12.7s\n",
      "890:\tlearn: 0.0862828\ttotal: 1m 42s\tremaining: 12.6s\n",
      "891:\tlearn: 0.0862649\ttotal: 1m 43s\tremaining: 12.5s\n",
      "892:\tlearn: 0.0862450\ttotal: 1m 43s\tremaining: 12.4s\n",
      "893:\tlearn: 0.0862196\ttotal: 1m 43s\tremaining: 12.3s\n",
      "894:\tlearn: 0.0862020\ttotal: 1m 43s\tremaining: 12.1s\n",
      "895:\tlearn: 0.0861614\ttotal: 1m 43s\tremaining: 12s\n",
      "896:\tlearn: 0.0861236\ttotal: 1m 43s\tremaining: 11.9s\n",
      "897:\tlearn: 0.0860942\ttotal: 1m 43s\tremaining: 11.8s\n",
      "898:\tlearn: 0.0860701\ttotal: 1m 43s\tremaining: 11.7s\n",
      "899:\tlearn: 0.0860295\ttotal: 1m 44s\tremaining: 11.6s\n",
      "900:\tlearn: 0.0859658\ttotal: 1m 44s\tremaining: 11.4s\n",
      "901:\tlearn: 0.0859392\ttotal: 1m 44s\tremaining: 11.3s\n",
      "902:\tlearn: 0.0858979\ttotal: 1m 44s\tremaining: 11.2s\n",
      "903:\tlearn: 0.0858273\ttotal: 1m 44s\tremaining: 11.1s\n",
      "904:\tlearn: 0.0858019\ttotal: 1m 44s\tremaining: 11s\n",
      "905:\tlearn: 0.0857796\ttotal: 1m 44s\tremaining: 10.9s\n",
      "906:\tlearn: 0.0857610\ttotal: 1m 44s\tremaining: 10.7s\n",
      "907:\tlearn: 0.0857458\ttotal: 1m 44s\tremaining: 10.6s\n",
      "908:\tlearn: 0.0856963\ttotal: 1m 45s\tremaining: 10.5s\n",
      "909:\tlearn: 0.0856751\ttotal: 1m 45s\tremaining: 10.4s\n",
      "910:\tlearn: 0.0856422\ttotal: 1m 45s\tremaining: 10.3s\n",
      "911:\tlearn: 0.0856178\ttotal: 1m 45s\tremaining: 10.2s\n",
      "912:\tlearn: 0.0855956\ttotal: 1m 45s\tremaining: 10.1s\n",
      "913:\tlearn: 0.0855672\ttotal: 1m 45s\tremaining: 9.94s\n",
      "914:\tlearn: 0.0855221\ttotal: 1m 45s\tremaining: 9.82s\n",
      "915:\tlearn: 0.0854767\ttotal: 1m 45s\tremaining: 9.7s\n",
      "916:\tlearn: 0.0854274\ttotal: 1m 45s\tremaining: 9.59s\n",
      "917:\tlearn: 0.0854112\ttotal: 1m 46s\tremaining: 9.47s\n",
      "918:\tlearn: 0.0854014\ttotal: 1m 46s\tremaining: 9.36s\n",
      "919:\tlearn: 0.0853469\ttotal: 1m 46s\tremaining: 9.24s\n",
      "920:\tlearn: 0.0853376\ttotal: 1m 46s\tremaining: 9.13s\n",
      "921:\tlearn: 0.0852883\ttotal: 1m 46s\tremaining: 9.01s\n",
      "922:\tlearn: 0.0852581\ttotal: 1m 46s\tremaining: 8.9s\n",
      "923:\tlearn: 0.0852218\ttotal: 1m 46s\tremaining: 8.78s\n",
      "924:\tlearn: 0.0852123\ttotal: 1m 46s\tremaining: 8.67s\n",
      "925:\tlearn: 0.0851745\ttotal: 1m 47s\tremaining: 8.55s\n",
      "926:\tlearn: 0.0851483\ttotal: 1m 47s\tremaining: 8.44s\n",
      "927:\tlearn: 0.0851125\ttotal: 1m 47s\tremaining: 8.32s\n",
      "928:\tlearn: 0.0850973\ttotal: 1m 47s\tremaining: 8.2s\n",
      "929:\tlearn: 0.0850635\ttotal: 1m 47s\tremaining: 8.09s\n",
      "930:\tlearn: 0.0850298\ttotal: 1m 47s\tremaining: 7.97s\n",
      "931:\tlearn: 0.0849672\ttotal: 1m 47s\tremaining: 7.86s\n",
      "932:\tlearn: 0.0849344\ttotal: 1m 47s\tremaining: 7.74s\n",
      "933:\tlearn: 0.0849193\ttotal: 1m 47s\tremaining: 7.63s\n",
      "934:\tlearn: 0.0848873\ttotal: 1m 48s\tremaining: 7.51s\n",
      "935:\tlearn: 0.0848482\ttotal: 1m 48s\tremaining: 7.4s\n",
      "936:\tlearn: 0.0848186\ttotal: 1m 48s\tremaining: 7.29s\n",
      "937:\tlearn: 0.0847909\ttotal: 1m 48s\tremaining: 7.17s\n",
      "938:\tlearn: 0.0847116\ttotal: 1m 48s\tremaining: 7.05s\n",
      "939:\tlearn: 0.0846814\ttotal: 1m 48s\tremaining: 6.94s\n",
      "940:\tlearn: 0.0846310\ttotal: 1m 48s\tremaining: 6.82s\n",
      "941:\tlearn: 0.0845595\ttotal: 1m 48s\tremaining: 6.71s\n",
      "942:\tlearn: 0.0845265\ttotal: 1m 49s\tremaining: 6.59s\n",
      "943:\tlearn: 0.0844973\ttotal: 1m 49s\tremaining: 6.47s\n",
      "944:\tlearn: 0.0844643\ttotal: 1m 49s\tremaining: 6.36s\n",
      "945:\tlearn: 0.0844174\ttotal: 1m 49s\tremaining: 6.25s\n",
      "946:\tlearn: 0.0843809\ttotal: 1m 49s\tremaining: 6.13s\n",
      "947:\tlearn: 0.0843568\ttotal: 1m 49s\tremaining: 6.01s\n",
      "948:\tlearn: 0.0843497\ttotal: 1m 49s\tremaining: 5.9s\n",
      "949:\tlearn: 0.0843379\ttotal: 1m 49s\tremaining: 5.78s\n",
      "950:\tlearn: 0.0843131\ttotal: 1m 49s\tremaining: 5.67s\n",
      "951:\tlearn: 0.0842720\ttotal: 1m 50s\tremaining: 5.55s\n",
      "952:\tlearn: 0.0842035\ttotal: 1m 50s\tremaining: 5.44s\n",
      "953:\tlearn: 0.0841816\ttotal: 1m 50s\tremaining: 5.32s\n",
      "954:\tlearn: 0.0841483\ttotal: 1m 50s\tremaining: 5.2s\n",
      "955:\tlearn: 0.0841163\ttotal: 1m 50s\tremaining: 5.09s\n",
      "956:\tlearn: 0.0840708\ttotal: 1m 50s\tremaining: 4.97s\n",
      "957:\tlearn: 0.0840458\ttotal: 1m 50s\tremaining: 4.86s\n",
      "958:\tlearn: 0.0840278\ttotal: 1m 50s\tremaining: 4.74s\n",
      "959:\tlearn: 0.0840101\ttotal: 1m 51s\tremaining: 4.63s\n",
      "960:\tlearn: 0.0839715\ttotal: 1m 51s\tremaining: 4.51s\n",
      "961:\tlearn: 0.0839475\ttotal: 1m 51s\tremaining: 4.39s\n",
      "962:\tlearn: 0.0838920\ttotal: 1m 51s\tremaining: 4.28s\n",
      "963:\tlearn: 0.0838489\ttotal: 1m 51s\tremaining: 4.16s\n",
      "964:\tlearn: 0.0838323\ttotal: 1m 51s\tremaining: 4.05s\n",
      "965:\tlearn: 0.0838054\ttotal: 1m 51s\tremaining: 3.93s\n",
      "966:\tlearn: 0.0837717\ttotal: 1m 51s\tremaining: 3.82s\n",
      "967:\tlearn: 0.0837395\ttotal: 1m 51s\tremaining: 3.7s\n",
      "968:\tlearn: 0.0837252\ttotal: 1m 52s\tremaining: 3.58s\n",
      "969:\tlearn: 0.0836906\ttotal: 1m 52s\tremaining: 3.47s\n",
      "970:\tlearn: 0.0836701\ttotal: 1m 52s\tremaining: 3.35s\n",
      "971:\tlearn: 0.0836395\ttotal: 1m 52s\tremaining: 3.24s\n",
      "972:\tlearn: 0.0836135\ttotal: 1m 52s\tremaining: 3.12s\n",
      "973:\tlearn: 0.0835668\ttotal: 1m 52s\tremaining: 3.01s\n",
      "974:\tlearn: 0.0835333\ttotal: 1m 52s\tremaining: 2.89s\n",
      "975:\tlearn: 0.0834934\ttotal: 1m 52s\tremaining: 2.77s\n",
      "976:\tlearn: 0.0834893\ttotal: 1m 52s\tremaining: 2.66s\n",
      "977:\tlearn: 0.0834664\ttotal: 1m 53s\tremaining: 2.54s\n",
      "978:\tlearn: 0.0834337\ttotal: 1m 53s\tremaining: 2.43s\n",
      "979:\tlearn: 0.0833878\ttotal: 1m 53s\tremaining: 2.31s\n",
      "980:\tlearn: 0.0833493\ttotal: 1m 53s\tremaining: 2.2s\n",
      "981:\tlearn: 0.0833195\ttotal: 1m 53s\tremaining: 2.08s\n",
      "982:\tlearn: 0.0833054\ttotal: 1m 53s\tremaining: 1.97s\n",
      "983:\tlearn: 0.0832825\ttotal: 1m 53s\tremaining: 1.85s\n",
      "984:\tlearn: 0.0832163\ttotal: 1m 53s\tremaining: 1.73s\n",
      "985:\tlearn: 0.0832013\ttotal: 1m 53s\tremaining: 1.62s\n",
      "986:\tlearn: 0.0831406\ttotal: 1m 54s\tremaining: 1.5s\n",
      "987:\tlearn: 0.0831149\ttotal: 1m 54s\tremaining: 1.39s\n",
      "988:\tlearn: 0.0830846\ttotal: 1m 54s\tremaining: 1.27s\n",
      "989:\tlearn: 0.0830533\ttotal: 1m 54s\tremaining: 1.16s\n",
      "990:\tlearn: 0.0829980\ttotal: 1m 54s\tremaining: 1.04s\n",
      "991:\tlearn: 0.0829579\ttotal: 1m 54s\tremaining: 925ms\n",
      "992:\tlearn: 0.0829255\ttotal: 1m 54s\tremaining: 809ms\n",
      "993:\tlearn: 0.0828893\ttotal: 1m 54s\tremaining: 694ms\n",
      "994:\tlearn: 0.0828734\ttotal: 1m 55s\tremaining: 578ms\n",
      "995:\tlearn: 0.0828214\ttotal: 1m 55s\tremaining: 462ms\n",
      "996:\tlearn: 0.0828077\ttotal: 1m 55s\tremaining: 347ms\n",
      "997:\tlearn: 0.0827562\ttotal: 1m 55s\tremaining: 231ms\n",
      "998:\tlearn: 0.0827315\ttotal: 1m 55s\tremaining: 116ms\n",
      "999:\tlearn: 0.0826931\ttotal: 1m 55s\tremaining: 0us\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     27361\n",
      "           1       0.90      0.88      0.89      6950\n",
      "\n",
      "    accuracy                           0.96     34311\n",
      "   macro avg       0.94      0.93      0.93     34311\n",
      "weighted avg       0.96      0.96      0.96     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "\n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "catboost_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', CatBoostClassifier(max_depth=10, one_hot_max_size=10))\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, test_size=0.05, random_state=42, stratify=y)\n",
    "\n",
    "catboost_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = catboost_pipeline.predict(X_test)\n",
    "\n",
    "# param_grid = {\n",
    "#     'num_leaves': [20, 30, 40],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'min_child_weight': [0.001, 0.05, 0.01]\n",
    "# }\n",
    "# # Créer une instance de GridSearchCV\n",
    "# grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Effectuer la recherche sur la grille\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Meilleurs hyperparamètres trouvés\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Meilleur score obtenu sur l'ensemble d'entraînement\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# # Meilleur modèle\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Utiliser le meilleur modèle pour prédire les étiquettes sur l'ensemble de test\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# print(\"Meilleurs hyperparamètres:\", best_params)\n",
    "# print(\"Meilleur score sur l'ensemble d'entraînement:\", best_score)\n",
    "# print(\"Rapport de classification:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5258156\ttotal: 99ms\tremaining: 1m 38s\n",
      "1:\tlearn: 0.4275702\ttotal: 197ms\tremaining: 1m 38s\n",
      "2:\tlearn: 0.3595619\ttotal: 299ms\tremaining: 1m 39s\n",
      "3:\tlearn: 0.3146526\ttotal: 396ms\tremaining: 1m 38s\n",
      "4:\tlearn: 0.2914315\ttotal: 485ms\tremaining: 1m 36s\n",
      "5:\tlearn: 0.2723684\ttotal: 585ms\tremaining: 1m 36s\n",
      "6:\tlearn: 0.2555552\ttotal: 687ms\tremaining: 1m 37s\n",
      "7:\tlearn: 0.2449120\ttotal: 785ms\tremaining: 1m 37s\n",
      "8:\tlearn: 0.2367658\ttotal: 877ms\tremaining: 1m 36s\n",
      "9:\tlearn: 0.2305051\ttotal: 973ms\tremaining: 1m 36s\n",
      "10:\tlearn: 0.2236396\ttotal: 1.06s\tremaining: 1m 35s\n",
      "11:\tlearn: 0.2149944\ttotal: 1.15s\tremaining: 1m 34s\n",
      "12:\tlearn: 0.2106634\ttotal: 1.24s\tremaining: 1m 34s\n",
      "13:\tlearn: 0.2070814\ttotal: 1.34s\tremaining: 1m 34s\n",
      "14:\tlearn: 0.2032672\ttotal: 1.44s\tremaining: 1m 34s\n",
      "15:\tlearn: 0.2011847\ttotal: 1.56s\tremaining: 1m 36s\n",
      "16:\tlearn: 0.1985610\ttotal: 1.66s\tremaining: 1m 35s\n",
      "17:\tlearn: 0.1964792\ttotal: 1.75s\tremaining: 1m 35s\n",
      "18:\tlearn: 0.1915291\ttotal: 1.85s\tremaining: 1m 35s\n",
      "19:\tlearn: 0.1889605\ttotal: 1.95s\tremaining: 1m 35s\n",
      "20:\tlearn: 0.1872897\ttotal: 2.05s\tremaining: 1m 35s\n",
      "21:\tlearn: 0.1856656\ttotal: 2.15s\tremaining: 1m 35s\n",
      "22:\tlearn: 0.1843362\ttotal: 2.24s\tremaining: 1m 35s\n",
      "23:\tlearn: 0.1832623\ttotal: 2.32s\tremaining: 1m 34s\n",
      "24:\tlearn: 0.1814054\ttotal: 2.42s\tremaining: 1m 34s\n",
      "25:\tlearn: 0.1800819\ttotal: 2.53s\tremaining: 1m 34s\n",
      "26:\tlearn: 0.1787554\ttotal: 2.63s\tremaining: 1m 34s\n",
      "27:\tlearn: 0.1776322\ttotal: 2.72s\tremaining: 1m 34s\n",
      "28:\tlearn: 0.1758824\ttotal: 2.82s\tremaining: 1m 34s\n",
      "29:\tlearn: 0.1746993\ttotal: 2.91s\tremaining: 1m 34s\n",
      "30:\tlearn: 0.1738582\ttotal: 3.02s\tremaining: 1m 34s\n",
      "31:\tlearn: 0.1726512\ttotal: 3.13s\tremaining: 1m 34s\n",
      "32:\tlearn: 0.1717344\ttotal: 3.25s\tremaining: 1m 35s\n",
      "33:\tlearn: 0.1711089\ttotal: 3.36s\tremaining: 1m 35s\n",
      "34:\tlearn: 0.1700743\ttotal: 3.46s\tremaining: 1m 35s\n",
      "35:\tlearn: 0.1693322\ttotal: 3.55s\tremaining: 1m 35s\n",
      "36:\tlearn: 0.1680947\ttotal: 3.65s\tremaining: 1m 34s\n",
      "37:\tlearn: 0.1673574\ttotal: 3.74s\tremaining: 1m 34s\n",
      "38:\tlearn: 0.1663959\ttotal: 3.84s\tremaining: 1m 34s\n",
      "39:\tlearn: 0.1654508\ttotal: 3.93s\tremaining: 1m 34s\n",
      "40:\tlearn: 0.1647577\ttotal: 4.03s\tremaining: 1m 34s\n",
      "41:\tlearn: 0.1640215\ttotal: 4.12s\tremaining: 1m 33s\n",
      "42:\tlearn: 0.1634221\ttotal: 4.21s\tremaining: 1m 33s\n",
      "43:\tlearn: 0.1627145\ttotal: 4.32s\tremaining: 1m 33s\n",
      "44:\tlearn: 0.1619436\ttotal: 4.42s\tremaining: 1m 33s\n",
      "45:\tlearn: 0.1610363\ttotal: 4.52s\tremaining: 1m 33s\n",
      "46:\tlearn: 0.1602128\ttotal: 4.62s\tremaining: 1m 33s\n",
      "47:\tlearn: 0.1594734\ttotal: 4.71s\tremaining: 1m 33s\n",
      "48:\tlearn: 0.1588850\ttotal: 4.81s\tremaining: 1m 33s\n",
      "49:\tlearn: 0.1583758\ttotal: 4.9s\tremaining: 1m 33s\n",
      "50:\tlearn: 0.1576319\ttotal: 5s\tremaining: 1m 32s\n",
      "51:\tlearn: 0.1572795\ttotal: 5.08s\tremaining: 1m 32s\n",
      "52:\tlearn: 0.1568771\ttotal: 5.18s\tremaining: 1m 32s\n",
      "53:\tlearn: 0.1563854\ttotal: 5.28s\tremaining: 1m 32s\n",
      "54:\tlearn: 0.1556445\ttotal: 5.37s\tremaining: 1m 32s\n",
      "55:\tlearn: 0.1550236\ttotal: 5.48s\tremaining: 1m 32s\n",
      "56:\tlearn: 0.1545078\ttotal: 5.59s\tremaining: 1m 32s\n",
      "57:\tlearn: 0.1541929\ttotal: 5.7s\tremaining: 1m 32s\n",
      "58:\tlearn: 0.1537567\ttotal: 5.8s\tremaining: 1m 32s\n",
      "59:\tlearn: 0.1531768\ttotal: 5.9s\tremaining: 1m 32s\n",
      "60:\tlearn: 0.1526770\ttotal: 6s\tremaining: 1m 32s\n",
      "61:\tlearn: 0.1523053\ttotal: 6.09s\tremaining: 1m 32s\n",
      "62:\tlearn: 0.1519712\ttotal: 6.18s\tremaining: 1m 31s\n",
      "63:\tlearn: 0.1514593\ttotal: 6.28s\tremaining: 1m 31s\n",
      "64:\tlearn: 0.1508909\ttotal: 6.37s\tremaining: 1m 31s\n",
      "65:\tlearn: 0.1505376\ttotal: 6.54s\tremaining: 1m 32s\n",
      "66:\tlearn: 0.1502094\ttotal: 6.66s\tremaining: 1m 32s\n",
      "67:\tlearn: 0.1497804\ttotal: 6.76s\tremaining: 1m 32s\n",
      "68:\tlearn: 0.1494810\ttotal: 6.86s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1490864\ttotal: 6.96s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1487769\ttotal: 7.05s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1485032\ttotal: 7.14s\tremaining: 1m 32s\n",
      "72:\tlearn: 0.1481410\ttotal: 7.25s\tremaining: 1m 32s\n",
      "73:\tlearn: 0.1478073\ttotal: 7.41s\tremaining: 1m 32s\n",
      "74:\tlearn: 0.1475518\ttotal: 7.5s\tremaining: 1m 32s\n",
      "75:\tlearn: 0.1472674\ttotal: 7.6s\tremaining: 1m 32s\n",
      "76:\tlearn: 0.1465009\ttotal: 7.68s\tremaining: 1m 32s\n",
      "77:\tlearn: 0.1461867\ttotal: 7.77s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1459362\ttotal: 7.86s\tremaining: 1m 31s\n",
      "79:\tlearn: 0.1455664\ttotal: 7.95s\tremaining: 1m 31s\n",
      "80:\tlearn: 0.1453026\ttotal: 8.12s\tremaining: 1m 32s\n",
      "81:\tlearn: 0.1448449\ttotal: 8.22s\tremaining: 1m 32s\n",
      "82:\tlearn: 0.1446525\ttotal: 8.31s\tremaining: 1m 31s\n",
      "83:\tlearn: 0.1444388\ttotal: 8.4s\tremaining: 1m 31s\n",
      "84:\tlearn: 0.1442546\ttotal: 8.5s\tremaining: 1m 31s\n",
      "85:\tlearn: 0.1439770\ttotal: 8.59s\tremaining: 1m 31s\n",
      "86:\tlearn: 0.1436316\ttotal: 8.68s\tremaining: 1m 31s\n",
      "87:\tlearn: 0.1433885\ttotal: 8.83s\tremaining: 1m 31s\n",
      "88:\tlearn: 0.1431889\ttotal: 8.92s\tremaining: 1m 31s\n",
      "89:\tlearn: 0.1428542\ttotal: 9.01s\tremaining: 1m 31s\n",
      "90:\tlearn: 0.1424933\ttotal: 9.11s\tremaining: 1m 30s\n",
      "91:\tlearn: 0.1422926\ttotal: 9.2s\tremaining: 1m 30s\n",
      "92:\tlearn: 0.1420365\ttotal: 9.29s\tremaining: 1m 30s\n",
      "93:\tlearn: 0.1418552\ttotal: 9.38s\tremaining: 1m 30s\n",
      "94:\tlearn: 0.1415318\ttotal: 9.48s\tremaining: 1m 30s\n",
      "95:\tlearn: 0.1411421\ttotal: 9.57s\tremaining: 1m 30s\n",
      "96:\tlearn: 0.1408045\ttotal: 9.67s\tremaining: 1m 29s\n",
      "97:\tlearn: 0.1405540\ttotal: 9.76s\tremaining: 1m 29s\n",
      "98:\tlearn: 0.1402427\ttotal: 9.86s\tremaining: 1m 29s\n",
      "99:\tlearn: 0.1398887\ttotal: 9.95s\tremaining: 1m 29s\n",
      "100:\tlearn: 0.1397203\ttotal: 10s\tremaining: 1m 29s\n",
      "101:\tlearn: 0.1394975\ttotal: 10.1s\tremaining: 1m 29s\n",
      "102:\tlearn: 0.1392873\ttotal: 10.2s\tremaining: 1m 29s\n",
      "103:\tlearn: 0.1389902\ttotal: 10.3s\tremaining: 1m 29s\n",
      "104:\tlearn: 0.1387788\ttotal: 10.4s\tremaining: 1m 28s\n",
      "105:\tlearn: 0.1384857\ttotal: 10.5s\tremaining: 1m 28s\n",
      "106:\tlearn: 0.1382386\ttotal: 10.6s\tremaining: 1m 28s\n",
      "107:\tlearn: 0.1380589\ttotal: 10.7s\tremaining: 1m 28s\n",
      "108:\tlearn: 0.1378432\ttotal: 10.8s\tremaining: 1m 28s\n",
      "109:\tlearn: 0.1376637\ttotal: 10.9s\tremaining: 1m 28s\n",
      "110:\tlearn: 0.1374519\ttotal: 11s\tremaining: 1m 28s\n",
      "111:\tlearn: 0.1372601\ttotal: 11.1s\tremaining: 1m 27s\n",
      "112:\tlearn: 0.1370462\ttotal: 11.2s\tremaining: 1m 27s\n",
      "113:\tlearn: 0.1368055\ttotal: 11.3s\tremaining: 1m 27s\n",
      "114:\tlearn: 0.1366886\ttotal: 11.4s\tremaining: 1m 27s\n",
      "115:\tlearn: 0.1365339\ttotal: 11.5s\tremaining: 1m 27s\n",
      "116:\tlearn: 0.1361956\ttotal: 11.6s\tremaining: 1m 27s\n",
      "117:\tlearn: 0.1360213\ttotal: 11.7s\tremaining: 1m 27s\n",
      "118:\tlearn: 0.1357781\ttotal: 11.8s\tremaining: 1m 27s\n",
      "119:\tlearn: 0.1355307\ttotal: 11.8s\tremaining: 1m 26s\n",
      "120:\tlearn: 0.1353288\ttotal: 11.9s\tremaining: 1m 26s\n",
      "121:\tlearn: 0.1351510\ttotal: 12s\tremaining: 1m 26s\n",
      "122:\tlearn: 0.1350057\ttotal: 12.1s\tremaining: 1m 26s\n",
      "123:\tlearn: 0.1347995\ttotal: 12.2s\tremaining: 1m 26s\n",
      "124:\tlearn: 0.1345595\ttotal: 12.3s\tremaining: 1m 26s\n",
      "125:\tlearn: 0.1344063\ttotal: 12.4s\tremaining: 1m 26s\n",
      "126:\tlearn: 0.1342242\ttotal: 12.5s\tremaining: 1m 25s\n",
      "127:\tlearn: 0.1340336\ttotal: 12.6s\tremaining: 1m 25s\n",
      "128:\tlearn: 0.1337690\ttotal: 12.7s\tremaining: 1m 25s\n",
      "129:\tlearn: 0.1335338\ttotal: 12.8s\tremaining: 1m 25s\n",
      "130:\tlearn: 0.1333892\ttotal: 12.9s\tremaining: 1m 25s\n",
      "131:\tlearn: 0.1332723\ttotal: 13s\tremaining: 1m 25s\n",
      "132:\tlearn: 0.1331502\ttotal: 13.1s\tremaining: 1m 25s\n",
      "133:\tlearn: 0.1330121\ttotal: 13.2s\tremaining: 1m 25s\n",
      "134:\tlearn: 0.1329061\ttotal: 13.2s\tremaining: 1m 24s\n",
      "135:\tlearn: 0.1327149\ttotal: 13.3s\tremaining: 1m 24s\n",
      "136:\tlearn: 0.1325452\ttotal: 13.4s\tremaining: 1m 24s\n",
      "137:\tlearn: 0.1323956\ttotal: 13.5s\tremaining: 1m 24s\n",
      "138:\tlearn: 0.1322082\ttotal: 13.6s\tremaining: 1m 24s\n",
      "139:\tlearn: 0.1320507\ttotal: 13.7s\tremaining: 1m 24s\n",
      "140:\tlearn: 0.1319117\ttotal: 13.8s\tremaining: 1m 24s\n",
      "141:\tlearn: 0.1317541\ttotal: 13.9s\tremaining: 1m 24s\n",
      "142:\tlearn: 0.1315899\ttotal: 14s\tremaining: 1m 23s\n",
      "143:\tlearn: 0.1313998\ttotal: 14.1s\tremaining: 1m 23s\n",
      "144:\tlearn: 0.1312715\ttotal: 14.2s\tremaining: 1m 23s\n",
      "145:\tlearn: 0.1311112\ttotal: 14.3s\tremaining: 1m 23s\n",
      "146:\tlearn: 0.1308869\ttotal: 14.4s\tremaining: 1m 23s\n",
      "147:\tlearn: 0.1306621\ttotal: 14.5s\tremaining: 1m 23s\n",
      "148:\tlearn: 0.1305288\ttotal: 14.6s\tremaining: 1m 23s\n",
      "149:\tlearn: 0.1304038\ttotal: 14.7s\tremaining: 1m 23s\n",
      "150:\tlearn: 0.1302783\ttotal: 14.8s\tremaining: 1m 23s\n",
      "151:\tlearn: 0.1301604\ttotal: 14.9s\tremaining: 1m 22s\n",
      "152:\tlearn: 0.1300270\ttotal: 15s\tremaining: 1m 22s\n",
      "153:\tlearn: 0.1299273\ttotal: 15s\tremaining: 1m 22s\n",
      "154:\tlearn: 0.1298137\ttotal: 15.1s\tremaining: 1m 22s\n",
      "155:\tlearn: 0.1297211\ttotal: 15.2s\tremaining: 1m 22s\n",
      "156:\tlearn: 0.1296072\ttotal: 15.3s\tremaining: 1m 22s\n",
      "157:\tlearn: 0.1295105\ttotal: 15.4s\tremaining: 1m 22s\n",
      "158:\tlearn: 0.1294048\ttotal: 15.5s\tremaining: 1m 22s\n",
      "159:\tlearn: 0.1292461\ttotal: 15.6s\tremaining: 1m 21s\n",
      "160:\tlearn: 0.1291276\ttotal: 15.7s\tremaining: 1m 21s\n",
      "161:\tlearn: 0.1289962\ttotal: 15.8s\tremaining: 1m 21s\n",
      "162:\tlearn: 0.1288591\ttotal: 15.9s\tremaining: 1m 21s\n",
      "163:\tlearn: 0.1287675\ttotal: 16s\tremaining: 1m 21s\n",
      "164:\tlearn: 0.1285956\ttotal: 16.1s\tremaining: 1m 21s\n",
      "165:\tlearn: 0.1284192\ttotal: 16.2s\tremaining: 1m 21s\n",
      "166:\tlearn: 0.1283228\ttotal: 16.3s\tremaining: 1m 21s\n",
      "167:\tlearn: 0.1281271\ttotal: 16.4s\tremaining: 1m 21s\n",
      "168:\tlearn: 0.1279557\ttotal: 16.5s\tremaining: 1m 20s\n",
      "169:\tlearn: 0.1278561\ttotal: 16.6s\tremaining: 1m 20s\n",
      "170:\tlearn: 0.1276495\ttotal: 16.7s\tremaining: 1m 20s\n",
      "171:\tlearn: 0.1275104\ttotal: 16.8s\tremaining: 1m 20s\n",
      "172:\tlearn: 0.1273983\ttotal: 16.8s\tremaining: 1m 20s\n",
      "173:\tlearn: 0.1272570\ttotal: 16.9s\tremaining: 1m 20s\n",
      "174:\tlearn: 0.1271269\ttotal: 17s\tremaining: 1m 20s\n",
      "175:\tlearn: 0.1270277\ttotal: 17.2s\tremaining: 1m 20s\n",
      "176:\tlearn: 0.1269040\ttotal: 17.3s\tremaining: 1m 20s\n",
      "177:\tlearn: 0.1267990\ttotal: 17.4s\tremaining: 1m 20s\n",
      "178:\tlearn: 0.1267441\ttotal: 17.5s\tremaining: 1m 20s\n",
      "179:\tlearn: 0.1265583\ttotal: 17.6s\tremaining: 1m 20s\n",
      "180:\tlearn: 0.1263702\ttotal: 17.7s\tremaining: 1m 20s\n",
      "181:\tlearn: 0.1262566\ttotal: 17.8s\tremaining: 1m 20s\n",
      "182:\tlearn: 0.1260798\ttotal: 17.9s\tremaining: 1m 19s\n",
      "183:\tlearn: 0.1259582\ttotal: 18s\tremaining: 1m 19s\n",
      "184:\tlearn: 0.1258243\ttotal: 18.1s\tremaining: 1m 19s\n",
      "185:\tlearn: 0.1257476\ttotal: 18.2s\tremaining: 1m 19s\n",
      "186:\tlearn: 0.1256174\ttotal: 18.3s\tremaining: 1m 19s\n",
      "187:\tlearn: 0.1254876\ttotal: 18.4s\tremaining: 1m 19s\n",
      "188:\tlearn: 0.1254090\ttotal: 18.5s\tremaining: 1m 19s\n",
      "189:\tlearn: 0.1253168\ttotal: 18.6s\tremaining: 1m 19s\n",
      "190:\tlearn: 0.1252451\ttotal: 18.7s\tremaining: 1m 19s\n",
      "191:\tlearn: 0.1251441\ttotal: 18.8s\tremaining: 1m 18s\n",
      "192:\tlearn: 0.1250396\ttotal: 18.9s\tremaining: 1m 18s\n",
      "193:\tlearn: 0.1249147\ttotal: 18.9s\tremaining: 1m 18s\n",
      "194:\tlearn: 0.1247739\ttotal: 19s\tremaining: 1m 18s\n",
      "195:\tlearn: 0.1245966\ttotal: 19.1s\tremaining: 1m 18s\n",
      "196:\tlearn: 0.1245060\ttotal: 19.2s\tremaining: 1m 18s\n",
      "197:\tlearn: 0.1243858\ttotal: 19.3s\tremaining: 1m 18s\n",
      "198:\tlearn: 0.1242927\ttotal: 19.4s\tremaining: 1m 18s\n",
      "199:\tlearn: 0.1241966\ttotal: 19.5s\tremaining: 1m 18s\n",
      "200:\tlearn: 0.1241192\ttotal: 19.6s\tremaining: 1m 17s\n",
      "201:\tlearn: 0.1239571\ttotal: 19.7s\tremaining: 1m 17s\n",
      "202:\tlearn: 0.1238523\ttotal: 19.8s\tremaining: 1m 17s\n",
      "203:\tlearn: 0.1237112\ttotal: 19.9s\tremaining: 1m 17s\n",
      "204:\tlearn: 0.1235913\ttotal: 20s\tremaining: 1m 17s\n",
      "205:\tlearn: 0.1235183\ttotal: 20.1s\tremaining: 1m 17s\n",
      "206:\tlearn: 0.1233837\ttotal: 20.2s\tremaining: 1m 17s\n",
      "207:\tlearn: 0.1233208\ttotal: 20.3s\tremaining: 1m 17s\n",
      "208:\tlearn: 0.1232357\ttotal: 20.4s\tremaining: 1m 17s\n",
      "209:\tlearn: 0.1231225\ttotal: 20.5s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1230434\ttotal: 20.5s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1229484\ttotal: 20.6s\tremaining: 1m 16s\n",
      "212:\tlearn: 0.1228616\ttotal: 20.7s\tremaining: 1m 16s\n",
      "213:\tlearn: 0.1227769\ttotal: 20.8s\tremaining: 1m 16s\n",
      "214:\tlearn: 0.1226802\ttotal: 20.9s\tremaining: 1m 16s\n",
      "215:\tlearn: 0.1226135\ttotal: 21s\tremaining: 1m 16s\n",
      "216:\tlearn: 0.1225393\ttotal: 21.1s\tremaining: 1m 16s\n",
      "217:\tlearn: 0.1223897\ttotal: 21.2s\tremaining: 1m 16s\n",
      "218:\tlearn: 0.1222908\ttotal: 21.3s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1221805\ttotal: 21.4s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1220267\ttotal: 21.5s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.1219132\ttotal: 21.6s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.1218110\ttotal: 21.7s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.1217200\ttotal: 21.8s\tremaining: 1m 15s\n",
      "224:\tlearn: 0.1215757\ttotal: 21.9s\tremaining: 1m 15s\n",
      "225:\tlearn: 0.1215034\ttotal: 22s\tremaining: 1m 15s\n",
      "226:\tlearn: 0.1213970\ttotal: 22s\tremaining: 1m 15s\n",
      "227:\tlearn: 0.1212845\ttotal: 22.1s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1211862\ttotal: 22.2s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1210999\ttotal: 22.3s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1210223\ttotal: 22.4s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1209428\ttotal: 22.5s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1208081\ttotal: 22.6s\tremaining: 1m 14s\n",
      "233:\tlearn: 0.1206547\ttotal: 22.7s\tremaining: 1m 14s\n",
      "234:\tlearn: 0.1205656\ttotal: 22.8s\tremaining: 1m 14s\n",
      "235:\tlearn: 0.1204955\ttotal: 22.9s\tremaining: 1m 14s\n",
      "236:\tlearn: 0.1203941\ttotal: 23s\tremaining: 1m 14s\n",
      "237:\tlearn: 0.1202786\ttotal: 23.1s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1201887\ttotal: 23.2s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1200797\ttotal: 23.3s\tremaining: 1m 13s\n",
      "240:\tlearn: 0.1200221\ttotal: 23.4s\tremaining: 1m 13s\n",
      "241:\tlearn: 0.1199261\ttotal: 23.5s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1198521\ttotal: 23.6s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1196802\ttotal: 23.7s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1196184\ttotal: 23.8s\tremaining: 1m 13s\n",
      "245:\tlearn: 0.1195320\ttotal: 23.9s\tremaining: 1m 13s\n",
      "246:\tlearn: 0.1194098\ttotal: 24s\tremaining: 1m 13s\n",
      "247:\tlearn: 0.1193075\ttotal: 24.1s\tremaining: 1m 12s\n",
      "248:\tlearn: 0.1192447\ttotal: 24.2s\tremaining: 1m 12s\n",
      "249:\tlearn: 0.1191508\ttotal: 24.3s\tremaining: 1m 12s\n",
      "250:\tlearn: 0.1190514\ttotal: 24.4s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.1189760\ttotal: 24.5s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.1187875\ttotal: 24.6s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.1186761\ttotal: 24.7s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.1186180\ttotal: 24.8s\tremaining: 1m 12s\n",
      "255:\tlearn: 0.1184723\ttotal: 24.9s\tremaining: 1m 12s\n",
      "256:\tlearn: 0.1183526\ttotal: 25s\tremaining: 1m 12s\n",
      "257:\tlearn: 0.1182457\ttotal: 25.1s\tremaining: 1m 12s\n",
      "258:\tlearn: 0.1181412\ttotal: 25.2s\tremaining: 1m 11s\n",
      "259:\tlearn: 0.1180766\ttotal: 25.2s\tremaining: 1m 11s\n",
      "260:\tlearn: 0.1179667\ttotal: 25.3s\tremaining: 1m 11s\n",
      "261:\tlearn: 0.1178890\ttotal: 25.4s\tremaining: 1m 11s\n",
      "262:\tlearn: 0.1177935\ttotal: 25.5s\tremaining: 1m 11s\n",
      "263:\tlearn: 0.1177041\ttotal: 25.6s\tremaining: 1m 11s\n",
      "264:\tlearn: 0.1176498\ttotal: 25.7s\tremaining: 1m 11s\n",
      "265:\tlearn: 0.1175866\ttotal: 25.8s\tremaining: 1m 11s\n",
      "266:\tlearn: 0.1175166\ttotal: 25.9s\tremaining: 1m 11s\n",
      "267:\tlearn: 0.1173913\ttotal: 26s\tremaining: 1m 11s\n",
      "268:\tlearn: 0.1173162\ttotal: 26.1s\tremaining: 1m 10s\n",
      "269:\tlearn: 0.1172632\ttotal: 26.2s\tremaining: 1m 10s\n",
      "270:\tlearn: 0.1171676\ttotal: 26.3s\tremaining: 1m 10s\n",
      "271:\tlearn: 0.1170599\ttotal: 26.4s\tremaining: 1m 10s\n",
      "272:\tlearn: 0.1169683\ttotal: 26.5s\tremaining: 1m 10s\n",
      "273:\tlearn: 0.1169000\ttotal: 26.6s\tremaining: 1m 10s\n",
      "274:\tlearn: 0.1168447\ttotal: 26.7s\tremaining: 1m 10s\n",
      "275:\tlearn: 0.1167840\ttotal: 26.8s\tremaining: 1m 10s\n",
      "276:\tlearn: 0.1166810\ttotal: 26.9s\tremaining: 1m 10s\n",
      "277:\tlearn: 0.1165916\ttotal: 27s\tremaining: 1m 10s\n",
      "278:\tlearn: 0.1165465\ttotal: 27.1s\tremaining: 1m 9s\n",
      "279:\tlearn: 0.1164246\ttotal: 27.2s\tremaining: 1m 9s\n",
      "280:\tlearn: 0.1163223\ttotal: 27.3s\tremaining: 1m 9s\n",
      "281:\tlearn: 0.1162536\ttotal: 27.4s\tremaining: 1m 9s\n",
      "282:\tlearn: 0.1161693\ttotal: 27.5s\tremaining: 1m 9s\n",
      "283:\tlearn: 0.1160696\ttotal: 27.5s\tremaining: 1m 9s\n",
      "284:\tlearn: 0.1159481\ttotal: 27.6s\tremaining: 1m 9s\n",
      "285:\tlearn: 0.1158446\ttotal: 27.8s\tremaining: 1m 9s\n",
      "286:\tlearn: 0.1158064\ttotal: 28s\tremaining: 1m 9s\n",
      "287:\tlearn: 0.1157317\ttotal: 28s\tremaining: 1m 9s\n",
      "288:\tlearn: 0.1156252\ttotal: 28.1s\tremaining: 1m 9s\n",
      "289:\tlearn: 0.1155437\ttotal: 28.2s\tremaining: 1m 9s\n",
      "290:\tlearn: 0.1154923\ttotal: 28.3s\tremaining: 1m 9s\n",
      "291:\tlearn: 0.1154226\ttotal: 28.4s\tremaining: 1m 8s\n",
      "292:\tlearn: 0.1153639\ttotal: 28.5s\tremaining: 1m 8s\n",
      "293:\tlearn: 0.1152655\ttotal: 28.6s\tremaining: 1m 8s\n",
      "294:\tlearn: 0.1152095\ttotal: 28.7s\tremaining: 1m 8s\n",
      "295:\tlearn: 0.1151149\ttotal: 28.8s\tremaining: 1m 8s\n",
      "296:\tlearn: 0.1150489\ttotal: 28.9s\tremaining: 1m 8s\n",
      "297:\tlearn: 0.1149891\ttotal: 29s\tremaining: 1m 8s\n",
      "298:\tlearn: 0.1148956\ttotal: 29.1s\tremaining: 1m 8s\n",
      "299:\tlearn: 0.1148263\ttotal: 29.2s\tremaining: 1m 8s\n",
      "300:\tlearn: 0.1147254\ttotal: 29.3s\tremaining: 1m 8s\n",
      "301:\tlearn: 0.1146407\ttotal: 29.4s\tremaining: 1m 7s\n",
      "302:\tlearn: 0.1145758\ttotal: 29.5s\tremaining: 1m 7s\n",
      "303:\tlearn: 0.1145075\ttotal: 29.6s\tremaining: 1m 7s\n",
      "304:\tlearn: 0.1144093\ttotal: 29.7s\tremaining: 1m 7s\n",
      "305:\tlearn: 0.1142887\ttotal: 29.8s\tremaining: 1m 7s\n",
      "306:\tlearn: 0.1141871\ttotal: 29.9s\tremaining: 1m 7s\n",
      "307:\tlearn: 0.1141307\ttotal: 30s\tremaining: 1m 7s\n",
      "308:\tlearn: 0.1140627\ttotal: 30s\tremaining: 1m 7s\n",
      "309:\tlearn: 0.1140052\ttotal: 30.1s\tremaining: 1m 7s\n",
      "310:\tlearn: 0.1139314\ttotal: 30.2s\tremaining: 1m 7s\n",
      "311:\tlearn: 0.1138762\ttotal: 30.3s\tremaining: 1m 6s\n",
      "312:\tlearn: 0.1138072\ttotal: 30.4s\tremaining: 1m 6s\n",
      "313:\tlearn: 0.1137323\ttotal: 30.5s\tremaining: 1m 6s\n",
      "314:\tlearn: 0.1136684\ttotal: 30.6s\tremaining: 1m 6s\n",
      "315:\tlearn: 0.1136112\ttotal: 30.7s\tremaining: 1m 6s\n",
      "316:\tlearn: 0.1135566\ttotal: 30.8s\tremaining: 1m 6s\n",
      "317:\tlearn: 0.1135028\ttotal: 30.9s\tremaining: 1m 6s\n",
      "318:\tlearn: 0.1134374\ttotal: 31s\tremaining: 1m 6s\n",
      "319:\tlearn: 0.1133728\ttotal: 31.1s\tremaining: 1m 6s\n",
      "320:\tlearn: 0.1132364\ttotal: 31.2s\tremaining: 1m 5s\n",
      "321:\tlearn: 0.1131448\ttotal: 31.3s\tremaining: 1m 5s\n",
      "322:\tlearn: 0.1130902\ttotal: 31.4s\tremaining: 1m 5s\n",
      "323:\tlearn: 0.1129988\ttotal: 31.5s\tremaining: 1m 5s\n",
      "324:\tlearn: 0.1129015\ttotal: 31.6s\tremaining: 1m 5s\n",
      "325:\tlearn: 0.1128635\ttotal: 31.7s\tremaining: 1m 5s\n",
      "326:\tlearn: 0.1128169\ttotal: 31.8s\tremaining: 1m 5s\n",
      "327:\tlearn: 0.1127460\ttotal: 31.9s\tremaining: 1m 5s\n",
      "328:\tlearn: 0.1126309\ttotal: 32s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1125885\ttotal: 32s\tremaining: 1m 5s\n",
      "330:\tlearn: 0.1125097\ttotal: 32.1s\tremaining: 1m 4s\n",
      "331:\tlearn: 0.1124096\ttotal: 32.2s\tremaining: 1m 4s\n",
      "332:\tlearn: 0.1123089\ttotal: 32.3s\tremaining: 1m 4s\n",
      "333:\tlearn: 0.1122214\ttotal: 32.4s\tremaining: 1m 4s\n",
      "334:\tlearn: 0.1121619\ttotal: 32.5s\tremaining: 1m 4s\n",
      "335:\tlearn: 0.1120752\ttotal: 32.6s\tremaining: 1m 4s\n",
      "336:\tlearn: 0.1119766\ttotal: 32.7s\tremaining: 1m 4s\n",
      "337:\tlearn: 0.1119360\ttotal: 32.8s\tremaining: 1m 4s\n",
      "338:\tlearn: 0.1118691\ttotal: 32.9s\tremaining: 1m 4s\n",
      "339:\tlearn: 0.1117882\ttotal: 33s\tremaining: 1m 4s\n",
      "340:\tlearn: 0.1117318\ttotal: 33.1s\tremaining: 1m 4s\n",
      "341:\tlearn: 0.1116664\ttotal: 33.2s\tremaining: 1m 3s\n",
      "342:\tlearn: 0.1116071\ttotal: 33.3s\tremaining: 1m 3s\n",
      "343:\tlearn: 0.1115561\ttotal: 33.4s\tremaining: 1m 3s\n",
      "344:\tlearn: 0.1114846\ttotal: 33.5s\tremaining: 1m 3s\n",
      "345:\tlearn: 0.1113984\ttotal: 33.6s\tremaining: 1m 3s\n",
      "346:\tlearn: 0.1113329\ttotal: 33.7s\tremaining: 1m 3s\n",
      "347:\tlearn: 0.1112619\ttotal: 33.8s\tremaining: 1m 3s\n",
      "348:\tlearn: 0.1112077\ttotal: 33.9s\tremaining: 1m 3s\n",
      "349:\tlearn: 0.1111519\ttotal: 34s\tremaining: 1m 3s\n",
      "350:\tlearn: 0.1111156\ttotal: 34.1s\tremaining: 1m 3s\n",
      "351:\tlearn: 0.1110531\ttotal: 34.2s\tremaining: 1m 2s\n",
      "352:\tlearn: 0.1109741\ttotal: 34.3s\tremaining: 1m 2s\n",
      "353:\tlearn: 0.1109330\ttotal: 34.4s\tremaining: 1m 2s\n",
      "354:\tlearn: 0.1108743\ttotal: 34.5s\tremaining: 1m 2s\n",
      "355:\tlearn: 0.1108165\ttotal: 34.6s\tremaining: 1m 2s\n",
      "356:\tlearn: 0.1107436\ttotal: 34.7s\tremaining: 1m 2s\n",
      "357:\tlearn: 0.1106966\ttotal: 34.8s\tremaining: 1m 2s\n",
      "358:\tlearn: 0.1106627\ttotal: 34.9s\tremaining: 1m 2s\n",
      "359:\tlearn: 0.1105864\ttotal: 34.9s\tremaining: 1m 2s\n",
      "360:\tlearn: 0.1105304\ttotal: 35s\tremaining: 1m 2s\n",
      "361:\tlearn: 0.1104290\ttotal: 35.1s\tremaining: 1m 1s\n",
      "362:\tlearn: 0.1103410\ttotal: 35.2s\tremaining: 1m 1s\n",
      "363:\tlearn: 0.1102466\ttotal: 35.3s\tremaining: 1m 1s\n",
      "364:\tlearn: 0.1102104\ttotal: 35.4s\tremaining: 1m 1s\n",
      "365:\tlearn: 0.1101551\ttotal: 35.5s\tremaining: 1m 1s\n",
      "366:\tlearn: 0.1101030\ttotal: 35.6s\tremaining: 1m 1s\n",
      "367:\tlearn: 0.1100458\ttotal: 35.7s\tremaining: 1m 1s\n",
      "368:\tlearn: 0.1099845\ttotal: 35.8s\tremaining: 1m 1s\n",
      "369:\tlearn: 0.1099090\ttotal: 35.9s\tremaining: 1m 1s\n",
      "370:\tlearn: 0.1098376\ttotal: 36s\tremaining: 1m 1s\n",
      "371:\tlearn: 0.1097221\ttotal: 36.1s\tremaining: 1m\n",
      "372:\tlearn: 0.1096605\ttotal: 36.2s\tremaining: 1m\n",
      "373:\tlearn: 0.1095837\ttotal: 36.3s\tremaining: 1m\n",
      "374:\tlearn: 0.1095105\ttotal: 36.4s\tremaining: 1m\n",
      "375:\tlearn: 0.1094312\ttotal: 36.5s\tremaining: 1m\n",
      "376:\tlearn: 0.1093434\ttotal: 36.6s\tremaining: 1m\n",
      "377:\tlearn: 0.1093040\ttotal: 36.7s\tremaining: 1m\n",
      "378:\tlearn: 0.1092519\ttotal: 36.8s\tremaining: 1m\n",
      "379:\tlearn: 0.1091940\ttotal: 36.9s\tremaining: 1m\n",
      "380:\tlearn: 0.1091011\ttotal: 37s\tremaining: 1m\n",
      "381:\tlearn: 0.1090519\ttotal: 37.1s\tremaining: 60s\n",
      "382:\tlearn: 0.1089955\ttotal: 37.2s\tremaining: 59.9s\n",
      "383:\tlearn: 0.1089547\ttotal: 37.3s\tremaining: 59.8s\n",
      "384:\tlearn: 0.1089092\ttotal: 37.4s\tremaining: 59.7s\n",
      "385:\tlearn: 0.1088552\ttotal: 37.4s\tremaining: 59.6s\n",
      "386:\tlearn: 0.1088072\ttotal: 37.5s\tremaining: 59.5s\n",
      "387:\tlearn: 0.1087356\ttotal: 37.6s\tremaining: 59.4s\n",
      "388:\tlearn: 0.1086892\ttotal: 37.7s\tremaining: 59.3s\n",
      "389:\tlearn: 0.1086363\ttotal: 37.8s\tremaining: 59.2s\n",
      "390:\tlearn: 0.1085912\ttotal: 37.9s\tremaining: 59.1s\n",
      "391:\tlearn: 0.1085322\ttotal: 38s\tremaining: 59s\n",
      "392:\tlearn: 0.1084876\ttotal: 38.1s\tremaining: 58.9s\n",
      "393:\tlearn: 0.1084022\ttotal: 38.2s\tremaining: 58.8s\n",
      "394:\tlearn: 0.1083265\ttotal: 38.3s\tremaining: 58.7s\n",
      "395:\tlearn: 0.1082353\ttotal: 38.4s\tremaining: 58.6s\n",
      "396:\tlearn: 0.1081728\ttotal: 38.6s\tremaining: 58.6s\n",
      "397:\tlearn: 0.1080952\ttotal: 38.7s\tremaining: 58.5s\n",
      "398:\tlearn: 0.1080419\ttotal: 38.8s\tremaining: 58.4s\n",
      "399:\tlearn: 0.1079772\ttotal: 38.9s\tremaining: 58.3s\n",
      "400:\tlearn: 0.1079106\ttotal: 39s\tremaining: 58.2s\n",
      "401:\tlearn: 0.1078669\ttotal: 39.1s\tremaining: 58.1s\n",
      "402:\tlearn: 0.1078238\ttotal: 39.2s\tremaining: 58s\n",
      "403:\tlearn: 0.1077651\ttotal: 39.3s\tremaining: 57.9s\n",
      "404:\tlearn: 0.1077040\ttotal: 39.4s\tremaining: 57.8s\n",
      "405:\tlearn: 0.1076433\ttotal: 39.5s\tremaining: 57.7s\n",
      "406:\tlearn: 0.1075618\ttotal: 39.6s\tremaining: 57.6s\n",
      "407:\tlearn: 0.1074988\ttotal: 39.7s\tremaining: 57.5s\n",
      "408:\tlearn: 0.1074053\ttotal: 39.7s\tremaining: 57.4s\n",
      "409:\tlearn: 0.1073068\ttotal: 39.8s\tremaining: 57.3s\n",
      "410:\tlearn: 0.1072657\ttotal: 39.9s\tremaining: 57.2s\n",
      "411:\tlearn: 0.1072315\ttotal: 40s\tremaining: 57.1s\n",
      "412:\tlearn: 0.1071882\ttotal: 40.1s\tremaining: 57s\n",
      "413:\tlearn: 0.1071472\ttotal: 40.2s\tremaining: 56.9s\n",
      "414:\tlearn: 0.1070933\ttotal: 40.3s\tremaining: 56.8s\n",
      "415:\tlearn: 0.1070432\ttotal: 40.4s\tremaining: 56.7s\n",
      "416:\tlearn: 0.1069935\ttotal: 40.5s\tremaining: 56.6s\n",
      "417:\tlearn: 0.1069595\ttotal: 40.6s\tremaining: 56.5s\n",
      "418:\tlearn: 0.1069269\ttotal: 40.7s\tremaining: 56.4s\n",
      "419:\tlearn: 0.1068526\ttotal: 40.8s\tremaining: 56.3s\n",
      "420:\tlearn: 0.1067973\ttotal: 40.9s\tremaining: 56.2s\n",
      "421:\tlearn: 0.1067752\ttotal: 41s\tremaining: 56.1s\n",
      "422:\tlearn: 0.1067318\ttotal: 41.1s\tremaining: 56s\n",
      "423:\tlearn: 0.1066570\ttotal: 41.2s\tremaining: 55.9s\n",
      "424:\tlearn: 0.1066336\ttotal: 41.3s\tremaining: 55.8s\n",
      "425:\tlearn: 0.1065610\ttotal: 41.4s\tremaining: 55.7s\n",
      "426:\tlearn: 0.1064957\ttotal: 41.5s\tremaining: 55.6s\n",
      "427:\tlearn: 0.1064288\ttotal: 41.6s\tremaining: 55.5s\n",
      "428:\tlearn: 0.1063885\ttotal: 41.6s\tremaining: 55.4s\n",
      "429:\tlearn: 0.1063123\ttotal: 41.7s\tremaining: 55.3s\n",
      "430:\tlearn: 0.1062341\ttotal: 41.8s\tremaining: 55.2s\n",
      "431:\tlearn: 0.1061859\ttotal: 41.9s\tremaining: 55.1s\n",
      "432:\tlearn: 0.1061336\ttotal: 42s\tremaining: 55s\n",
      "433:\tlearn: 0.1060379\ttotal: 42.1s\tremaining: 54.9s\n",
      "434:\tlearn: 0.1060070\ttotal: 42.2s\tremaining: 54.8s\n",
      "435:\tlearn: 0.1059620\ttotal: 42.3s\tremaining: 54.7s\n",
      "436:\tlearn: 0.1059318\ttotal: 42.4s\tremaining: 54.6s\n",
      "437:\tlearn: 0.1058861\ttotal: 42.5s\tremaining: 54.5s\n",
      "438:\tlearn: 0.1058443\ttotal: 42.6s\tremaining: 54.4s\n",
      "439:\tlearn: 0.1057875\ttotal: 42.7s\tremaining: 54.3s\n",
      "440:\tlearn: 0.1057515\ttotal: 42.8s\tremaining: 54.2s\n",
      "441:\tlearn: 0.1057092\ttotal: 42.9s\tremaining: 54.1s\n",
      "442:\tlearn: 0.1056275\ttotal: 43s\tremaining: 54s\n",
      "443:\tlearn: 0.1055677\ttotal: 43.1s\tremaining: 53.9s\n",
      "444:\tlearn: 0.1055492\ttotal: 43.2s\tremaining: 53.8s\n",
      "445:\tlearn: 0.1055123\ttotal: 43.3s\tremaining: 53.7s\n",
      "446:\tlearn: 0.1054823\ttotal: 43.4s\tremaining: 53.6s\n",
      "447:\tlearn: 0.1054491\ttotal: 43.5s\tremaining: 53.5s\n",
      "448:\tlearn: 0.1053506\ttotal: 43.6s\tremaining: 53.5s\n",
      "449:\tlearn: 0.1052901\ttotal: 43.7s\tremaining: 53.4s\n",
      "450:\tlearn: 0.1052523\ttotal: 43.8s\tremaining: 53.3s\n",
      "451:\tlearn: 0.1051563\ttotal: 43.9s\tremaining: 53.2s\n",
      "452:\tlearn: 0.1050992\ttotal: 44s\tremaining: 53.1s\n",
      "453:\tlearn: 0.1050467\ttotal: 44.1s\tremaining: 53s\n",
      "454:\tlearn: 0.1049697\ttotal: 44.2s\tremaining: 52.9s\n",
      "455:\tlearn: 0.1049340\ttotal: 44.3s\tremaining: 52.8s\n",
      "456:\tlearn: 0.1048861\ttotal: 44.4s\tremaining: 52.7s\n",
      "457:\tlearn: 0.1048527\ttotal: 44.5s\tremaining: 52.6s\n",
      "458:\tlearn: 0.1047802\ttotal: 44.6s\tremaining: 52.5s\n",
      "459:\tlearn: 0.1047423\ttotal: 44.7s\tremaining: 52.4s\n",
      "460:\tlearn: 0.1046720\ttotal: 44.8s\tremaining: 52.3s\n",
      "461:\tlearn: 0.1046226\ttotal: 44.9s\tremaining: 52.2s\n",
      "462:\tlearn: 0.1045529\ttotal: 45s\tremaining: 52.1s\n",
      "463:\tlearn: 0.1044806\ttotal: 45.1s\tremaining: 52.1s\n",
      "464:\tlearn: 0.1043959\ttotal: 45.2s\tremaining: 52s\n",
      "465:\tlearn: 0.1042882\ttotal: 45.3s\tremaining: 51.9s\n",
      "466:\tlearn: 0.1042310\ttotal: 45.4s\tremaining: 51.8s\n",
      "467:\tlearn: 0.1041251\ttotal: 45.5s\tremaining: 51.8s\n",
      "468:\tlearn: 0.1040247\ttotal: 45.6s\tremaining: 51.7s\n",
      "469:\tlearn: 0.1039779\ttotal: 45.7s\tremaining: 51.6s\n",
      "470:\tlearn: 0.1039398\ttotal: 45.8s\tremaining: 51.5s\n",
      "471:\tlearn: 0.1038936\ttotal: 45.9s\tremaining: 51.4s\n",
      "472:\tlearn: 0.1038063\ttotal: 46s\tremaining: 51.3s\n",
      "473:\tlearn: 0.1037577\ttotal: 46.2s\tremaining: 51.2s\n",
      "474:\tlearn: 0.1037177\ttotal: 46.3s\tremaining: 51.1s\n",
      "475:\tlearn: 0.1036616\ttotal: 46.4s\tremaining: 51s\n",
      "476:\tlearn: 0.1036321\ttotal: 46.5s\tremaining: 50.9s\n",
      "477:\tlearn: 0.1035682\ttotal: 46.6s\tremaining: 50.9s\n",
      "478:\tlearn: 0.1035216\ttotal: 46.7s\tremaining: 50.8s\n",
      "479:\tlearn: 0.1034574\ttotal: 46.8s\tremaining: 50.7s\n",
      "480:\tlearn: 0.1034118\ttotal: 46.9s\tremaining: 50.6s\n",
      "481:\tlearn: 0.1033725\ttotal: 47s\tremaining: 50.5s\n",
      "482:\tlearn: 0.1033055\ttotal: 47.1s\tremaining: 50.4s\n",
      "483:\tlearn: 0.1032163\ttotal: 47.2s\tremaining: 50.3s\n",
      "484:\tlearn: 0.1031632\ttotal: 47.3s\tremaining: 50.3s\n",
      "485:\tlearn: 0.1031129\ttotal: 47.5s\tremaining: 50.2s\n",
      "486:\tlearn: 0.1030485\ttotal: 47.6s\tremaining: 50.1s\n",
      "487:\tlearn: 0.1029897\ttotal: 47.7s\tremaining: 50s\n",
      "488:\tlearn: 0.1029513\ttotal: 47.8s\tremaining: 49.9s\n",
      "489:\tlearn: 0.1028164\ttotal: 47.9s\tremaining: 49.8s\n",
      "490:\tlearn: 0.1027818\ttotal: 48s\tremaining: 49.7s\n",
      "491:\tlearn: 0.1027290\ttotal: 48.1s\tremaining: 49.7s\n",
      "492:\tlearn: 0.1026487\ttotal: 48.2s\tremaining: 49.6s\n",
      "493:\tlearn: 0.1026069\ttotal: 48.3s\tremaining: 49.5s\n",
      "494:\tlearn: 0.1025419\ttotal: 48.4s\tremaining: 49.4s\n",
      "495:\tlearn: 0.1024861\ttotal: 48.5s\tremaining: 49.3s\n",
      "496:\tlearn: 0.1024461\ttotal: 48.6s\tremaining: 49.2s\n",
      "497:\tlearn: 0.1023815\ttotal: 48.7s\tremaining: 49.1s\n",
      "498:\tlearn: 0.1023297\ttotal: 48.9s\tremaining: 49.1s\n",
      "499:\tlearn: 0.1022593\ttotal: 49s\tremaining: 49s\n",
      "500:\tlearn: 0.1022286\ttotal: 49.1s\tremaining: 48.9s\n",
      "501:\tlearn: 0.1021687\ttotal: 49.2s\tremaining: 48.8s\n",
      "502:\tlearn: 0.1021339\ttotal: 49.3s\tremaining: 48.7s\n",
      "503:\tlearn: 0.1020858\ttotal: 49.4s\tremaining: 48.6s\n",
      "504:\tlearn: 0.1020536\ttotal: 49.5s\tremaining: 48.5s\n",
      "505:\tlearn: 0.1020113\ttotal: 49.6s\tremaining: 48.4s\n",
      "506:\tlearn: 0.1019590\ttotal: 49.7s\tremaining: 48.3s\n",
      "507:\tlearn: 0.1018748\ttotal: 49.8s\tremaining: 48.2s\n",
      "508:\tlearn: 0.1018161\ttotal: 49.9s\tremaining: 48.1s\n",
      "509:\tlearn: 0.1017445\ttotal: 50s\tremaining: 48.1s\n",
      "510:\tlearn: 0.1016810\ttotal: 50.2s\tremaining: 48s\n",
      "511:\tlearn: 0.1016089\ttotal: 50.3s\tremaining: 47.9s\n",
      "512:\tlearn: 0.1015713\ttotal: 50.4s\tremaining: 47.8s\n",
      "513:\tlearn: 0.1015362\ttotal: 50.5s\tremaining: 47.7s\n",
      "514:\tlearn: 0.1014996\ttotal: 50.6s\tremaining: 47.7s\n",
      "515:\tlearn: 0.1014597\ttotal: 50.7s\tremaining: 47.6s\n",
      "516:\tlearn: 0.1014275\ttotal: 50.8s\tremaining: 47.5s\n",
      "517:\tlearn: 0.1014170\ttotal: 50.9s\tremaining: 47.3s\n",
      "518:\tlearn: 0.1013824\ttotal: 51s\tremaining: 47.3s\n",
      "519:\tlearn: 0.1013222\ttotal: 51.1s\tremaining: 47.2s\n",
      "520:\tlearn: 0.1012755\ttotal: 51.2s\tremaining: 47.1s\n",
      "521:\tlearn: 0.1011839\ttotal: 51.3s\tremaining: 47s\n",
      "522:\tlearn: 0.1011128\ttotal: 51.4s\tremaining: 46.9s\n",
      "523:\tlearn: 0.1010742\ttotal: 51.5s\tremaining: 46.8s\n",
      "524:\tlearn: 0.1010050\ttotal: 51.6s\tremaining: 46.7s\n",
      "525:\tlearn: 0.1009357\ttotal: 51.7s\tremaining: 46.6s\n",
      "526:\tlearn: 0.1008995\ttotal: 51.8s\tremaining: 46.5s\n",
      "527:\tlearn: 0.1008382\ttotal: 51.9s\tremaining: 46.4s\n",
      "528:\tlearn: 0.1007782\ttotal: 52s\tremaining: 46.3s\n",
      "529:\tlearn: 0.1007341\ttotal: 52.1s\tremaining: 46.2s\n",
      "530:\tlearn: 0.1006717\ttotal: 52.2s\tremaining: 46.1s\n",
      "531:\tlearn: 0.1006349\ttotal: 52.3s\tremaining: 46s\n",
      "532:\tlearn: 0.1005962\ttotal: 52.4s\tremaining: 45.9s\n",
      "533:\tlearn: 0.1005529\ttotal: 52.5s\tremaining: 45.8s\n",
      "534:\tlearn: 0.1005067\ttotal: 52.6s\tremaining: 45.7s\n",
      "535:\tlearn: 0.1004736\ttotal: 52.7s\tremaining: 45.6s\n",
      "536:\tlearn: 0.1004253\ttotal: 52.8s\tremaining: 45.5s\n",
      "537:\tlearn: 0.1003952\ttotal: 52.9s\tremaining: 45.4s\n",
      "538:\tlearn: 0.1003680\ttotal: 53s\tremaining: 45.3s\n",
      "539:\tlearn: 0.1003096\ttotal: 53.1s\tremaining: 45.2s\n",
      "540:\tlearn: 0.1002832\ttotal: 53.2s\tremaining: 45.1s\n",
      "541:\tlearn: 0.1002196\ttotal: 53.2s\tremaining: 45s\n",
      "542:\tlearn: 0.1001721\ttotal: 53.3s\tremaining: 44.9s\n",
      "543:\tlearn: 0.1001298\ttotal: 53.4s\tremaining: 44.8s\n",
      "544:\tlearn: 0.1001006\ttotal: 53.5s\tremaining: 44.7s\n",
      "545:\tlearn: 0.1000774\ttotal: 53.6s\tremaining: 44.6s\n",
      "546:\tlearn: 0.1000347\ttotal: 53.7s\tremaining: 44.5s\n",
      "547:\tlearn: 0.1000184\ttotal: 53.8s\tremaining: 44.4s\n",
      "548:\tlearn: 0.0999327\ttotal: 53.9s\tremaining: 44.3s\n",
      "549:\tlearn: 0.0998931\ttotal: 54s\tremaining: 44.2s\n",
      "550:\tlearn: 0.0998526\ttotal: 54.1s\tremaining: 44.1s\n",
      "551:\tlearn: 0.0997918\ttotal: 54.2s\tremaining: 44s\n",
      "552:\tlearn: 0.0997313\ttotal: 54.3s\tremaining: 43.9s\n",
      "553:\tlearn: 0.0996972\ttotal: 54.4s\tremaining: 43.8s\n",
      "554:\tlearn: 0.0996578\ttotal: 54.5s\tremaining: 43.7s\n",
      "555:\tlearn: 0.0995861\ttotal: 54.6s\tremaining: 43.6s\n",
      "556:\tlearn: 0.0995600\ttotal: 54.7s\tremaining: 43.5s\n",
      "557:\tlearn: 0.0995316\ttotal: 54.8s\tremaining: 43.4s\n",
      "558:\tlearn: 0.0994770\ttotal: 54.9s\tremaining: 43.3s\n",
      "559:\tlearn: 0.0994026\ttotal: 55s\tremaining: 43.2s\n",
      "560:\tlearn: 0.0993559\ttotal: 55.1s\tremaining: 43.1s\n",
      "561:\tlearn: 0.0993146\ttotal: 55.2s\tremaining: 43s\n",
      "562:\tlearn: 0.0992730\ttotal: 55.3s\tremaining: 42.9s\n",
      "563:\tlearn: 0.0992537\ttotal: 55.4s\tremaining: 42.8s\n",
      "564:\tlearn: 0.0992030\ttotal: 55.5s\tremaining: 42.7s\n",
      "565:\tlearn: 0.0991652\ttotal: 55.6s\tremaining: 42.6s\n",
      "566:\tlearn: 0.0991110\ttotal: 55.7s\tremaining: 42.5s\n",
      "567:\tlearn: 0.0990691\ttotal: 55.8s\tremaining: 42.4s\n",
      "568:\tlearn: 0.0990244\ttotal: 55.8s\tremaining: 42.3s\n",
      "569:\tlearn: 0.0989606\ttotal: 55.9s\tremaining: 42.2s\n",
      "570:\tlearn: 0.0989046\ttotal: 56s\tremaining: 42.1s\n",
      "571:\tlearn: 0.0988791\ttotal: 56.1s\tremaining: 42s\n",
      "572:\tlearn: 0.0987839\ttotal: 56.3s\tremaining: 42s\n",
      "573:\tlearn: 0.0987060\ttotal: 56.4s\tremaining: 41.9s\n",
      "574:\tlearn: 0.0987045\ttotal: 56.5s\tremaining: 41.8s\n",
      "575:\tlearn: 0.0986104\ttotal: 56.6s\tremaining: 41.7s\n",
      "576:\tlearn: 0.0985745\ttotal: 56.7s\tremaining: 41.6s\n",
      "577:\tlearn: 0.0985396\ttotal: 56.8s\tremaining: 41.5s\n",
      "578:\tlearn: 0.0984767\ttotal: 56.9s\tremaining: 41.4s\n",
      "579:\tlearn: 0.0984393\ttotal: 57s\tremaining: 41.3s\n",
      "580:\tlearn: 0.0984321\ttotal: 57.1s\tremaining: 41.2s\n",
      "581:\tlearn: 0.0983673\ttotal: 57.2s\tremaining: 41.1s\n",
      "582:\tlearn: 0.0982753\ttotal: 57.3s\tremaining: 41s\n",
      "583:\tlearn: 0.0982333\ttotal: 57.4s\tremaining: 40.9s\n",
      "584:\tlearn: 0.0982015\ttotal: 57.5s\tremaining: 40.8s\n",
      "585:\tlearn: 0.0981375\ttotal: 57.6s\tremaining: 40.7s\n",
      "586:\tlearn: 0.0980923\ttotal: 57.7s\tremaining: 40.6s\n",
      "587:\tlearn: 0.0979703\ttotal: 57.8s\tremaining: 40.5s\n",
      "588:\tlearn: 0.0979366\ttotal: 57.9s\tremaining: 40.4s\n",
      "589:\tlearn: 0.0979099\ttotal: 58s\tremaining: 40.3s\n",
      "590:\tlearn: 0.0978675\ttotal: 58.1s\tremaining: 40.2s\n",
      "591:\tlearn: 0.0978423\ttotal: 58.2s\tremaining: 40.1s\n",
      "592:\tlearn: 0.0977861\ttotal: 58.3s\tremaining: 40s\n",
      "593:\tlearn: 0.0977394\ttotal: 58.4s\tremaining: 39.9s\n",
      "594:\tlearn: 0.0976892\ttotal: 58.5s\tremaining: 39.8s\n",
      "595:\tlearn: 0.0976653\ttotal: 58.6s\tremaining: 39.7s\n",
      "596:\tlearn: 0.0976280\ttotal: 58.7s\tremaining: 39.6s\n",
      "597:\tlearn: 0.0975611\ttotal: 58.8s\tremaining: 39.5s\n",
      "598:\tlearn: 0.0974966\ttotal: 58.9s\tremaining: 39.4s\n",
      "599:\tlearn: 0.0974552\ttotal: 59s\tremaining: 39.3s\n",
      "600:\tlearn: 0.0973875\ttotal: 59.1s\tremaining: 39.2s\n",
      "601:\tlearn: 0.0973265\ttotal: 59.2s\tremaining: 39.2s\n",
      "602:\tlearn: 0.0972902\ttotal: 59.4s\tremaining: 39.1s\n",
      "603:\tlearn: 0.0972698\ttotal: 59.5s\tremaining: 39s\n",
      "604:\tlearn: 0.0972168\ttotal: 59.6s\tremaining: 38.9s\n",
      "605:\tlearn: 0.0971434\ttotal: 59.7s\tremaining: 38.8s\n",
      "606:\tlearn: 0.0971099\ttotal: 59.8s\tremaining: 38.7s\n",
      "607:\tlearn: 0.0970572\ttotal: 59.9s\tremaining: 38.6s\n",
      "608:\tlearn: 0.0970238\ttotal: 1m\tremaining: 38.5s\n",
      "609:\tlearn: 0.0970195\ttotal: 1m\tremaining: 38.4s\n",
      "610:\tlearn: 0.0969825\ttotal: 1m\tremaining: 38.3s\n",
      "611:\tlearn: 0.0969221\ttotal: 1m\tremaining: 38.2s\n",
      "612:\tlearn: 0.0968616\ttotal: 1m\tremaining: 38.1s\n",
      "613:\tlearn: 0.0968193\ttotal: 1m\tremaining: 38s\n",
      "614:\tlearn: 0.0967837\ttotal: 1m\tremaining: 37.9s\n",
      "615:\tlearn: 0.0967517\ttotal: 1m\tremaining: 37.8s\n",
      "616:\tlearn: 0.0967063\ttotal: 1m\tremaining: 37.7s\n",
      "617:\tlearn: 0.0966762\ttotal: 1m\tremaining: 37.6s\n",
      "618:\tlearn: 0.0966525\ttotal: 1m\tremaining: 37.5s\n",
      "619:\tlearn: 0.0965922\ttotal: 1m 1s\tremaining: 37.4s\n",
      "620:\tlearn: 0.0965531\ttotal: 1m 1s\tremaining: 37.3s\n",
      "621:\tlearn: 0.0965160\ttotal: 1m 1s\tremaining: 37.2s\n",
      "622:\tlearn: 0.0964863\ttotal: 1m 1s\tremaining: 37.1s\n",
      "623:\tlearn: 0.0964282\ttotal: 1m 1s\tremaining: 37s\n",
      "624:\tlearn: 0.0963706\ttotal: 1m 1s\tremaining: 36.9s\n",
      "625:\tlearn: 0.0962877\ttotal: 1m 1s\tremaining: 36.8s\n",
      "626:\tlearn: 0.0962467\ttotal: 1m 1s\tremaining: 36.7s\n",
      "627:\tlearn: 0.0961915\ttotal: 1m 1s\tremaining: 36.6s\n",
      "628:\tlearn: 0.0961580\ttotal: 1m 1s\tremaining: 36.5s\n",
      "629:\tlearn: 0.0961169\ttotal: 1m 2s\tremaining: 36.4s\n",
      "630:\tlearn: 0.0960518\ttotal: 1m 2s\tremaining: 36.3s\n",
      "631:\tlearn: 0.0960081\ttotal: 1m 2s\tremaining: 36.2s\n",
      "632:\tlearn: 0.0959750\ttotal: 1m 2s\tremaining: 36.1s\n",
      "633:\tlearn: 0.0959415\ttotal: 1m 2s\tremaining: 36s\n",
      "634:\tlearn: 0.0958993\ttotal: 1m 2s\tremaining: 35.9s\n",
      "635:\tlearn: 0.0958602\ttotal: 1m 2s\tremaining: 35.8s\n",
      "636:\tlearn: 0.0958526\ttotal: 1m 2s\tremaining: 35.7s\n",
      "637:\tlearn: 0.0958146\ttotal: 1m 2s\tremaining: 35.6s\n",
      "638:\tlearn: 0.0957812\ttotal: 1m 2s\tremaining: 35.5s\n",
      "639:\tlearn: 0.0957375\ttotal: 1m 2s\tremaining: 35.4s\n",
      "640:\tlearn: 0.0956979\ttotal: 1m 3s\tremaining: 35.3s\n",
      "641:\tlearn: 0.0956417\ttotal: 1m 3s\tremaining: 35.2s\n",
      "642:\tlearn: 0.0956193\ttotal: 1m 3s\tremaining: 35.1s\n",
      "643:\tlearn: 0.0954900\ttotal: 1m 3s\tremaining: 35s\n",
      "644:\tlearn: 0.0954614\ttotal: 1m 3s\tremaining: 34.9s\n",
      "645:\tlearn: 0.0954090\ttotal: 1m 3s\tremaining: 34.8s\n",
      "646:\tlearn: 0.0953562\ttotal: 1m 3s\tremaining: 34.7s\n",
      "647:\tlearn: 0.0952877\ttotal: 1m 3s\tremaining: 34.6s\n",
      "648:\tlearn: 0.0952360\ttotal: 1m 3s\tremaining: 34.5s\n",
      "649:\tlearn: 0.0951742\ttotal: 1m 3s\tremaining: 34.4s\n",
      "650:\tlearn: 0.0951245\ttotal: 1m 4s\tremaining: 34.3s\n",
      "651:\tlearn: 0.0950830\ttotal: 1m 4s\tremaining: 34.2s\n",
      "652:\tlearn: 0.0950629\ttotal: 1m 4s\tremaining: 34.1s\n",
      "653:\tlearn: 0.0950083\ttotal: 1m 4s\tremaining: 34s\n",
      "654:\tlearn: 0.0949639\ttotal: 1m 4s\tremaining: 34s\n",
      "655:\tlearn: 0.0949304\ttotal: 1m 4s\tremaining: 33.9s\n",
      "656:\tlearn: 0.0948993\ttotal: 1m 4s\tremaining: 33.8s\n",
      "657:\tlearn: 0.0948420\ttotal: 1m 4s\tremaining: 33.7s\n",
      "658:\tlearn: 0.0947979\ttotal: 1m 4s\tremaining: 33.6s\n",
      "659:\tlearn: 0.0947978\ttotal: 1m 4s\tremaining: 33.5s\n",
      "660:\tlearn: 0.0947904\ttotal: 1m 5s\tremaining: 33.4s\n",
      "661:\tlearn: 0.0947683\ttotal: 1m 5s\tremaining: 33.3s\n",
      "662:\tlearn: 0.0947166\ttotal: 1m 5s\tremaining: 33.2s\n",
      "663:\tlearn: 0.0946892\ttotal: 1m 5s\tremaining: 33.1s\n",
      "664:\tlearn: 0.0946406\ttotal: 1m 5s\tremaining: 33s\n",
      "665:\tlearn: 0.0945721\ttotal: 1m 5s\tremaining: 32.9s\n",
      "666:\tlearn: 0.0945175\ttotal: 1m 5s\tremaining: 32.8s\n",
      "667:\tlearn: 0.0944683\ttotal: 1m 5s\tremaining: 32.7s\n",
      "668:\tlearn: 0.0944467\ttotal: 1m 5s\tremaining: 32.6s\n",
      "669:\tlearn: 0.0943961\ttotal: 1m 5s\tremaining: 32.5s\n",
      "670:\tlearn: 0.0943504\ttotal: 1m 6s\tremaining: 32.4s\n",
      "671:\tlearn: 0.0942559\ttotal: 1m 6s\tremaining: 32.3s\n",
      "672:\tlearn: 0.0942184\ttotal: 1m 6s\tremaining: 32.2s\n",
      "673:\tlearn: 0.0941736\ttotal: 1m 6s\tremaining: 32.1s\n",
      "674:\tlearn: 0.0940708\ttotal: 1m 6s\tremaining: 32s\n",
      "675:\tlearn: 0.0940202\ttotal: 1m 6s\tremaining: 31.9s\n",
      "676:\tlearn: 0.0939885\ttotal: 1m 6s\tremaining: 31.8s\n",
      "677:\tlearn: 0.0939372\ttotal: 1m 6s\tremaining: 31.7s\n",
      "678:\tlearn: 0.0939008\ttotal: 1m 6s\tremaining: 31.6s\n",
      "679:\tlearn: 0.0938618\ttotal: 1m 6s\tremaining: 31.5s\n",
      "680:\tlearn: 0.0937987\ttotal: 1m 7s\tremaining: 31.4s\n",
      "681:\tlearn: 0.0937435\ttotal: 1m 7s\tremaining: 31.3s\n",
      "682:\tlearn: 0.0937242\ttotal: 1m 7s\tremaining: 31.2s\n",
      "683:\tlearn: 0.0936932\ttotal: 1m 7s\tremaining: 31.1s\n",
      "684:\tlearn: 0.0936387\ttotal: 1m 7s\tremaining: 31s\n",
      "685:\tlearn: 0.0935697\ttotal: 1m 7s\tremaining: 30.9s\n",
      "686:\tlearn: 0.0935264\ttotal: 1m 7s\tremaining: 30.8s\n",
      "687:\tlearn: 0.0934636\ttotal: 1m 7s\tremaining: 30.7s\n",
      "688:\tlearn: 0.0934377\ttotal: 1m 7s\tremaining: 30.6s\n",
      "689:\tlearn: 0.0933903\ttotal: 1m 7s\tremaining: 30.5s\n",
      "690:\tlearn: 0.0933404\ttotal: 1m 7s\tremaining: 30.4s\n",
      "691:\tlearn: 0.0933071\ttotal: 1m 8s\tremaining: 30.3s\n",
      "692:\tlearn: 0.0932768\ttotal: 1m 8s\tremaining: 30.2s\n",
      "693:\tlearn: 0.0932143\ttotal: 1m 8s\tremaining: 30.1s\n",
      "694:\tlearn: 0.0931650\ttotal: 1m 8s\tremaining: 30s\n",
      "695:\tlearn: 0.0931434\ttotal: 1m 8s\tremaining: 29.9s\n",
      "696:\tlearn: 0.0930959\ttotal: 1m 8s\tremaining: 29.9s\n",
      "697:\tlearn: 0.0929880\ttotal: 1m 8s\tremaining: 29.8s\n",
      "698:\tlearn: 0.0929511\ttotal: 1m 8s\tremaining: 29.7s\n",
      "699:\tlearn: 0.0929026\ttotal: 1m 8s\tremaining: 29.6s\n",
      "700:\tlearn: 0.0928536\ttotal: 1m 9s\tremaining: 29.5s\n",
      "701:\tlearn: 0.0928037\ttotal: 1m 9s\tremaining: 29.4s\n",
      "702:\tlearn: 0.0927661\ttotal: 1m 9s\tremaining: 29.3s\n",
      "703:\tlearn: 0.0927254\ttotal: 1m 9s\tremaining: 29.2s\n",
      "704:\tlearn: 0.0927027\ttotal: 1m 9s\tremaining: 29.1s\n",
      "705:\tlearn: 0.0926579\ttotal: 1m 9s\tremaining: 29s\n",
      "706:\tlearn: 0.0925911\ttotal: 1m 9s\tremaining: 28.9s\n",
      "707:\tlearn: 0.0925088\ttotal: 1m 9s\tremaining: 28.8s\n",
      "708:\tlearn: 0.0924422\ttotal: 1m 10s\tremaining: 28.7s\n",
      "709:\tlearn: 0.0923786\ttotal: 1m 10s\tremaining: 28.6s\n",
      "710:\tlearn: 0.0923524\ttotal: 1m 10s\tremaining: 28.5s\n",
      "711:\tlearn: 0.0923065\ttotal: 1m 10s\tremaining: 28.4s\n",
      "712:\tlearn: 0.0922176\ttotal: 1m 10s\tremaining: 28.3s\n",
      "713:\tlearn: 0.0921936\ttotal: 1m 10s\tremaining: 28.2s\n",
      "714:\tlearn: 0.0921441\ttotal: 1m 10s\tremaining: 28.1s\n",
      "715:\tlearn: 0.0920978\ttotal: 1m 10s\tremaining: 28s\n",
      "716:\tlearn: 0.0920327\ttotal: 1m 10s\tremaining: 27.9s\n",
      "717:\tlearn: 0.0919831\ttotal: 1m 10s\tremaining: 27.8s\n",
      "718:\tlearn: 0.0919415\ttotal: 1m 11s\tremaining: 27.8s\n",
      "719:\tlearn: 0.0918931\ttotal: 1m 11s\tremaining: 27.7s\n",
      "720:\tlearn: 0.0918540\ttotal: 1m 11s\tremaining: 27.6s\n",
      "721:\tlearn: 0.0918127\ttotal: 1m 11s\tremaining: 27.5s\n",
      "722:\tlearn: 0.0917840\ttotal: 1m 11s\tremaining: 27.4s\n",
      "723:\tlearn: 0.0917657\ttotal: 1m 11s\tremaining: 27.3s\n",
      "724:\tlearn: 0.0917396\ttotal: 1m 11s\tremaining: 27.2s\n",
      "725:\tlearn: 0.0917157\ttotal: 1m 11s\tremaining: 27.1s\n",
      "726:\tlearn: 0.0916484\ttotal: 1m 11s\tremaining: 27s\n",
      "727:\tlearn: 0.0916033\ttotal: 1m 11s\tremaining: 26.9s\n",
      "728:\tlearn: 0.0915751\ttotal: 1m 12s\tremaining: 26.8s\n",
      "729:\tlearn: 0.0915510\ttotal: 1m 12s\tremaining: 26.7s\n",
      "730:\tlearn: 0.0915024\ttotal: 1m 12s\tremaining: 26.6s\n",
      "731:\tlearn: 0.0914671\ttotal: 1m 12s\tremaining: 26.5s\n",
      "732:\tlearn: 0.0914259\ttotal: 1m 12s\tremaining: 26.4s\n",
      "733:\tlearn: 0.0914209\ttotal: 1m 12s\tremaining: 26.3s\n",
      "734:\tlearn: 0.0913872\ttotal: 1m 12s\tremaining: 26.2s\n",
      "735:\tlearn: 0.0913615\ttotal: 1m 12s\tremaining: 26.1s\n",
      "736:\tlearn: 0.0913355\ttotal: 1m 12s\tremaining: 26s\n",
      "737:\tlearn: 0.0913114\ttotal: 1m 12s\tremaining: 25.9s\n",
      "738:\tlearn: 0.0912754\ttotal: 1m 13s\tremaining: 25.8s\n",
      "739:\tlearn: 0.0912499\ttotal: 1m 13s\tremaining: 25.7s\n",
      "740:\tlearn: 0.0912071\ttotal: 1m 13s\tremaining: 25.6s\n",
      "741:\tlearn: 0.0911646\ttotal: 1m 13s\tremaining: 25.5s\n",
      "742:\tlearn: 0.0911263\ttotal: 1m 13s\tremaining: 25.4s\n",
      "743:\tlearn: 0.0910837\ttotal: 1m 13s\tremaining: 25.3s\n",
      "744:\tlearn: 0.0910316\ttotal: 1m 13s\tremaining: 25.2s\n",
      "745:\tlearn: 0.0909896\ttotal: 1m 13s\tremaining: 25.1s\n",
      "746:\tlearn: 0.0909725\ttotal: 1m 13s\tremaining: 25s\n",
      "747:\tlearn: 0.0909478\ttotal: 1m 14s\tremaining: 24.9s\n",
      "748:\tlearn: 0.0909221\ttotal: 1m 14s\tremaining: 24.8s\n",
      "749:\tlearn: 0.0908869\ttotal: 1m 14s\tremaining: 24.8s\n",
      "750:\tlearn: 0.0908449\ttotal: 1m 14s\tremaining: 24.7s\n",
      "751:\tlearn: 0.0908228\ttotal: 1m 14s\tremaining: 24.6s\n",
      "752:\tlearn: 0.0907795\ttotal: 1m 14s\tremaining: 24.5s\n",
      "753:\tlearn: 0.0907704\ttotal: 1m 14s\tremaining: 24.4s\n",
      "754:\tlearn: 0.0907184\ttotal: 1m 14s\tremaining: 24.3s\n",
      "755:\tlearn: 0.0906947\ttotal: 1m 14s\tremaining: 24.2s\n",
      "756:\tlearn: 0.0906486\ttotal: 1m 15s\tremaining: 24.1s\n",
      "757:\tlearn: 0.0906171\ttotal: 1m 15s\tremaining: 24s\n",
      "758:\tlearn: 0.0905605\ttotal: 1m 15s\tremaining: 23.9s\n",
      "759:\tlearn: 0.0904890\ttotal: 1m 15s\tremaining: 23.8s\n",
      "760:\tlearn: 0.0904203\ttotal: 1m 15s\tremaining: 23.7s\n",
      "761:\tlearn: 0.0903949\ttotal: 1m 15s\tremaining: 23.6s\n",
      "762:\tlearn: 0.0903148\ttotal: 1m 15s\tremaining: 23.5s\n",
      "763:\tlearn: 0.0902233\ttotal: 1m 15s\tremaining: 23.4s\n",
      "764:\tlearn: 0.0901904\ttotal: 1m 15s\tremaining: 23.3s\n",
      "765:\tlearn: 0.0901655\ttotal: 1m 16s\tremaining: 23.2s\n",
      "766:\tlearn: 0.0901331\ttotal: 1m 16s\tremaining: 23.1s\n",
      "767:\tlearn: 0.0900723\ttotal: 1m 16s\tremaining: 23s\n",
      "768:\tlearn: 0.0900385\ttotal: 1m 16s\tremaining: 22.9s\n",
      "769:\tlearn: 0.0899908\ttotal: 1m 16s\tremaining: 22.8s\n",
      "770:\tlearn: 0.0899223\ttotal: 1m 16s\tremaining: 22.7s\n",
      "771:\tlearn: 0.0898896\ttotal: 1m 16s\tremaining: 22.6s\n",
      "772:\tlearn: 0.0898337\ttotal: 1m 16s\tremaining: 22.6s\n",
      "773:\tlearn: 0.0898015\ttotal: 1m 16s\tremaining: 22.5s\n",
      "774:\tlearn: 0.0897681\ttotal: 1m 17s\tremaining: 22.4s\n",
      "775:\tlearn: 0.0897276\ttotal: 1m 17s\tremaining: 22.3s\n",
      "776:\tlearn: 0.0896881\ttotal: 1m 17s\tremaining: 22.2s\n",
      "777:\tlearn: 0.0896332\ttotal: 1m 17s\tremaining: 22.1s\n",
      "778:\tlearn: 0.0895695\ttotal: 1m 17s\tremaining: 22s\n",
      "779:\tlearn: 0.0895234\ttotal: 1m 17s\tremaining: 21.9s\n",
      "780:\tlearn: 0.0894866\ttotal: 1m 17s\tremaining: 21.8s\n",
      "781:\tlearn: 0.0894560\ttotal: 1m 17s\tremaining: 21.7s\n",
      "782:\tlearn: 0.0894273\ttotal: 1m 17s\tremaining: 21.6s\n",
      "783:\tlearn: 0.0893932\ttotal: 1m 18s\tremaining: 21.5s\n",
      "784:\tlearn: 0.0893508\ttotal: 1m 18s\tremaining: 21.4s\n",
      "785:\tlearn: 0.0893131\ttotal: 1m 18s\tremaining: 21.3s\n",
      "786:\tlearn: 0.0892929\ttotal: 1m 18s\tremaining: 21.2s\n",
      "787:\tlearn: 0.0892542\ttotal: 1m 18s\tremaining: 21.1s\n",
      "788:\tlearn: 0.0892388\ttotal: 1m 18s\tremaining: 21s\n",
      "789:\tlearn: 0.0892172\ttotal: 1m 18s\tremaining: 20.9s\n",
      "790:\tlearn: 0.0891676\ttotal: 1m 18s\tremaining: 20.8s\n",
      "791:\tlearn: 0.0891521\ttotal: 1m 18s\tremaining: 20.7s\n",
      "792:\tlearn: 0.0891108\ttotal: 1m 18s\tremaining: 20.6s\n",
      "793:\tlearn: 0.0890796\ttotal: 1m 19s\tremaining: 20.5s\n",
      "794:\tlearn: 0.0890341\ttotal: 1m 19s\tremaining: 20.4s\n",
      "795:\tlearn: 0.0889726\ttotal: 1m 19s\tremaining: 20.3s\n",
      "796:\tlearn: 0.0889252\ttotal: 1m 19s\tremaining: 20.2s\n",
      "797:\tlearn: 0.0888970\ttotal: 1m 19s\tremaining: 20.1s\n",
      "798:\tlearn: 0.0888452\ttotal: 1m 19s\tremaining: 20s\n",
      "799:\tlearn: 0.0887619\ttotal: 1m 19s\tremaining: 19.9s\n",
      "800:\tlearn: 0.0887527\ttotal: 1m 19s\tremaining: 19.8s\n",
      "801:\tlearn: 0.0886950\ttotal: 1m 19s\tremaining: 19.7s\n",
      "802:\tlearn: 0.0886582\ttotal: 1m 20s\tremaining: 19.6s\n",
      "803:\tlearn: 0.0886219\ttotal: 1m 20s\tremaining: 19.5s\n",
      "804:\tlearn: 0.0885604\ttotal: 1m 20s\tremaining: 19.4s\n",
      "805:\tlearn: 0.0885246\ttotal: 1m 20s\tremaining: 19.3s\n",
      "806:\tlearn: 0.0884702\ttotal: 1m 20s\tremaining: 19.2s\n",
      "807:\tlearn: 0.0884335\ttotal: 1m 20s\tremaining: 19.1s\n",
      "808:\tlearn: 0.0883859\ttotal: 1m 20s\tremaining: 19s\n",
      "809:\tlearn: 0.0883515\ttotal: 1m 20s\tremaining: 18.9s\n",
      "810:\tlearn: 0.0883279\ttotal: 1m 20s\tremaining: 18.8s\n",
      "811:\tlearn: 0.0883022\ttotal: 1m 20s\tremaining: 18.7s\n",
      "812:\tlearn: 0.0882558\ttotal: 1m 20s\tremaining: 18.6s\n",
      "813:\tlearn: 0.0882056\ttotal: 1m 21s\tremaining: 18.5s\n",
      "814:\tlearn: 0.0881872\ttotal: 1m 21s\tremaining: 18.4s\n",
      "815:\tlearn: 0.0881719\ttotal: 1m 21s\tremaining: 18.3s\n",
      "816:\tlearn: 0.0881365\ttotal: 1m 21s\tremaining: 18.2s\n",
      "817:\tlearn: 0.0880918\ttotal: 1m 21s\tremaining: 18.1s\n",
      "818:\tlearn: 0.0880654\ttotal: 1m 21s\tremaining: 18s\n",
      "819:\tlearn: 0.0880242\ttotal: 1m 21s\tremaining: 17.9s\n",
      "820:\tlearn: 0.0879757\ttotal: 1m 21s\tremaining: 17.8s\n",
      "821:\tlearn: 0.0879375\ttotal: 1m 21s\tremaining: 17.7s\n",
      "822:\tlearn: 0.0878922\ttotal: 1m 21s\tremaining: 17.6s\n",
      "823:\tlearn: 0.0878533\ttotal: 1m 22s\tremaining: 17.5s\n",
      "824:\tlearn: 0.0878129\ttotal: 1m 22s\tremaining: 17.4s\n",
      "825:\tlearn: 0.0877690\ttotal: 1m 22s\tremaining: 17.3s\n",
      "826:\tlearn: 0.0877247\ttotal: 1m 22s\tremaining: 17.2s\n",
      "827:\tlearn: 0.0876759\ttotal: 1m 22s\tremaining: 17.1s\n",
      "828:\tlearn: 0.0876048\ttotal: 1m 22s\tremaining: 17s\n",
      "829:\tlearn: 0.0875982\ttotal: 1m 22s\tremaining: 16.9s\n",
      "830:\tlearn: 0.0875385\ttotal: 1m 22s\tremaining: 16.8s\n",
      "831:\tlearn: 0.0875069\ttotal: 1m 22s\tremaining: 16.7s\n",
      "832:\tlearn: 0.0874996\ttotal: 1m 22s\tremaining: 16.6s\n",
      "833:\tlearn: 0.0874893\ttotal: 1m 22s\tremaining: 16.5s\n",
      "834:\tlearn: 0.0874466\ttotal: 1m 23s\tremaining: 16.4s\n",
      "835:\tlearn: 0.0874111\ttotal: 1m 23s\tremaining: 16.3s\n",
      "836:\tlearn: 0.0873855\ttotal: 1m 23s\tremaining: 16.2s\n",
      "837:\tlearn: 0.0873502\ttotal: 1m 23s\tremaining: 16.1s\n",
      "838:\tlearn: 0.0873031\ttotal: 1m 23s\tremaining: 16s\n",
      "839:\tlearn: 0.0872634\ttotal: 1m 23s\tremaining: 15.9s\n",
      "840:\tlearn: 0.0871989\ttotal: 1m 23s\tremaining: 15.8s\n",
      "841:\tlearn: 0.0871655\ttotal: 1m 23s\tremaining: 15.7s\n",
      "842:\tlearn: 0.0871489\ttotal: 1m 23s\tremaining: 15.6s\n",
      "843:\tlearn: 0.0870871\ttotal: 1m 23s\tremaining: 15.5s\n",
      "844:\tlearn: 0.0870428\ttotal: 1m 24s\tremaining: 15.4s\n",
      "845:\tlearn: 0.0870028\ttotal: 1m 24s\tremaining: 15.3s\n",
      "846:\tlearn: 0.0869636\ttotal: 1m 24s\tremaining: 15.2s\n",
      "847:\tlearn: 0.0869377\ttotal: 1m 24s\tremaining: 15.1s\n",
      "848:\tlearn: 0.0869139\ttotal: 1m 24s\tremaining: 15s\n",
      "849:\tlearn: 0.0869065\ttotal: 1m 24s\tremaining: 14.9s\n",
      "850:\tlearn: 0.0868707\ttotal: 1m 24s\tremaining: 14.8s\n",
      "851:\tlearn: 0.0868534\ttotal: 1m 24s\tremaining: 14.7s\n",
      "852:\tlearn: 0.0867982\ttotal: 1m 24s\tremaining: 14.6s\n",
      "853:\tlearn: 0.0867629\ttotal: 1m 24s\tremaining: 14.5s\n",
      "854:\tlearn: 0.0867342\ttotal: 1m 25s\tremaining: 14.4s\n",
      "855:\tlearn: 0.0866834\ttotal: 1m 25s\tremaining: 14.3s\n",
      "856:\tlearn: 0.0866574\ttotal: 1m 25s\tremaining: 14.2s\n",
      "857:\tlearn: 0.0866377\ttotal: 1m 25s\tremaining: 14.1s\n",
      "858:\tlearn: 0.0866003\ttotal: 1m 25s\tremaining: 14s\n",
      "859:\tlearn: 0.0865713\ttotal: 1m 25s\tremaining: 13.9s\n",
      "860:\tlearn: 0.0865280\ttotal: 1m 25s\tremaining: 13.8s\n",
      "861:\tlearn: 0.0864837\ttotal: 1m 25s\tremaining: 13.7s\n",
      "862:\tlearn: 0.0864522\ttotal: 1m 25s\tremaining: 13.6s\n",
      "863:\tlearn: 0.0864126\ttotal: 1m 25s\tremaining: 13.5s\n",
      "864:\tlearn: 0.0863769\ttotal: 1m 25s\tremaining: 13.4s\n",
      "865:\tlearn: 0.0863151\ttotal: 1m 26s\tremaining: 13.3s\n",
      "866:\tlearn: 0.0862606\ttotal: 1m 26s\tremaining: 13.2s\n",
      "867:\tlearn: 0.0862213\ttotal: 1m 26s\tremaining: 13.1s\n",
      "868:\tlearn: 0.0862024\ttotal: 1m 26s\tremaining: 13s\n",
      "869:\tlearn: 0.0861501\ttotal: 1m 26s\tremaining: 12.9s\n",
      "870:\tlearn: 0.0861123\ttotal: 1m 26s\tremaining: 12.8s\n",
      "871:\tlearn: 0.0860864\ttotal: 1m 26s\tremaining: 12.7s\n",
      "872:\tlearn: 0.0860367\ttotal: 1m 26s\tremaining: 12.6s\n",
      "873:\tlearn: 0.0859837\ttotal: 1m 26s\tremaining: 12.5s\n",
      "874:\tlearn: 0.0859636\ttotal: 1m 26s\tremaining: 12.4s\n",
      "875:\tlearn: 0.0859343\ttotal: 1m 27s\tremaining: 12.3s\n",
      "876:\tlearn: 0.0858571\ttotal: 1m 27s\tremaining: 12.2s\n",
      "877:\tlearn: 0.0858371\ttotal: 1m 27s\tremaining: 12.1s\n",
      "878:\tlearn: 0.0857947\ttotal: 1m 27s\tremaining: 12s\n",
      "879:\tlearn: 0.0857689\ttotal: 1m 27s\tremaining: 11.9s\n",
      "880:\tlearn: 0.0857270\ttotal: 1m 27s\tremaining: 11.8s\n",
      "881:\tlearn: 0.0857049\ttotal: 1m 27s\tremaining: 11.7s\n",
      "882:\tlearn: 0.0856735\ttotal: 1m 27s\tremaining: 11.6s\n",
      "883:\tlearn: 0.0856233\ttotal: 1m 27s\tremaining: 11.5s\n",
      "884:\tlearn: 0.0855643\ttotal: 1m 27s\tremaining: 11.4s\n",
      "885:\tlearn: 0.0855360\ttotal: 1m 27s\tremaining: 11.3s\n",
      "886:\tlearn: 0.0855098\ttotal: 1m 28s\tremaining: 11.2s\n",
      "887:\tlearn: 0.0854904\ttotal: 1m 28s\tremaining: 11.1s\n",
      "888:\tlearn: 0.0854593\ttotal: 1m 28s\tremaining: 11s\n",
      "889:\tlearn: 0.0854361\ttotal: 1m 28s\tremaining: 10.9s\n",
      "890:\tlearn: 0.0853891\ttotal: 1m 28s\tremaining: 10.8s\n",
      "891:\tlearn: 0.0853455\ttotal: 1m 28s\tremaining: 10.7s\n",
      "892:\tlearn: 0.0852821\ttotal: 1m 28s\tremaining: 10.6s\n",
      "893:\tlearn: 0.0852635\ttotal: 1m 28s\tremaining: 10.5s\n",
      "894:\tlearn: 0.0852087\ttotal: 1m 28s\tremaining: 10.4s\n",
      "895:\tlearn: 0.0851357\ttotal: 1m 28s\tremaining: 10.3s\n",
      "896:\tlearn: 0.0850698\ttotal: 1m 29s\tremaining: 10.2s\n",
      "897:\tlearn: 0.0850258\ttotal: 1m 29s\tremaining: 10.1s\n",
      "898:\tlearn: 0.0849860\ttotal: 1m 29s\tremaining: 10s\n",
      "899:\tlearn: 0.0849355\ttotal: 1m 29s\tremaining: 9.92s\n",
      "900:\tlearn: 0.0848915\ttotal: 1m 29s\tremaining: 9.82s\n",
      "901:\tlearn: 0.0848527\ttotal: 1m 29s\tremaining: 9.72s\n",
      "902:\tlearn: 0.0848169\ttotal: 1m 29s\tremaining: 9.63s\n",
      "903:\tlearn: 0.0847838\ttotal: 1m 29s\tremaining: 9.53s\n",
      "904:\tlearn: 0.0847368\ttotal: 1m 29s\tremaining: 9.43s\n",
      "905:\tlearn: 0.0846891\ttotal: 1m 29s\tremaining: 9.33s\n",
      "906:\tlearn: 0.0846436\ttotal: 1m 30s\tremaining: 9.23s\n",
      "907:\tlearn: 0.0846121\ttotal: 1m 30s\tremaining: 9.13s\n",
      "908:\tlearn: 0.0845931\ttotal: 1m 30s\tremaining: 9.03s\n",
      "909:\tlearn: 0.0845158\ttotal: 1m 30s\tremaining: 8.94s\n",
      "910:\tlearn: 0.0844811\ttotal: 1m 30s\tremaining: 8.84s\n",
      "911:\tlearn: 0.0844382\ttotal: 1m 30s\tremaining: 8.74s\n",
      "912:\tlearn: 0.0844285\ttotal: 1m 30s\tremaining: 8.64s\n",
      "913:\tlearn: 0.0843971\ttotal: 1m 30s\tremaining: 8.54s\n",
      "914:\tlearn: 0.0843494\ttotal: 1m 30s\tremaining: 8.44s\n",
      "915:\tlearn: 0.0843141\ttotal: 1m 30s\tremaining: 8.34s\n",
      "916:\tlearn: 0.0842547\ttotal: 1m 31s\tremaining: 8.24s\n",
      "917:\tlearn: 0.0842140\ttotal: 1m 31s\tremaining: 8.14s\n",
      "918:\tlearn: 0.0841420\ttotal: 1m 31s\tremaining: 8.04s\n",
      "919:\tlearn: 0.0841131\ttotal: 1m 31s\tremaining: 7.94s\n",
      "920:\tlearn: 0.0840436\ttotal: 1m 31s\tremaining: 7.84s\n",
      "921:\tlearn: 0.0840373\ttotal: 1m 31s\tremaining: 7.75s\n",
      "922:\tlearn: 0.0840040\ttotal: 1m 31s\tremaining: 7.65s\n",
      "923:\tlearn: 0.0839728\ttotal: 1m 31s\tremaining: 7.55s\n",
      "924:\tlearn: 0.0839494\ttotal: 1m 31s\tremaining: 7.45s\n",
      "925:\tlearn: 0.0839308\ttotal: 1m 31s\tremaining: 7.35s\n",
      "926:\tlearn: 0.0838985\ttotal: 1m 32s\tremaining: 7.25s\n",
      "927:\tlearn: 0.0838924\ttotal: 1m 32s\tremaining: 7.15s\n",
      "928:\tlearn: 0.0838326\ttotal: 1m 32s\tremaining: 7.05s\n",
      "929:\tlearn: 0.0837899\ttotal: 1m 32s\tremaining: 6.95s\n",
      "930:\tlearn: 0.0837534\ttotal: 1m 32s\tremaining: 6.85s\n",
      "931:\tlearn: 0.0837308\ttotal: 1m 32s\tremaining: 6.75s\n",
      "932:\tlearn: 0.0836582\ttotal: 1m 32s\tremaining: 6.65s\n",
      "933:\tlearn: 0.0836322\ttotal: 1m 32s\tremaining: 6.55s\n",
      "934:\tlearn: 0.0836091\ttotal: 1m 32s\tremaining: 6.45s\n",
      "935:\tlearn: 0.0835631\ttotal: 1m 32s\tremaining: 6.35s\n",
      "936:\tlearn: 0.0835241\ttotal: 1m 32s\tremaining: 6.25s\n",
      "937:\tlearn: 0.0835185\ttotal: 1m 33s\tremaining: 6.15s\n",
      "938:\tlearn: 0.0834941\ttotal: 1m 33s\tremaining: 6.05s\n",
      "939:\tlearn: 0.0834744\ttotal: 1m 33s\tremaining: 5.95s\n",
      "940:\tlearn: 0.0834348\ttotal: 1m 33s\tremaining: 5.85s\n",
      "941:\tlearn: 0.0834199\ttotal: 1m 33s\tremaining: 5.75s\n",
      "942:\tlearn: 0.0833884\ttotal: 1m 33s\tremaining: 5.66s\n",
      "943:\tlearn: 0.0833762\ttotal: 1m 33s\tremaining: 5.56s\n",
      "944:\tlearn: 0.0833449\ttotal: 1m 33s\tremaining: 5.46s\n",
      "945:\tlearn: 0.0832930\ttotal: 1m 33s\tremaining: 5.36s\n",
      "946:\tlearn: 0.0832396\ttotal: 1m 33s\tremaining: 5.26s\n",
      "947:\tlearn: 0.0832043\ttotal: 1m 34s\tremaining: 5.16s\n",
      "948:\tlearn: 0.0831565\ttotal: 1m 34s\tremaining: 5.06s\n",
      "949:\tlearn: 0.0831350\ttotal: 1m 34s\tremaining: 4.96s\n",
      "950:\tlearn: 0.0831013\ttotal: 1m 34s\tremaining: 4.86s\n",
      "951:\tlearn: 0.0830370\ttotal: 1m 34s\tremaining: 4.76s\n",
      "952:\tlearn: 0.0829900\ttotal: 1m 34s\tremaining: 4.66s\n",
      "953:\tlearn: 0.0829587\ttotal: 1m 34s\tremaining: 4.56s\n",
      "954:\tlearn: 0.0829541\ttotal: 1m 34s\tremaining: 4.46s\n",
      "955:\tlearn: 0.0829360\ttotal: 1m 34s\tremaining: 4.36s\n",
      "956:\tlearn: 0.0828946\ttotal: 1m 34s\tremaining: 4.26s\n",
      "957:\tlearn: 0.0828632\ttotal: 1m 35s\tremaining: 4.17s\n",
      "958:\tlearn: 0.0828385\ttotal: 1m 35s\tremaining: 4.07s\n",
      "959:\tlearn: 0.0828076\ttotal: 1m 35s\tremaining: 3.97s\n",
      "960:\tlearn: 0.0827560\ttotal: 1m 35s\tremaining: 3.87s\n",
      "961:\tlearn: 0.0827265\ttotal: 1m 35s\tremaining: 3.77s\n",
      "962:\tlearn: 0.0826841\ttotal: 1m 35s\tremaining: 3.67s\n",
      "963:\tlearn: 0.0826125\ttotal: 1m 35s\tremaining: 3.57s\n",
      "964:\tlearn: 0.0825614\ttotal: 1m 35s\tremaining: 3.47s\n",
      "965:\tlearn: 0.0825101\ttotal: 1m 35s\tremaining: 3.37s\n",
      "966:\tlearn: 0.0824345\ttotal: 1m 35s\tremaining: 3.27s\n",
      "967:\tlearn: 0.0824062\ttotal: 1m 36s\tremaining: 3.17s\n",
      "968:\tlearn: 0.0823855\ttotal: 1m 36s\tremaining: 3.07s\n",
      "969:\tlearn: 0.0823408\ttotal: 1m 36s\tremaining: 2.98s\n",
      "970:\tlearn: 0.0822999\ttotal: 1m 36s\tremaining: 2.88s\n",
      "971:\tlearn: 0.0822614\ttotal: 1m 36s\tremaining: 2.78s\n",
      "972:\tlearn: 0.0822224\ttotal: 1m 36s\tremaining: 2.68s\n",
      "973:\tlearn: 0.0821842\ttotal: 1m 36s\tremaining: 2.58s\n",
      "974:\tlearn: 0.0821246\ttotal: 1m 36s\tremaining: 2.48s\n",
      "975:\tlearn: 0.0821012\ttotal: 1m 36s\tremaining: 2.38s\n",
      "976:\tlearn: 0.0820600\ttotal: 1m 36s\tremaining: 2.28s\n",
      "977:\tlearn: 0.0820165\ttotal: 1m 36s\tremaining: 2.18s\n",
      "978:\tlearn: 0.0819582\ttotal: 1m 37s\tremaining: 2.08s\n",
      "979:\tlearn: 0.0819294\ttotal: 1m 37s\tremaining: 1.98s\n",
      "980:\tlearn: 0.0819049\ttotal: 1m 37s\tremaining: 1.88s\n",
      "981:\tlearn: 0.0818808\ttotal: 1m 37s\tremaining: 1.78s\n",
      "982:\tlearn: 0.0818464\ttotal: 1m 37s\tremaining: 1.69s\n",
      "983:\tlearn: 0.0818416\ttotal: 1m 37s\tremaining: 1.59s\n",
      "984:\tlearn: 0.0818161\ttotal: 1m 37s\tremaining: 1.49s\n",
      "985:\tlearn: 0.0817881\ttotal: 1m 37s\tremaining: 1.39s\n",
      "986:\tlearn: 0.0817652\ttotal: 1m 37s\tremaining: 1.29s\n",
      "987:\tlearn: 0.0817244\ttotal: 1m 37s\tremaining: 1.19s\n",
      "988:\tlearn: 0.0817086\ttotal: 1m 38s\tremaining: 1.09s\n",
      "989:\tlearn: 0.0816760\ttotal: 1m 38s\tremaining: 992ms\n",
      "990:\tlearn: 0.0816094\ttotal: 1m 38s\tremaining: 892ms\n",
      "991:\tlearn: 0.0815574\ttotal: 1m 38s\tremaining: 793ms\n",
      "992:\tlearn: 0.0815480\ttotal: 1m 38s\tremaining: 694ms\n",
      "993:\tlearn: 0.0814742\ttotal: 1m 38s\tremaining: 595ms\n",
      "994:\tlearn: 0.0814373\ttotal: 1m 38s\tremaining: 496ms\n",
      "995:\tlearn: 0.0814186\ttotal: 1m 38s\tremaining: 397ms\n",
      "996:\tlearn: 0.0813767\ttotal: 1m 38s\tremaining: 297ms\n",
      "997:\tlearn: 0.0813466\ttotal: 1m 38s\tremaining: 198ms\n",
      "998:\tlearn: 0.0813239\ttotal: 1m 39s\tremaining: 99.1ms\n",
      "999:\tlearn: 0.0812905\ttotal: 1m 39s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5247142\ttotal: 96.5ms\tremaining: 1m 36s\n",
      "1:\tlearn: 0.4290319\ttotal: 194ms\tremaining: 1m 36s\n",
      "2:\tlearn: 0.3662552\ttotal: 287ms\tremaining: 1m 35s\n",
      "3:\tlearn: 0.3285835\ttotal: 380ms\tremaining: 1m 34s\n",
      "4:\tlearn: 0.2969836\ttotal: 491ms\tremaining: 1m 37s\n",
      "5:\tlearn: 0.2747354\ttotal: 583ms\tremaining: 1m 36s\n",
      "6:\tlearn: 0.2611399\ttotal: 676ms\tremaining: 1m 35s\n",
      "7:\tlearn: 0.2520892\ttotal: 765ms\tremaining: 1m 34s\n",
      "8:\tlearn: 0.2410174\ttotal: 866ms\tremaining: 1m 35s\n",
      "9:\tlearn: 0.2324716\ttotal: 960ms\tremaining: 1m 35s\n",
      "10:\tlearn: 0.2217792\ttotal: 1.05s\tremaining: 1m 34s\n",
      "11:\tlearn: 0.2160220\ttotal: 1.16s\tremaining: 1m 35s\n",
      "12:\tlearn: 0.2115084\ttotal: 1.24s\tremaining: 1m 34s\n",
      "13:\tlearn: 0.2060661\ttotal: 1.34s\tremaining: 1m 34s\n",
      "14:\tlearn: 0.2027406\ttotal: 1.43s\tremaining: 1m 33s\n",
      "15:\tlearn: 0.2006642\ttotal: 1.52s\tremaining: 1m 33s\n",
      "16:\tlearn: 0.1979357\ttotal: 1.61s\tremaining: 1m 33s\n",
      "17:\tlearn: 0.1952674\ttotal: 1.71s\tremaining: 1m 33s\n",
      "18:\tlearn: 0.1926978\ttotal: 1.8s\tremaining: 1m 33s\n",
      "19:\tlearn: 0.1907816\ttotal: 1.9s\tremaining: 1m 33s\n",
      "20:\tlearn: 0.1880369\ttotal: 1.99s\tremaining: 1m 32s\n",
      "21:\tlearn: 0.1866016\ttotal: 2.08s\tremaining: 1m 32s\n",
      "22:\tlearn: 0.1846220\ttotal: 2.18s\tremaining: 1m 32s\n",
      "23:\tlearn: 0.1823096\ttotal: 2.26s\tremaining: 1m 32s\n",
      "24:\tlearn: 0.1804778\ttotal: 2.35s\tremaining: 1m 31s\n",
      "25:\tlearn: 0.1793183\ttotal: 2.45s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1777770\ttotal: 2.55s\tremaining: 1m 31s\n",
      "27:\tlearn: 0.1764437\ttotal: 2.66s\tremaining: 1m 32s\n",
      "28:\tlearn: 0.1752544\ttotal: 2.76s\tremaining: 1m 32s\n",
      "29:\tlearn: 0.1742391\ttotal: 2.85s\tremaining: 1m 32s\n",
      "30:\tlearn: 0.1729539\ttotal: 2.96s\tremaining: 1m 32s\n",
      "31:\tlearn: 0.1712130\ttotal: 3.05s\tremaining: 1m 32s\n",
      "32:\tlearn: 0.1700782\ttotal: 3.14s\tremaining: 1m 31s\n",
      "33:\tlearn: 0.1691012\ttotal: 3.23s\tremaining: 1m 31s\n",
      "34:\tlearn: 0.1682603\ttotal: 3.33s\tremaining: 1m 31s\n",
      "35:\tlearn: 0.1673643\ttotal: 3.42s\tremaining: 1m 31s\n",
      "36:\tlearn: 0.1665535\ttotal: 3.52s\tremaining: 1m 31s\n",
      "37:\tlearn: 0.1656842\ttotal: 3.62s\tremaining: 1m 31s\n",
      "38:\tlearn: 0.1648665\ttotal: 3.71s\tremaining: 1m 31s\n",
      "39:\tlearn: 0.1633246\ttotal: 3.81s\tremaining: 1m 31s\n",
      "40:\tlearn: 0.1625337\ttotal: 3.9s\tremaining: 1m 31s\n",
      "41:\tlearn: 0.1618969\ttotal: 3.99s\tremaining: 1m 30s\n",
      "42:\tlearn: 0.1614084\ttotal: 4.08s\tremaining: 1m 30s\n",
      "43:\tlearn: 0.1609816\ttotal: 4.18s\tremaining: 1m 30s\n",
      "44:\tlearn: 0.1605900\ttotal: 4.27s\tremaining: 1m 30s\n",
      "45:\tlearn: 0.1600429\ttotal: 4.36s\tremaining: 1m 30s\n",
      "46:\tlearn: 0.1593968\ttotal: 4.46s\tremaining: 1m 30s\n",
      "47:\tlearn: 0.1589328\ttotal: 4.55s\tremaining: 1m 30s\n",
      "48:\tlearn: 0.1580936\ttotal: 4.68s\tremaining: 1m 30s\n",
      "49:\tlearn: 0.1572806\ttotal: 4.78s\tremaining: 1m 30s\n",
      "50:\tlearn: 0.1566601\ttotal: 4.88s\tremaining: 1m 30s\n",
      "51:\tlearn: 0.1559921\ttotal: 4.97s\tremaining: 1m 30s\n",
      "52:\tlearn: 0.1554978\ttotal: 5.07s\tremaining: 1m 30s\n",
      "53:\tlearn: 0.1549274\ttotal: 5.16s\tremaining: 1m 30s\n",
      "54:\tlearn: 0.1544377\ttotal: 5.26s\tremaining: 1m 30s\n",
      "55:\tlearn: 0.1537820\ttotal: 5.36s\tremaining: 1m 30s\n",
      "56:\tlearn: 0.1532621\ttotal: 5.45s\tremaining: 1m 30s\n",
      "57:\tlearn: 0.1529369\ttotal: 5.54s\tremaining: 1m 30s\n",
      "58:\tlearn: 0.1524606\ttotal: 5.64s\tremaining: 1m 29s\n",
      "59:\tlearn: 0.1521997\ttotal: 5.73s\tremaining: 1m 29s\n",
      "60:\tlearn: 0.1517807\ttotal: 5.82s\tremaining: 1m 29s\n",
      "61:\tlearn: 0.1512709\ttotal: 5.92s\tremaining: 1m 29s\n",
      "62:\tlearn: 0.1507872\ttotal: 6.01s\tremaining: 1m 29s\n",
      "63:\tlearn: 0.1502769\ttotal: 6.11s\tremaining: 1m 29s\n",
      "64:\tlearn: 0.1497003\ttotal: 6.21s\tremaining: 1m 29s\n",
      "65:\tlearn: 0.1493667\ttotal: 6.3s\tremaining: 1m 29s\n",
      "66:\tlearn: 0.1490641\ttotal: 6.39s\tremaining: 1m 29s\n",
      "67:\tlearn: 0.1487439\ttotal: 6.49s\tremaining: 1m 28s\n",
      "68:\tlearn: 0.1482392\ttotal: 6.58s\tremaining: 1m 28s\n",
      "69:\tlearn: 0.1478802\ttotal: 6.68s\tremaining: 1m 28s\n",
      "70:\tlearn: 0.1474656\ttotal: 6.78s\tremaining: 1m 28s\n",
      "71:\tlearn: 0.1472309\ttotal: 6.87s\tremaining: 1m 28s\n",
      "72:\tlearn: 0.1469294\ttotal: 6.97s\tremaining: 1m 28s\n",
      "73:\tlearn: 0.1466011\ttotal: 7.06s\tremaining: 1m 28s\n",
      "74:\tlearn: 0.1462527\ttotal: 7.16s\tremaining: 1m 28s\n",
      "75:\tlearn: 0.1459718\ttotal: 7.26s\tremaining: 1m 28s\n",
      "76:\tlearn: 0.1456238\ttotal: 7.35s\tremaining: 1m 28s\n",
      "77:\tlearn: 0.1453612\ttotal: 7.45s\tremaining: 1m 28s\n",
      "78:\tlearn: 0.1450347\ttotal: 7.55s\tremaining: 1m 27s\n",
      "79:\tlearn: 0.1447586\ttotal: 7.65s\tremaining: 1m 27s\n",
      "80:\tlearn: 0.1445564\ttotal: 7.74s\tremaining: 1m 27s\n",
      "81:\tlearn: 0.1442456\ttotal: 7.84s\tremaining: 1m 27s\n",
      "82:\tlearn: 0.1440459\ttotal: 7.93s\tremaining: 1m 27s\n",
      "83:\tlearn: 0.1436258\ttotal: 8.02s\tremaining: 1m 27s\n",
      "84:\tlearn: 0.1433357\ttotal: 8.12s\tremaining: 1m 27s\n",
      "85:\tlearn: 0.1430034\ttotal: 8.21s\tremaining: 1m 27s\n",
      "86:\tlearn: 0.1428288\ttotal: 8.3s\tremaining: 1m 27s\n",
      "87:\tlearn: 0.1425692\ttotal: 8.4s\tremaining: 1m 27s\n",
      "88:\tlearn: 0.1423890\ttotal: 8.49s\tremaining: 1m 26s\n",
      "89:\tlearn: 0.1421052\ttotal: 8.58s\tremaining: 1m 26s\n",
      "90:\tlearn: 0.1418699\ttotal: 8.67s\tremaining: 1m 26s\n",
      "91:\tlearn: 0.1415642\ttotal: 8.77s\tremaining: 1m 26s\n",
      "92:\tlearn: 0.1413346\ttotal: 8.86s\tremaining: 1m 26s\n",
      "93:\tlearn: 0.1411255\ttotal: 8.96s\tremaining: 1m 26s\n",
      "94:\tlearn: 0.1408647\ttotal: 9.09s\tremaining: 1m 26s\n",
      "95:\tlearn: 0.1406386\ttotal: 9.26s\tremaining: 1m 27s\n",
      "96:\tlearn: 0.1404526\ttotal: 9.36s\tremaining: 1m 27s\n",
      "97:\tlearn: 0.1403146\ttotal: 9.45s\tremaining: 1m 26s\n",
      "98:\tlearn: 0.1399154\ttotal: 9.54s\tremaining: 1m 26s\n",
      "99:\tlearn: 0.1396737\ttotal: 9.64s\tremaining: 1m 26s\n",
      "100:\tlearn: 0.1393900\ttotal: 9.73s\tremaining: 1m 26s\n",
      "101:\tlearn: 0.1390799\ttotal: 9.83s\tremaining: 1m 26s\n",
      "102:\tlearn: 0.1388730\ttotal: 9.92s\tremaining: 1m 26s\n",
      "103:\tlearn: 0.1386459\ttotal: 10s\tremaining: 1m 26s\n",
      "104:\tlearn: 0.1382075\ttotal: 10.1s\tremaining: 1m 26s\n",
      "105:\tlearn: 0.1380331\ttotal: 10.2s\tremaining: 1m 25s\n",
      "106:\tlearn: 0.1378826\ttotal: 10.3s\tremaining: 1m 25s\n",
      "107:\tlearn: 0.1376540\ttotal: 10.4s\tremaining: 1m 25s\n",
      "108:\tlearn: 0.1374066\ttotal: 10.5s\tremaining: 1m 25s\n",
      "109:\tlearn: 0.1371449\ttotal: 10.6s\tremaining: 1m 25s\n",
      "110:\tlearn: 0.1368167\ttotal: 10.7s\tremaining: 1m 25s\n",
      "111:\tlearn: 0.1366158\ttotal: 10.8s\tremaining: 1m 25s\n",
      "112:\tlearn: 0.1363906\ttotal: 10.9s\tremaining: 1m 25s\n",
      "113:\tlearn: 0.1362661\ttotal: 11s\tremaining: 1m 25s\n",
      "114:\tlearn: 0.1359261\ttotal: 11s\tremaining: 1m 24s\n",
      "115:\tlearn: 0.1356938\ttotal: 11.1s\tremaining: 1m 24s\n",
      "116:\tlearn: 0.1355054\ttotal: 11.2s\tremaining: 1m 24s\n",
      "117:\tlearn: 0.1352392\ttotal: 11.3s\tremaining: 1m 24s\n",
      "118:\tlearn: 0.1350467\ttotal: 11.4s\tremaining: 1m 24s\n",
      "119:\tlearn: 0.1348777\ttotal: 11.5s\tremaining: 1m 24s\n",
      "120:\tlearn: 0.1347154\ttotal: 11.6s\tremaining: 1m 24s\n",
      "121:\tlearn: 0.1345329\ttotal: 11.7s\tremaining: 1m 24s\n",
      "122:\tlearn: 0.1342786\ttotal: 11.8s\tremaining: 1m 24s\n",
      "123:\tlearn: 0.1341378\ttotal: 11.9s\tremaining: 1m 23s\n",
      "124:\tlearn: 0.1339675\ttotal: 12s\tremaining: 1m 23s\n",
      "125:\tlearn: 0.1337047\ttotal: 12.1s\tremaining: 1m 23s\n",
      "126:\tlearn: 0.1334960\ttotal: 12.2s\tremaining: 1m 23s\n",
      "127:\tlearn: 0.1332800\ttotal: 12.3s\tremaining: 1m 23s\n",
      "128:\tlearn: 0.1330483\ttotal: 12.4s\tremaining: 1m 23s\n",
      "129:\tlearn: 0.1327978\ttotal: 12.5s\tremaining: 1m 23s\n",
      "130:\tlearn: 0.1326519\ttotal: 12.6s\tremaining: 1m 23s\n",
      "131:\tlearn: 0.1324372\ttotal: 12.7s\tremaining: 1m 23s\n",
      "132:\tlearn: 0.1322280\ttotal: 12.8s\tremaining: 1m 23s\n",
      "133:\tlearn: 0.1320908\ttotal: 12.8s\tremaining: 1m 22s\n",
      "134:\tlearn: 0.1319449\ttotal: 13s\tremaining: 1m 23s\n",
      "135:\tlearn: 0.1317987\ttotal: 13.1s\tremaining: 1m 22s\n",
      "136:\tlearn: 0.1315724\ttotal: 13.2s\tremaining: 1m 22s\n",
      "137:\tlearn: 0.1313807\ttotal: 13.3s\tremaining: 1m 22s\n",
      "138:\tlearn: 0.1312960\ttotal: 13.4s\tremaining: 1m 22s\n",
      "139:\tlearn: 0.1310957\ttotal: 13.5s\tremaining: 1m 22s\n",
      "140:\tlearn: 0.1309753\ttotal: 13.6s\tremaining: 1m 22s\n",
      "141:\tlearn: 0.1307997\ttotal: 13.6s\tremaining: 1m 22s\n",
      "142:\tlearn: 0.1306800\ttotal: 13.7s\tremaining: 1m 22s\n",
      "143:\tlearn: 0.1305669\ttotal: 13.8s\tremaining: 1m 22s\n",
      "144:\tlearn: 0.1304110\ttotal: 13.9s\tremaining: 1m 22s\n",
      "145:\tlearn: 0.1302813\ttotal: 14s\tremaining: 1m 22s\n",
      "146:\tlearn: 0.1300555\ttotal: 14.1s\tremaining: 1m 21s\n",
      "147:\tlearn: 0.1299303\ttotal: 14.2s\tremaining: 1m 21s\n",
      "148:\tlearn: 0.1297095\ttotal: 14.3s\tremaining: 1m 21s\n",
      "149:\tlearn: 0.1295947\ttotal: 14.4s\tremaining: 1m 21s\n",
      "150:\tlearn: 0.1294462\ttotal: 14.5s\tremaining: 1m 21s\n",
      "151:\tlearn: 0.1292897\ttotal: 14.6s\tremaining: 1m 21s\n",
      "152:\tlearn: 0.1291531\ttotal: 14.7s\tremaining: 1m 21s\n",
      "153:\tlearn: 0.1289635\ttotal: 14.8s\tremaining: 1m 21s\n",
      "154:\tlearn: 0.1287964\ttotal: 14.9s\tremaining: 1m 21s\n",
      "155:\tlearn: 0.1286321\ttotal: 15s\tremaining: 1m 21s\n",
      "156:\tlearn: 0.1285308\ttotal: 15.1s\tremaining: 1m 21s\n",
      "157:\tlearn: 0.1284044\ttotal: 15.2s\tremaining: 1m 20s\n",
      "158:\tlearn: 0.1282614\ttotal: 15.3s\tremaining: 1m 20s\n",
      "159:\tlearn: 0.1281640\ttotal: 15.4s\tremaining: 1m 20s\n",
      "160:\tlearn: 0.1278987\ttotal: 15.5s\tremaining: 1m 20s\n",
      "161:\tlearn: 0.1277917\ttotal: 15.6s\tremaining: 1m 20s\n",
      "162:\tlearn: 0.1276562\ttotal: 15.7s\tremaining: 1m 20s\n",
      "163:\tlearn: 0.1275424\ttotal: 15.8s\tremaining: 1m 20s\n",
      "164:\tlearn: 0.1273804\ttotal: 15.9s\tremaining: 1m 20s\n",
      "165:\tlearn: 0.1272900\ttotal: 16s\tremaining: 1m 20s\n",
      "166:\tlearn: 0.1271990\ttotal: 16s\tremaining: 1m 20s\n",
      "167:\tlearn: 0.1270662\ttotal: 16.1s\tremaining: 1m 19s\n",
      "168:\tlearn: 0.1269807\ttotal: 16.2s\tremaining: 1m 19s\n",
      "169:\tlearn: 0.1268347\ttotal: 16.3s\tremaining: 1m 19s\n",
      "170:\tlearn: 0.1267478\ttotal: 16.4s\tremaining: 1m 19s\n",
      "171:\tlearn: 0.1265774\ttotal: 16.5s\tremaining: 1m 19s\n",
      "172:\tlearn: 0.1264081\ttotal: 16.6s\tremaining: 1m 19s\n",
      "173:\tlearn: 0.1262483\ttotal: 16.7s\tremaining: 1m 19s\n",
      "174:\tlearn: 0.1261476\ttotal: 16.8s\tremaining: 1m 19s\n",
      "175:\tlearn: 0.1259788\ttotal: 16.9s\tremaining: 1m 19s\n",
      "176:\tlearn: 0.1257798\ttotal: 17s\tremaining: 1m 19s\n",
      "177:\tlearn: 0.1256822\ttotal: 17.1s\tremaining: 1m 18s\n",
      "178:\tlearn: 0.1255589\ttotal: 17.2s\tremaining: 1m 18s\n",
      "179:\tlearn: 0.1254087\ttotal: 17.3s\tremaining: 1m 18s\n",
      "180:\tlearn: 0.1253234\ttotal: 17.4s\tremaining: 1m 18s\n",
      "181:\tlearn: 0.1251857\ttotal: 17.5s\tremaining: 1m 18s\n",
      "182:\tlearn: 0.1249929\ttotal: 17.6s\tremaining: 1m 18s\n",
      "183:\tlearn: 0.1248791\ttotal: 17.7s\tremaining: 1m 18s\n",
      "184:\tlearn: 0.1247574\ttotal: 17.8s\tremaining: 1m 18s\n",
      "185:\tlearn: 0.1246110\ttotal: 17.9s\tremaining: 1m 18s\n",
      "186:\tlearn: 0.1244564\ttotal: 18s\tremaining: 1m 18s\n",
      "187:\tlearn: 0.1243859\ttotal: 18.1s\tremaining: 1m 18s\n",
      "188:\tlearn: 0.1242768\ttotal: 18.2s\tremaining: 1m 18s\n",
      "189:\tlearn: 0.1241866\ttotal: 18.3s\tremaining: 1m 17s\n",
      "190:\tlearn: 0.1240816\ttotal: 18.4s\tremaining: 1m 17s\n",
      "191:\tlearn: 0.1239623\ttotal: 18.5s\tremaining: 1m 17s\n",
      "192:\tlearn: 0.1238748\ttotal: 18.6s\tremaining: 1m 17s\n",
      "193:\tlearn: 0.1237862\ttotal: 18.7s\tremaining: 1m 17s\n",
      "194:\tlearn: 0.1236979\ttotal: 18.7s\tremaining: 1m 17s\n",
      "195:\tlearn: 0.1235935\ttotal: 18.8s\tremaining: 1m 17s\n",
      "196:\tlearn: 0.1233832\ttotal: 18.9s\tremaining: 1m 17s\n",
      "197:\tlearn: 0.1232541\ttotal: 19s\tremaining: 1m 17s\n",
      "198:\tlearn: 0.1230974\ttotal: 19.1s\tremaining: 1m 17s\n",
      "199:\tlearn: 0.1230000\ttotal: 19.2s\tremaining: 1m 16s\n",
      "200:\tlearn: 0.1228879\ttotal: 19.3s\tremaining: 1m 16s\n",
      "201:\tlearn: 0.1227618\ttotal: 19.4s\tremaining: 1m 16s\n",
      "202:\tlearn: 0.1226634\ttotal: 19.5s\tremaining: 1m 16s\n",
      "203:\tlearn: 0.1225659\ttotal: 19.6s\tremaining: 1m 16s\n",
      "204:\tlearn: 0.1224876\ttotal: 19.7s\tremaining: 1m 16s\n",
      "205:\tlearn: 0.1224038\ttotal: 19.8s\tremaining: 1m 16s\n",
      "206:\tlearn: 0.1222986\ttotal: 19.9s\tremaining: 1m 16s\n",
      "207:\tlearn: 0.1221665\ttotal: 20.1s\tremaining: 1m 16s\n",
      "208:\tlearn: 0.1220021\ttotal: 20.2s\tremaining: 1m 16s\n",
      "209:\tlearn: 0.1218890\ttotal: 20.3s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1218059\ttotal: 20.4s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1217227\ttotal: 20.5s\tremaining: 1m 16s\n",
      "212:\tlearn: 0.1216273\ttotal: 20.6s\tremaining: 1m 16s\n",
      "213:\tlearn: 0.1214810\ttotal: 20.7s\tremaining: 1m 16s\n",
      "214:\tlearn: 0.1213770\ttotal: 20.8s\tremaining: 1m 15s\n",
      "215:\tlearn: 0.1212872\ttotal: 20.9s\tremaining: 1m 15s\n",
      "216:\tlearn: 0.1212118\ttotal: 21s\tremaining: 1m 15s\n",
      "217:\tlearn: 0.1211335\ttotal: 21.1s\tremaining: 1m 15s\n",
      "218:\tlearn: 0.1210574\ttotal: 21.2s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1208791\ttotal: 21.3s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1207165\ttotal: 21.4s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.1206384\ttotal: 21.5s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.1205391\ttotal: 21.6s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.1204514\ttotal: 21.7s\tremaining: 1m 15s\n",
      "224:\tlearn: 0.1203252\ttotal: 21.8s\tremaining: 1m 14s\n",
      "225:\tlearn: 0.1202140\ttotal: 21.9s\tremaining: 1m 14s\n",
      "226:\tlearn: 0.1201345\ttotal: 22s\tremaining: 1m 14s\n",
      "227:\tlearn: 0.1200165\ttotal: 22.1s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1199505\ttotal: 22.1s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1198620\ttotal: 22.2s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1197694\ttotal: 22.3s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1196419\ttotal: 22.4s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1195453\ttotal: 22.5s\tremaining: 1m 14s\n",
      "233:\tlearn: 0.1194415\ttotal: 22.6s\tremaining: 1m 14s\n",
      "234:\tlearn: 0.1193641\ttotal: 22.7s\tremaining: 1m 13s\n",
      "235:\tlearn: 0.1192952\ttotal: 22.8s\tremaining: 1m 13s\n",
      "236:\tlearn: 0.1191535\ttotal: 22.9s\tremaining: 1m 13s\n",
      "237:\tlearn: 0.1190850\ttotal: 23s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1189564\ttotal: 23.1s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1188672\ttotal: 23.2s\tremaining: 1m 13s\n",
      "240:\tlearn: 0.1188024\ttotal: 23.3s\tremaining: 1m 13s\n",
      "241:\tlearn: 0.1186929\ttotal: 23.4s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1186097\ttotal: 23.5s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1185232\ttotal: 23.6s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1183942\ttotal: 23.7s\tremaining: 1m 12s\n",
      "245:\tlearn: 0.1183201\ttotal: 23.8s\tremaining: 1m 12s\n",
      "246:\tlearn: 0.1182528\ttotal: 23.9s\tremaining: 1m 12s\n",
      "247:\tlearn: 0.1181763\ttotal: 24s\tremaining: 1m 12s\n",
      "248:\tlearn: 0.1181012\ttotal: 24.1s\tremaining: 1m 12s\n",
      "249:\tlearn: 0.1180398\ttotal: 24.2s\tremaining: 1m 12s\n",
      "250:\tlearn: 0.1179590\ttotal: 24.3s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.1178490\ttotal: 24.4s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.1177588\ttotal: 24.4s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.1176626\ttotal: 24.5s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.1175720\ttotal: 24.6s\tremaining: 1m 11s\n",
      "255:\tlearn: 0.1174761\ttotal: 24.7s\tremaining: 1m 11s\n",
      "256:\tlearn: 0.1173988\ttotal: 24.8s\tremaining: 1m 11s\n",
      "257:\tlearn: 0.1173485\ttotal: 24.9s\tremaining: 1m 11s\n",
      "258:\tlearn: 0.1173083\ttotal: 25s\tremaining: 1m 11s\n",
      "259:\tlearn: 0.1172119\ttotal: 25.1s\tremaining: 1m 11s\n",
      "260:\tlearn: 0.1170693\ttotal: 25.2s\tremaining: 1m 11s\n",
      "261:\tlearn: 0.1170081\ttotal: 25.3s\tremaining: 1m 11s\n",
      "262:\tlearn: 0.1169300\ttotal: 25.4s\tremaining: 1m 11s\n",
      "263:\tlearn: 0.1168331\ttotal: 25.5s\tremaining: 1m 11s\n",
      "264:\tlearn: 0.1167321\ttotal: 25.6s\tremaining: 1m 11s\n",
      "265:\tlearn: 0.1166709\ttotal: 25.7s\tremaining: 1m 10s\n",
      "266:\tlearn: 0.1165897\ttotal: 25.8s\tremaining: 1m 10s\n",
      "267:\tlearn: 0.1164707\ttotal: 25.9s\tremaining: 1m 10s\n",
      "268:\tlearn: 0.1163407\ttotal: 26s\tremaining: 1m 10s\n",
      "269:\tlearn: 0.1162691\ttotal: 26.1s\tremaining: 1m 10s\n",
      "270:\tlearn: 0.1161864\ttotal: 26.2s\tremaining: 1m 10s\n",
      "271:\tlearn: 0.1161165\ttotal: 26.3s\tremaining: 1m 10s\n",
      "272:\tlearn: 0.1160044\ttotal: 26.4s\tremaining: 1m 10s\n",
      "273:\tlearn: 0.1158990\ttotal: 26.5s\tremaining: 1m 10s\n",
      "274:\tlearn: 0.1158360\ttotal: 26.6s\tremaining: 1m 10s\n",
      "275:\tlearn: 0.1157836\ttotal: 26.7s\tremaining: 1m 10s\n",
      "276:\tlearn: 0.1156688\ttotal: 26.8s\tremaining: 1m 9s\n",
      "277:\tlearn: 0.1155881\ttotal: 26.9s\tremaining: 1m 9s\n",
      "278:\tlearn: 0.1154832\ttotal: 27s\tremaining: 1m 9s\n",
      "279:\tlearn: 0.1153797\ttotal: 27.1s\tremaining: 1m 9s\n",
      "280:\tlearn: 0.1152845\ttotal: 27.2s\tremaining: 1m 9s\n",
      "281:\tlearn: 0.1152215\ttotal: 27.3s\tremaining: 1m 9s\n",
      "282:\tlearn: 0.1151648\ttotal: 27.4s\tremaining: 1m 9s\n",
      "283:\tlearn: 0.1150675\ttotal: 27.5s\tremaining: 1m 9s\n",
      "284:\tlearn: 0.1149638\ttotal: 27.6s\tremaining: 1m 9s\n",
      "285:\tlearn: 0.1148778\ttotal: 27.7s\tremaining: 1m 9s\n",
      "286:\tlearn: 0.1148189\ttotal: 27.8s\tremaining: 1m 8s\n",
      "287:\tlearn: 0.1147735\ttotal: 27.8s\tremaining: 1m 8s\n",
      "288:\tlearn: 0.1147135\ttotal: 27.9s\tremaining: 1m 8s\n",
      "289:\tlearn: 0.1146410\ttotal: 28s\tremaining: 1m 8s\n",
      "290:\tlearn: 0.1145926\ttotal: 28.1s\tremaining: 1m 8s\n",
      "291:\tlearn: 0.1145223\ttotal: 28.2s\tremaining: 1m 8s\n",
      "292:\tlearn: 0.1144283\ttotal: 28.3s\tremaining: 1m 8s\n",
      "293:\tlearn: 0.1143778\ttotal: 28.4s\tremaining: 1m 8s\n",
      "294:\tlearn: 0.1142480\ttotal: 28.5s\tremaining: 1m 8s\n",
      "295:\tlearn: 0.1142257\ttotal: 28.6s\tremaining: 1m 8s\n",
      "296:\tlearn: 0.1141463\ttotal: 28.7s\tremaining: 1m 7s\n",
      "297:\tlearn: 0.1140559\ttotal: 28.8s\tremaining: 1m 7s\n",
      "298:\tlearn: 0.1140196\ttotal: 28.9s\tremaining: 1m 7s\n",
      "299:\tlearn: 0.1139539\ttotal: 29s\tremaining: 1m 7s\n",
      "300:\tlearn: 0.1138379\ttotal: 29.1s\tremaining: 1m 7s\n",
      "301:\tlearn: 0.1137363\ttotal: 29.2s\tremaining: 1m 7s\n",
      "302:\tlearn: 0.1136611\ttotal: 29.3s\tremaining: 1m 7s\n",
      "303:\tlearn: 0.1135326\ttotal: 29.4s\tremaining: 1m 7s\n",
      "304:\tlearn: 0.1134628\ttotal: 29.5s\tremaining: 1m 7s\n",
      "305:\tlearn: 0.1134153\ttotal: 29.6s\tremaining: 1m 7s\n",
      "306:\tlearn: 0.1133590\ttotal: 29.7s\tremaining: 1m 6s\n",
      "307:\tlearn: 0.1132577\ttotal: 29.8s\tremaining: 1m 6s\n",
      "308:\tlearn: 0.1131683\ttotal: 29.9s\tremaining: 1m 6s\n",
      "309:\tlearn: 0.1131015\ttotal: 30s\tremaining: 1m 6s\n",
      "310:\tlearn: 0.1130395\ttotal: 30.1s\tremaining: 1m 6s\n",
      "311:\tlearn: 0.1129564\ttotal: 30.1s\tremaining: 1m 6s\n",
      "312:\tlearn: 0.1128990\ttotal: 30.2s\tremaining: 1m 6s\n",
      "313:\tlearn: 0.1128224\ttotal: 30.3s\tremaining: 1m 6s\n",
      "314:\tlearn: 0.1127848\ttotal: 30.4s\tremaining: 1m 6s\n",
      "315:\tlearn: 0.1127165\ttotal: 30.5s\tremaining: 1m 6s\n",
      "316:\tlearn: 0.1126764\ttotal: 30.6s\tremaining: 1m 5s\n",
      "317:\tlearn: 0.1126074\ttotal: 30.8s\tremaining: 1m 5s\n",
      "318:\tlearn: 0.1125427\ttotal: 30.9s\tremaining: 1m 5s\n",
      "319:\tlearn: 0.1125312\ttotal: 31s\tremaining: 1m 5s\n",
      "320:\tlearn: 0.1124177\ttotal: 31.1s\tremaining: 1m 5s\n",
      "321:\tlearn: 0.1123354\ttotal: 31.2s\tremaining: 1m 5s\n",
      "322:\tlearn: 0.1122505\ttotal: 31.3s\tremaining: 1m 5s\n",
      "323:\tlearn: 0.1121877\ttotal: 31.4s\tremaining: 1m 5s\n",
      "324:\tlearn: 0.1121278\ttotal: 31.5s\tremaining: 1m 5s\n",
      "325:\tlearn: 0.1120654\ttotal: 31.6s\tremaining: 1m 5s\n",
      "326:\tlearn: 0.1119621\ttotal: 31.7s\tremaining: 1m 5s\n",
      "327:\tlearn: 0.1118909\ttotal: 31.8s\tremaining: 1m 5s\n",
      "328:\tlearn: 0.1117945\ttotal: 31.9s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1117305\ttotal: 32s\tremaining: 1m 4s\n",
      "330:\tlearn: 0.1116406\ttotal: 32.1s\tremaining: 1m 4s\n",
      "331:\tlearn: 0.1115625\ttotal: 32.2s\tremaining: 1m 4s\n",
      "332:\tlearn: 0.1114836\ttotal: 32.3s\tremaining: 1m 4s\n",
      "333:\tlearn: 0.1113890\ttotal: 32.4s\tremaining: 1m 4s\n",
      "334:\tlearn: 0.1113620\ttotal: 32.4s\tremaining: 1m 4s\n",
      "335:\tlearn: 0.1113127\ttotal: 32.5s\tremaining: 1m 4s\n",
      "336:\tlearn: 0.1112471\ttotal: 32.6s\tremaining: 1m 4s\n",
      "337:\tlearn: 0.1111673\ttotal: 32.8s\tremaining: 1m 4s\n",
      "338:\tlearn: 0.1111143\ttotal: 32.8s\tremaining: 1m 4s\n",
      "339:\tlearn: 0.1110427\ttotal: 32.9s\tremaining: 1m 3s\n",
      "340:\tlearn: 0.1109699\ttotal: 33s\tremaining: 1m 3s\n",
      "341:\tlearn: 0.1109105\ttotal: 33.1s\tremaining: 1m 3s\n",
      "342:\tlearn: 0.1108035\ttotal: 33.2s\tremaining: 1m 3s\n",
      "343:\tlearn: 0.1107678\ttotal: 33.3s\tremaining: 1m 3s\n",
      "344:\tlearn: 0.1107121\ttotal: 33.4s\tremaining: 1m 3s\n",
      "345:\tlearn: 0.1106216\ttotal: 33.5s\tremaining: 1m 3s\n",
      "346:\tlearn: 0.1105299\ttotal: 33.6s\tremaining: 1m 3s\n",
      "347:\tlearn: 0.1104339\ttotal: 33.7s\tremaining: 1m 3s\n",
      "348:\tlearn: 0.1103471\ttotal: 33.8s\tremaining: 1m 3s\n",
      "349:\tlearn: 0.1102855\ttotal: 33.9s\tremaining: 1m 3s\n",
      "350:\tlearn: 0.1102040\ttotal: 34s\tremaining: 1m 2s\n",
      "351:\tlearn: 0.1101308\ttotal: 34.1s\tremaining: 1m 2s\n",
      "352:\tlearn: 0.1100712\ttotal: 34.2s\tremaining: 1m 2s\n",
      "353:\tlearn: 0.1099928\ttotal: 34.3s\tremaining: 1m 2s\n",
      "354:\tlearn: 0.1099539\ttotal: 34.4s\tremaining: 1m 2s\n",
      "355:\tlearn: 0.1099014\ttotal: 34.5s\tremaining: 1m 2s\n",
      "356:\tlearn: 0.1098632\ttotal: 34.6s\tremaining: 1m 2s\n",
      "357:\tlearn: 0.1097862\ttotal: 34.7s\tremaining: 1m 2s\n",
      "358:\tlearn: 0.1097106\ttotal: 34.8s\tremaining: 1m 2s\n",
      "359:\tlearn: 0.1096077\ttotal: 34.9s\tremaining: 1m 2s\n",
      "360:\tlearn: 0.1095293\ttotal: 35s\tremaining: 1m 1s\n",
      "361:\tlearn: 0.1094603\ttotal: 35.1s\tremaining: 1m 1s\n",
      "362:\tlearn: 0.1093909\ttotal: 35.2s\tremaining: 1m 1s\n",
      "363:\tlearn: 0.1093445\ttotal: 35.3s\tremaining: 1m 1s\n",
      "364:\tlearn: 0.1092544\ttotal: 35.4s\tremaining: 1m 1s\n",
      "365:\tlearn: 0.1091960\ttotal: 35.5s\tremaining: 1m 1s\n",
      "366:\tlearn: 0.1091193\ttotal: 35.6s\tremaining: 1m 1s\n",
      "367:\tlearn: 0.1090714\ttotal: 35.7s\tremaining: 1m 1s\n",
      "368:\tlearn: 0.1090255\ttotal: 35.8s\tremaining: 1m 1s\n",
      "369:\tlearn: 0.1089614\ttotal: 35.9s\tremaining: 1m 1s\n",
      "370:\tlearn: 0.1088988\ttotal: 35.9s\tremaining: 1m\n",
      "371:\tlearn: 0.1088537\ttotal: 36s\tremaining: 1m\n",
      "372:\tlearn: 0.1088012\ttotal: 36.1s\tremaining: 1m\n",
      "373:\tlearn: 0.1087195\ttotal: 36.2s\tremaining: 1m\n",
      "374:\tlearn: 0.1086712\ttotal: 36.3s\tremaining: 1m\n",
      "375:\tlearn: 0.1085692\ttotal: 36.4s\tremaining: 1m\n",
      "376:\tlearn: 0.1085141\ttotal: 36.5s\tremaining: 1m\n",
      "377:\tlearn: 0.1084217\ttotal: 36.6s\tremaining: 1m\n",
      "378:\tlearn: 0.1083925\ttotal: 36.7s\tremaining: 1m\n",
      "379:\tlearn: 0.1083423\ttotal: 36.8s\tremaining: 1m\n",
      "380:\tlearn: 0.1082509\ttotal: 36.9s\tremaining: 60s\n",
      "381:\tlearn: 0.1081810\ttotal: 37s\tremaining: 59.9s\n",
      "382:\tlearn: 0.1080899\ttotal: 37.1s\tremaining: 59.8s\n",
      "383:\tlearn: 0.1080243\ttotal: 37.2s\tremaining: 59.7s\n",
      "384:\tlearn: 0.1079494\ttotal: 37.3s\tremaining: 59.6s\n",
      "385:\tlearn: 0.1079197\ttotal: 37.4s\tremaining: 59.5s\n",
      "386:\tlearn: 0.1078795\ttotal: 37.5s\tremaining: 59.4s\n",
      "387:\tlearn: 0.1078409\ttotal: 37.6s\tremaining: 59.3s\n",
      "388:\tlearn: 0.1077796\ttotal: 37.7s\tremaining: 59.2s\n",
      "389:\tlearn: 0.1077103\ttotal: 37.8s\tremaining: 59.1s\n",
      "390:\tlearn: 0.1076629\ttotal: 37.9s\tremaining: 59s\n",
      "391:\tlearn: 0.1076156\ttotal: 38s\tremaining: 58.9s\n",
      "392:\tlearn: 0.1075478\ttotal: 38.1s\tremaining: 58.8s\n",
      "393:\tlearn: 0.1074825\ttotal: 38.1s\tremaining: 58.7s\n",
      "394:\tlearn: 0.1074023\ttotal: 38.2s\tremaining: 58.6s\n",
      "395:\tlearn: 0.1073141\ttotal: 38.3s\tremaining: 58.5s\n",
      "396:\tlearn: 0.1071882\ttotal: 38.4s\tremaining: 58.4s\n",
      "397:\tlearn: 0.1071273\ttotal: 38.5s\tremaining: 58.3s\n",
      "398:\tlearn: 0.1070407\ttotal: 38.6s\tremaining: 58.2s\n",
      "399:\tlearn: 0.1069970\ttotal: 38.7s\tremaining: 58.1s\n",
      "400:\tlearn: 0.1069517\ttotal: 38.8s\tremaining: 58s\n",
      "401:\tlearn: 0.1069176\ttotal: 38.9s\tremaining: 57.9s\n",
      "402:\tlearn: 0.1068584\ttotal: 39s\tremaining: 57.8s\n",
      "403:\tlearn: 0.1068030\ttotal: 39.1s\tremaining: 57.7s\n",
      "404:\tlearn: 0.1067088\ttotal: 39.2s\tremaining: 57.6s\n",
      "405:\tlearn: 0.1066504\ttotal: 39.3s\tremaining: 57.5s\n",
      "406:\tlearn: 0.1066023\ttotal: 39.4s\tremaining: 57.4s\n",
      "407:\tlearn: 0.1065658\ttotal: 39.5s\tremaining: 57.3s\n",
      "408:\tlearn: 0.1065242\ttotal: 39.6s\tremaining: 57.2s\n",
      "409:\tlearn: 0.1064198\ttotal: 39.7s\tremaining: 57.1s\n",
      "410:\tlearn: 0.1063658\ttotal: 39.8s\tremaining: 57s\n",
      "411:\tlearn: 0.1063027\ttotal: 39.9s\tremaining: 56.9s\n",
      "412:\tlearn: 0.1062231\ttotal: 40s\tremaining: 56.8s\n",
      "413:\tlearn: 0.1061911\ttotal: 40.1s\tremaining: 56.7s\n",
      "414:\tlearn: 0.1061737\ttotal: 40.2s\tremaining: 56.6s\n",
      "415:\tlearn: 0.1061040\ttotal: 40.3s\tremaining: 56.5s\n",
      "416:\tlearn: 0.1060320\ttotal: 40.4s\tremaining: 56.4s\n",
      "417:\tlearn: 0.1059529\ttotal: 40.5s\tremaining: 56.4s\n",
      "418:\tlearn: 0.1058797\ttotal: 40.6s\tremaining: 56.3s\n",
      "419:\tlearn: 0.1058025\ttotal: 40.7s\tremaining: 56.2s\n",
      "420:\tlearn: 0.1057431\ttotal: 40.8s\tremaining: 56.1s\n",
      "421:\tlearn: 0.1056825\ttotal: 40.9s\tremaining: 56s\n",
      "422:\tlearn: 0.1056206\ttotal: 41s\tremaining: 55.9s\n",
      "423:\tlearn: 0.1055255\ttotal: 41.2s\tremaining: 55.9s\n",
      "424:\tlearn: 0.1054663\ttotal: 41.3s\tremaining: 55.8s\n",
      "425:\tlearn: 0.1054177\ttotal: 41.4s\tremaining: 55.7s\n",
      "426:\tlearn: 0.1053717\ttotal: 41.5s\tremaining: 55.6s\n",
      "427:\tlearn: 0.1053123\ttotal: 41.6s\tremaining: 55.5s\n",
      "428:\tlearn: 0.1052497\ttotal: 41.7s\tremaining: 55.4s\n",
      "429:\tlearn: 0.1052099\ttotal: 41.7s\tremaining: 55.3s\n",
      "430:\tlearn: 0.1051328\ttotal: 41.8s\tremaining: 55.2s\n",
      "431:\tlearn: 0.1050592\ttotal: 41.9s\tremaining: 55.1s\n",
      "432:\tlearn: 0.1050008\ttotal: 42s\tremaining: 55s\n",
      "433:\tlearn: 0.1049483\ttotal: 42.1s\tremaining: 54.9s\n",
      "434:\tlearn: 0.1049195\ttotal: 42.2s\tremaining: 54.8s\n",
      "435:\tlearn: 0.1048727\ttotal: 42.3s\tremaining: 54.7s\n",
      "436:\tlearn: 0.1047717\ttotal: 42.4s\tremaining: 54.6s\n",
      "437:\tlearn: 0.1047148\ttotal: 42.5s\tremaining: 54.5s\n",
      "438:\tlearn: 0.1046330\ttotal: 42.6s\tremaining: 54.5s\n",
      "439:\tlearn: 0.1045799\ttotal: 42.7s\tremaining: 54.4s\n",
      "440:\tlearn: 0.1045062\ttotal: 42.8s\tremaining: 54.3s\n",
      "441:\tlearn: 0.1044288\ttotal: 42.9s\tremaining: 54.2s\n",
      "442:\tlearn: 0.1043689\ttotal: 43s\tremaining: 54.1s\n",
      "443:\tlearn: 0.1042701\ttotal: 43.1s\tremaining: 54s\n",
      "444:\tlearn: 0.1041864\ttotal: 43.2s\tremaining: 53.9s\n",
      "445:\tlearn: 0.1041280\ttotal: 43.3s\tremaining: 53.8s\n",
      "446:\tlearn: 0.1040582\ttotal: 43.4s\tremaining: 53.7s\n",
      "447:\tlearn: 0.1039699\ttotal: 43.5s\tremaining: 53.6s\n",
      "448:\tlearn: 0.1038992\ttotal: 43.6s\tremaining: 53.5s\n",
      "449:\tlearn: 0.1038498\ttotal: 43.7s\tremaining: 53.4s\n",
      "450:\tlearn: 0.1037764\ttotal: 43.8s\tremaining: 53.3s\n",
      "451:\tlearn: 0.1037288\ttotal: 43.9s\tremaining: 53.2s\n",
      "452:\tlearn: 0.1036958\ttotal: 44s\tremaining: 53.1s\n",
      "453:\tlearn: 0.1036384\ttotal: 44.1s\tremaining: 53s\n",
      "454:\tlearn: 0.1035788\ttotal: 44.2s\tremaining: 52.9s\n",
      "455:\tlearn: 0.1035553\ttotal: 44.3s\tremaining: 52.8s\n",
      "456:\tlearn: 0.1034935\ttotal: 44.4s\tremaining: 52.7s\n",
      "457:\tlearn: 0.1034194\ttotal: 44.4s\tremaining: 52.6s\n",
      "458:\tlearn: 0.1033821\ttotal: 44.5s\tremaining: 52.5s\n",
      "459:\tlearn: 0.1033191\ttotal: 44.6s\tremaining: 52.4s\n",
      "460:\tlearn: 0.1032763\ttotal: 44.7s\tremaining: 52.3s\n",
      "461:\tlearn: 0.1032393\ttotal: 44.8s\tremaining: 52.2s\n",
      "462:\tlearn: 0.1031833\ttotal: 44.9s\tremaining: 52.1s\n",
      "463:\tlearn: 0.1030917\ttotal: 45s\tremaining: 52s\n",
      "464:\tlearn: 0.1030507\ttotal: 45.1s\tremaining: 51.9s\n",
      "465:\tlearn: 0.1029780\ttotal: 45.2s\tremaining: 51.8s\n",
      "466:\tlearn: 0.1029087\ttotal: 45.3s\tremaining: 51.7s\n",
      "467:\tlearn: 0.1028589\ttotal: 45.4s\tremaining: 51.6s\n",
      "468:\tlearn: 0.1027918\ttotal: 45.5s\tremaining: 51.5s\n",
      "469:\tlearn: 0.1027470\ttotal: 45.6s\tremaining: 51.4s\n",
      "470:\tlearn: 0.1026799\ttotal: 45.7s\tremaining: 51.3s\n",
      "471:\tlearn: 0.1026214\ttotal: 45.8s\tremaining: 51.2s\n",
      "472:\tlearn: 0.1025731\ttotal: 45.9s\tremaining: 51.1s\n",
      "473:\tlearn: 0.1024982\ttotal: 46s\tremaining: 51s\n",
      "474:\tlearn: 0.1024458\ttotal: 46.1s\tremaining: 50.9s\n",
      "475:\tlearn: 0.1024106\ttotal: 46.2s\tremaining: 50.8s\n",
      "476:\tlearn: 0.1023227\ttotal: 46.3s\tremaining: 50.7s\n",
      "477:\tlearn: 0.1022996\ttotal: 46.4s\tremaining: 50.6s\n",
      "478:\tlearn: 0.1021949\ttotal: 46.5s\tremaining: 50.5s\n",
      "479:\tlearn: 0.1021555\ttotal: 46.6s\tremaining: 50.4s\n",
      "480:\tlearn: 0.1020973\ttotal: 46.7s\tremaining: 50.3s\n",
      "481:\tlearn: 0.1020433\ttotal: 46.8s\tremaining: 50.2s\n",
      "482:\tlearn: 0.1020022\ttotal: 46.8s\tremaining: 50.1s\n",
      "483:\tlearn: 0.1019250\ttotal: 46.9s\tremaining: 50s\n",
      "484:\tlearn: 0.1018330\ttotal: 47s\tremaining: 50s\n",
      "485:\tlearn: 0.1017642\ttotal: 47.2s\tremaining: 49.9s\n",
      "486:\tlearn: 0.1017150\ttotal: 47.3s\tremaining: 49.8s\n",
      "487:\tlearn: 0.1016704\ttotal: 47.4s\tremaining: 49.7s\n",
      "488:\tlearn: 0.1016317\ttotal: 47.5s\tremaining: 49.6s\n",
      "489:\tlearn: 0.1015952\ttotal: 47.5s\tremaining: 49.5s\n",
      "490:\tlearn: 0.1015046\ttotal: 47.6s\tremaining: 49.4s\n",
      "491:\tlearn: 0.1014442\ttotal: 47.7s\tremaining: 49.3s\n",
      "492:\tlearn: 0.1013940\ttotal: 47.8s\tremaining: 49.2s\n",
      "493:\tlearn: 0.1013620\ttotal: 47.9s\tremaining: 49.1s\n",
      "494:\tlearn: 0.1013268\ttotal: 48s\tremaining: 49s\n",
      "495:\tlearn: 0.1012717\ttotal: 48.1s\tremaining: 48.9s\n",
      "496:\tlearn: 0.1012335\ttotal: 48.2s\tremaining: 48.8s\n",
      "497:\tlearn: 0.1011966\ttotal: 48.3s\tremaining: 48.7s\n",
      "498:\tlearn: 0.1011366\ttotal: 48.4s\tremaining: 48.6s\n",
      "499:\tlearn: 0.1010900\ttotal: 48.5s\tremaining: 48.5s\n",
      "500:\tlearn: 0.1010434\ttotal: 48.6s\tremaining: 48.4s\n",
      "501:\tlearn: 0.1010235\ttotal: 48.7s\tremaining: 48.3s\n",
      "502:\tlearn: 0.1010025\ttotal: 48.8s\tremaining: 48.2s\n",
      "503:\tlearn: 0.1009616\ttotal: 48.9s\tremaining: 48.1s\n",
      "504:\tlearn: 0.1008572\ttotal: 49s\tremaining: 48s\n",
      "505:\tlearn: 0.1007294\ttotal: 49.1s\tremaining: 47.9s\n",
      "506:\tlearn: 0.1006762\ttotal: 49.2s\tremaining: 47.8s\n",
      "507:\tlearn: 0.1005986\ttotal: 49.3s\tremaining: 47.7s\n",
      "508:\tlearn: 0.1005551\ttotal: 49.4s\tremaining: 47.6s\n",
      "509:\tlearn: 0.1005271\ttotal: 49.5s\tremaining: 47.5s\n",
      "510:\tlearn: 0.1004580\ttotal: 49.6s\tremaining: 47.4s\n",
      "511:\tlearn: 0.1004165\ttotal: 49.7s\tremaining: 47.3s\n",
      "512:\tlearn: 0.1003703\ttotal: 49.8s\tremaining: 47.2s\n",
      "513:\tlearn: 0.1003258\ttotal: 49.8s\tremaining: 47.1s\n",
      "514:\tlearn: 0.1002892\ttotal: 49.9s\tremaining: 47s\n",
      "515:\tlearn: 0.1002356\ttotal: 50.1s\tremaining: 47s\n",
      "516:\tlearn: 0.1001919\ttotal: 50.3s\tremaining: 46.9s\n",
      "517:\tlearn: 0.1001565\ttotal: 50.3s\tremaining: 46.8s\n",
      "518:\tlearn: 0.1001263\ttotal: 50.4s\tremaining: 46.7s\n",
      "519:\tlearn: 0.1000407\ttotal: 50.5s\tremaining: 46.6s\n",
      "520:\tlearn: 0.0999781\ttotal: 50.6s\tremaining: 46.6s\n",
      "521:\tlearn: 0.0999309\ttotal: 50.7s\tremaining: 46.4s\n",
      "522:\tlearn: 0.0998879\ttotal: 50.8s\tremaining: 46.3s\n",
      "523:\tlearn: 0.0998279\ttotal: 50.9s\tremaining: 46.3s\n",
      "524:\tlearn: 0.0997863\ttotal: 51s\tremaining: 46.2s\n",
      "525:\tlearn: 0.0997450\ttotal: 51.1s\tremaining: 46.1s\n",
      "526:\tlearn: 0.0996984\ttotal: 51.2s\tremaining: 46s\n",
      "527:\tlearn: 0.0996361\ttotal: 51.3s\tremaining: 45.9s\n",
      "528:\tlearn: 0.0995630\ttotal: 51.4s\tremaining: 45.8s\n",
      "529:\tlearn: 0.0994694\ttotal: 51.5s\tremaining: 45.7s\n",
      "530:\tlearn: 0.0994002\ttotal: 51.6s\tremaining: 45.6s\n",
      "531:\tlearn: 0.0993757\ttotal: 51.7s\tremaining: 45.5s\n",
      "532:\tlearn: 0.0993106\ttotal: 51.8s\tremaining: 45.4s\n",
      "533:\tlearn: 0.0992626\ttotal: 51.9s\tremaining: 45.3s\n",
      "534:\tlearn: 0.0992287\ttotal: 52s\tremaining: 45.2s\n",
      "535:\tlearn: 0.0991991\ttotal: 52.1s\tremaining: 45.1s\n",
      "536:\tlearn: 0.0991473\ttotal: 52.2s\tremaining: 45s\n",
      "537:\tlearn: 0.0990854\ttotal: 52.3s\tremaining: 44.9s\n",
      "538:\tlearn: 0.0990240\ttotal: 52.4s\tremaining: 44.8s\n",
      "539:\tlearn: 0.0989039\ttotal: 52.5s\tremaining: 44.7s\n",
      "540:\tlearn: 0.0988664\ttotal: 52.6s\tremaining: 44.6s\n",
      "541:\tlearn: 0.0988102\ttotal: 52.7s\tremaining: 44.5s\n",
      "542:\tlearn: 0.0987354\ttotal: 52.8s\tremaining: 44.4s\n",
      "543:\tlearn: 0.0986677\ttotal: 52.9s\tremaining: 44.3s\n",
      "544:\tlearn: 0.0986036\ttotal: 53s\tremaining: 44.2s\n",
      "545:\tlearn: 0.0985366\ttotal: 53.1s\tremaining: 44.1s\n",
      "546:\tlearn: 0.0984925\ttotal: 53.2s\tremaining: 44.1s\n",
      "547:\tlearn: 0.0984245\ttotal: 53.3s\tremaining: 44s\n",
      "548:\tlearn: 0.0983782\ttotal: 53.4s\tremaining: 43.9s\n",
      "549:\tlearn: 0.0983185\ttotal: 53.5s\tremaining: 43.8s\n",
      "550:\tlearn: 0.0982978\ttotal: 53.6s\tremaining: 43.7s\n",
      "551:\tlearn: 0.0982428\ttotal: 53.7s\tremaining: 43.6s\n",
      "552:\tlearn: 0.0982013\ttotal: 53.8s\tremaining: 43.5s\n",
      "553:\tlearn: 0.0981329\ttotal: 53.9s\tremaining: 43.4s\n",
      "554:\tlearn: 0.0980611\ttotal: 54s\tremaining: 43.3s\n",
      "555:\tlearn: 0.0980184\ttotal: 54.1s\tremaining: 43.2s\n",
      "556:\tlearn: 0.0979936\ttotal: 54.2s\tremaining: 43.1s\n",
      "557:\tlearn: 0.0979407\ttotal: 54.2s\tremaining: 43s\n",
      "558:\tlearn: 0.0979112\ttotal: 54.3s\tremaining: 42.9s\n",
      "559:\tlearn: 0.0978388\ttotal: 54.4s\tremaining: 42.8s\n",
      "560:\tlearn: 0.0977724\ttotal: 54.5s\tremaining: 42.7s\n",
      "561:\tlearn: 0.0977483\ttotal: 54.6s\tremaining: 42.6s\n",
      "562:\tlearn: 0.0977087\ttotal: 54.7s\tremaining: 42.5s\n",
      "563:\tlearn: 0.0976697\ttotal: 54.8s\tremaining: 42.4s\n",
      "564:\tlearn: 0.0976197\ttotal: 54.9s\tremaining: 42.3s\n",
      "565:\tlearn: 0.0975644\ttotal: 55s\tremaining: 42.2s\n",
      "566:\tlearn: 0.0975348\ttotal: 55.1s\tremaining: 42.1s\n",
      "567:\tlearn: 0.0975029\ttotal: 55.2s\tremaining: 42s\n",
      "568:\tlearn: 0.0974182\ttotal: 55.3s\tremaining: 41.9s\n",
      "569:\tlearn: 0.0973887\ttotal: 55.4s\tremaining: 41.8s\n",
      "570:\tlearn: 0.0973501\ttotal: 55.5s\tremaining: 41.7s\n",
      "571:\tlearn: 0.0973217\ttotal: 55.6s\tremaining: 41.6s\n",
      "572:\tlearn: 0.0972851\ttotal: 55.7s\tremaining: 41.5s\n",
      "573:\tlearn: 0.0972322\ttotal: 55.8s\tremaining: 41.4s\n",
      "574:\tlearn: 0.0971787\ttotal: 55.9s\tremaining: 41.3s\n",
      "575:\tlearn: 0.0971393\ttotal: 56s\tremaining: 41.2s\n",
      "576:\tlearn: 0.0971052\ttotal: 56.1s\tremaining: 41.1s\n",
      "577:\tlearn: 0.0970532\ttotal: 56.2s\tremaining: 41s\n",
      "578:\tlearn: 0.0970136\ttotal: 56.3s\tremaining: 40.9s\n",
      "579:\tlearn: 0.0970010\ttotal: 56.4s\tremaining: 40.8s\n",
      "580:\tlearn: 0.0969718\ttotal: 56.5s\tremaining: 40.7s\n",
      "581:\tlearn: 0.0969182\ttotal: 56.6s\tremaining: 40.6s\n",
      "582:\tlearn: 0.0968720\ttotal: 56.7s\tremaining: 40.5s\n",
      "583:\tlearn: 0.0968000\ttotal: 56.8s\tremaining: 40.4s\n",
      "584:\tlearn: 0.0967489\ttotal: 56.9s\tremaining: 40.3s\n",
      "585:\tlearn: 0.0966840\ttotal: 57s\tremaining: 40.3s\n",
      "586:\tlearn: 0.0966337\ttotal: 57.1s\tremaining: 40.2s\n",
      "587:\tlearn: 0.0965517\ttotal: 57.2s\tremaining: 40.1s\n",
      "588:\tlearn: 0.0965223\ttotal: 57.3s\tremaining: 40s\n",
      "589:\tlearn: 0.0964838\ttotal: 57.4s\tremaining: 39.9s\n",
      "590:\tlearn: 0.0964383\ttotal: 57.5s\tremaining: 39.8s\n",
      "591:\tlearn: 0.0964028\ttotal: 57.5s\tremaining: 39.7s\n",
      "592:\tlearn: 0.0963440\ttotal: 57.7s\tremaining: 39.6s\n",
      "593:\tlearn: 0.0963153\ttotal: 57.7s\tremaining: 39.5s\n",
      "594:\tlearn: 0.0962773\ttotal: 57.8s\tremaining: 39.4s\n",
      "595:\tlearn: 0.0962070\ttotal: 57.9s\tremaining: 39.3s\n",
      "596:\tlearn: 0.0961200\ttotal: 58s\tremaining: 39.2s\n",
      "597:\tlearn: 0.0960626\ttotal: 58.1s\tremaining: 39.1s\n",
      "598:\tlearn: 0.0960301\ttotal: 58.2s\tremaining: 39s\n",
      "599:\tlearn: 0.0960038\ttotal: 58.3s\tremaining: 38.9s\n",
      "600:\tlearn: 0.0959707\ttotal: 58.4s\tremaining: 38.8s\n",
      "601:\tlearn: 0.0959057\ttotal: 58.5s\tremaining: 38.7s\n",
      "602:\tlearn: 0.0958773\ttotal: 58.6s\tremaining: 38.6s\n",
      "603:\tlearn: 0.0958132\ttotal: 58.7s\tremaining: 38.5s\n",
      "604:\tlearn: 0.0957860\ttotal: 58.8s\tremaining: 38.4s\n",
      "605:\tlearn: 0.0957418\ttotal: 58.9s\tremaining: 38.3s\n",
      "606:\tlearn: 0.0956878\ttotal: 59s\tremaining: 38.2s\n",
      "607:\tlearn: 0.0956250\ttotal: 59.1s\tremaining: 38.1s\n",
      "608:\tlearn: 0.0955911\ttotal: 59.2s\tremaining: 38s\n",
      "609:\tlearn: 0.0955369\ttotal: 59.3s\tremaining: 37.9s\n",
      "610:\tlearn: 0.0954880\ttotal: 59.4s\tremaining: 37.8s\n",
      "611:\tlearn: 0.0954033\ttotal: 59.5s\tremaining: 37.7s\n",
      "612:\tlearn: 0.0953683\ttotal: 59.6s\tremaining: 37.6s\n",
      "613:\tlearn: 0.0953139\ttotal: 59.7s\tremaining: 37.5s\n",
      "614:\tlearn: 0.0952857\ttotal: 59.8s\tremaining: 37.5s\n",
      "615:\tlearn: 0.0952583\ttotal: 59.9s\tremaining: 37.4s\n",
      "616:\tlearn: 0.0951996\ttotal: 1m\tremaining: 37.3s\n",
      "617:\tlearn: 0.0951709\ttotal: 1m\tremaining: 37.2s\n",
      "618:\tlearn: 0.0951247\ttotal: 1m\tremaining: 37.1s\n",
      "619:\tlearn: 0.0950662\ttotal: 1m\tremaining: 37s\n",
      "620:\tlearn: 0.0950134\ttotal: 1m\tremaining: 36.9s\n",
      "621:\tlearn: 0.0949668\ttotal: 1m\tremaining: 36.8s\n",
      "622:\tlearn: 0.0948745\ttotal: 1m\tremaining: 36.7s\n",
      "623:\tlearn: 0.0948320\ttotal: 1m\tremaining: 36.6s\n",
      "624:\tlearn: 0.0947919\ttotal: 1m\tremaining: 36.5s\n",
      "625:\tlearn: 0.0947517\ttotal: 1m\tremaining: 36.4s\n",
      "626:\tlearn: 0.0946994\ttotal: 1m 1s\tremaining: 36.3s\n",
      "627:\tlearn: 0.0946438\ttotal: 1m 1s\tremaining: 36.2s\n",
      "628:\tlearn: 0.0946123\ttotal: 1m 1s\tremaining: 36.1s\n",
      "629:\tlearn: 0.0945857\ttotal: 1m 1s\tremaining: 36s\n",
      "630:\tlearn: 0.0945242\ttotal: 1m 1s\tremaining: 36s\n",
      "631:\tlearn: 0.0944519\ttotal: 1m 1s\tremaining: 35.9s\n",
      "632:\tlearn: 0.0943896\ttotal: 1m 1s\tremaining: 35.8s\n",
      "633:\tlearn: 0.0943680\ttotal: 1m 1s\tremaining: 35.7s\n",
      "634:\tlearn: 0.0943149\ttotal: 1m 1s\tremaining: 35.6s\n",
      "635:\tlearn: 0.0942781\ttotal: 1m 1s\tremaining: 35.5s\n",
      "636:\tlearn: 0.0942227\ttotal: 1m 2s\tremaining: 35.4s\n",
      "637:\tlearn: 0.0942062\ttotal: 1m 2s\tremaining: 35.3s\n",
      "638:\tlearn: 0.0941511\ttotal: 1m 2s\tremaining: 35.2s\n",
      "639:\tlearn: 0.0940965\ttotal: 1m 2s\tremaining: 35.1s\n",
      "640:\tlearn: 0.0939992\ttotal: 1m 2s\tremaining: 35s\n",
      "641:\tlearn: 0.0939727\ttotal: 1m 2s\tremaining: 34.9s\n",
      "642:\tlearn: 0.0938989\ttotal: 1m 2s\tremaining: 34.8s\n",
      "643:\tlearn: 0.0938695\ttotal: 1m 2s\tremaining: 34.7s\n",
      "644:\tlearn: 0.0938430\ttotal: 1m 2s\tremaining: 34.6s\n",
      "645:\tlearn: 0.0937603\ttotal: 1m 2s\tremaining: 34.5s\n",
      "646:\tlearn: 0.0937297\ttotal: 1m 3s\tremaining: 34.4s\n",
      "647:\tlearn: 0.0936739\ttotal: 1m 3s\tremaining: 34.3s\n",
      "648:\tlearn: 0.0935922\ttotal: 1m 3s\tremaining: 34.2s\n",
      "649:\tlearn: 0.0935634\ttotal: 1m 3s\tremaining: 34.1s\n",
      "650:\tlearn: 0.0935174\ttotal: 1m 3s\tremaining: 34s\n",
      "651:\tlearn: 0.0934939\ttotal: 1m 3s\tremaining: 33.9s\n",
      "652:\tlearn: 0.0934586\ttotal: 1m 3s\tremaining: 33.8s\n",
      "653:\tlearn: 0.0934270\ttotal: 1m 3s\tremaining: 33.7s\n",
      "654:\tlearn: 0.0933917\ttotal: 1m 3s\tremaining: 33.6s\n",
      "655:\tlearn: 0.0933731\ttotal: 1m 3s\tremaining: 33.5s\n",
      "656:\tlearn: 0.0933062\ttotal: 1m 3s\tremaining: 33.4s\n",
      "657:\tlearn: 0.0932697\ttotal: 1m 4s\tremaining: 33.3s\n",
      "658:\tlearn: 0.0931930\ttotal: 1m 4s\tremaining: 33.2s\n",
      "659:\tlearn: 0.0931703\ttotal: 1m 4s\tremaining: 33.1s\n",
      "660:\tlearn: 0.0931057\ttotal: 1m 4s\tremaining: 33s\n",
      "661:\tlearn: 0.0930534\ttotal: 1m 4s\tremaining: 32.9s\n",
      "662:\tlearn: 0.0929970\ttotal: 1m 4s\tremaining: 32.8s\n",
      "663:\tlearn: 0.0929380\ttotal: 1m 4s\tremaining: 32.7s\n",
      "664:\tlearn: 0.0929184\ttotal: 1m 4s\tremaining: 32.6s\n",
      "665:\tlearn: 0.0928565\ttotal: 1m 4s\tremaining: 32.5s\n",
      "666:\tlearn: 0.0928416\ttotal: 1m 4s\tremaining: 32.4s\n",
      "667:\tlearn: 0.0928150\ttotal: 1m 5s\tremaining: 32.3s\n",
      "668:\tlearn: 0.0927774\ttotal: 1m 5s\tremaining: 32.2s\n",
      "669:\tlearn: 0.0927526\ttotal: 1m 5s\tremaining: 32.1s\n",
      "670:\tlearn: 0.0927269\ttotal: 1m 5s\tremaining: 32s\n",
      "671:\tlearn: 0.0927023\ttotal: 1m 5s\tremaining: 31.9s\n",
      "672:\tlearn: 0.0926683\ttotal: 1m 5s\tremaining: 31.8s\n",
      "673:\tlearn: 0.0926427\ttotal: 1m 5s\tremaining: 31.7s\n",
      "674:\tlearn: 0.0925956\ttotal: 1m 5s\tremaining: 31.6s\n",
      "675:\tlearn: 0.0925817\ttotal: 1m 5s\tremaining: 31.5s\n",
      "676:\tlearn: 0.0925528\ttotal: 1m 5s\tremaining: 31.4s\n",
      "677:\tlearn: 0.0925189\ttotal: 1m 6s\tremaining: 31.3s\n",
      "678:\tlearn: 0.0924958\ttotal: 1m 6s\tremaining: 31.2s\n",
      "679:\tlearn: 0.0924574\ttotal: 1m 6s\tremaining: 31.1s\n",
      "680:\tlearn: 0.0924115\ttotal: 1m 6s\tremaining: 31.1s\n",
      "681:\tlearn: 0.0923862\ttotal: 1m 6s\tremaining: 30.9s\n",
      "682:\tlearn: 0.0923623\ttotal: 1m 6s\tremaining: 30.9s\n",
      "683:\tlearn: 0.0923244\ttotal: 1m 6s\tremaining: 30.8s\n",
      "684:\tlearn: 0.0922825\ttotal: 1m 6s\tremaining: 30.7s\n",
      "685:\tlearn: 0.0922624\ttotal: 1m 6s\tremaining: 30.6s\n",
      "686:\tlearn: 0.0922021\ttotal: 1m 6s\tremaining: 30.5s\n",
      "687:\tlearn: 0.0921669\ttotal: 1m 6s\tremaining: 30.4s\n",
      "688:\tlearn: 0.0921166\ttotal: 1m 7s\tremaining: 30.3s\n",
      "689:\tlearn: 0.0920537\ttotal: 1m 7s\tremaining: 30.2s\n",
      "690:\tlearn: 0.0920110\ttotal: 1m 7s\tremaining: 30.1s\n",
      "691:\tlearn: 0.0919552\ttotal: 1m 7s\tremaining: 30s\n",
      "692:\tlearn: 0.0919221\ttotal: 1m 7s\tremaining: 29.9s\n",
      "693:\tlearn: 0.0918869\ttotal: 1m 7s\tremaining: 29.8s\n",
      "694:\tlearn: 0.0918613\ttotal: 1m 7s\tremaining: 29.7s\n",
      "695:\tlearn: 0.0918101\ttotal: 1m 7s\tremaining: 29.6s\n",
      "696:\tlearn: 0.0917783\ttotal: 1m 7s\tremaining: 29.5s\n",
      "697:\tlearn: 0.0917265\ttotal: 1m 7s\tremaining: 29.4s\n",
      "698:\tlearn: 0.0916624\ttotal: 1m 8s\tremaining: 29.3s\n",
      "699:\tlearn: 0.0916257\ttotal: 1m 8s\tremaining: 29.2s\n",
      "700:\tlearn: 0.0915839\ttotal: 1m 8s\tremaining: 29.1s\n",
      "701:\tlearn: 0.0915477\ttotal: 1m 8s\tremaining: 29s\n",
      "702:\tlearn: 0.0915072\ttotal: 1m 8s\tremaining: 28.9s\n",
      "703:\tlearn: 0.0914788\ttotal: 1m 8s\tremaining: 28.8s\n",
      "704:\tlearn: 0.0914121\ttotal: 1m 8s\tremaining: 28.7s\n",
      "705:\tlearn: 0.0913583\ttotal: 1m 8s\tremaining: 28.6s\n",
      "706:\tlearn: 0.0912947\ttotal: 1m 8s\tremaining: 28.5s\n",
      "707:\tlearn: 0.0912508\ttotal: 1m 8s\tremaining: 28.4s\n",
      "708:\tlearn: 0.0912033\ttotal: 1m 8s\tremaining: 28.3s\n",
      "709:\tlearn: 0.0911463\ttotal: 1m 9s\tremaining: 28.2s\n",
      "710:\tlearn: 0.0911169\ttotal: 1m 9s\tremaining: 28.1s\n",
      "711:\tlearn: 0.0910756\ttotal: 1m 9s\tremaining: 28s\n",
      "712:\tlearn: 0.0910300\ttotal: 1m 9s\tremaining: 27.9s\n",
      "713:\tlearn: 0.0909858\ttotal: 1m 9s\tremaining: 27.8s\n",
      "714:\tlearn: 0.0909662\ttotal: 1m 9s\tremaining: 27.7s\n",
      "715:\tlearn: 0.0909228\ttotal: 1m 9s\tremaining: 27.6s\n",
      "716:\tlearn: 0.0908939\ttotal: 1m 9s\tremaining: 27.5s\n",
      "717:\tlearn: 0.0908362\ttotal: 1m 9s\tremaining: 27.4s\n",
      "718:\tlearn: 0.0907940\ttotal: 1m 9s\tremaining: 27.3s\n",
      "719:\tlearn: 0.0907799\ttotal: 1m 10s\tremaining: 27.2s\n",
      "720:\tlearn: 0.0907379\ttotal: 1m 10s\tremaining: 27.1s\n",
      "721:\tlearn: 0.0907042\ttotal: 1m 10s\tremaining: 27s\n",
      "722:\tlearn: 0.0906373\ttotal: 1m 10s\tremaining: 26.9s\n",
      "723:\tlearn: 0.0905966\ttotal: 1m 10s\tremaining: 26.8s\n",
      "724:\tlearn: 0.0905850\ttotal: 1m 10s\tremaining: 26.7s\n",
      "725:\tlearn: 0.0905540\ttotal: 1m 10s\tremaining: 26.7s\n",
      "726:\tlearn: 0.0905265\ttotal: 1m 10s\tremaining: 26.6s\n",
      "727:\tlearn: 0.0904739\ttotal: 1m 10s\tremaining: 26.5s\n",
      "728:\tlearn: 0.0904352\ttotal: 1m 11s\tremaining: 26.4s\n",
      "729:\tlearn: 0.0904147\ttotal: 1m 11s\tremaining: 26.3s\n",
      "730:\tlearn: 0.0903831\ttotal: 1m 11s\tremaining: 26.2s\n",
      "731:\tlearn: 0.0903431\ttotal: 1m 11s\tremaining: 26.1s\n",
      "732:\tlearn: 0.0902671\ttotal: 1m 11s\tremaining: 26s\n",
      "733:\tlearn: 0.0902373\ttotal: 1m 11s\tremaining: 25.9s\n",
      "734:\tlearn: 0.0902039\ttotal: 1m 11s\tremaining: 25.8s\n",
      "735:\tlearn: 0.0901708\ttotal: 1m 11s\tremaining: 25.7s\n",
      "736:\tlearn: 0.0901340\ttotal: 1m 11s\tremaining: 25.6s\n",
      "737:\tlearn: 0.0901062\ttotal: 1m 11s\tremaining: 25.5s\n",
      "738:\tlearn: 0.0900809\ttotal: 1m 11s\tremaining: 25.4s\n",
      "739:\tlearn: 0.0900472\ttotal: 1m 12s\tremaining: 25.3s\n",
      "740:\tlearn: 0.0900023\ttotal: 1m 12s\tremaining: 25.2s\n",
      "741:\tlearn: 0.0899825\ttotal: 1m 12s\tremaining: 25.1s\n",
      "742:\tlearn: 0.0899528\ttotal: 1m 12s\tremaining: 25s\n",
      "743:\tlearn: 0.0898788\ttotal: 1m 12s\tremaining: 24.9s\n",
      "744:\tlearn: 0.0898182\ttotal: 1m 12s\tremaining: 24.8s\n",
      "745:\tlearn: 0.0897689\ttotal: 1m 12s\tremaining: 24.7s\n",
      "746:\tlearn: 0.0897516\ttotal: 1m 12s\tremaining: 24.6s\n",
      "747:\tlearn: 0.0896990\ttotal: 1m 12s\tremaining: 24.5s\n",
      "748:\tlearn: 0.0896552\ttotal: 1m 12s\tremaining: 24.4s\n",
      "749:\tlearn: 0.0895864\ttotal: 1m 13s\tremaining: 24.3s\n",
      "750:\tlearn: 0.0895579\ttotal: 1m 13s\tremaining: 24.2s\n",
      "751:\tlearn: 0.0895101\ttotal: 1m 13s\tremaining: 24.1s\n",
      "752:\tlearn: 0.0894651\ttotal: 1m 13s\tremaining: 24.1s\n",
      "753:\tlearn: 0.0894205\ttotal: 1m 13s\tremaining: 24s\n",
      "754:\tlearn: 0.0893906\ttotal: 1m 13s\tremaining: 23.9s\n",
      "755:\tlearn: 0.0893472\ttotal: 1m 13s\tremaining: 23.8s\n",
      "756:\tlearn: 0.0893120\ttotal: 1m 13s\tremaining: 23.7s\n",
      "757:\tlearn: 0.0892566\ttotal: 1m 13s\tremaining: 23.6s\n",
      "758:\tlearn: 0.0891749\ttotal: 1m 13s\tremaining: 23.5s\n",
      "759:\tlearn: 0.0891449\ttotal: 1m 14s\tremaining: 23.4s\n",
      "760:\tlearn: 0.0891251\ttotal: 1m 14s\tremaining: 23.3s\n",
      "761:\tlearn: 0.0890641\ttotal: 1m 14s\tremaining: 23.2s\n",
      "762:\tlearn: 0.0890540\ttotal: 1m 14s\tremaining: 23.1s\n",
      "763:\tlearn: 0.0890391\ttotal: 1m 14s\tremaining: 23s\n",
      "764:\tlearn: 0.0890096\ttotal: 1m 14s\tremaining: 22.9s\n",
      "765:\tlearn: 0.0889963\ttotal: 1m 14s\tremaining: 22.8s\n",
      "766:\tlearn: 0.0889511\ttotal: 1m 14s\tremaining: 22.7s\n",
      "767:\tlearn: 0.0889281\ttotal: 1m 14s\tremaining: 22.6s\n",
      "768:\tlearn: 0.0888707\ttotal: 1m 14s\tremaining: 22.5s\n",
      "769:\tlearn: 0.0888094\ttotal: 1m 14s\tremaining: 22.4s\n",
      "770:\tlearn: 0.0887553\ttotal: 1m 15s\tremaining: 22.3s\n",
      "771:\tlearn: 0.0887528\ttotal: 1m 15s\tremaining: 22.2s\n",
      "772:\tlearn: 0.0887100\ttotal: 1m 15s\tremaining: 22.1s\n",
      "773:\tlearn: 0.0886850\ttotal: 1m 15s\tremaining: 22s\n",
      "774:\tlearn: 0.0886306\ttotal: 1m 15s\tremaining: 21.9s\n",
      "775:\tlearn: 0.0885762\ttotal: 1m 15s\tremaining: 21.8s\n",
      "776:\tlearn: 0.0885551\ttotal: 1m 15s\tremaining: 21.7s\n",
      "777:\tlearn: 0.0884973\ttotal: 1m 15s\tremaining: 21.6s\n",
      "778:\tlearn: 0.0884624\ttotal: 1m 15s\tremaining: 21.5s\n",
      "779:\tlearn: 0.0884329\ttotal: 1m 15s\tremaining: 21.4s\n",
      "780:\tlearn: 0.0883726\ttotal: 1m 16s\tremaining: 21.3s\n",
      "781:\tlearn: 0.0883501\ttotal: 1m 16s\tremaining: 21.2s\n",
      "782:\tlearn: 0.0883023\ttotal: 1m 16s\tremaining: 21.1s\n",
      "783:\tlearn: 0.0882461\ttotal: 1m 16s\tremaining: 21s\n",
      "784:\tlearn: 0.0882135\ttotal: 1m 16s\tremaining: 20.9s\n",
      "785:\tlearn: 0.0881875\ttotal: 1m 16s\tremaining: 20.8s\n",
      "786:\tlearn: 0.0881398\ttotal: 1m 16s\tremaining: 20.7s\n",
      "787:\tlearn: 0.0881252\ttotal: 1m 16s\tremaining: 20.6s\n",
      "788:\tlearn: 0.0881102\ttotal: 1m 16s\tremaining: 20.5s\n",
      "789:\tlearn: 0.0880761\ttotal: 1m 16s\tremaining: 20.4s\n",
      "790:\tlearn: 0.0880679\ttotal: 1m 16s\tremaining: 20.3s\n",
      "791:\tlearn: 0.0880031\ttotal: 1m 17s\tremaining: 20.2s\n",
      "792:\tlearn: 0.0879792\ttotal: 1m 17s\tremaining: 20.1s\n",
      "793:\tlearn: 0.0879500\ttotal: 1m 17s\tremaining: 20s\n",
      "794:\tlearn: 0.0879113\ttotal: 1m 17s\tremaining: 19.9s\n",
      "795:\tlearn: 0.0878496\ttotal: 1m 17s\tremaining: 19.9s\n",
      "796:\tlearn: 0.0878167\ttotal: 1m 17s\tremaining: 19.8s\n",
      "797:\tlearn: 0.0877706\ttotal: 1m 17s\tremaining: 19.7s\n",
      "798:\tlearn: 0.0877317\ttotal: 1m 17s\tremaining: 19.6s\n",
      "799:\tlearn: 0.0876804\ttotal: 1m 17s\tremaining: 19.5s\n",
      "800:\tlearn: 0.0875981\ttotal: 1m 17s\tremaining: 19.4s\n",
      "801:\tlearn: 0.0875397\ttotal: 1m 18s\tremaining: 19.3s\n",
      "802:\tlearn: 0.0875099\ttotal: 1m 18s\tremaining: 19.2s\n",
      "803:\tlearn: 0.0874639\ttotal: 1m 18s\tremaining: 19.1s\n",
      "804:\tlearn: 0.0874333\ttotal: 1m 18s\tremaining: 19s\n",
      "805:\tlearn: 0.0873800\ttotal: 1m 18s\tremaining: 18.9s\n",
      "806:\tlearn: 0.0873603\ttotal: 1m 18s\tremaining: 18.8s\n",
      "807:\tlearn: 0.0873175\ttotal: 1m 18s\tremaining: 18.7s\n",
      "808:\tlearn: 0.0872650\ttotal: 1m 18s\tremaining: 18.6s\n",
      "809:\tlearn: 0.0872146\ttotal: 1m 18s\tremaining: 18.5s\n",
      "810:\tlearn: 0.0871979\ttotal: 1m 18s\tremaining: 18.4s\n",
      "811:\tlearn: 0.0871585\ttotal: 1m 19s\tremaining: 18.3s\n",
      "812:\tlearn: 0.0871138\ttotal: 1m 19s\tremaining: 18.2s\n",
      "813:\tlearn: 0.0870591\ttotal: 1m 19s\tremaining: 18.1s\n",
      "814:\tlearn: 0.0869760\ttotal: 1m 19s\tremaining: 18s\n",
      "815:\tlearn: 0.0868909\ttotal: 1m 19s\tremaining: 17.9s\n",
      "816:\tlearn: 0.0868353\ttotal: 1m 19s\tremaining: 17.8s\n",
      "817:\tlearn: 0.0868092\ttotal: 1m 19s\tremaining: 17.7s\n",
      "818:\tlearn: 0.0867720\ttotal: 1m 19s\tremaining: 17.6s\n",
      "819:\tlearn: 0.0867048\ttotal: 1m 19s\tremaining: 17.5s\n",
      "820:\tlearn: 0.0866692\ttotal: 1m 19s\tremaining: 17.4s\n",
      "821:\tlearn: 0.0866411\ttotal: 1m 19s\tremaining: 17.3s\n",
      "822:\tlearn: 0.0866114\ttotal: 1m 20s\tremaining: 17.2s\n",
      "823:\tlearn: 0.0865458\ttotal: 1m 20s\tremaining: 17.1s\n",
      "824:\tlearn: 0.0865330\ttotal: 1m 20s\tremaining: 17s\n",
      "825:\tlearn: 0.0865094\ttotal: 1m 20s\tremaining: 16.9s\n",
      "826:\tlearn: 0.0864632\ttotal: 1m 20s\tremaining: 16.8s\n",
      "827:\tlearn: 0.0864417\ttotal: 1m 20s\tremaining: 16.7s\n",
      "828:\tlearn: 0.0863862\ttotal: 1m 20s\tremaining: 16.6s\n",
      "829:\tlearn: 0.0863553\ttotal: 1m 20s\tremaining: 16.5s\n",
      "830:\tlearn: 0.0863196\ttotal: 1m 20s\tremaining: 16.4s\n",
      "831:\tlearn: 0.0862798\ttotal: 1m 20s\tremaining: 16.3s\n",
      "832:\tlearn: 0.0862313\ttotal: 1m 21s\tremaining: 16.2s\n",
      "833:\tlearn: 0.0862040\ttotal: 1m 21s\tremaining: 16.1s\n",
      "834:\tlearn: 0.0861628\ttotal: 1m 21s\tremaining: 16.1s\n",
      "835:\tlearn: 0.0861286\ttotal: 1m 21s\tremaining: 16s\n",
      "836:\tlearn: 0.0860798\ttotal: 1m 21s\tremaining: 15.9s\n",
      "837:\tlearn: 0.0860447\ttotal: 1m 21s\tremaining: 15.8s\n",
      "838:\tlearn: 0.0860167\ttotal: 1m 21s\tremaining: 15.7s\n",
      "839:\tlearn: 0.0859995\ttotal: 1m 21s\tremaining: 15.6s\n",
      "840:\tlearn: 0.0859493\ttotal: 1m 21s\tremaining: 15.5s\n",
      "841:\tlearn: 0.0859006\ttotal: 1m 22s\tremaining: 15.4s\n",
      "842:\tlearn: 0.0858559\ttotal: 1m 22s\tremaining: 15.3s\n",
      "843:\tlearn: 0.0858183\ttotal: 1m 22s\tremaining: 15.2s\n",
      "844:\tlearn: 0.0857898\ttotal: 1m 22s\tremaining: 15.1s\n",
      "845:\tlearn: 0.0857495\ttotal: 1m 22s\tremaining: 15s\n",
      "846:\tlearn: 0.0857337\ttotal: 1m 22s\tremaining: 14.9s\n",
      "847:\tlearn: 0.0856939\ttotal: 1m 22s\tremaining: 14.8s\n",
      "848:\tlearn: 0.0856560\ttotal: 1m 22s\tremaining: 14.7s\n",
      "849:\tlearn: 0.0856257\ttotal: 1m 22s\tremaining: 14.6s\n",
      "850:\tlearn: 0.0855721\ttotal: 1m 22s\tremaining: 14.5s\n",
      "851:\tlearn: 0.0855038\ttotal: 1m 23s\tremaining: 14.4s\n",
      "852:\tlearn: 0.0854352\ttotal: 1m 23s\tremaining: 14.3s\n",
      "853:\tlearn: 0.0854168\ttotal: 1m 23s\tremaining: 14.2s\n",
      "854:\tlearn: 0.0853916\ttotal: 1m 23s\tremaining: 14.1s\n",
      "855:\tlearn: 0.0853702\ttotal: 1m 23s\tremaining: 14s\n",
      "856:\tlearn: 0.0853265\ttotal: 1m 23s\tremaining: 13.9s\n",
      "857:\tlearn: 0.0852338\ttotal: 1m 23s\tremaining: 13.8s\n",
      "858:\tlearn: 0.0852052\ttotal: 1m 23s\tremaining: 13.7s\n",
      "859:\tlearn: 0.0851692\ttotal: 1m 23s\tremaining: 13.6s\n",
      "860:\tlearn: 0.0851256\ttotal: 1m 23s\tremaining: 13.5s\n",
      "861:\tlearn: 0.0850633\ttotal: 1m 23s\tremaining: 13.4s\n",
      "862:\tlearn: 0.0850110\ttotal: 1m 24s\tremaining: 13.3s\n",
      "863:\tlearn: 0.0849932\ttotal: 1m 24s\tremaining: 13.2s\n",
      "864:\tlearn: 0.0849428\ttotal: 1m 24s\tremaining: 13.1s\n",
      "865:\tlearn: 0.0848883\ttotal: 1m 24s\tremaining: 13s\n",
      "866:\tlearn: 0.0848320\ttotal: 1m 24s\tremaining: 13s\n",
      "867:\tlearn: 0.0847992\ttotal: 1m 24s\tremaining: 12.9s\n",
      "868:\tlearn: 0.0847429\ttotal: 1m 24s\tremaining: 12.8s\n",
      "869:\tlearn: 0.0847029\ttotal: 1m 24s\tremaining: 12.7s\n",
      "870:\tlearn: 0.0846868\ttotal: 1m 24s\tremaining: 12.6s\n",
      "871:\tlearn: 0.0846543\ttotal: 1m 24s\tremaining: 12.5s\n",
      "872:\tlearn: 0.0846193\ttotal: 1m 24s\tremaining: 12.4s\n",
      "873:\tlearn: 0.0845928\ttotal: 1m 25s\tremaining: 12.3s\n",
      "874:\tlearn: 0.0845448\ttotal: 1m 25s\tremaining: 12.2s\n",
      "875:\tlearn: 0.0845223\ttotal: 1m 25s\tremaining: 12.1s\n",
      "876:\tlearn: 0.0844718\ttotal: 1m 25s\tremaining: 12s\n",
      "877:\tlearn: 0.0844383\ttotal: 1m 25s\tremaining: 11.9s\n",
      "878:\tlearn: 0.0844146\ttotal: 1m 25s\tremaining: 11.8s\n",
      "879:\tlearn: 0.0843985\ttotal: 1m 25s\tremaining: 11.7s\n",
      "880:\tlearn: 0.0843768\ttotal: 1m 25s\tremaining: 11.6s\n",
      "881:\tlearn: 0.0843109\ttotal: 1m 25s\tremaining: 11.5s\n",
      "882:\tlearn: 0.0842590\ttotal: 1m 25s\tremaining: 11.4s\n",
      "883:\tlearn: 0.0842166\ttotal: 1m 26s\tremaining: 11.3s\n",
      "884:\tlearn: 0.0841655\ttotal: 1m 26s\tremaining: 11.2s\n",
      "885:\tlearn: 0.0841257\ttotal: 1m 26s\tremaining: 11.1s\n",
      "886:\tlearn: 0.0840654\ttotal: 1m 26s\tremaining: 11s\n",
      "887:\tlearn: 0.0840329\ttotal: 1m 26s\tremaining: 10.9s\n",
      "888:\tlearn: 0.0840329\ttotal: 1m 26s\tremaining: 10.8s\n",
      "889:\tlearn: 0.0839770\ttotal: 1m 26s\tremaining: 10.7s\n",
      "890:\tlearn: 0.0839545\ttotal: 1m 26s\tremaining: 10.6s\n",
      "891:\tlearn: 0.0839030\ttotal: 1m 26s\tremaining: 10.5s\n",
      "892:\tlearn: 0.0838786\ttotal: 1m 26s\tremaining: 10.4s\n",
      "893:\tlearn: 0.0838238\ttotal: 1m 27s\tremaining: 10.3s\n",
      "894:\tlearn: 0.0838125\ttotal: 1m 27s\tremaining: 10.2s\n",
      "895:\tlearn: 0.0837976\ttotal: 1m 27s\tremaining: 10.1s\n",
      "896:\tlearn: 0.0837728\ttotal: 1m 27s\tremaining: 10s\n",
      "897:\tlearn: 0.0837276\ttotal: 1m 27s\tremaining: 9.93s\n",
      "898:\tlearn: 0.0836914\ttotal: 1m 27s\tremaining: 9.83s\n",
      "899:\tlearn: 0.0836363\ttotal: 1m 27s\tremaining: 9.73s\n",
      "900:\tlearn: 0.0836126\ttotal: 1m 27s\tremaining: 9.63s\n",
      "901:\tlearn: 0.0835736\ttotal: 1m 27s\tremaining: 9.54s\n",
      "902:\tlearn: 0.0835612\ttotal: 1m 27s\tremaining: 9.44s\n",
      "903:\tlearn: 0.0835279\ttotal: 1m 27s\tremaining: 9.34s\n",
      "904:\tlearn: 0.0834971\ttotal: 1m 28s\tremaining: 9.24s\n",
      "905:\tlearn: 0.0834487\ttotal: 1m 28s\tremaining: 9.15s\n",
      "906:\tlearn: 0.0834079\ttotal: 1m 28s\tremaining: 9.05s\n",
      "907:\tlearn: 0.0833896\ttotal: 1m 28s\tremaining: 8.95s\n",
      "908:\tlearn: 0.0833366\ttotal: 1m 28s\tremaining: 8.85s\n",
      "909:\tlearn: 0.0832746\ttotal: 1m 28s\tremaining: 8.76s\n",
      "910:\tlearn: 0.0832493\ttotal: 1m 28s\tremaining: 8.66s\n",
      "911:\tlearn: 0.0832261\ttotal: 1m 28s\tremaining: 8.56s\n",
      "912:\tlearn: 0.0831808\ttotal: 1m 28s\tremaining: 8.46s\n",
      "913:\tlearn: 0.0831697\ttotal: 1m 28s\tremaining: 8.37s\n",
      "914:\tlearn: 0.0831285\ttotal: 1m 29s\tremaining: 8.27s\n",
      "915:\tlearn: 0.0831185\ttotal: 1m 29s\tremaining: 8.17s\n",
      "916:\tlearn: 0.0830861\ttotal: 1m 29s\tremaining: 8.07s\n",
      "917:\tlearn: 0.0830597\ttotal: 1m 29s\tremaining: 7.97s\n",
      "918:\tlearn: 0.0830202\ttotal: 1m 29s\tremaining: 7.88s\n",
      "919:\tlearn: 0.0829947\ttotal: 1m 29s\tremaining: 7.78s\n",
      "920:\tlearn: 0.0829801\ttotal: 1m 29s\tremaining: 7.68s\n",
      "921:\tlearn: 0.0829598\ttotal: 1m 29s\tremaining: 7.58s\n",
      "922:\tlearn: 0.0829211\ttotal: 1m 29s\tremaining: 7.49s\n",
      "923:\tlearn: 0.0828566\ttotal: 1m 29s\tremaining: 7.39s\n",
      "924:\tlearn: 0.0828198\ttotal: 1m 29s\tremaining: 7.29s\n",
      "925:\tlearn: 0.0827822\ttotal: 1m 30s\tremaining: 7.2s\n",
      "926:\tlearn: 0.0827541\ttotal: 1m 30s\tremaining: 7.1s\n",
      "927:\tlearn: 0.0827390\ttotal: 1m 30s\tremaining: 7s\n",
      "928:\tlearn: 0.0826856\ttotal: 1m 30s\tremaining: 6.9s\n",
      "929:\tlearn: 0.0826460\ttotal: 1m 30s\tremaining: 6.8s\n",
      "930:\tlearn: 0.0825962\ttotal: 1m 30s\tremaining: 6.71s\n",
      "931:\tlearn: 0.0825322\ttotal: 1m 30s\tremaining: 6.61s\n",
      "932:\tlearn: 0.0824746\ttotal: 1m 30s\tremaining: 6.51s\n",
      "933:\tlearn: 0.0824422\ttotal: 1m 30s\tremaining: 6.42s\n",
      "934:\tlearn: 0.0823927\ttotal: 1m 30s\tremaining: 6.32s\n",
      "935:\tlearn: 0.0823597\ttotal: 1m 31s\tremaining: 6.22s\n",
      "936:\tlearn: 0.0823478\ttotal: 1m 31s\tremaining: 6.12s\n",
      "937:\tlearn: 0.0823014\ttotal: 1m 31s\tremaining: 6.03s\n",
      "938:\tlearn: 0.0822396\ttotal: 1m 31s\tremaining: 5.94s\n",
      "939:\tlearn: 0.0821560\ttotal: 1m 31s\tremaining: 5.84s\n",
      "940:\tlearn: 0.0821245\ttotal: 1m 31s\tremaining: 5.74s\n",
      "941:\tlearn: 0.0820913\ttotal: 1m 31s\tremaining: 5.64s\n",
      "942:\tlearn: 0.0820449\ttotal: 1m 31s\tremaining: 5.55s\n",
      "943:\tlearn: 0.0820052\ttotal: 1m 31s\tremaining: 5.45s\n",
      "944:\tlearn: 0.0819635\ttotal: 1m 32s\tremaining: 5.35s\n",
      "945:\tlearn: 0.0819127\ttotal: 1m 32s\tremaining: 5.26s\n",
      "946:\tlearn: 0.0818708\ttotal: 1m 32s\tremaining: 5.16s\n",
      "947:\tlearn: 0.0818362\ttotal: 1m 32s\tremaining: 5.06s\n",
      "948:\tlearn: 0.0818216\ttotal: 1m 32s\tremaining: 4.96s\n",
      "949:\tlearn: 0.0817785\ttotal: 1m 32s\tremaining: 4.87s\n",
      "950:\tlearn: 0.0817456\ttotal: 1m 32s\tremaining: 4.77s\n",
      "951:\tlearn: 0.0817131\ttotal: 1m 32s\tremaining: 4.67s\n",
      "952:\tlearn: 0.0816924\ttotal: 1m 32s\tremaining: 4.58s\n",
      "953:\tlearn: 0.0816717\ttotal: 1m 32s\tremaining: 4.48s\n",
      "954:\tlearn: 0.0816484\ttotal: 1m 32s\tremaining: 4.38s\n",
      "955:\tlearn: 0.0815842\ttotal: 1m 33s\tremaining: 4.28s\n",
      "956:\tlearn: 0.0815408\ttotal: 1m 33s\tremaining: 4.18s\n",
      "957:\tlearn: 0.0815180\ttotal: 1m 33s\tremaining: 4.09s\n",
      "958:\tlearn: 0.0814931\ttotal: 1m 33s\tremaining: 3.99s\n",
      "959:\tlearn: 0.0814700\ttotal: 1m 33s\tremaining: 3.89s\n",
      "960:\tlearn: 0.0814452\ttotal: 1m 33s\tremaining: 3.79s\n",
      "961:\tlearn: 0.0813897\ttotal: 1m 33s\tremaining: 3.7s\n",
      "962:\tlearn: 0.0813728\ttotal: 1m 33s\tremaining: 3.6s\n",
      "963:\tlearn: 0.0813506\ttotal: 1m 33s\tremaining: 3.5s\n",
      "964:\tlearn: 0.0813274\ttotal: 1m 33s\tremaining: 3.41s\n",
      "965:\tlearn: 0.0813172\ttotal: 1m 33s\tremaining: 3.31s\n",
      "966:\tlearn: 0.0812696\ttotal: 1m 34s\tremaining: 3.21s\n",
      "967:\tlearn: 0.0812535\ttotal: 1m 34s\tremaining: 3.11s\n",
      "968:\tlearn: 0.0812192\ttotal: 1m 34s\tremaining: 3.02s\n",
      "969:\tlearn: 0.0811859\ttotal: 1m 34s\tremaining: 2.92s\n",
      "970:\tlearn: 0.0811561\ttotal: 1m 34s\tremaining: 2.82s\n",
      "971:\tlearn: 0.0811038\ttotal: 1m 34s\tremaining: 2.72s\n",
      "972:\tlearn: 0.0810386\ttotal: 1m 34s\tremaining: 2.63s\n",
      "973:\tlearn: 0.0810154\ttotal: 1m 34s\tremaining: 2.53s\n",
      "974:\tlearn: 0.0809596\ttotal: 1m 34s\tremaining: 2.43s\n",
      "975:\tlearn: 0.0809224\ttotal: 1m 34s\tremaining: 2.33s\n",
      "976:\tlearn: 0.0808828\ttotal: 1m 35s\tremaining: 2.24s\n",
      "977:\tlearn: 0.0808228\ttotal: 1m 35s\tremaining: 2.14s\n",
      "978:\tlearn: 0.0807972\ttotal: 1m 35s\tremaining: 2.04s\n",
      "979:\tlearn: 0.0807559\ttotal: 1m 35s\tremaining: 1.95s\n",
      "980:\tlearn: 0.0807050\ttotal: 1m 35s\tremaining: 1.85s\n",
      "981:\tlearn: 0.0806688\ttotal: 1m 35s\tremaining: 1.75s\n",
      "982:\tlearn: 0.0806022\ttotal: 1m 35s\tremaining: 1.65s\n",
      "983:\tlearn: 0.0805649\ttotal: 1m 35s\tremaining: 1.56s\n",
      "984:\tlearn: 0.0805124\ttotal: 1m 35s\tremaining: 1.46s\n",
      "985:\tlearn: 0.0804644\ttotal: 1m 35s\tremaining: 1.36s\n",
      "986:\tlearn: 0.0804398\ttotal: 1m 36s\tremaining: 1.26s\n",
      "987:\tlearn: 0.0803963\ttotal: 1m 36s\tremaining: 1.17s\n",
      "988:\tlearn: 0.0803384\ttotal: 1m 36s\tremaining: 1.07s\n",
      "989:\tlearn: 0.0802596\ttotal: 1m 36s\tremaining: 973ms\n",
      "990:\tlearn: 0.0802293\ttotal: 1m 36s\tremaining: 876ms\n",
      "991:\tlearn: 0.0802097\ttotal: 1m 36s\tremaining: 778ms\n",
      "992:\tlearn: 0.0801766\ttotal: 1m 36s\tremaining: 681ms\n",
      "993:\tlearn: 0.0801464\ttotal: 1m 36s\tremaining: 584ms\n",
      "994:\tlearn: 0.0801145\ttotal: 1m 36s\tremaining: 487ms\n",
      "995:\tlearn: 0.0800984\ttotal: 1m 36s\tremaining: 389ms\n",
      "996:\tlearn: 0.0800487\ttotal: 1m 37s\tremaining: 292ms\n",
      "997:\tlearn: 0.0800181\ttotal: 1m 37s\tremaining: 195ms\n",
      "998:\tlearn: 0.0799809\ttotal: 1m 37s\tremaining: 97.3ms\n",
      "999:\tlearn: 0.0799424\ttotal: 1m 37s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5202879\ttotal: 95ms\tremaining: 1m 34s\n",
      "1:\tlearn: 0.4233036\ttotal: 194ms\tremaining: 1m 36s\n",
      "2:\tlearn: 0.3584844\ttotal: 290ms\tremaining: 1m 36s\n",
      "3:\tlearn: 0.3223838\ttotal: 385ms\tremaining: 1m 35s\n",
      "4:\tlearn: 0.2949049\ttotal: 486ms\tremaining: 1m 36s\n",
      "5:\tlearn: 0.2779582\ttotal: 573ms\tremaining: 1m 34s\n",
      "6:\tlearn: 0.2574987\ttotal: 665ms\tremaining: 1m 34s\n",
      "7:\tlearn: 0.2471196\ttotal: 763ms\tremaining: 1m 34s\n",
      "8:\tlearn: 0.2386418\ttotal: 854ms\tremaining: 1m 34s\n",
      "9:\tlearn: 0.2260424\ttotal: 947ms\tremaining: 1m 33s\n",
      "10:\tlearn: 0.2174175\ttotal: 1.04s\tremaining: 1m 33s\n",
      "11:\tlearn: 0.2134280\ttotal: 1.13s\tremaining: 1m 33s\n",
      "12:\tlearn: 0.2101899\ttotal: 1.22s\tremaining: 1m 32s\n",
      "13:\tlearn: 0.2047555\ttotal: 1.31s\tremaining: 1m 32s\n",
      "14:\tlearn: 0.2024618\ttotal: 1.41s\tremaining: 1m 32s\n",
      "15:\tlearn: 0.2004523\ttotal: 1.51s\tremaining: 1m 32s\n",
      "16:\tlearn: 0.1979402\ttotal: 1.6s\tremaining: 1m 32s\n",
      "17:\tlearn: 0.1941637\ttotal: 1.7s\tremaining: 1m 32s\n",
      "18:\tlearn: 0.1920484\ttotal: 1.79s\tremaining: 1m 32s\n",
      "19:\tlearn: 0.1900425\ttotal: 1.89s\tremaining: 1m 32s\n",
      "20:\tlearn: 0.1873963\ttotal: 1.99s\tremaining: 1m 32s\n",
      "21:\tlearn: 0.1861834\ttotal: 2.08s\tremaining: 1m 32s\n",
      "22:\tlearn: 0.1846827\ttotal: 2.17s\tremaining: 1m 32s\n",
      "23:\tlearn: 0.1824957\ttotal: 2.27s\tremaining: 1m 32s\n",
      "24:\tlearn: 0.1809935\ttotal: 2.36s\tremaining: 1m 32s\n",
      "25:\tlearn: 0.1796033\ttotal: 2.45s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1781335\ttotal: 2.54s\tremaining: 1m 31s\n",
      "27:\tlearn: 0.1757106\ttotal: 2.63s\tremaining: 1m 31s\n",
      "28:\tlearn: 0.1745203\ttotal: 2.74s\tremaining: 1m 31s\n",
      "29:\tlearn: 0.1732687\ttotal: 2.83s\tremaining: 1m 31s\n",
      "30:\tlearn: 0.1717565\ttotal: 2.93s\tremaining: 1m 31s\n",
      "31:\tlearn: 0.1710146\ttotal: 3.03s\tremaining: 1m 31s\n",
      "32:\tlearn: 0.1699484\ttotal: 3.12s\tremaining: 1m 31s\n",
      "33:\tlearn: 0.1686906\ttotal: 3.21s\tremaining: 1m 31s\n",
      "34:\tlearn: 0.1680522\ttotal: 3.31s\tremaining: 1m 31s\n",
      "35:\tlearn: 0.1672166\ttotal: 3.4s\tremaining: 1m 31s\n",
      "36:\tlearn: 0.1664976\ttotal: 3.5s\tremaining: 1m 30s\n",
      "37:\tlearn: 0.1654004\ttotal: 3.6s\tremaining: 1m 31s\n",
      "38:\tlearn: 0.1646917\ttotal: 3.69s\tremaining: 1m 30s\n",
      "39:\tlearn: 0.1634072\ttotal: 3.78s\tremaining: 1m 30s\n",
      "40:\tlearn: 0.1625236\ttotal: 3.87s\tremaining: 1m 30s\n",
      "41:\tlearn: 0.1618867\ttotal: 3.96s\tremaining: 1m 30s\n",
      "42:\tlearn: 0.1612370\ttotal: 4.06s\tremaining: 1m 30s\n",
      "43:\tlearn: 0.1605362\ttotal: 4.16s\tremaining: 1m 30s\n",
      "44:\tlearn: 0.1599859\ttotal: 4.25s\tremaining: 1m 30s\n",
      "45:\tlearn: 0.1592201\ttotal: 4.34s\tremaining: 1m 30s\n",
      "46:\tlearn: 0.1587677\ttotal: 4.46s\tremaining: 1m 30s\n",
      "47:\tlearn: 0.1581722\ttotal: 4.55s\tremaining: 1m 30s\n",
      "48:\tlearn: 0.1573938\ttotal: 4.65s\tremaining: 1m 30s\n",
      "49:\tlearn: 0.1566059\ttotal: 4.75s\tremaining: 1m 30s\n",
      "50:\tlearn: 0.1557287\ttotal: 4.84s\tremaining: 1m 30s\n",
      "51:\tlearn: 0.1551930\ttotal: 4.94s\tremaining: 1m 30s\n",
      "52:\tlearn: 0.1546719\ttotal: 5.04s\tremaining: 1m 29s\n",
      "53:\tlearn: 0.1542653\ttotal: 5.13s\tremaining: 1m 29s\n",
      "54:\tlearn: 0.1538120\ttotal: 5.23s\tremaining: 1m 29s\n",
      "55:\tlearn: 0.1534961\ttotal: 5.32s\tremaining: 1m 29s\n",
      "56:\tlearn: 0.1527439\ttotal: 5.42s\tremaining: 1m 29s\n",
      "57:\tlearn: 0.1520798\ttotal: 5.51s\tremaining: 1m 29s\n",
      "58:\tlearn: 0.1517715\ttotal: 5.61s\tremaining: 1m 29s\n",
      "59:\tlearn: 0.1514190\ttotal: 5.7s\tremaining: 1m 29s\n",
      "60:\tlearn: 0.1510939\ttotal: 5.79s\tremaining: 1m 29s\n",
      "61:\tlearn: 0.1507517\ttotal: 5.89s\tremaining: 1m 29s\n",
      "62:\tlearn: 0.1504627\ttotal: 5.98s\tremaining: 1m 28s\n",
      "63:\tlearn: 0.1501569\ttotal: 6.07s\tremaining: 1m 28s\n",
      "64:\tlearn: 0.1497491\ttotal: 6.17s\tremaining: 1m 28s\n",
      "65:\tlearn: 0.1493173\ttotal: 6.26s\tremaining: 1m 28s\n",
      "66:\tlearn: 0.1488112\ttotal: 6.36s\tremaining: 1m 28s\n",
      "67:\tlearn: 0.1485700\ttotal: 6.45s\tremaining: 1m 28s\n",
      "68:\tlearn: 0.1482102\ttotal: 6.55s\tremaining: 1m 28s\n",
      "69:\tlearn: 0.1476959\ttotal: 6.63s\tremaining: 1m 28s\n",
      "70:\tlearn: 0.1471820\ttotal: 6.73s\tremaining: 1m 28s\n",
      "71:\tlearn: 0.1468548\ttotal: 6.83s\tremaining: 1m 28s\n",
      "72:\tlearn: 0.1463526\ttotal: 6.93s\tremaining: 1m 27s\n",
      "73:\tlearn: 0.1460063\ttotal: 7.03s\tremaining: 1m 27s\n",
      "74:\tlearn: 0.1456899\ttotal: 7.12s\tremaining: 1m 27s\n",
      "75:\tlearn: 0.1453766\ttotal: 7.21s\tremaining: 1m 27s\n",
      "76:\tlearn: 0.1450181\ttotal: 7.31s\tremaining: 1m 27s\n",
      "77:\tlearn: 0.1447886\ttotal: 7.4s\tremaining: 1m 27s\n",
      "78:\tlearn: 0.1444716\ttotal: 7.5s\tremaining: 1m 27s\n",
      "79:\tlearn: 0.1442052\ttotal: 7.59s\tremaining: 1m 27s\n",
      "80:\tlearn: 0.1438696\ttotal: 7.69s\tremaining: 1m 27s\n",
      "81:\tlearn: 0.1436511\ttotal: 7.78s\tremaining: 1m 27s\n",
      "82:\tlearn: 0.1433436\ttotal: 7.88s\tremaining: 1m 27s\n",
      "83:\tlearn: 0.1429985\ttotal: 7.98s\tremaining: 1m 27s\n",
      "84:\tlearn: 0.1427558\ttotal: 8.07s\tremaining: 1m 26s\n",
      "85:\tlearn: 0.1424134\ttotal: 8.16s\tremaining: 1m 26s\n",
      "86:\tlearn: 0.1422554\ttotal: 8.26s\tremaining: 1m 26s\n",
      "87:\tlearn: 0.1419037\ttotal: 8.35s\tremaining: 1m 26s\n",
      "88:\tlearn: 0.1416295\ttotal: 8.54s\tremaining: 1m 27s\n",
      "89:\tlearn: 0.1414106\ttotal: 8.65s\tremaining: 1m 27s\n",
      "90:\tlearn: 0.1410792\ttotal: 8.75s\tremaining: 1m 27s\n",
      "91:\tlearn: 0.1408227\ttotal: 8.85s\tremaining: 1m 27s\n",
      "92:\tlearn: 0.1405724\ttotal: 8.94s\tremaining: 1m 27s\n",
      "93:\tlearn: 0.1404036\ttotal: 9.06s\tremaining: 1m 27s\n",
      "94:\tlearn: 0.1400766\ttotal: 9.15s\tremaining: 1m 27s\n",
      "95:\tlearn: 0.1397773\ttotal: 9.24s\tremaining: 1m 27s\n",
      "96:\tlearn: 0.1393887\ttotal: 9.35s\tremaining: 1m 27s\n",
      "97:\tlearn: 0.1392482\ttotal: 9.44s\tremaining: 1m 26s\n",
      "98:\tlearn: 0.1388778\ttotal: 9.53s\tremaining: 1m 26s\n",
      "99:\tlearn: 0.1386668\ttotal: 9.63s\tremaining: 1m 26s\n",
      "100:\tlearn: 0.1383983\ttotal: 9.73s\tremaining: 1m 26s\n",
      "101:\tlearn: 0.1381434\ttotal: 9.82s\tremaining: 1m 26s\n",
      "102:\tlearn: 0.1377997\ttotal: 9.91s\tremaining: 1m 26s\n",
      "103:\tlearn: 0.1375880\ttotal: 10s\tremaining: 1m 26s\n",
      "104:\tlearn: 0.1374422\ttotal: 10.1s\tremaining: 1m 26s\n",
      "105:\tlearn: 0.1372619\ttotal: 10.2s\tremaining: 1m 26s\n",
      "106:\tlearn: 0.1369503\ttotal: 10.3s\tremaining: 1m 25s\n",
      "107:\tlearn: 0.1366777\ttotal: 10.4s\tremaining: 1m 25s\n",
      "108:\tlearn: 0.1363947\ttotal: 10.5s\tremaining: 1m 25s\n",
      "109:\tlearn: 0.1362098\ttotal: 10.6s\tremaining: 1m 25s\n",
      "110:\tlearn: 0.1359351\ttotal: 10.7s\tremaining: 1m 25s\n",
      "111:\tlearn: 0.1357820\ttotal: 10.8s\tremaining: 1m 25s\n",
      "112:\tlearn: 0.1355856\ttotal: 10.9s\tremaining: 1m 25s\n",
      "113:\tlearn: 0.1354284\ttotal: 11s\tremaining: 1m 25s\n",
      "114:\tlearn: 0.1352133\ttotal: 11.1s\tremaining: 1m 25s\n",
      "115:\tlearn: 0.1349720\ttotal: 11.2s\tremaining: 1m 25s\n",
      "116:\tlearn: 0.1347276\ttotal: 11.3s\tremaining: 1m 24s\n",
      "117:\tlearn: 0.1345699\ttotal: 11.4s\tremaining: 1m 24s\n",
      "118:\tlearn: 0.1344476\ttotal: 11.5s\tremaining: 1m 24s\n",
      "119:\tlearn: 0.1342577\ttotal: 11.6s\tremaining: 1m 24s\n",
      "120:\tlearn: 0.1341392\ttotal: 11.6s\tremaining: 1m 24s\n",
      "121:\tlearn: 0.1339629\ttotal: 11.7s\tremaining: 1m 24s\n",
      "122:\tlearn: 0.1337657\ttotal: 11.8s\tremaining: 1m 24s\n",
      "123:\tlearn: 0.1335919\ttotal: 11.9s\tremaining: 1m 24s\n",
      "124:\tlearn: 0.1334004\ttotal: 12s\tremaining: 1m 24s\n",
      "125:\tlearn: 0.1332171\ttotal: 12.1s\tremaining: 1m 24s\n",
      "126:\tlearn: 0.1330186\ttotal: 12.2s\tremaining: 1m 24s\n",
      "127:\tlearn: 0.1328003\ttotal: 12.3s\tremaining: 1m 24s\n",
      "128:\tlearn: 0.1326064\ttotal: 12.5s\tremaining: 1m 24s\n",
      "129:\tlearn: 0.1324480\ttotal: 12.6s\tremaining: 1m 24s\n",
      "130:\tlearn: 0.1323101\ttotal: 12.7s\tremaining: 1m 23s\n",
      "131:\tlearn: 0.1322080\ttotal: 12.8s\tremaining: 1m 23s\n",
      "132:\tlearn: 0.1319428\ttotal: 12.8s\tremaining: 1m 23s\n",
      "133:\tlearn: 0.1318032\ttotal: 12.9s\tremaining: 1m 23s\n",
      "134:\tlearn: 0.1316087\ttotal: 13s\tremaining: 1m 23s\n",
      "135:\tlearn: 0.1314734\ttotal: 13.1s\tremaining: 1m 23s\n",
      "136:\tlearn: 0.1313295\ttotal: 13.2s\tremaining: 1m 23s\n",
      "137:\tlearn: 0.1311329\ttotal: 13.3s\tremaining: 1m 23s\n",
      "138:\tlearn: 0.1309771\ttotal: 13.4s\tremaining: 1m 23s\n",
      "139:\tlearn: 0.1307498\ttotal: 13.5s\tremaining: 1m 22s\n",
      "140:\tlearn: 0.1304703\ttotal: 13.6s\tremaining: 1m 23s\n",
      "141:\tlearn: 0.1303216\ttotal: 13.7s\tremaining: 1m 22s\n",
      "142:\tlearn: 0.1301026\ttotal: 13.8s\tremaining: 1m 22s\n",
      "143:\tlearn: 0.1298645\ttotal: 13.9s\tremaining: 1m 22s\n",
      "144:\tlearn: 0.1296306\ttotal: 14s\tremaining: 1m 22s\n",
      "145:\tlearn: 0.1295015\ttotal: 14.1s\tremaining: 1m 22s\n",
      "146:\tlearn: 0.1293417\ttotal: 14.2s\tremaining: 1m 22s\n",
      "147:\tlearn: 0.1291925\ttotal: 14.3s\tremaining: 1m 22s\n",
      "148:\tlearn: 0.1290488\ttotal: 14.4s\tremaining: 1m 22s\n",
      "149:\tlearn: 0.1288731\ttotal: 14.5s\tremaining: 1m 22s\n",
      "150:\tlearn: 0.1287745\ttotal: 14.6s\tremaining: 1m 21s\n",
      "151:\tlearn: 0.1286384\ttotal: 14.7s\tremaining: 1m 21s\n",
      "152:\tlearn: 0.1285143\ttotal: 14.8s\tremaining: 1m 21s\n",
      "153:\tlearn: 0.1283022\ttotal: 14.9s\tremaining: 1m 21s\n",
      "154:\tlearn: 0.1280641\ttotal: 15s\tremaining: 1m 21s\n",
      "155:\tlearn: 0.1279204\ttotal: 15.1s\tremaining: 1m 21s\n",
      "156:\tlearn: 0.1277804\ttotal: 15.2s\tremaining: 1m 21s\n",
      "157:\tlearn: 0.1276370\ttotal: 15.3s\tremaining: 1m 21s\n",
      "158:\tlearn: 0.1275417\ttotal: 15.4s\tremaining: 1m 21s\n",
      "159:\tlearn: 0.1273958\ttotal: 15.4s\tremaining: 1m 21s\n",
      "160:\tlearn: 0.1272892\ttotal: 15.5s\tremaining: 1m 21s\n",
      "161:\tlearn: 0.1271030\ttotal: 15.7s\tremaining: 1m 20s\n",
      "162:\tlearn: 0.1270077\ttotal: 15.7s\tremaining: 1m 20s\n",
      "163:\tlearn: 0.1268669\ttotal: 15.8s\tremaining: 1m 20s\n",
      "164:\tlearn: 0.1267244\ttotal: 15.9s\tremaining: 1m 20s\n",
      "165:\tlearn: 0.1266211\ttotal: 16s\tremaining: 1m 20s\n",
      "166:\tlearn: 0.1264340\ttotal: 16.1s\tremaining: 1m 20s\n",
      "167:\tlearn: 0.1262935\ttotal: 16.2s\tremaining: 1m 20s\n",
      "168:\tlearn: 0.1261899\ttotal: 16.3s\tremaining: 1m 20s\n",
      "169:\tlearn: 0.1260848\ttotal: 16.4s\tremaining: 1m 20s\n",
      "170:\tlearn: 0.1259992\ttotal: 16.5s\tremaining: 1m 20s\n",
      "171:\tlearn: 0.1258176\ttotal: 16.6s\tremaining: 1m 19s\n",
      "172:\tlearn: 0.1256927\ttotal: 16.7s\tremaining: 1m 19s\n",
      "173:\tlearn: 0.1255513\ttotal: 16.8s\tremaining: 1m 19s\n",
      "174:\tlearn: 0.1254354\ttotal: 16.9s\tremaining: 1m 19s\n",
      "175:\tlearn: 0.1252639\ttotal: 17s\tremaining: 1m 19s\n",
      "176:\tlearn: 0.1251941\ttotal: 17.1s\tremaining: 1m 19s\n",
      "177:\tlearn: 0.1251105\ttotal: 17.2s\tremaining: 1m 19s\n",
      "178:\tlearn: 0.1249704\ttotal: 17.3s\tremaining: 1m 19s\n",
      "179:\tlearn: 0.1248085\ttotal: 17.4s\tremaining: 1m 19s\n",
      "180:\tlearn: 0.1246514\ttotal: 17.5s\tremaining: 1m 19s\n",
      "181:\tlearn: 0.1245686\ttotal: 17.5s\tremaining: 1m 18s\n",
      "182:\tlearn: 0.1244722\ttotal: 17.6s\tremaining: 1m 18s\n",
      "183:\tlearn: 0.1243482\ttotal: 17.7s\tremaining: 1m 18s\n",
      "184:\tlearn: 0.1242375\ttotal: 17.8s\tremaining: 1m 18s\n",
      "185:\tlearn: 0.1241323\ttotal: 18s\tremaining: 1m 18s\n",
      "186:\tlearn: 0.1239512\ttotal: 18.1s\tremaining: 1m 18s\n",
      "187:\tlearn: 0.1238090\ttotal: 18.2s\tremaining: 1m 18s\n",
      "188:\tlearn: 0.1236879\ttotal: 18.3s\tremaining: 1m 18s\n",
      "189:\tlearn: 0.1235623\ttotal: 18.3s\tremaining: 1m 18s\n",
      "190:\tlearn: 0.1234524\ttotal: 18.4s\tremaining: 1m 18s\n",
      "191:\tlearn: 0.1233731\ttotal: 18.5s\tremaining: 1m 18s\n",
      "192:\tlearn: 0.1232854\ttotal: 18.6s\tremaining: 1m 17s\n",
      "193:\tlearn: 0.1232227\ttotal: 18.7s\tremaining: 1m 17s\n",
      "194:\tlearn: 0.1231062\ttotal: 18.8s\tremaining: 1m 17s\n",
      "195:\tlearn: 0.1229690\ttotal: 19s\tremaining: 1m 18s\n",
      "196:\tlearn: 0.1228751\ttotal: 19.1s\tremaining: 1m 17s\n",
      "197:\tlearn: 0.1227930\ttotal: 19.2s\tremaining: 1m 17s\n",
      "198:\tlearn: 0.1227288\ttotal: 19.3s\tremaining: 1m 17s\n",
      "199:\tlearn: 0.1225978\ttotal: 19.4s\tremaining: 1m 17s\n",
      "200:\tlearn: 0.1224208\ttotal: 19.5s\tremaining: 1m 17s\n",
      "201:\tlearn: 0.1223269\ttotal: 19.6s\tremaining: 1m 17s\n",
      "202:\tlearn: 0.1222359\ttotal: 19.7s\tremaining: 1m 17s\n",
      "203:\tlearn: 0.1221697\ttotal: 19.8s\tremaining: 1m 17s\n",
      "204:\tlearn: 0.1220919\ttotal: 19.9s\tremaining: 1m 17s\n",
      "205:\tlearn: 0.1220124\ttotal: 20s\tremaining: 1m 17s\n",
      "206:\tlearn: 0.1219422\ttotal: 20.1s\tremaining: 1m 17s\n",
      "207:\tlearn: 0.1218490\ttotal: 20.2s\tremaining: 1m 16s\n",
      "208:\tlearn: 0.1217777\ttotal: 20.3s\tremaining: 1m 16s\n",
      "209:\tlearn: 0.1216703\ttotal: 20.4s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1215809\ttotal: 20.5s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1215300\ttotal: 20.6s\tremaining: 1m 16s\n",
      "212:\tlearn: 0.1214200\ttotal: 20.7s\tremaining: 1m 16s\n",
      "213:\tlearn: 0.1213492\ttotal: 20.8s\tremaining: 1m 16s\n",
      "214:\tlearn: 0.1212171\ttotal: 20.9s\tremaining: 1m 16s\n",
      "215:\tlearn: 0.1211333\ttotal: 20.9s\tremaining: 1m 16s\n",
      "216:\tlearn: 0.1209849\ttotal: 21s\tremaining: 1m 15s\n",
      "217:\tlearn: 0.1208524\ttotal: 21.1s\tremaining: 1m 15s\n",
      "218:\tlearn: 0.1207570\ttotal: 21.2s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1206696\ttotal: 21.3s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1205814\ttotal: 21.4s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.1205084\ttotal: 21.5s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.1204526\ttotal: 21.6s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.1203039\ttotal: 21.7s\tremaining: 1m 15s\n",
      "224:\tlearn: 0.1201944\ttotal: 21.8s\tremaining: 1m 15s\n",
      "225:\tlearn: 0.1201260\ttotal: 21.9s\tremaining: 1m 15s\n",
      "226:\tlearn: 0.1200675\ttotal: 22s\tremaining: 1m 14s\n",
      "227:\tlearn: 0.1199782\ttotal: 22.1s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1198787\ttotal: 22.2s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1197470\ttotal: 22.3s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1195795\ttotal: 22.4s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1195441\ttotal: 22.5s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1194321\ttotal: 22.6s\tremaining: 1m 14s\n",
      "233:\tlearn: 0.1193388\ttotal: 22.7s\tremaining: 1m 14s\n",
      "234:\tlearn: 0.1192196\ttotal: 22.8s\tremaining: 1m 14s\n",
      "235:\tlearn: 0.1191617\ttotal: 22.9s\tremaining: 1m 14s\n",
      "236:\tlearn: 0.1190994\ttotal: 23s\tremaining: 1m 13s\n",
      "237:\tlearn: 0.1190339\ttotal: 23.1s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1189300\ttotal: 23.2s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1188731\ttotal: 23.3s\tremaining: 1m 13s\n",
      "240:\tlearn: 0.1187465\ttotal: 23.4s\tremaining: 1m 13s\n",
      "241:\tlearn: 0.1185917\ttotal: 23.5s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1184509\ttotal: 23.6s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1183171\ttotal: 23.7s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1182474\ttotal: 23.8s\tremaining: 1m 13s\n",
      "245:\tlearn: 0.1181438\ttotal: 23.9s\tremaining: 1m 13s\n",
      "246:\tlearn: 0.1180360\ttotal: 24s\tremaining: 1m 13s\n",
      "247:\tlearn: 0.1179527\ttotal: 24.1s\tremaining: 1m 13s\n",
      "248:\tlearn: 0.1178847\ttotal: 24.2s\tremaining: 1m 12s\n",
      "249:\tlearn: 0.1177990\ttotal: 24.3s\tremaining: 1m 12s\n",
      "250:\tlearn: 0.1177112\ttotal: 24.4s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.1176156\ttotal: 24.5s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.1175522\ttotal: 24.6s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.1174925\ttotal: 24.7s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.1173682\ttotal: 24.8s\tremaining: 1m 12s\n",
      "255:\tlearn: 0.1172844\ttotal: 24.8s\tremaining: 1m 12s\n",
      "256:\tlearn: 0.1172253\ttotal: 24.9s\tremaining: 1m 12s\n",
      "257:\tlearn: 0.1171145\ttotal: 25s\tremaining: 1m 12s\n",
      "258:\tlearn: 0.1169486\ttotal: 25.1s\tremaining: 1m 11s\n",
      "259:\tlearn: 0.1168518\ttotal: 25.3s\tremaining: 1m 11s\n",
      "260:\tlearn: 0.1167141\ttotal: 25.4s\tremaining: 1m 11s\n",
      "261:\tlearn: 0.1166056\ttotal: 25.5s\tremaining: 1m 11s\n",
      "262:\tlearn: 0.1165067\ttotal: 25.6s\tremaining: 1m 11s\n",
      "263:\tlearn: 0.1163971\ttotal: 25.7s\tremaining: 1m 11s\n",
      "264:\tlearn: 0.1163305\ttotal: 25.8s\tremaining: 1m 11s\n",
      "265:\tlearn: 0.1162906\ttotal: 25.9s\tremaining: 1m 11s\n",
      "266:\tlearn: 0.1161653\ttotal: 26s\tremaining: 1m 11s\n",
      "267:\tlearn: 0.1161059\ttotal: 26s\tremaining: 1m 11s\n",
      "268:\tlearn: 0.1160543\ttotal: 26.1s\tremaining: 1m 11s\n",
      "269:\tlearn: 0.1159719\ttotal: 26.2s\tremaining: 1m 10s\n",
      "270:\tlearn: 0.1159439\ttotal: 26.3s\tremaining: 1m 10s\n",
      "271:\tlearn: 0.1158694\ttotal: 26.4s\tremaining: 1m 10s\n",
      "272:\tlearn: 0.1157780\ttotal: 26.5s\tremaining: 1m 10s\n",
      "273:\tlearn: 0.1156777\ttotal: 26.6s\tremaining: 1m 10s\n",
      "274:\tlearn: 0.1155581\ttotal: 26.7s\tremaining: 1m 10s\n",
      "275:\tlearn: 0.1154564\ttotal: 26.8s\tremaining: 1m 10s\n",
      "276:\tlearn: 0.1153883\ttotal: 26.9s\tremaining: 1m 10s\n",
      "277:\tlearn: 0.1153035\ttotal: 27s\tremaining: 1m 10s\n",
      "278:\tlearn: 0.1152216\ttotal: 27.1s\tremaining: 1m 10s\n",
      "279:\tlearn: 0.1151446\ttotal: 27.2s\tremaining: 1m 9s\n",
      "280:\tlearn: 0.1150910\ttotal: 27.3s\tremaining: 1m 9s\n",
      "281:\tlearn: 0.1149703\ttotal: 27.4s\tremaining: 1m 9s\n",
      "282:\tlearn: 0.1149283\ttotal: 27.5s\tremaining: 1m 9s\n",
      "283:\tlearn: 0.1148753\ttotal: 27.6s\tremaining: 1m 9s\n",
      "284:\tlearn: 0.1148054\ttotal: 27.7s\tremaining: 1m 9s\n",
      "285:\tlearn: 0.1147372\ttotal: 27.8s\tremaining: 1m 9s\n",
      "286:\tlearn: 0.1146632\ttotal: 27.9s\tremaining: 1m 9s\n",
      "287:\tlearn: 0.1145697\ttotal: 28s\tremaining: 1m 9s\n",
      "288:\tlearn: 0.1144803\ttotal: 28.1s\tremaining: 1m 9s\n",
      "289:\tlearn: 0.1143799\ttotal: 28.2s\tremaining: 1m 8s\n",
      "290:\tlearn: 0.1142951\ttotal: 28.3s\tremaining: 1m 8s\n",
      "291:\tlearn: 0.1141846\ttotal: 28.4s\tremaining: 1m 8s\n",
      "292:\tlearn: 0.1141160\ttotal: 28.5s\tremaining: 1m 8s\n",
      "293:\tlearn: 0.1140380\ttotal: 28.6s\tremaining: 1m 8s\n",
      "294:\tlearn: 0.1139657\ttotal: 28.7s\tremaining: 1m 8s\n",
      "295:\tlearn: 0.1139092\ttotal: 28.8s\tremaining: 1m 8s\n",
      "296:\tlearn: 0.1138104\ttotal: 29s\tremaining: 1m 8s\n",
      "297:\tlearn: 0.1137226\ttotal: 29.1s\tremaining: 1m 8s\n",
      "298:\tlearn: 0.1136778\ttotal: 29.2s\tremaining: 1m 8s\n",
      "299:\tlearn: 0.1136180\ttotal: 29.3s\tremaining: 1m 8s\n",
      "300:\tlearn: 0.1135102\ttotal: 29.4s\tremaining: 1m 8s\n",
      "301:\tlearn: 0.1134408\ttotal: 29.5s\tremaining: 1m 8s\n",
      "302:\tlearn: 0.1133739\ttotal: 29.6s\tremaining: 1m 8s\n",
      "303:\tlearn: 0.1132882\ttotal: 29.7s\tremaining: 1m 8s\n",
      "304:\tlearn: 0.1132080\ttotal: 29.8s\tremaining: 1m 7s\n",
      "305:\tlearn: 0.1131298\ttotal: 29.9s\tremaining: 1m 7s\n",
      "306:\tlearn: 0.1130438\ttotal: 30s\tremaining: 1m 7s\n",
      "307:\tlearn: 0.1129823\ttotal: 30.1s\tremaining: 1m 7s\n",
      "308:\tlearn: 0.1129176\ttotal: 30.2s\tremaining: 1m 7s\n",
      "309:\tlearn: 0.1128365\ttotal: 30.4s\tremaining: 1m 7s\n",
      "310:\tlearn: 0.1127855\ttotal: 30.5s\tremaining: 1m 7s\n",
      "311:\tlearn: 0.1127301\ttotal: 30.6s\tremaining: 1m 7s\n",
      "312:\tlearn: 0.1126855\ttotal: 30.7s\tremaining: 1m 7s\n",
      "313:\tlearn: 0.1126111\ttotal: 30.8s\tremaining: 1m 7s\n",
      "314:\tlearn: 0.1125252\ttotal: 30.9s\tremaining: 1m 7s\n",
      "315:\tlearn: 0.1124428\ttotal: 31s\tremaining: 1m 7s\n",
      "316:\tlearn: 0.1123970\ttotal: 31.1s\tremaining: 1m 6s\n",
      "317:\tlearn: 0.1122772\ttotal: 31.2s\tremaining: 1m 6s\n",
      "318:\tlearn: 0.1121640\ttotal: 31.3s\tremaining: 1m 6s\n",
      "319:\tlearn: 0.1120998\ttotal: 31.4s\tremaining: 1m 6s\n",
      "320:\tlearn: 0.1120326\ttotal: 31.5s\tremaining: 1m 6s\n",
      "321:\tlearn: 0.1119446\ttotal: 31.6s\tremaining: 1m 6s\n",
      "322:\tlearn: 0.1118592\ttotal: 31.7s\tremaining: 1m 6s\n",
      "323:\tlearn: 0.1118031\ttotal: 31.8s\tremaining: 1m 6s\n",
      "324:\tlearn: 0.1117431\ttotal: 31.9s\tremaining: 1m 6s\n",
      "325:\tlearn: 0.1116729\ttotal: 32s\tremaining: 1m 6s\n",
      "326:\tlearn: 0.1115677\ttotal: 32.1s\tremaining: 1m 6s\n",
      "327:\tlearn: 0.1115249\ttotal: 32.2s\tremaining: 1m 6s\n",
      "328:\tlearn: 0.1114121\ttotal: 32.3s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1113030\ttotal: 32.4s\tremaining: 1m 5s\n",
      "330:\tlearn: 0.1112434\ttotal: 32.5s\tremaining: 1m 5s\n",
      "331:\tlearn: 0.1111463\ttotal: 32.7s\tremaining: 1m 5s\n",
      "332:\tlearn: 0.1111192\ttotal: 32.8s\tremaining: 1m 5s\n",
      "333:\tlearn: 0.1110348\ttotal: 32.9s\tremaining: 1m 5s\n",
      "334:\tlearn: 0.1109454\ttotal: 33s\tremaining: 1m 5s\n",
      "335:\tlearn: 0.1108782\ttotal: 33.1s\tremaining: 1m 5s\n",
      "336:\tlearn: 0.1107875\ttotal: 33.2s\tremaining: 1m 5s\n",
      "337:\tlearn: 0.1106851\ttotal: 33.3s\tremaining: 1m 5s\n",
      "338:\tlearn: 0.1105861\ttotal: 33.4s\tremaining: 1m 5s\n",
      "339:\tlearn: 0.1105106\ttotal: 33.5s\tremaining: 1m 5s\n",
      "340:\tlearn: 0.1104357\ttotal: 33.6s\tremaining: 1m 4s\n",
      "341:\tlearn: 0.1103875\ttotal: 33.7s\tremaining: 1m 4s\n",
      "342:\tlearn: 0.1103168\ttotal: 33.8s\tremaining: 1m 4s\n",
      "343:\tlearn: 0.1102018\ttotal: 33.9s\tremaining: 1m 4s\n",
      "344:\tlearn: 0.1101294\ttotal: 34s\tremaining: 1m 4s\n",
      "345:\tlearn: 0.1100577\ttotal: 34.1s\tremaining: 1m 4s\n",
      "346:\tlearn: 0.1099968\ttotal: 34.2s\tremaining: 1m 4s\n",
      "347:\tlearn: 0.1099362\ttotal: 34.3s\tremaining: 1m 4s\n",
      "348:\tlearn: 0.1098737\ttotal: 34.4s\tremaining: 1m 4s\n",
      "349:\tlearn: 0.1098092\ttotal: 34.5s\tremaining: 1m 4s\n",
      "350:\tlearn: 0.1097635\ttotal: 34.6s\tremaining: 1m 4s\n",
      "351:\tlearn: 0.1097189\ttotal: 34.7s\tremaining: 1m 3s\n",
      "352:\tlearn: 0.1096532\ttotal: 34.8s\tremaining: 1m 3s\n",
      "353:\tlearn: 0.1095847\ttotal: 34.9s\tremaining: 1m 3s\n",
      "354:\tlearn: 0.1095200\ttotal: 35s\tremaining: 1m 3s\n",
      "355:\tlearn: 0.1094191\ttotal: 35.1s\tremaining: 1m 3s\n",
      "356:\tlearn: 0.1093657\ttotal: 35.2s\tremaining: 1m 3s\n",
      "357:\tlearn: 0.1093055\ttotal: 35.5s\tremaining: 1m 3s\n",
      "358:\tlearn: 0.1092443\ttotal: 35.6s\tremaining: 1m 3s\n",
      "359:\tlearn: 0.1092005\ttotal: 35.8s\tremaining: 1m 3s\n",
      "360:\tlearn: 0.1091248\ttotal: 35.9s\tremaining: 1m 3s\n",
      "361:\tlearn: 0.1090493\ttotal: 36s\tremaining: 1m 3s\n",
      "362:\tlearn: 0.1090259\ttotal: 36s\tremaining: 1m 3s\n",
      "363:\tlearn: 0.1089916\ttotal: 36.1s\tremaining: 1m 3s\n",
      "364:\tlearn: 0.1088839\ttotal: 36.2s\tremaining: 1m 3s\n",
      "365:\tlearn: 0.1087817\ttotal: 36.4s\tremaining: 1m 2s\n",
      "366:\tlearn: 0.1087012\ttotal: 36.5s\tremaining: 1m 2s\n",
      "367:\tlearn: 0.1086499\ttotal: 36.6s\tremaining: 1m 2s\n",
      "368:\tlearn: 0.1085936\ttotal: 36.7s\tremaining: 1m 2s\n",
      "369:\tlearn: 0.1085467\ttotal: 36.8s\tremaining: 1m 2s\n",
      "370:\tlearn: 0.1084983\ttotal: 36.9s\tremaining: 1m 2s\n",
      "371:\tlearn: 0.1084301\ttotal: 37s\tremaining: 1m 2s\n",
      "372:\tlearn: 0.1083620\ttotal: 37.1s\tremaining: 1m 2s\n",
      "373:\tlearn: 0.1083142\ttotal: 37.2s\tremaining: 1m 2s\n",
      "374:\tlearn: 0.1082587\ttotal: 37.3s\tremaining: 1m 2s\n",
      "375:\tlearn: 0.1082202\ttotal: 37.4s\tremaining: 1m 2s\n",
      "376:\tlearn: 0.1081757\ttotal: 37.5s\tremaining: 1m 1s\n",
      "377:\tlearn: 0.1081284\ttotal: 37.6s\tremaining: 1m 1s\n",
      "378:\tlearn: 0.1080837\ttotal: 37.7s\tremaining: 1m 1s\n",
      "379:\tlearn: 0.1080105\ttotal: 37.8s\tremaining: 1m 1s\n",
      "380:\tlearn: 0.1079709\ttotal: 37.9s\tremaining: 1m 1s\n",
      "381:\tlearn: 0.1078845\ttotal: 38s\tremaining: 1m 1s\n",
      "382:\tlearn: 0.1078084\ttotal: 38.1s\tremaining: 1m 1s\n",
      "383:\tlearn: 0.1077160\ttotal: 38.2s\tremaining: 1m 1s\n",
      "384:\tlearn: 0.1076457\ttotal: 38.3s\tremaining: 1m 1s\n",
      "385:\tlearn: 0.1075944\ttotal: 38.4s\tremaining: 1m 1s\n",
      "386:\tlearn: 0.1075337\ttotal: 38.5s\tremaining: 1m 1s\n",
      "387:\tlearn: 0.1074716\ttotal: 38.6s\tremaining: 1m\n",
      "388:\tlearn: 0.1074126\ttotal: 38.7s\tremaining: 1m\n",
      "389:\tlearn: 0.1073613\ttotal: 38.8s\tremaining: 1m\n",
      "390:\tlearn: 0.1072946\ttotal: 39s\tremaining: 1m\n",
      "391:\tlearn: 0.1071978\ttotal: 39.1s\tremaining: 1m\n",
      "392:\tlearn: 0.1071514\ttotal: 39.2s\tremaining: 1m\n",
      "393:\tlearn: 0.1070677\ttotal: 39.3s\tremaining: 1m\n",
      "394:\tlearn: 0.1069788\ttotal: 39.4s\tremaining: 1m\n",
      "395:\tlearn: 0.1069103\ttotal: 39.5s\tremaining: 1m\n",
      "396:\tlearn: 0.1068366\ttotal: 39.6s\tremaining: 1m\n",
      "397:\tlearn: 0.1067630\ttotal: 39.7s\tremaining: 1m\n",
      "398:\tlearn: 0.1066849\ttotal: 39.8s\tremaining: 1m\n",
      "399:\tlearn: 0.1066019\ttotal: 40s\tremaining: 59.9s\n",
      "400:\tlearn: 0.1065090\ttotal: 40.1s\tremaining: 59.8s\n",
      "401:\tlearn: 0.1064396\ttotal: 40.1s\tremaining: 59.7s\n",
      "402:\tlearn: 0.1063973\ttotal: 40.3s\tremaining: 59.6s\n",
      "403:\tlearn: 0.1063481\ttotal: 40.3s\tremaining: 59.5s\n",
      "404:\tlearn: 0.1062770\ttotal: 40.4s\tremaining: 59.4s\n",
      "405:\tlearn: 0.1062303\ttotal: 40.5s\tremaining: 59.3s\n",
      "406:\tlearn: 0.1061704\ttotal: 40.6s\tremaining: 59.2s\n",
      "407:\tlearn: 0.1061245\ttotal: 40.7s\tremaining: 59.1s\n",
      "408:\tlearn: 0.1060988\ttotal: 40.9s\tremaining: 59.1s\n",
      "409:\tlearn: 0.1060247\ttotal: 41s\tremaining: 59s\n",
      "410:\tlearn: 0.1059980\ttotal: 41.1s\tremaining: 58.9s\n",
      "411:\tlearn: 0.1059226\ttotal: 41.2s\tremaining: 58.8s\n",
      "412:\tlearn: 0.1058734\ttotal: 41.3s\tremaining: 58.8s\n",
      "413:\tlearn: 0.1058276\ttotal: 41.5s\tremaining: 58.7s\n",
      "414:\tlearn: 0.1057688\ttotal: 41.6s\tremaining: 58.6s\n",
      "415:\tlearn: 0.1057226\ttotal: 41.7s\tremaining: 58.5s\n",
      "416:\tlearn: 0.1056749\ttotal: 41.8s\tremaining: 58.4s\n",
      "417:\tlearn: 0.1056240\ttotal: 41.9s\tremaining: 58.3s\n",
      "418:\tlearn: 0.1055933\ttotal: 42s\tremaining: 58.3s\n",
      "419:\tlearn: 0.1055730\ttotal: 42.2s\tremaining: 58.3s\n",
      "420:\tlearn: 0.1055468\ttotal: 42.3s\tremaining: 58.2s\n",
      "421:\tlearn: 0.1054895\ttotal: 42.4s\tremaining: 58.1s\n",
      "422:\tlearn: 0.1054299\ttotal: 42.5s\tremaining: 58s\n",
      "423:\tlearn: 0.1053506\ttotal: 42.6s\tremaining: 57.9s\n",
      "424:\tlearn: 0.1052914\ttotal: 42.7s\tremaining: 57.8s\n",
      "425:\tlearn: 0.1052305\ttotal: 42.8s\tremaining: 57.7s\n",
      "426:\tlearn: 0.1051949\ttotal: 42.9s\tremaining: 57.6s\n",
      "427:\tlearn: 0.1051494\ttotal: 43s\tremaining: 57.5s\n",
      "428:\tlearn: 0.1051117\ttotal: 43.1s\tremaining: 57.4s\n",
      "429:\tlearn: 0.1050474\ttotal: 43.2s\tremaining: 57.3s\n",
      "430:\tlearn: 0.1049814\ttotal: 43.3s\tremaining: 57.2s\n",
      "431:\tlearn: 0.1049244\ttotal: 43.5s\tremaining: 57.1s\n",
      "432:\tlearn: 0.1048423\ttotal: 43.6s\tremaining: 57s\n",
      "433:\tlearn: 0.1047374\ttotal: 43.7s\tremaining: 56.9s\n",
      "434:\tlearn: 0.1046912\ttotal: 43.7s\tremaining: 56.8s\n",
      "435:\tlearn: 0.1046606\ttotal: 43.8s\tremaining: 56.7s\n",
      "436:\tlearn: 0.1046121\ttotal: 43.9s\tremaining: 56.6s\n",
      "437:\tlearn: 0.1045662\ttotal: 44s\tremaining: 56.5s\n",
      "438:\tlearn: 0.1045301\ttotal: 44.1s\tremaining: 56.4s\n",
      "439:\tlearn: 0.1044950\ttotal: 44.2s\tremaining: 56.3s\n",
      "440:\tlearn: 0.1044465\ttotal: 44.3s\tremaining: 56.2s\n",
      "441:\tlearn: 0.1043102\ttotal: 44.4s\tremaining: 56.1s\n",
      "442:\tlearn: 0.1042005\ttotal: 44.5s\tremaining: 56s\n",
      "443:\tlearn: 0.1041277\ttotal: 44.6s\tremaining: 55.9s\n",
      "444:\tlearn: 0.1040635\ttotal: 44.7s\tremaining: 55.8s\n",
      "445:\tlearn: 0.1039909\ttotal: 44.8s\tremaining: 55.7s\n",
      "446:\tlearn: 0.1039576\ttotal: 44.9s\tremaining: 55.6s\n",
      "447:\tlearn: 0.1039084\ttotal: 45s\tremaining: 55.5s\n",
      "448:\tlearn: 0.1038744\ttotal: 45.1s\tremaining: 55.4s\n",
      "449:\tlearn: 0.1038494\ttotal: 45.2s\tremaining: 55.2s\n",
      "450:\tlearn: 0.1037860\ttotal: 45.3s\tremaining: 55.1s\n",
      "451:\tlearn: 0.1037561\ttotal: 45.4s\tremaining: 55s\n",
      "452:\tlearn: 0.1037093\ttotal: 45.5s\tremaining: 54.9s\n",
      "453:\tlearn: 0.1036297\ttotal: 45.6s\tremaining: 54.8s\n",
      "454:\tlearn: 0.1035982\ttotal: 45.7s\tremaining: 54.7s\n",
      "455:\tlearn: 0.1035144\ttotal: 45.8s\tremaining: 54.7s\n",
      "456:\tlearn: 0.1034706\ttotal: 46s\tremaining: 54.6s\n",
      "457:\tlearn: 0.1034171\ttotal: 46.1s\tremaining: 54.5s\n",
      "458:\tlearn: 0.1033626\ttotal: 46.2s\tremaining: 54.5s\n",
      "459:\tlearn: 0.1032723\ttotal: 46.3s\tremaining: 54.4s\n",
      "460:\tlearn: 0.1031601\ttotal: 46.4s\tremaining: 54.3s\n",
      "461:\tlearn: 0.1031226\ttotal: 46.5s\tremaining: 54.2s\n",
      "462:\tlearn: 0.1030834\ttotal: 46.6s\tremaining: 54.1s\n",
      "463:\tlearn: 0.1030446\ttotal: 46.8s\tremaining: 54.1s\n",
      "464:\tlearn: 0.1029850\ttotal: 46.9s\tremaining: 54s\n",
      "465:\tlearn: 0.1028897\ttotal: 47.1s\tremaining: 54s\n",
      "466:\tlearn: 0.1028639\ttotal: 47.3s\tremaining: 54s\n",
      "467:\tlearn: 0.1028516\ttotal: 47.4s\tremaining: 53.9s\n",
      "468:\tlearn: 0.1028094\ttotal: 47.5s\tremaining: 53.8s\n",
      "469:\tlearn: 0.1027897\ttotal: 47.6s\tremaining: 53.7s\n",
      "470:\tlearn: 0.1027187\ttotal: 47.8s\tremaining: 53.6s\n",
      "471:\tlearn: 0.1026495\ttotal: 47.9s\tremaining: 53.6s\n",
      "472:\tlearn: 0.1025851\ttotal: 48s\tremaining: 53.5s\n",
      "473:\tlearn: 0.1025098\ttotal: 48.2s\tremaining: 53.5s\n",
      "474:\tlearn: 0.1024712\ttotal: 48.3s\tremaining: 53.4s\n",
      "475:\tlearn: 0.1024305\ttotal: 48.4s\tremaining: 53.3s\n",
      "476:\tlearn: 0.1024087\ttotal: 48.5s\tremaining: 53.1s\n",
      "477:\tlearn: 0.1023255\ttotal: 48.6s\tremaining: 53s\n",
      "478:\tlearn: 0.1022617\ttotal: 48.7s\tremaining: 53s\n",
      "479:\tlearn: 0.1022078\ttotal: 48.8s\tremaining: 52.9s\n",
      "480:\tlearn: 0.1021759\ttotal: 48.9s\tremaining: 52.8s\n",
      "481:\tlearn: 0.1021272\ttotal: 49s\tremaining: 52.7s\n",
      "482:\tlearn: 0.1020570\ttotal: 49.1s\tremaining: 52.6s\n",
      "483:\tlearn: 0.1020120\ttotal: 49.2s\tremaining: 52.5s\n",
      "484:\tlearn: 0.1019934\ttotal: 49.3s\tremaining: 52.4s\n",
      "485:\tlearn: 0.1019420\ttotal: 49.5s\tremaining: 52.3s\n",
      "486:\tlearn: 0.1018721\ttotal: 49.6s\tremaining: 52.2s\n",
      "487:\tlearn: 0.1018246\ttotal: 49.7s\tremaining: 52.1s\n",
      "488:\tlearn: 0.1017632\ttotal: 49.8s\tremaining: 52s\n",
      "489:\tlearn: 0.1016866\ttotal: 49.9s\tremaining: 51.9s\n",
      "490:\tlearn: 0.1016198\ttotal: 50s\tremaining: 51.8s\n",
      "491:\tlearn: 0.1015667\ttotal: 50.1s\tremaining: 51.7s\n",
      "492:\tlearn: 0.1015109\ttotal: 50.2s\tremaining: 51.7s\n",
      "493:\tlearn: 0.1014447\ttotal: 50.3s\tremaining: 51.6s\n",
      "494:\tlearn: 0.1013670\ttotal: 50.4s\tremaining: 51.5s\n",
      "495:\tlearn: 0.1013284\ttotal: 50.5s\tremaining: 51.4s\n",
      "496:\tlearn: 0.1012985\ttotal: 50.6s\tremaining: 51.3s\n",
      "497:\tlearn: 0.1012371\ttotal: 50.8s\tremaining: 51.2s\n",
      "498:\tlearn: 0.1011709\ttotal: 50.9s\tremaining: 51.1s\n",
      "499:\tlearn: 0.1011129\ttotal: 51s\tremaining: 51s\n",
      "500:\tlearn: 0.1010682\ttotal: 51.1s\tremaining: 50.9s\n",
      "501:\tlearn: 0.1010568\ttotal: 51.3s\tremaining: 50.8s\n",
      "502:\tlearn: 0.1009981\ttotal: 51.4s\tremaining: 50.8s\n",
      "503:\tlearn: 0.1009505\ttotal: 51.5s\tremaining: 50.7s\n",
      "504:\tlearn: 0.1009093\ttotal: 51.6s\tremaining: 50.6s\n",
      "505:\tlearn: 0.1008739\ttotal: 51.7s\tremaining: 50.5s\n",
      "506:\tlearn: 0.1008095\ttotal: 51.8s\tremaining: 50.4s\n",
      "507:\tlearn: 0.1007630\ttotal: 51.9s\tremaining: 50.3s\n",
      "508:\tlearn: 0.1007177\ttotal: 52s\tremaining: 50.2s\n",
      "509:\tlearn: 0.1006971\ttotal: 52.2s\tremaining: 50.1s\n",
      "510:\tlearn: 0.1006391\ttotal: 52.3s\tremaining: 50.1s\n",
      "511:\tlearn: 0.1006089\ttotal: 52.4s\tremaining: 50s\n",
      "512:\tlearn: 0.1005440\ttotal: 52.5s\tremaining: 49.8s\n",
      "513:\tlearn: 0.1005097\ttotal: 52.6s\tremaining: 49.8s\n",
      "514:\tlearn: 0.1004431\ttotal: 52.7s\tremaining: 49.7s\n",
      "515:\tlearn: 0.1004166\ttotal: 52.8s\tremaining: 49.6s\n",
      "516:\tlearn: 0.1003514\ttotal: 52.9s\tremaining: 49.5s\n",
      "517:\tlearn: 0.1002334\ttotal: 53s\tremaining: 49.4s\n",
      "518:\tlearn: 0.1001722\ttotal: 53.1s\tremaining: 49.3s\n",
      "519:\tlearn: 0.1001560\ttotal: 53.2s\tremaining: 49.1s\n",
      "520:\tlearn: 0.1001316\ttotal: 53.4s\tremaining: 49.1s\n",
      "521:\tlearn: 0.1000732\ttotal: 53.5s\tremaining: 48.9s\n",
      "522:\tlearn: 0.0999689\ttotal: 53.5s\tremaining: 48.8s\n",
      "523:\tlearn: 0.0999286\ttotal: 53.7s\tremaining: 48.7s\n",
      "524:\tlearn: 0.0998779\ttotal: 53.8s\tremaining: 48.6s\n",
      "525:\tlearn: 0.0998419\ttotal: 53.9s\tremaining: 48.6s\n",
      "526:\tlearn: 0.0998039\ttotal: 54s\tremaining: 48.5s\n",
      "527:\tlearn: 0.0997711\ttotal: 54.1s\tremaining: 48.3s\n",
      "528:\tlearn: 0.0997213\ttotal: 54.2s\tremaining: 48.3s\n",
      "529:\tlearn: 0.0996611\ttotal: 54.3s\tremaining: 48.1s\n",
      "530:\tlearn: 0.0996231\ttotal: 54.4s\tremaining: 48s\n",
      "531:\tlearn: 0.0995576\ttotal: 54.5s\tremaining: 47.9s\n",
      "532:\tlearn: 0.0994983\ttotal: 54.6s\tremaining: 47.9s\n",
      "533:\tlearn: 0.0994420\ttotal: 54.7s\tremaining: 47.8s\n",
      "534:\tlearn: 0.0993978\ttotal: 54.9s\tremaining: 47.7s\n",
      "535:\tlearn: 0.0993676\ttotal: 55s\tremaining: 47.6s\n",
      "536:\tlearn: 0.0993292\ttotal: 55.1s\tremaining: 47.5s\n",
      "537:\tlearn: 0.0992897\ttotal: 55.3s\tremaining: 47.5s\n",
      "538:\tlearn: 0.0992303\ttotal: 55.4s\tremaining: 47.4s\n",
      "539:\tlearn: 0.0991846\ttotal: 55.5s\tremaining: 47.3s\n",
      "540:\tlearn: 0.0991548\ttotal: 55.6s\tremaining: 47.2s\n",
      "541:\tlearn: 0.0990973\ttotal: 55.7s\tremaining: 47.1s\n",
      "542:\tlearn: 0.0990487\ttotal: 55.8s\tremaining: 47s\n",
      "543:\tlearn: 0.0990165\ttotal: 55.9s\tremaining: 46.8s\n",
      "544:\tlearn: 0.0989818\ttotal: 56s\tremaining: 46.7s\n",
      "545:\tlearn: 0.0989544\ttotal: 56.1s\tremaining: 46.6s\n",
      "546:\tlearn: 0.0989144\ttotal: 56.2s\tremaining: 46.6s\n",
      "547:\tlearn: 0.0988441\ttotal: 56.4s\tremaining: 46.5s\n",
      "548:\tlearn: 0.0988188\ttotal: 56.5s\tremaining: 46.4s\n",
      "549:\tlearn: 0.0987599\ttotal: 56.6s\tremaining: 46.3s\n",
      "550:\tlearn: 0.0987347\ttotal: 56.7s\tremaining: 46.2s\n",
      "551:\tlearn: 0.0986112\ttotal: 56.8s\tremaining: 46.1s\n",
      "552:\tlearn: 0.0985955\ttotal: 56.9s\tremaining: 46s\n",
      "553:\tlearn: 0.0985364\ttotal: 57s\tremaining: 45.9s\n",
      "554:\tlearn: 0.0984992\ttotal: 57.2s\tremaining: 45.8s\n",
      "555:\tlearn: 0.0984520\ttotal: 57.3s\tremaining: 45.8s\n",
      "556:\tlearn: 0.0984267\ttotal: 57.4s\tremaining: 45.7s\n",
      "557:\tlearn: 0.0983984\ttotal: 57.5s\tremaining: 45.6s\n",
      "558:\tlearn: 0.0983543\ttotal: 57.6s\tremaining: 45.5s\n",
      "559:\tlearn: 0.0982917\ttotal: 57.7s\tremaining: 45.3s\n",
      "560:\tlearn: 0.0982316\ttotal: 57.8s\tremaining: 45.2s\n",
      "561:\tlearn: 0.0982021\ttotal: 57.9s\tremaining: 45.1s\n",
      "562:\tlearn: 0.0981403\ttotal: 58s\tremaining: 45s\n",
      "563:\tlearn: 0.0980819\ttotal: 58.1s\tremaining: 44.9s\n",
      "564:\tlearn: 0.0980419\ttotal: 58.2s\tremaining: 44.8s\n",
      "565:\tlearn: 0.0980127\ttotal: 58.3s\tremaining: 44.7s\n",
      "566:\tlearn: 0.0979841\ttotal: 58.5s\tremaining: 44.6s\n",
      "567:\tlearn: 0.0979551\ttotal: 58.6s\tremaining: 44.5s\n",
      "568:\tlearn: 0.0978722\ttotal: 58.7s\tremaining: 44.5s\n",
      "569:\tlearn: 0.0978135\ttotal: 58.9s\tremaining: 44.4s\n",
      "570:\tlearn: 0.0977486\ttotal: 59.1s\tremaining: 44.4s\n",
      "571:\tlearn: 0.0976903\ttotal: 59.2s\tremaining: 44.3s\n",
      "572:\tlearn: 0.0976470\ttotal: 59.3s\tremaining: 44.2s\n",
      "573:\tlearn: 0.0976066\ttotal: 59.4s\tremaining: 44.1s\n",
      "574:\tlearn: 0.0975362\ttotal: 59.5s\tremaining: 44s\n",
      "575:\tlearn: 0.0975175\ttotal: 59.6s\tremaining: 43.9s\n",
      "576:\tlearn: 0.0974623\ttotal: 59.7s\tremaining: 43.8s\n",
      "577:\tlearn: 0.0974023\ttotal: 59.8s\tremaining: 43.7s\n",
      "578:\tlearn: 0.0973777\ttotal: 59.9s\tremaining: 43.6s\n",
      "579:\tlearn: 0.0973283\ttotal: 1m\tremaining: 43.5s\n",
      "580:\tlearn: 0.0972728\ttotal: 1m\tremaining: 43.4s\n",
      "581:\tlearn: 0.0972431\ttotal: 1m\tremaining: 43.2s\n",
      "582:\tlearn: 0.0971697\ttotal: 1m\tremaining: 43.2s\n",
      "583:\tlearn: 0.0971254\ttotal: 1m\tremaining: 43.1s\n",
      "584:\tlearn: 0.0970966\ttotal: 1m\tremaining: 43s\n",
      "585:\tlearn: 0.0970490\ttotal: 1m\tremaining: 42.8s\n",
      "586:\tlearn: 0.0970250\ttotal: 1m\tremaining: 42.7s\n",
      "587:\tlearn: 0.0969928\ttotal: 1m\tremaining: 42.6s\n",
      "588:\tlearn: 0.0969317\ttotal: 1m\tremaining: 42.5s\n",
      "589:\tlearn: 0.0968890\ttotal: 1m 1s\tremaining: 42.4s\n",
      "590:\tlearn: 0.0968087\ttotal: 1m 1s\tremaining: 42.3s\n",
      "591:\tlearn: 0.0967654\ttotal: 1m 1s\tremaining: 42.2s\n",
      "592:\tlearn: 0.0966826\ttotal: 1m 1s\tremaining: 42.1s\n",
      "593:\tlearn: 0.0966534\ttotal: 1m 1s\tremaining: 42s\n",
      "594:\tlearn: 0.0966093\ttotal: 1m 1s\tremaining: 41.9s\n",
      "595:\tlearn: 0.0965423\ttotal: 1m 1s\tremaining: 41.8s\n",
      "596:\tlearn: 0.0964747\ttotal: 1m 1s\tremaining: 41.7s\n",
      "597:\tlearn: 0.0964195\ttotal: 1m 1s\tremaining: 41.6s\n",
      "598:\tlearn: 0.0964055\ttotal: 1m 1s\tremaining: 41.5s\n",
      "599:\tlearn: 0.0963747\ttotal: 1m 2s\tremaining: 41.4s\n",
      "600:\tlearn: 0.0963286\ttotal: 1m 2s\tremaining: 41.3s\n",
      "601:\tlearn: 0.0962854\ttotal: 1m 2s\tremaining: 41.2s\n",
      "602:\tlearn: 0.0962474\ttotal: 1m 2s\tremaining: 41.1s\n",
      "603:\tlearn: 0.0962329\ttotal: 1m 2s\tremaining: 41s\n",
      "604:\tlearn: 0.0962036\ttotal: 1m 2s\tremaining: 40.9s\n",
      "605:\tlearn: 0.0961761\ttotal: 1m 2s\tremaining: 40.8s\n",
      "606:\tlearn: 0.0961306\ttotal: 1m 2s\tremaining: 40.7s\n",
      "607:\tlearn: 0.0961111\ttotal: 1m 3s\tremaining: 40.6s\n",
      "608:\tlearn: 0.0960691\ttotal: 1m 3s\tremaining: 40.5s\n",
      "609:\tlearn: 0.0960163\ttotal: 1m 3s\tremaining: 40.4s\n",
      "610:\tlearn: 0.0959849\ttotal: 1m 3s\tremaining: 40.3s\n",
      "611:\tlearn: 0.0959745\ttotal: 1m 3s\tremaining: 40.2s\n",
      "612:\tlearn: 0.0959069\ttotal: 1m 3s\tremaining: 40.1s\n",
      "613:\tlearn: 0.0958691\ttotal: 1m 3s\tremaining: 40s\n",
      "614:\tlearn: 0.0958413\ttotal: 1m 3s\tremaining: 39.9s\n",
      "615:\tlearn: 0.0958053\ttotal: 1m 3s\tremaining: 39.8s\n",
      "616:\tlearn: 0.0957898\ttotal: 1m 4s\tremaining: 39.7s\n",
      "617:\tlearn: 0.0957515\ttotal: 1m 4s\tremaining: 39.7s\n",
      "618:\tlearn: 0.0957222\ttotal: 1m 4s\tremaining: 39.5s\n",
      "619:\tlearn: 0.0956704\ttotal: 1m 4s\tremaining: 39.4s\n",
      "620:\tlearn: 0.0956502\ttotal: 1m 4s\tremaining: 39.3s\n",
      "621:\tlearn: 0.0956242\ttotal: 1m 4s\tremaining: 39.2s\n",
      "622:\tlearn: 0.0955402\ttotal: 1m 4s\tremaining: 39.1s\n",
      "623:\tlearn: 0.0954731\ttotal: 1m 4s\tremaining: 39s\n",
      "624:\tlearn: 0.0953690\ttotal: 1m 4s\tremaining: 38.9s\n",
      "625:\tlearn: 0.0953135\ttotal: 1m 4s\tremaining: 38.8s\n",
      "626:\tlearn: 0.0953015\ttotal: 1m 5s\tremaining: 38.7s\n",
      "627:\tlearn: 0.0952752\ttotal: 1m 5s\tremaining: 38.6s\n",
      "628:\tlearn: 0.0952290\ttotal: 1m 5s\tremaining: 38.5s\n",
      "629:\tlearn: 0.0951999\ttotal: 1m 5s\tremaining: 38.4s\n",
      "630:\tlearn: 0.0951619\ttotal: 1m 5s\tremaining: 38.3s\n",
      "631:\tlearn: 0.0951113\ttotal: 1m 5s\tremaining: 38.2s\n",
      "632:\tlearn: 0.0950917\ttotal: 1m 5s\tremaining: 38.1s\n",
      "633:\tlearn: 0.0950475\ttotal: 1m 5s\tremaining: 38s\n",
      "634:\tlearn: 0.0950097\ttotal: 1m 5s\tremaining: 37.8s\n",
      "635:\tlearn: 0.0949405\ttotal: 1m 5s\tremaining: 37.7s\n",
      "636:\tlearn: 0.0948975\ttotal: 1m 6s\tremaining: 37.6s\n",
      "637:\tlearn: 0.0948659\ttotal: 1m 6s\tremaining: 37.5s\n",
      "638:\tlearn: 0.0948211\ttotal: 1m 6s\tremaining: 37.4s\n",
      "639:\tlearn: 0.0947552\ttotal: 1m 6s\tremaining: 37.3s\n",
      "640:\tlearn: 0.0946921\ttotal: 1m 6s\tremaining: 37.2s\n",
      "641:\tlearn: 0.0946100\ttotal: 1m 6s\tremaining: 37.1s\n",
      "642:\tlearn: 0.0945603\ttotal: 1m 6s\tremaining: 37s\n",
      "643:\tlearn: 0.0944899\ttotal: 1m 6s\tremaining: 36.9s\n",
      "644:\tlearn: 0.0944635\ttotal: 1m 6s\tremaining: 36.8s\n",
      "645:\tlearn: 0.0943775\ttotal: 1m 6s\tremaining: 36.7s\n",
      "646:\tlearn: 0.0943328\ttotal: 1m 7s\tremaining: 36.6s\n",
      "647:\tlearn: 0.0942536\ttotal: 1m 7s\tremaining: 36.5s\n",
      "648:\tlearn: 0.0942012\ttotal: 1m 7s\tremaining: 36.4s\n",
      "649:\tlearn: 0.0941685\ttotal: 1m 7s\tremaining: 36.3s\n",
      "650:\tlearn: 0.0941108\ttotal: 1m 7s\tremaining: 36.2s\n",
      "651:\tlearn: 0.0941067\ttotal: 1m 7s\tremaining: 36.1s\n",
      "652:\tlearn: 0.0940960\ttotal: 1m 7s\tremaining: 36s\n",
      "653:\tlearn: 0.0940707\ttotal: 1m 7s\tremaining: 35.9s\n",
      "654:\tlearn: 0.0940020\ttotal: 1m 8s\tremaining: 35.8s\n",
      "655:\tlearn: 0.0939653\ttotal: 1m 8s\tremaining: 35.7s\n",
      "656:\tlearn: 0.0939446\ttotal: 1m 8s\tremaining: 35.6s\n",
      "657:\tlearn: 0.0939106\ttotal: 1m 8s\tremaining: 35.5s\n",
      "658:\tlearn: 0.0938682\ttotal: 1m 8s\tremaining: 35.4s\n",
      "659:\tlearn: 0.0938046\ttotal: 1m 8s\tremaining: 35.3s\n",
      "660:\tlearn: 0.0937536\ttotal: 1m 8s\tremaining: 35.2s\n",
      "661:\tlearn: 0.0936947\ttotal: 1m 8s\tremaining: 35.1s\n",
      "662:\tlearn: 0.0936237\ttotal: 1m 8s\tremaining: 35s\n",
      "663:\tlearn: 0.0935985\ttotal: 1m 8s\tremaining: 34.9s\n",
      "664:\tlearn: 0.0935660\ttotal: 1m 9s\tremaining: 34.8s\n",
      "665:\tlearn: 0.0935289\ttotal: 1m 9s\tremaining: 34.7s\n",
      "666:\tlearn: 0.0935121\ttotal: 1m 9s\tremaining: 34.6s\n",
      "667:\tlearn: 0.0934773\ttotal: 1m 9s\tremaining: 34.5s\n",
      "668:\tlearn: 0.0934054\ttotal: 1m 9s\tremaining: 34.4s\n",
      "669:\tlearn: 0.0933405\ttotal: 1m 9s\tremaining: 34.3s\n",
      "670:\tlearn: 0.0933001\ttotal: 1m 9s\tremaining: 34.2s\n",
      "671:\tlearn: 0.0932363\ttotal: 1m 9s\tremaining: 34.1s\n",
      "672:\tlearn: 0.0932198\ttotal: 1m 10s\tremaining: 34s\n",
      "673:\tlearn: 0.0931718\ttotal: 1m 10s\tremaining: 33.9s\n",
      "674:\tlearn: 0.0931233\ttotal: 1m 10s\tremaining: 33.8s\n",
      "675:\tlearn: 0.0930840\ttotal: 1m 10s\tremaining: 33.7s\n",
      "676:\tlearn: 0.0930522\ttotal: 1m 10s\tremaining: 33.6s\n",
      "677:\tlearn: 0.0930260\ttotal: 1m 10s\tremaining: 33.5s\n",
      "678:\tlearn: 0.0929311\ttotal: 1m 10s\tremaining: 33.4s\n",
      "679:\tlearn: 0.0928908\ttotal: 1m 10s\tremaining: 33.3s\n",
      "680:\tlearn: 0.0928066\ttotal: 1m 10s\tremaining: 33.2s\n",
      "681:\tlearn: 0.0927777\ttotal: 1m 11s\tremaining: 33.1s\n",
      "682:\tlearn: 0.0927311\ttotal: 1m 11s\tremaining: 33s\n",
      "683:\tlearn: 0.0926918\ttotal: 1m 11s\tremaining: 32.9s\n",
      "684:\tlearn: 0.0926583\ttotal: 1m 11s\tremaining: 32.8s\n",
      "685:\tlearn: 0.0926200\ttotal: 1m 11s\tremaining: 32.8s\n",
      "686:\tlearn: 0.0925984\ttotal: 1m 11s\tremaining: 32.7s\n",
      "687:\tlearn: 0.0925639\ttotal: 1m 11s\tremaining: 32.5s\n",
      "688:\tlearn: 0.0925054\ttotal: 1m 11s\tremaining: 32.4s\n",
      "689:\tlearn: 0.0924581\ttotal: 1m 11s\tremaining: 32.3s\n",
      "690:\tlearn: 0.0924097\ttotal: 1m 12s\tremaining: 32.2s\n",
      "691:\tlearn: 0.0923712\ttotal: 1m 12s\tremaining: 32.1s\n",
      "692:\tlearn: 0.0923367\ttotal: 1m 12s\tremaining: 32s\n",
      "693:\tlearn: 0.0922838\ttotal: 1m 12s\tremaining: 31.9s\n",
      "694:\tlearn: 0.0922674\ttotal: 1m 12s\tremaining: 31.8s\n",
      "695:\tlearn: 0.0922156\ttotal: 1m 12s\tremaining: 31.7s\n",
      "696:\tlearn: 0.0921529\ttotal: 1m 12s\tremaining: 31.6s\n",
      "697:\tlearn: 0.0921155\ttotal: 1m 12s\tremaining: 31.5s\n",
      "698:\tlearn: 0.0920815\ttotal: 1m 12s\tremaining: 31.4s\n",
      "699:\tlearn: 0.0920465\ttotal: 1m 12s\tremaining: 31.3s\n",
      "700:\tlearn: 0.0920050\ttotal: 1m 13s\tremaining: 31.2s\n",
      "701:\tlearn: 0.0919578\ttotal: 1m 13s\tremaining: 31.1s\n",
      "702:\tlearn: 0.0918957\ttotal: 1m 13s\tremaining: 31s\n",
      "703:\tlearn: 0.0918069\ttotal: 1m 13s\tremaining: 30.9s\n",
      "704:\tlearn: 0.0917625\ttotal: 1m 13s\tremaining: 30.8s\n",
      "705:\tlearn: 0.0917528\ttotal: 1m 13s\tremaining: 30.6s\n",
      "706:\tlearn: 0.0917013\ttotal: 1m 13s\tremaining: 30.6s\n",
      "707:\tlearn: 0.0916161\ttotal: 1m 14s\tremaining: 30.5s\n",
      "708:\tlearn: 0.0915640\ttotal: 1m 14s\tremaining: 30.4s\n",
      "709:\tlearn: 0.0915297\ttotal: 1m 14s\tremaining: 30.3s\n",
      "710:\tlearn: 0.0914605\ttotal: 1m 14s\tremaining: 30.2s\n",
      "711:\tlearn: 0.0914177\ttotal: 1m 14s\tremaining: 30.2s\n",
      "712:\tlearn: 0.0913685\ttotal: 1m 14s\tremaining: 30.1s\n",
      "713:\tlearn: 0.0913253\ttotal: 1m 14s\tremaining: 30s\n",
      "714:\tlearn: 0.0912628\ttotal: 1m 14s\tremaining: 29.9s\n",
      "715:\tlearn: 0.0912335\ttotal: 1m 15s\tremaining: 29.8s\n",
      "716:\tlearn: 0.0912103\ttotal: 1m 15s\tremaining: 29.7s\n",
      "717:\tlearn: 0.0911808\ttotal: 1m 15s\tremaining: 29.6s\n",
      "718:\tlearn: 0.0911409\ttotal: 1m 15s\tremaining: 29.4s\n",
      "719:\tlearn: 0.0910928\ttotal: 1m 15s\tremaining: 29.3s\n",
      "720:\tlearn: 0.0910531\ttotal: 1m 15s\tremaining: 29.2s\n",
      "721:\tlearn: 0.0910265\ttotal: 1m 15s\tremaining: 29.1s\n",
      "722:\tlearn: 0.0910051\ttotal: 1m 15s\tremaining: 29s\n",
      "723:\tlearn: 0.0909816\ttotal: 1m 15s\tremaining: 28.9s\n",
      "724:\tlearn: 0.0909676\ttotal: 1m 15s\tremaining: 28.8s\n",
      "725:\tlearn: 0.0909414\ttotal: 1m 16s\tremaining: 28.7s\n",
      "726:\tlearn: 0.0908701\ttotal: 1m 16s\tremaining: 28.6s\n",
      "727:\tlearn: 0.0908116\ttotal: 1m 16s\tremaining: 28.5s\n",
      "728:\tlearn: 0.0907559\ttotal: 1m 16s\tremaining: 28.4s\n",
      "729:\tlearn: 0.0907224\ttotal: 1m 16s\tremaining: 28.3s\n",
      "730:\tlearn: 0.0907091\ttotal: 1m 16s\tremaining: 28.2s\n",
      "731:\tlearn: 0.0906616\ttotal: 1m 16s\tremaining: 28.1s\n",
      "732:\tlearn: 0.0906132\ttotal: 1m 16s\tremaining: 28s\n",
      "733:\tlearn: 0.0905816\ttotal: 1m 16s\tremaining: 27.9s\n",
      "734:\tlearn: 0.0905536\ttotal: 1m 17s\tremaining: 27.8s\n",
      "735:\tlearn: 0.0904917\ttotal: 1m 17s\tremaining: 27.7s\n",
      "736:\tlearn: 0.0904597\ttotal: 1m 17s\tremaining: 27.6s\n",
      "737:\tlearn: 0.0904320\ttotal: 1m 17s\tremaining: 27.5s\n",
      "738:\tlearn: 0.0903927\ttotal: 1m 17s\tremaining: 27.4s\n",
      "739:\tlearn: 0.0903545\ttotal: 1m 17s\tremaining: 27.3s\n",
      "740:\tlearn: 0.0902901\ttotal: 1m 17s\tremaining: 27.2s\n",
      "741:\tlearn: 0.0902699\ttotal: 1m 17s\tremaining: 27.1s\n",
      "742:\tlearn: 0.0902449\ttotal: 1m 18s\tremaining: 27s\n",
      "743:\tlearn: 0.0901986\ttotal: 1m 18s\tremaining: 26.9s\n",
      "744:\tlearn: 0.0901728\ttotal: 1m 18s\tremaining: 26.8s\n",
      "745:\tlearn: 0.0901181\ttotal: 1m 18s\tremaining: 26.7s\n",
      "746:\tlearn: 0.0900709\ttotal: 1m 18s\tremaining: 26.6s\n",
      "747:\tlearn: 0.0900304\ttotal: 1m 18s\tremaining: 26.5s\n",
      "748:\tlearn: 0.0899971\ttotal: 1m 18s\tremaining: 26.4s\n",
      "749:\tlearn: 0.0899646\ttotal: 1m 18s\tremaining: 26.3s\n",
      "750:\tlearn: 0.0899158\ttotal: 1m 18s\tremaining: 26.2s\n",
      "751:\tlearn: 0.0898911\ttotal: 1m 19s\tremaining: 26.1s\n",
      "752:\tlearn: 0.0897966\ttotal: 1m 19s\tremaining: 26s\n",
      "753:\tlearn: 0.0897492\ttotal: 1m 19s\tremaining: 25.8s\n",
      "754:\tlearn: 0.0897102\ttotal: 1m 19s\tremaining: 25.7s\n",
      "755:\tlearn: 0.0896679\ttotal: 1m 19s\tremaining: 25.6s\n",
      "756:\tlearn: 0.0896491\ttotal: 1m 19s\tremaining: 25.5s\n",
      "757:\tlearn: 0.0896233\ttotal: 1m 19s\tremaining: 25.4s\n",
      "758:\tlearn: 0.0895850\ttotal: 1m 19s\tremaining: 25.3s\n",
      "759:\tlearn: 0.0895459\ttotal: 1m 19s\tremaining: 25.2s\n",
      "760:\tlearn: 0.0895293\ttotal: 1m 19s\tremaining: 25.1s\n",
      "761:\tlearn: 0.0894688\ttotal: 1m 19s\tremaining: 25s\n",
      "762:\tlearn: 0.0894685\ttotal: 1m 20s\tremaining: 24.9s\n",
      "763:\tlearn: 0.0894477\ttotal: 1m 20s\tremaining: 24.8s\n",
      "764:\tlearn: 0.0893886\ttotal: 1m 20s\tremaining: 24.7s\n",
      "765:\tlearn: 0.0893262\ttotal: 1m 20s\tremaining: 24.6s\n",
      "766:\tlearn: 0.0892941\ttotal: 1m 20s\tremaining: 24.5s\n",
      "767:\tlearn: 0.0892204\ttotal: 1m 20s\tremaining: 24.4s\n",
      "768:\tlearn: 0.0891957\ttotal: 1m 20s\tremaining: 24.3s\n",
      "769:\tlearn: 0.0891777\ttotal: 1m 20s\tremaining: 24.2s\n",
      "770:\tlearn: 0.0891554\ttotal: 1m 20s\tremaining: 24s\n",
      "771:\tlearn: 0.0890855\ttotal: 1m 21s\tremaining: 23.9s\n",
      "772:\tlearn: 0.0890434\ttotal: 1m 21s\tremaining: 23.8s\n",
      "773:\tlearn: 0.0889934\ttotal: 1m 21s\tremaining: 23.7s\n",
      "774:\tlearn: 0.0889568\ttotal: 1m 21s\tremaining: 23.6s\n",
      "775:\tlearn: 0.0889091\ttotal: 1m 21s\tremaining: 23.5s\n",
      "776:\tlearn: 0.0888714\ttotal: 1m 21s\tremaining: 23.4s\n",
      "777:\tlearn: 0.0888332\ttotal: 1m 21s\tremaining: 23.3s\n",
      "778:\tlearn: 0.0887842\ttotal: 1m 21s\tremaining: 23.2s\n",
      "779:\tlearn: 0.0887516\ttotal: 1m 21s\tremaining: 23.1s\n",
      "780:\tlearn: 0.0887378\ttotal: 1m 21s\tremaining: 23s\n",
      "781:\tlearn: 0.0886768\ttotal: 1m 22s\tremaining: 22.9s\n",
      "782:\tlearn: 0.0886270\ttotal: 1m 22s\tremaining: 22.8s\n",
      "783:\tlearn: 0.0885925\ttotal: 1m 22s\tremaining: 22.6s\n",
      "784:\tlearn: 0.0885284\ttotal: 1m 22s\tremaining: 22.5s\n",
      "785:\tlearn: 0.0885122\ttotal: 1m 22s\tremaining: 22.4s\n",
      "786:\tlearn: 0.0884830\ttotal: 1m 22s\tremaining: 22.3s\n",
      "787:\tlearn: 0.0884510\ttotal: 1m 22s\tremaining: 22.2s\n",
      "788:\tlearn: 0.0884095\ttotal: 1m 22s\tremaining: 22.1s\n",
      "789:\tlearn: 0.0883806\ttotal: 1m 22s\tremaining: 22s\n",
      "790:\tlearn: 0.0883319\ttotal: 1m 22s\tremaining: 21.9s\n",
      "791:\tlearn: 0.0883054\ttotal: 1m 22s\tremaining: 21.8s\n",
      "792:\tlearn: 0.0882745\ttotal: 1m 23s\tremaining: 21.7s\n",
      "793:\tlearn: 0.0881953\ttotal: 1m 23s\tremaining: 21.6s\n",
      "794:\tlearn: 0.0881480\ttotal: 1m 23s\tremaining: 21.5s\n",
      "795:\tlearn: 0.0881202\ttotal: 1m 23s\tremaining: 21.4s\n",
      "796:\tlearn: 0.0880851\ttotal: 1m 23s\tremaining: 21.3s\n",
      "797:\tlearn: 0.0880665\ttotal: 1m 23s\tremaining: 21.2s\n",
      "798:\tlearn: 0.0880256\ttotal: 1m 23s\tremaining: 21.1s\n",
      "799:\tlearn: 0.0879920\ttotal: 1m 24s\tremaining: 21s\n",
      "800:\tlearn: 0.0879332\ttotal: 1m 24s\tremaining: 20.9s\n",
      "801:\tlearn: 0.0878767\ttotal: 1m 24s\tremaining: 20.8s\n",
      "802:\tlearn: 0.0878582\ttotal: 1m 24s\tremaining: 20.7s\n",
      "803:\tlearn: 0.0878028\ttotal: 1m 24s\tremaining: 20.6s\n",
      "804:\tlearn: 0.0877544\ttotal: 1m 24s\tremaining: 20.5s\n",
      "805:\tlearn: 0.0877127\ttotal: 1m 24s\tremaining: 20.4s\n",
      "806:\tlearn: 0.0876856\ttotal: 1m 24s\tremaining: 20.3s\n",
      "807:\tlearn: 0.0876560\ttotal: 1m 24s\tremaining: 20.2s\n",
      "808:\tlearn: 0.0876057\ttotal: 1m 25s\tremaining: 20.1s\n",
      "809:\tlearn: 0.0875698\ttotal: 1m 25s\tremaining: 20s\n",
      "810:\tlearn: 0.0875404\ttotal: 1m 25s\tremaining: 19.9s\n",
      "811:\tlearn: 0.0875089\ttotal: 1m 25s\tremaining: 19.8s\n",
      "812:\tlearn: 0.0874707\ttotal: 1m 25s\tremaining: 19.6s\n",
      "813:\tlearn: 0.0874318\ttotal: 1m 25s\tremaining: 19.5s\n",
      "814:\tlearn: 0.0873839\ttotal: 1m 25s\tremaining: 19.4s\n",
      "815:\tlearn: 0.0873508\ttotal: 1m 25s\tremaining: 19.3s\n",
      "816:\tlearn: 0.0873012\ttotal: 1m 25s\tremaining: 19.2s\n",
      "817:\tlearn: 0.0872633\ttotal: 1m 25s\tremaining: 19.1s\n",
      "818:\tlearn: 0.0872307\ttotal: 1m 25s\tremaining: 19s\n",
      "819:\tlearn: 0.0872026\ttotal: 1m 26s\tremaining: 18.9s\n",
      "820:\tlearn: 0.0871680\ttotal: 1m 26s\tremaining: 18.8s\n",
      "821:\tlearn: 0.0871111\ttotal: 1m 26s\tremaining: 18.7s\n",
      "822:\tlearn: 0.0870565\ttotal: 1m 26s\tremaining: 18.6s\n",
      "823:\tlearn: 0.0870229\ttotal: 1m 26s\tremaining: 18.5s\n",
      "824:\tlearn: 0.0869691\ttotal: 1m 26s\tremaining: 18.4s\n",
      "825:\tlearn: 0.0869314\ttotal: 1m 26s\tremaining: 18.3s\n",
      "826:\tlearn: 0.0869090\ttotal: 1m 26s\tremaining: 18.2s\n",
      "827:\tlearn: 0.0868684\ttotal: 1m 26s\tremaining: 18.1s\n",
      "828:\tlearn: 0.0868413\ttotal: 1m 27s\tremaining: 18s\n",
      "829:\tlearn: 0.0867739\ttotal: 1m 27s\tremaining: 17.8s\n",
      "830:\tlearn: 0.0867357\ttotal: 1m 27s\tremaining: 17.7s\n",
      "831:\tlearn: 0.0867078\ttotal: 1m 27s\tremaining: 17.6s\n",
      "832:\tlearn: 0.0866788\ttotal: 1m 27s\tremaining: 17.5s\n",
      "833:\tlearn: 0.0866262\ttotal: 1m 27s\tremaining: 17.4s\n",
      "834:\tlearn: 0.0865770\ttotal: 1m 27s\tremaining: 17.3s\n",
      "835:\tlearn: 0.0865532\ttotal: 1m 27s\tremaining: 17.2s\n",
      "836:\tlearn: 0.0865147\ttotal: 1m 27s\tremaining: 17.1s\n",
      "837:\tlearn: 0.0864291\ttotal: 1m 27s\tremaining: 17s\n",
      "838:\tlearn: 0.0863785\ttotal: 1m 28s\tremaining: 16.9s\n",
      "839:\tlearn: 0.0863620\ttotal: 1m 28s\tremaining: 16.8s\n",
      "840:\tlearn: 0.0863212\ttotal: 1m 28s\tremaining: 16.7s\n",
      "841:\tlearn: 0.0862502\ttotal: 1m 28s\tremaining: 16.6s\n",
      "842:\tlearn: 0.0862121\ttotal: 1m 28s\tremaining: 16.5s\n",
      "843:\tlearn: 0.0861792\ttotal: 1m 28s\tremaining: 16.4s\n",
      "844:\tlearn: 0.0861377\ttotal: 1m 28s\tremaining: 16.3s\n",
      "845:\tlearn: 0.0860605\ttotal: 1m 28s\tremaining: 16.2s\n",
      "846:\tlearn: 0.0859945\ttotal: 1m 28s\tremaining: 16.1s\n",
      "847:\tlearn: 0.0859622\ttotal: 1m 29s\tremaining: 16s\n",
      "848:\tlearn: 0.0859259\ttotal: 1m 29s\tremaining: 15.9s\n",
      "849:\tlearn: 0.0858951\ttotal: 1m 29s\tremaining: 15.8s\n",
      "850:\tlearn: 0.0858548\ttotal: 1m 29s\tremaining: 15.7s\n",
      "851:\tlearn: 0.0858097\ttotal: 1m 29s\tremaining: 15.6s\n",
      "852:\tlearn: 0.0857899\ttotal: 1m 29s\tremaining: 15.5s\n",
      "853:\tlearn: 0.0857700\ttotal: 1m 30s\tremaining: 15.4s\n",
      "854:\tlearn: 0.0857642\ttotal: 1m 30s\tremaining: 15.3s\n",
      "855:\tlearn: 0.0857464\ttotal: 1m 30s\tremaining: 15.2s\n",
      "856:\tlearn: 0.0857145\ttotal: 1m 30s\tremaining: 15.1s\n",
      "857:\tlearn: 0.0856779\ttotal: 1m 30s\tremaining: 15s\n",
      "858:\tlearn: 0.0856487\ttotal: 1m 30s\tremaining: 14.9s\n",
      "859:\tlearn: 0.0856165\ttotal: 1m 30s\tremaining: 14.8s\n",
      "860:\tlearn: 0.0855713\ttotal: 1m 30s\tremaining: 14.6s\n",
      "861:\tlearn: 0.0855261\ttotal: 1m 30s\tremaining: 14.5s\n",
      "862:\tlearn: 0.0854616\ttotal: 1m 30s\tremaining: 14.4s\n",
      "863:\tlearn: 0.0854125\ttotal: 1m 31s\tremaining: 14.3s\n",
      "864:\tlearn: 0.0853790\ttotal: 1m 31s\tremaining: 14.2s\n",
      "865:\tlearn: 0.0853309\ttotal: 1m 31s\tremaining: 14.1s\n",
      "866:\tlearn: 0.0852617\ttotal: 1m 31s\tremaining: 14s\n",
      "867:\tlearn: 0.0852362\ttotal: 1m 31s\tremaining: 13.9s\n",
      "868:\tlearn: 0.0851707\ttotal: 1m 31s\tremaining: 13.8s\n",
      "869:\tlearn: 0.0851354\ttotal: 1m 31s\tremaining: 13.7s\n",
      "870:\tlearn: 0.0851178\ttotal: 1m 31s\tremaining: 13.6s\n",
      "871:\tlearn: 0.0850674\ttotal: 1m 31s\tremaining: 13.5s\n",
      "872:\tlearn: 0.0850531\ttotal: 1m 32s\tremaining: 13.4s\n",
      "873:\tlearn: 0.0850319\ttotal: 1m 32s\tremaining: 13.3s\n",
      "874:\tlearn: 0.0850172\ttotal: 1m 32s\tremaining: 13.2s\n",
      "875:\tlearn: 0.0849881\ttotal: 1m 32s\tremaining: 13.1s\n",
      "876:\tlearn: 0.0849788\ttotal: 1m 32s\tremaining: 13s\n",
      "877:\tlearn: 0.0849525\ttotal: 1m 32s\tremaining: 12.9s\n",
      "878:\tlearn: 0.0849083\ttotal: 1m 32s\tremaining: 12.8s\n",
      "879:\tlearn: 0.0848459\ttotal: 1m 32s\tremaining: 12.7s\n",
      "880:\tlearn: 0.0848094\ttotal: 1m 32s\tremaining: 12.6s\n",
      "881:\tlearn: 0.0847855\ttotal: 1m 33s\tremaining: 12.4s\n",
      "882:\tlearn: 0.0847424\ttotal: 1m 33s\tremaining: 12.3s\n",
      "883:\tlearn: 0.0847184\ttotal: 1m 33s\tremaining: 12.2s\n",
      "884:\tlearn: 0.0846537\ttotal: 1m 33s\tremaining: 12.2s\n",
      "885:\tlearn: 0.0846005\ttotal: 1m 33s\tremaining: 12.1s\n",
      "886:\tlearn: 0.0846004\ttotal: 1m 33s\tremaining: 12s\n",
      "887:\tlearn: 0.0845753\ttotal: 1m 33s\tremaining: 11.8s\n",
      "888:\tlearn: 0.0845222\ttotal: 1m 34s\tremaining: 11.7s\n",
      "889:\tlearn: 0.0844958\ttotal: 1m 34s\tremaining: 11.6s\n",
      "890:\tlearn: 0.0844726\ttotal: 1m 34s\tremaining: 11.5s\n",
      "891:\tlearn: 0.0844472\ttotal: 1m 34s\tremaining: 11.4s\n",
      "892:\tlearn: 0.0844280\ttotal: 1m 34s\tremaining: 11.3s\n",
      "893:\tlearn: 0.0844021\ttotal: 1m 34s\tremaining: 11.2s\n",
      "894:\tlearn: 0.0843710\ttotal: 1m 34s\tremaining: 11.1s\n",
      "895:\tlearn: 0.0843454\ttotal: 1m 34s\tremaining: 11s\n",
      "896:\tlearn: 0.0843021\ttotal: 1m 34s\tremaining: 10.9s\n",
      "897:\tlearn: 0.0842736\ttotal: 1m 34s\tremaining: 10.8s\n",
      "898:\tlearn: 0.0842391\ttotal: 1m 35s\tremaining: 10.7s\n",
      "899:\tlearn: 0.0842048\ttotal: 1m 35s\tremaining: 10.6s\n",
      "900:\tlearn: 0.0841616\ttotal: 1m 35s\tremaining: 10.5s\n",
      "901:\tlearn: 0.0840625\ttotal: 1m 35s\tremaining: 10.4s\n",
      "902:\tlearn: 0.0840008\ttotal: 1m 35s\tremaining: 10.3s\n",
      "903:\tlearn: 0.0839645\ttotal: 1m 35s\tremaining: 10.2s\n",
      "904:\tlearn: 0.0839448\ttotal: 1m 35s\tremaining: 10s\n",
      "905:\tlearn: 0.0838825\ttotal: 1m 35s\tremaining: 9.94s\n",
      "906:\tlearn: 0.0838384\ttotal: 1m 35s\tremaining: 9.83s\n",
      "907:\tlearn: 0.0838068\ttotal: 1m 35s\tremaining: 9.73s\n",
      "908:\tlearn: 0.0837793\ttotal: 1m 36s\tremaining: 9.62s\n",
      "909:\tlearn: 0.0837165\ttotal: 1m 36s\tremaining: 9.52s\n",
      "910:\tlearn: 0.0836649\ttotal: 1m 36s\tremaining: 9.41s\n",
      "911:\tlearn: 0.0836462\ttotal: 1m 36s\tremaining: 9.3s\n",
      "912:\tlearn: 0.0836235\ttotal: 1m 36s\tremaining: 9.2s\n",
      "913:\tlearn: 0.0835971\ttotal: 1m 36s\tremaining: 9.09s\n",
      "914:\tlearn: 0.0835806\ttotal: 1m 36s\tremaining: 8.98s\n",
      "915:\tlearn: 0.0835506\ttotal: 1m 36s\tremaining: 8.88s\n",
      "916:\tlearn: 0.0835352\ttotal: 1m 36s\tremaining: 8.77s\n",
      "917:\tlearn: 0.0835104\ttotal: 1m 37s\tremaining: 8.66s\n",
      "918:\tlearn: 0.0834614\ttotal: 1m 37s\tremaining: 8.56s\n",
      "919:\tlearn: 0.0834295\ttotal: 1m 37s\tremaining: 8.45s\n",
      "920:\tlearn: 0.0833724\ttotal: 1m 37s\tremaining: 8.35s\n",
      "921:\tlearn: 0.0833408\ttotal: 1m 37s\tremaining: 8.24s\n",
      "922:\tlearn: 0.0833221\ttotal: 1m 37s\tremaining: 8.14s\n",
      "923:\tlearn: 0.0832632\ttotal: 1m 37s\tremaining: 8.03s\n",
      "924:\tlearn: 0.0832081\ttotal: 1m 37s\tremaining: 7.93s\n",
      "925:\tlearn: 0.0831443\ttotal: 1m 37s\tremaining: 7.82s\n",
      "926:\tlearn: 0.0831193\ttotal: 1m 37s\tremaining: 7.71s\n",
      "927:\tlearn: 0.0831108\ttotal: 1m 38s\tremaining: 7.61s\n",
      "928:\tlearn: 0.0830449\ttotal: 1m 38s\tremaining: 7.5s\n",
      "929:\tlearn: 0.0830224\ttotal: 1m 38s\tremaining: 7.4s\n",
      "930:\tlearn: 0.0829839\ttotal: 1m 38s\tremaining: 7.29s\n",
      "931:\tlearn: 0.0829338\ttotal: 1m 38s\tremaining: 7.19s\n",
      "932:\tlearn: 0.0829068\ttotal: 1m 38s\tremaining: 7.08s\n",
      "933:\tlearn: 0.0828895\ttotal: 1m 38s\tremaining: 6.97s\n",
      "934:\tlearn: 0.0828461\ttotal: 1m 38s\tremaining: 6.87s\n",
      "935:\tlearn: 0.0828058\ttotal: 1m 38s\tremaining: 6.76s\n",
      "936:\tlearn: 0.0827554\ttotal: 1m 38s\tremaining: 6.65s\n",
      "937:\tlearn: 0.0827414\ttotal: 1m 39s\tremaining: 6.55s\n",
      "938:\tlearn: 0.0826730\ttotal: 1m 39s\tremaining: 6.44s\n",
      "939:\tlearn: 0.0826427\ttotal: 1m 39s\tremaining: 6.33s\n",
      "940:\tlearn: 0.0825755\ttotal: 1m 39s\tremaining: 6.23s\n",
      "941:\tlearn: 0.0825289\ttotal: 1m 39s\tremaining: 6.12s\n",
      "942:\tlearn: 0.0825020\ttotal: 1m 39s\tremaining: 6.02s\n",
      "943:\tlearn: 0.0824464\ttotal: 1m 39s\tremaining: 5.91s\n",
      "944:\tlearn: 0.0823814\ttotal: 1m 39s\tremaining: 5.8s\n",
      "945:\tlearn: 0.0823327\ttotal: 1m 39s\tremaining: 5.7s\n",
      "946:\tlearn: 0.0823176\ttotal: 1m 39s\tremaining: 5.59s\n",
      "947:\tlearn: 0.0822751\ttotal: 1m 40s\tremaining: 5.49s\n",
      "948:\tlearn: 0.0822603\ttotal: 1m 40s\tremaining: 5.38s\n",
      "949:\tlearn: 0.0822381\ttotal: 1m 40s\tremaining: 5.27s\n",
      "950:\tlearn: 0.0822162\ttotal: 1m 40s\tremaining: 5.17s\n",
      "951:\tlearn: 0.0821859\ttotal: 1m 40s\tremaining: 5.06s\n",
      "952:\tlearn: 0.0821718\ttotal: 1m 40s\tremaining: 4.96s\n",
      "953:\tlearn: 0.0820853\ttotal: 1m 40s\tremaining: 4.86s\n",
      "954:\tlearn: 0.0820641\ttotal: 1m 40s\tremaining: 4.75s\n",
      "955:\tlearn: 0.0820000\ttotal: 1m 40s\tremaining: 4.65s\n",
      "956:\tlearn: 0.0819605\ttotal: 1m 41s\tremaining: 4.54s\n",
      "957:\tlearn: 0.0819225\ttotal: 1m 41s\tremaining: 4.43s\n",
      "958:\tlearn: 0.0818876\ttotal: 1m 41s\tremaining: 4.33s\n",
      "959:\tlearn: 0.0818305\ttotal: 1m 41s\tremaining: 4.22s\n",
      "960:\tlearn: 0.0817802\ttotal: 1m 41s\tremaining: 4.12s\n",
      "961:\tlearn: 0.0817356\ttotal: 1m 41s\tremaining: 4.01s\n",
      "962:\tlearn: 0.0816880\ttotal: 1m 41s\tremaining: 3.9s\n",
      "963:\tlearn: 0.0816655\ttotal: 1m 41s\tremaining: 3.8s\n",
      "964:\tlearn: 0.0816360\ttotal: 1m 41s\tremaining: 3.69s\n",
      "965:\tlearn: 0.0816063\ttotal: 1m 41s\tremaining: 3.59s\n",
      "966:\tlearn: 0.0815848\ttotal: 1m 42s\tremaining: 3.48s\n",
      "967:\tlearn: 0.0815504\ttotal: 1m 42s\tremaining: 3.38s\n",
      "968:\tlearn: 0.0814867\ttotal: 1m 42s\tremaining: 3.27s\n",
      "969:\tlearn: 0.0814561\ttotal: 1m 42s\tremaining: 3.16s\n",
      "970:\tlearn: 0.0814161\ttotal: 1m 42s\tremaining: 3.06s\n",
      "971:\tlearn: 0.0813838\ttotal: 1m 42s\tremaining: 2.95s\n",
      "972:\tlearn: 0.0813570\ttotal: 1m 42s\tremaining: 2.85s\n",
      "973:\tlearn: 0.0813233\ttotal: 1m 42s\tremaining: 2.74s\n",
      "974:\tlearn: 0.0812977\ttotal: 1m 42s\tremaining: 2.63s\n",
      "975:\tlearn: 0.0812669\ttotal: 1m 42s\tremaining: 2.53s\n",
      "976:\tlearn: 0.0812301\ttotal: 1m 42s\tremaining: 2.42s\n",
      "977:\tlearn: 0.0812177\ttotal: 1m 43s\tremaining: 2.32s\n",
      "978:\tlearn: 0.0812135\ttotal: 1m 43s\tremaining: 2.21s\n",
      "979:\tlearn: 0.0812043\ttotal: 1m 43s\tremaining: 2.11s\n",
      "980:\tlearn: 0.0811982\ttotal: 1m 43s\tremaining: 2s\n",
      "981:\tlearn: 0.0811786\ttotal: 1m 43s\tremaining: 1.9s\n",
      "982:\tlearn: 0.0811494\ttotal: 1m 43s\tremaining: 1.79s\n",
      "983:\tlearn: 0.0810933\ttotal: 1m 43s\tremaining: 1.68s\n",
      "984:\tlearn: 0.0810540\ttotal: 1m 43s\tremaining: 1.58s\n",
      "985:\tlearn: 0.0809934\ttotal: 1m 43s\tremaining: 1.47s\n",
      "986:\tlearn: 0.0809544\ttotal: 1m 43s\tremaining: 1.37s\n",
      "987:\tlearn: 0.0809314\ttotal: 1m 43s\tremaining: 1.26s\n",
      "988:\tlearn: 0.0809095\ttotal: 1m 44s\tremaining: 1.16s\n",
      "989:\tlearn: 0.0808853\ttotal: 1m 44s\tremaining: 1.05s\n",
      "990:\tlearn: 0.0808420\ttotal: 1m 44s\tremaining: 947ms\n",
      "991:\tlearn: 0.0808417\ttotal: 1m 44s\tremaining: 841ms\n",
      "992:\tlearn: 0.0808080\ttotal: 1m 44s\tremaining: 736ms\n",
      "993:\tlearn: 0.0807731\ttotal: 1m 44s\tremaining: 631ms\n",
      "994:\tlearn: 0.0807251\ttotal: 1m 44s\tremaining: 526ms\n",
      "995:\tlearn: 0.0806791\ttotal: 1m 44s\tremaining: 420ms\n",
      "996:\tlearn: 0.0806544\ttotal: 1m 44s\tremaining: 315ms\n",
      "997:\tlearn: 0.0806219\ttotal: 1m 44s\tremaining: 210ms\n",
      "998:\tlearn: 0.0806011\ttotal: 1m 44s\tremaining: 105ms\n",
      "999:\tlearn: 0.0805827\ttotal: 1m 45s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5293080\ttotal: 98.4ms\tremaining: 1m 38s\n",
      "1:\tlearn: 0.4226092\ttotal: 201ms\tremaining: 1m 40s\n",
      "2:\tlearn: 0.3612002\ttotal: 293ms\tremaining: 1m 37s\n",
      "3:\tlearn: 0.3216669\ttotal: 383ms\tremaining: 1m 35s\n",
      "4:\tlearn: 0.2950487\ttotal: 476ms\tremaining: 1m 34s\n",
      "5:\tlearn: 0.2777141\ttotal: 567ms\tremaining: 1m 34s\n",
      "6:\tlearn: 0.2619102\ttotal: 659ms\tremaining: 1m 33s\n",
      "7:\tlearn: 0.2515128\ttotal: 749ms\tremaining: 1m 32s\n",
      "8:\tlearn: 0.2357007\ttotal: 841ms\tremaining: 1m 32s\n",
      "9:\tlearn: 0.2275091\ttotal: 935ms\tremaining: 1m 32s\n",
      "10:\tlearn: 0.2214348\ttotal: 1.02s\tremaining: 1m 32s\n",
      "11:\tlearn: 0.2165170\ttotal: 1.12s\tremaining: 1m 32s\n",
      "12:\tlearn: 0.2126396\ttotal: 1.21s\tremaining: 1m 32s\n",
      "13:\tlearn: 0.2098227\ttotal: 1.32s\tremaining: 1m 32s\n",
      "14:\tlearn: 0.2062856\ttotal: 1.42s\tremaining: 1m 32s\n",
      "15:\tlearn: 0.2000575\ttotal: 1.51s\tremaining: 1m 32s\n",
      "16:\tlearn: 0.1960049\ttotal: 1.6s\tremaining: 1m 32s\n",
      "17:\tlearn: 0.1934664\ttotal: 1.69s\tremaining: 1m 32s\n",
      "18:\tlearn: 0.1911542\ttotal: 1.78s\tremaining: 1m 32s\n",
      "19:\tlearn: 0.1882423\ttotal: 1.87s\tremaining: 1m 31s\n",
      "20:\tlearn: 0.1866385\ttotal: 1.97s\tremaining: 1m 31s\n",
      "21:\tlearn: 0.1845704\ttotal: 2.06s\tremaining: 1m 31s\n",
      "22:\tlearn: 0.1832431\ttotal: 2.15s\tremaining: 1m 31s\n",
      "23:\tlearn: 0.1821054\ttotal: 2.25s\tremaining: 1m 31s\n",
      "24:\tlearn: 0.1802291\ttotal: 2.33s\tremaining: 1m 31s\n",
      "25:\tlearn: 0.1788500\ttotal: 2.43s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1771895\ttotal: 2.52s\tremaining: 1m 30s\n",
      "27:\tlearn: 0.1761279\ttotal: 2.61s\tremaining: 1m 30s\n",
      "28:\tlearn: 0.1744508\ttotal: 2.7s\tremaining: 1m 30s\n",
      "29:\tlearn: 0.1732665\ttotal: 2.8s\tremaining: 1m 30s\n",
      "30:\tlearn: 0.1719429\ttotal: 2.89s\tremaining: 1m 30s\n",
      "31:\tlearn: 0.1710706\ttotal: 2.98s\tremaining: 1m 30s\n",
      "32:\tlearn: 0.1693279\ttotal: 3.07s\tremaining: 1m 30s\n",
      "33:\tlearn: 0.1680570\ttotal: 3.17s\tremaining: 1m 29s\n",
      "34:\tlearn: 0.1669637\ttotal: 3.26s\tremaining: 1m 29s\n",
      "35:\tlearn: 0.1660921\ttotal: 3.35s\tremaining: 1m 29s\n",
      "36:\tlearn: 0.1651614\ttotal: 3.44s\tremaining: 1m 29s\n",
      "37:\tlearn: 0.1642093\ttotal: 3.54s\tremaining: 1m 29s\n",
      "38:\tlearn: 0.1633595\ttotal: 3.63s\tremaining: 1m 29s\n",
      "39:\tlearn: 0.1624492\ttotal: 3.72s\tremaining: 1m 29s\n",
      "40:\tlearn: 0.1617956\ttotal: 3.81s\tremaining: 1m 29s\n",
      "41:\tlearn: 0.1609815\ttotal: 3.9s\tremaining: 1m 29s\n",
      "42:\tlearn: 0.1601256\ttotal: 4s\tremaining: 1m 28s\n",
      "43:\tlearn: 0.1592379\ttotal: 4.09s\tremaining: 1m 28s\n",
      "44:\tlearn: 0.1586588\ttotal: 4.19s\tremaining: 1m 28s\n",
      "45:\tlearn: 0.1581984\ttotal: 4.28s\tremaining: 1m 28s\n",
      "46:\tlearn: 0.1577032\ttotal: 4.37s\tremaining: 1m 28s\n",
      "47:\tlearn: 0.1572804\ttotal: 4.47s\tremaining: 1m 28s\n",
      "48:\tlearn: 0.1566725\ttotal: 4.55s\tremaining: 1m 28s\n",
      "49:\tlearn: 0.1560189\ttotal: 4.65s\tremaining: 1m 28s\n",
      "50:\tlearn: 0.1554191\ttotal: 4.75s\tremaining: 1m 28s\n",
      "51:\tlearn: 0.1550057\ttotal: 4.84s\tremaining: 1m 28s\n",
      "52:\tlearn: 0.1544170\ttotal: 4.93s\tremaining: 1m 28s\n",
      "53:\tlearn: 0.1536813\ttotal: 5.02s\tremaining: 1m 27s\n",
      "54:\tlearn: 0.1531630\ttotal: 5.11s\tremaining: 1m 27s\n",
      "55:\tlearn: 0.1526945\ttotal: 5.21s\tremaining: 1m 27s\n",
      "56:\tlearn: 0.1522736\ttotal: 5.3s\tremaining: 1m 27s\n",
      "57:\tlearn: 0.1517863\ttotal: 5.39s\tremaining: 1m 27s\n",
      "58:\tlearn: 0.1514114\ttotal: 5.48s\tremaining: 1m 27s\n",
      "59:\tlearn: 0.1507929\ttotal: 5.58s\tremaining: 1m 27s\n",
      "60:\tlearn: 0.1503935\ttotal: 5.67s\tremaining: 1m 27s\n",
      "61:\tlearn: 0.1499971\ttotal: 5.77s\tremaining: 1m 27s\n",
      "62:\tlearn: 0.1494896\ttotal: 5.86s\tremaining: 1m 27s\n",
      "63:\tlearn: 0.1487996\ttotal: 5.95s\tremaining: 1m 27s\n",
      "64:\tlearn: 0.1483984\ttotal: 6.05s\tremaining: 1m 27s\n",
      "65:\tlearn: 0.1481213\ttotal: 6.14s\tremaining: 1m 26s\n",
      "66:\tlearn: 0.1478019\ttotal: 6.23s\tremaining: 1m 26s\n",
      "67:\tlearn: 0.1474684\ttotal: 6.32s\tremaining: 1m 26s\n",
      "68:\tlearn: 0.1471235\ttotal: 6.41s\tremaining: 1m 26s\n",
      "69:\tlearn: 0.1467792\ttotal: 6.51s\tremaining: 1m 26s\n",
      "70:\tlearn: 0.1463975\ttotal: 6.6s\tremaining: 1m 26s\n",
      "71:\tlearn: 0.1461927\ttotal: 6.69s\tremaining: 1m 26s\n",
      "72:\tlearn: 0.1457639\ttotal: 6.79s\tremaining: 1m 26s\n",
      "73:\tlearn: 0.1454365\ttotal: 6.88s\tremaining: 1m 26s\n",
      "74:\tlearn: 0.1450070\ttotal: 6.99s\tremaining: 1m 26s\n",
      "75:\tlearn: 0.1447017\ttotal: 7.08s\tremaining: 1m 26s\n",
      "76:\tlearn: 0.1443045\ttotal: 7.18s\tremaining: 1m 26s\n",
      "77:\tlearn: 0.1440663\ttotal: 7.27s\tremaining: 1m 25s\n",
      "78:\tlearn: 0.1438511\ttotal: 7.36s\tremaining: 1m 25s\n",
      "79:\tlearn: 0.1436094\ttotal: 7.46s\tremaining: 1m 25s\n",
      "80:\tlearn: 0.1433231\ttotal: 7.55s\tremaining: 1m 25s\n",
      "81:\tlearn: 0.1430424\ttotal: 7.64s\tremaining: 1m 25s\n",
      "82:\tlearn: 0.1427550\ttotal: 7.73s\tremaining: 1m 25s\n",
      "83:\tlearn: 0.1425547\ttotal: 7.82s\tremaining: 1m 25s\n",
      "84:\tlearn: 0.1423532\ttotal: 7.91s\tremaining: 1m 25s\n",
      "85:\tlearn: 0.1420926\ttotal: 8.01s\tremaining: 1m 25s\n",
      "86:\tlearn: 0.1418380\ttotal: 8.1s\tremaining: 1m 24s\n",
      "87:\tlearn: 0.1414727\ttotal: 8.19s\tremaining: 1m 24s\n",
      "88:\tlearn: 0.1412386\ttotal: 8.28s\tremaining: 1m 24s\n",
      "89:\tlearn: 0.1410639\ttotal: 8.37s\tremaining: 1m 24s\n",
      "90:\tlearn: 0.1408371\ttotal: 8.46s\tremaining: 1m 24s\n",
      "91:\tlearn: 0.1406135\ttotal: 8.56s\tremaining: 1m 24s\n",
      "92:\tlearn: 0.1402551\ttotal: 8.65s\tremaining: 1m 24s\n",
      "93:\tlearn: 0.1398866\ttotal: 8.74s\tremaining: 1m 24s\n",
      "94:\tlearn: 0.1397079\ttotal: 8.83s\tremaining: 1m 24s\n",
      "95:\tlearn: 0.1394619\ttotal: 8.92s\tremaining: 1m 24s\n",
      "96:\tlearn: 0.1391811\ttotal: 9.02s\tremaining: 1m 23s\n",
      "97:\tlearn: 0.1387832\ttotal: 9.11s\tremaining: 1m 23s\n",
      "98:\tlearn: 0.1385119\ttotal: 9.19s\tremaining: 1m 23s\n",
      "99:\tlearn: 0.1381249\ttotal: 9.29s\tremaining: 1m 23s\n",
      "100:\tlearn: 0.1378867\ttotal: 9.38s\tremaining: 1m 23s\n",
      "101:\tlearn: 0.1376255\ttotal: 9.47s\tremaining: 1m 23s\n",
      "102:\tlearn: 0.1373538\ttotal: 9.56s\tremaining: 1m 23s\n",
      "103:\tlearn: 0.1371233\ttotal: 9.65s\tremaining: 1m 23s\n",
      "104:\tlearn: 0.1369557\ttotal: 9.74s\tremaining: 1m 23s\n",
      "105:\tlearn: 0.1366956\ttotal: 9.83s\tremaining: 1m 22s\n",
      "106:\tlearn: 0.1364532\ttotal: 10s\tremaining: 1m 23s\n",
      "107:\tlearn: 0.1361191\ttotal: 10.1s\tremaining: 1m 23s\n",
      "108:\tlearn: 0.1359297\ttotal: 10.2s\tremaining: 1m 23s\n",
      "109:\tlearn: 0.1357129\ttotal: 10.3s\tremaining: 1m 23s\n",
      "110:\tlearn: 0.1355331\ttotal: 10.4s\tremaining: 1m 23s\n",
      "111:\tlearn: 0.1353467\ttotal: 10.5s\tremaining: 1m 23s\n",
      "112:\tlearn: 0.1351601\ttotal: 10.6s\tremaining: 1m 23s\n",
      "113:\tlearn: 0.1349837\ttotal: 10.7s\tremaining: 1m 23s\n",
      "114:\tlearn: 0.1348412\ttotal: 10.8s\tremaining: 1m 23s\n",
      "115:\tlearn: 0.1346663\ttotal: 10.9s\tremaining: 1m 23s\n",
      "116:\tlearn: 0.1343817\ttotal: 11s\tremaining: 1m 23s\n",
      "117:\tlearn: 0.1341648\ttotal: 11.1s\tremaining: 1m 23s\n",
      "118:\tlearn: 0.1339557\ttotal: 11.2s\tremaining: 1m 22s\n",
      "119:\tlearn: 0.1337692\ttotal: 11.3s\tremaining: 1m 22s\n",
      "120:\tlearn: 0.1335241\ttotal: 11.4s\tremaining: 1m 22s\n",
      "121:\tlearn: 0.1330884\ttotal: 11.5s\tremaining: 1m 22s\n",
      "122:\tlearn: 0.1328285\ttotal: 11.6s\tremaining: 1m 22s\n",
      "123:\tlearn: 0.1326496\ttotal: 11.7s\tremaining: 1m 22s\n",
      "124:\tlearn: 0.1324967\ttotal: 11.8s\tremaining: 1m 22s\n",
      "125:\tlearn: 0.1323372\ttotal: 11.9s\tremaining: 1m 22s\n",
      "126:\tlearn: 0.1321751\ttotal: 12s\tremaining: 1m 22s\n",
      "127:\tlearn: 0.1319166\ttotal: 12.1s\tremaining: 1m 22s\n",
      "128:\tlearn: 0.1317441\ttotal: 12.2s\tremaining: 1m 22s\n",
      "129:\tlearn: 0.1315653\ttotal: 12.3s\tremaining: 1m 22s\n",
      "130:\tlearn: 0.1313062\ttotal: 12.5s\tremaining: 1m 22s\n",
      "131:\tlearn: 0.1310611\ttotal: 12.6s\tremaining: 1m 22s\n",
      "132:\tlearn: 0.1308715\ttotal: 12.6s\tremaining: 1m 22s\n",
      "133:\tlearn: 0.1307184\ttotal: 12.7s\tremaining: 1m 22s\n",
      "134:\tlearn: 0.1306044\ttotal: 12.8s\tremaining: 1m 22s\n",
      "135:\tlearn: 0.1304553\ttotal: 12.9s\tremaining: 1m 22s\n",
      "136:\tlearn: 0.1303426\ttotal: 13s\tremaining: 1m 21s\n",
      "137:\tlearn: 0.1301554\ttotal: 13.1s\tremaining: 1m 21s\n",
      "138:\tlearn: 0.1300372\ttotal: 13.2s\tremaining: 1m 21s\n",
      "139:\tlearn: 0.1299172\ttotal: 13.3s\tremaining: 1m 21s\n",
      "140:\tlearn: 0.1297603\ttotal: 13.4s\tremaining: 1m 21s\n",
      "141:\tlearn: 0.1296765\ttotal: 13.5s\tremaining: 1m 21s\n",
      "142:\tlearn: 0.1295244\ttotal: 13.6s\tremaining: 1m 21s\n",
      "143:\tlearn: 0.1293954\ttotal: 13.7s\tremaining: 1m 21s\n",
      "144:\tlearn: 0.1292827\ttotal: 13.7s\tremaining: 1m 21s\n",
      "145:\tlearn: 0.1290705\ttotal: 13.8s\tremaining: 1m 20s\n",
      "146:\tlearn: 0.1288874\ttotal: 13.9s\tremaining: 1m 20s\n",
      "147:\tlearn: 0.1286961\ttotal: 14s\tremaining: 1m 20s\n",
      "148:\tlearn: 0.1284669\ttotal: 14.1s\tremaining: 1m 20s\n",
      "149:\tlearn: 0.1283067\ttotal: 14.2s\tremaining: 1m 20s\n",
      "150:\tlearn: 0.1281617\ttotal: 14.3s\tremaining: 1m 20s\n",
      "151:\tlearn: 0.1279691\ttotal: 14.4s\tremaining: 1m 20s\n",
      "152:\tlearn: 0.1278535\ttotal: 14.5s\tremaining: 1m 20s\n",
      "153:\tlearn: 0.1277367\ttotal: 14.6s\tremaining: 1m 20s\n",
      "154:\tlearn: 0.1275999\ttotal: 14.7s\tremaining: 1m 20s\n",
      "155:\tlearn: 0.1274381\ttotal: 14.8s\tremaining: 1m 19s\n",
      "156:\tlearn: 0.1273420\ttotal: 14.9s\tremaining: 1m 19s\n",
      "157:\tlearn: 0.1272095\ttotal: 15s\tremaining: 1m 19s\n",
      "158:\tlearn: 0.1270823\ttotal: 15.1s\tremaining: 1m 19s\n",
      "159:\tlearn: 0.1269454\ttotal: 15.2s\tremaining: 1m 19s\n",
      "160:\tlearn: 0.1267693\ttotal: 15.2s\tremaining: 1m 19s\n",
      "161:\tlearn: 0.1266322\ttotal: 15.3s\tremaining: 1m 19s\n",
      "162:\tlearn: 0.1264930\ttotal: 15.4s\tremaining: 1m 19s\n",
      "163:\tlearn: 0.1263851\ttotal: 15.5s\tremaining: 1m 19s\n",
      "164:\tlearn: 0.1263081\ttotal: 15.6s\tremaining: 1m 19s\n",
      "165:\tlearn: 0.1261634\ttotal: 15.7s\tremaining: 1m 18s\n",
      "166:\tlearn: 0.1260117\ttotal: 15.8s\tremaining: 1m 18s\n",
      "167:\tlearn: 0.1259086\ttotal: 15.9s\tremaining: 1m 18s\n",
      "168:\tlearn: 0.1257956\ttotal: 16s\tremaining: 1m 18s\n",
      "169:\tlearn: 0.1256874\ttotal: 16.1s\tremaining: 1m 18s\n",
      "170:\tlearn: 0.1255701\ttotal: 16.2s\tremaining: 1m 18s\n",
      "171:\tlearn: 0.1254256\ttotal: 16.3s\tremaining: 1m 18s\n",
      "172:\tlearn: 0.1253388\ttotal: 16.4s\tremaining: 1m 18s\n",
      "173:\tlearn: 0.1252366\ttotal: 16.5s\tremaining: 1m 18s\n",
      "174:\tlearn: 0.1251333\ttotal: 16.6s\tremaining: 1m 18s\n",
      "175:\tlearn: 0.1250125\ttotal: 16.7s\tremaining: 1m 17s\n",
      "176:\tlearn: 0.1248992\ttotal: 16.7s\tremaining: 1m 17s\n",
      "177:\tlearn: 0.1247949\ttotal: 16.8s\tremaining: 1m 17s\n",
      "178:\tlearn: 0.1247006\ttotal: 16.9s\tremaining: 1m 17s\n",
      "179:\tlearn: 0.1245899\ttotal: 17s\tremaining: 1m 17s\n",
      "180:\tlearn: 0.1244511\ttotal: 17.1s\tremaining: 1m 17s\n",
      "181:\tlearn: 0.1243299\ttotal: 17.2s\tremaining: 1m 17s\n",
      "182:\tlearn: 0.1242695\ttotal: 17.3s\tremaining: 1m 17s\n",
      "183:\tlearn: 0.1241739\ttotal: 17.4s\tremaining: 1m 17s\n",
      "184:\tlearn: 0.1239830\ttotal: 17.5s\tremaining: 1m 17s\n",
      "185:\tlearn: 0.1239137\ttotal: 17.6s\tremaining: 1m 17s\n",
      "186:\tlearn: 0.1238218\ttotal: 17.7s\tremaining: 1m 16s\n",
      "187:\tlearn: 0.1236566\ttotal: 17.8s\tremaining: 1m 16s\n",
      "188:\tlearn: 0.1235604\ttotal: 17.9s\tremaining: 1m 16s\n",
      "189:\tlearn: 0.1234640\ttotal: 18s\tremaining: 1m 16s\n",
      "190:\tlearn: 0.1233674\ttotal: 18.1s\tremaining: 1m 16s\n",
      "191:\tlearn: 0.1232854\ttotal: 18.2s\tremaining: 1m 16s\n",
      "192:\tlearn: 0.1231753\ttotal: 18.3s\tremaining: 1m 16s\n",
      "193:\tlearn: 0.1230735\ttotal: 18.4s\tremaining: 1m 16s\n",
      "194:\tlearn: 0.1229752\ttotal: 18.5s\tremaining: 1m 16s\n",
      "195:\tlearn: 0.1228621\ttotal: 18.5s\tremaining: 1m 16s\n",
      "196:\tlearn: 0.1227566\ttotal: 18.6s\tremaining: 1m 15s\n",
      "197:\tlearn: 0.1226199\ttotal: 18.7s\tremaining: 1m 15s\n",
      "198:\tlearn: 0.1225385\ttotal: 18.8s\tremaining: 1m 15s\n",
      "199:\tlearn: 0.1223389\ttotal: 18.9s\tremaining: 1m 15s\n",
      "200:\tlearn: 0.1222637\ttotal: 19s\tremaining: 1m 15s\n",
      "201:\tlearn: 0.1221273\ttotal: 19.1s\tremaining: 1m 15s\n",
      "202:\tlearn: 0.1220487\ttotal: 19.2s\tremaining: 1m 15s\n",
      "203:\tlearn: 0.1219758\ttotal: 19.3s\tremaining: 1m 15s\n",
      "204:\tlearn: 0.1218384\ttotal: 19.4s\tremaining: 1m 15s\n",
      "205:\tlearn: 0.1217301\ttotal: 19.5s\tremaining: 1m 15s\n",
      "206:\tlearn: 0.1216044\ttotal: 19.6s\tremaining: 1m 14s\n",
      "207:\tlearn: 0.1214759\ttotal: 19.7s\tremaining: 1m 14s\n",
      "208:\tlearn: 0.1214465\ttotal: 19.7s\tremaining: 1m 14s\n",
      "209:\tlearn: 0.1213383\ttotal: 19.8s\tremaining: 1m 14s\n",
      "210:\tlearn: 0.1212023\ttotal: 20s\tremaining: 1m 14s\n",
      "211:\tlearn: 0.1211154\ttotal: 20.1s\tremaining: 1m 14s\n",
      "212:\tlearn: 0.1210364\ttotal: 20.2s\tremaining: 1m 14s\n",
      "213:\tlearn: 0.1209798\ttotal: 20.3s\tremaining: 1m 14s\n",
      "214:\tlearn: 0.1208472\ttotal: 20.4s\tremaining: 1m 14s\n",
      "215:\tlearn: 0.1207502\ttotal: 20.5s\tremaining: 1m 14s\n",
      "216:\tlearn: 0.1206316\ttotal: 20.6s\tremaining: 1m 14s\n",
      "217:\tlearn: 0.1205820\ttotal: 20.7s\tremaining: 1m 14s\n",
      "218:\tlearn: 0.1205096\ttotal: 20.8s\tremaining: 1m 14s\n",
      "219:\tlearn: 0.1204042\ttotal: 20.9s\tremaining: 1m 13s\n",
      "220:\tlearn: 0.1202602\ttotal: 21s\tremaining: 1m 13s\n",
      "221:\tlearn: 0.1201899\ttotal: 21.1s\tremaining: 1m 13s\n",
      "222:\tlearn: 0.1201110\ttotal: 21.1s\tremaining: 1m 13s\n",
      "223:\tlearn: 0.1200338\ttotal: 21.2s\tremaining: 1m 13s\n",
      "224:\tlearn: 0.1199118\ttotal: 21.3s\tremaining: 1m 13s\n",
      "225:\tlearn: 0.1198088\ttotal: 21.4s\tremaining: 1m 13s\n",
      "226:\tlearn: 0.1197030\ttotal: 21.5s\tremaining: 1m 13s\n",
      "227:\tlearn: 0.1196269\ttotal: 21.6s\tremaining: 1m 13s\n",
      "228:\tlearn: 0.1195353\ttotal: 21.7s\tremaining: 1m 13s\n",
      "229:\tlearn: 0.1194716\ttotal: 21.8s\tremaining: 1m 12s\n",
      "230:\tlearn: 0.1192973\ttotal: 21.9s\tremaining: 1m 12s\n",
      "231:\tlearn: 0.1192011\ttotal: 22s\tremaining: 1m 12s\n",
      "232:\tlearn: 0.1190930\ttotal: 22.1s\tremaining: 1m 12s\n",
      "233:\tlearn: 0.1190215\ttotal: 22.2s\tremaining: 1m 12s\n",
      "234:\tlearn: 0.1189324\ttotal: 22.3s\tremaining: 1m 12s\n",
      "235:\tlearn: 0.1188551\ttotal: 22.4s\tremaining: 1m 12s\n",
      "236:\tlearn: 0.1187111\ttotal: 22.5s\tremaining: 1m 12s\n",
      "237:\tlearn: 0.1186098\ttotal: 22.6s\tremaining: 1m 12s\n",
      "238:\tlearn: 0.1184877\ttotal: 22.6s\tremaining: 1m 12s\n",
      "239:\tlearn: 0.1184049\ttotal: 22.7s\tremaining: 1m 12s\n",
      "240:\tlearn: 0.1183113\ttotal: 22.8s\tremaining: 1m 11s\n",
      "241:\tlearn: 0.1182756\ttotal: 22.9s\tremaining: 1m 11s\n",
      "242:\tlearn: 0.1181876\ttotal: 23s\tremaining: 1m 11s\n",
      "243:\tlearn: 0.1181325\ttotal: 23.1s\tremaining: 1m 11s\n",
      "244:\tlearn: 0.1180662\ttotal: 23.2s\tremaining: 1m 11s\n",
      "245:\tlearn: 0.1179074\ttotal: 23.3s\tremaining: 1m 11s\n",
      "246:\tlearn: 0.1177943\ttotal: 23.4s\tremaining: 1m 11s\n",
      "247:\tlearn: 0.1177028\ttotal: 23.5s\tremaining: 1m 11s\n",
      "248:\tlearn: 0.1176175\ttotal: 23.6s\tremaining: 1m 11s\n",
      "249:\tlearn: 0.1175514\ttotal: 23.7s\tremaining: 1m 10s\n",
      "250:\tlearn: 0.1174594\ttotal: 23.8s\tremaining: 1m 10s\n",
      "251:\tlearn: 0.1173926\ttotal: 23.9s\tremaining: 1m 10s\n",
      "252:\tlearn: 0.1173197\ttotal: 23.9s\tremaining: 1m 10s\n",
      "253:\tlearn: 0.1172508\ttotal: 24s\tremaining: 1m 10s\n",
      "254:\tlearn: 0.1171741\ttotal: 24.1s\tremaining: 1m 10s\n",
      "255:\tlearn: 0.1171083\ttotal: 24.2s\tremaining: 1m 10s\n",
      "256:\tlearn: 0.1170511\ttotal: 24.3s\tremaining: 1m 10s\n",
      "257:\tlearn: 0.1169564\ttotal: 24.4s\tremaining: 1m 10s\n",
      "258:\tlearn: 0.1168809\ttotal: 24.5s\tremaining: 1m 10s\n",
      "259:\tlearn: 0.1167987\ttotal: 24.6s\tremaining: 1m 9s\n",
      "260:\tlearn: 0.1166938\ttotal: 24.7s\tremaining: 1m 9s\n",
      "261:\tlearn: 0.1165997\ttotal: 24.8s\tremaining: 1m 9s\n",
      "262:\tlearn: 0.1165177\ttotal: 24.9s\tremaining: 1m 9s\n",
      "263:\tlearn: 0.1164810\ttotal: 25s\tremaining: 1m 9s\n",
      "264:\tlearn: 0.1163665\ttotal: 25.1s\tremaining: 1m 9s\n",
      "265:\tlearn: 0.1162853\ttotal: 25.2s\tremaining: 1m 9s\n",
      "266:\tlearn: 0.1162034\ttotal: 25.2s\tremaining: 1m 9s\n",
      "267:\tlearn: 0.1161514\ttotal: 25.4s\tremaining: 1m 9s\n",
      "268:\tlearn: 0.1160851\ttotal: 25.4s\tremaining: 1m 9s\n",
      "269:\tlearn: 0.1159893\ttotal: 25.5s\tremaining: 1m 9s\n",
      "270:\tlearn: 0.1159016\ttotal: 25.6s\tremaining: 1m 8s\n",
      "271:\tlearn: 0.1158227\ttotal: 25.7s\tremaining: 1m 8s\n",
      "272:\tlearn: 0.1157314\ttotal: 25.8s\tremaining: 1m 8s\n",
      "273:\tlearn: 0.1156703\ttotal: 25.9s\tremaining: 1m 8s\n",
      "274:\tlearn: 0.1156180\ttotal: 26s\tremaining: 1m 8s\n",
      "275:\tlearn: 0.1155099\ttotal: 26.1s\tremaining: 1m 8s\n",
      "276:\tlearn: 0.1154311\ttotal: 26.2s\tremaining: 1m 8s\n",
      "277:\tlearn: 0.1153263\ttotal: 26.3s\tremaining: 1m 8s\n",
      "278:\tlearn: 0.1152737\ttotal: 26.4s\tremaining: 1m 8s\n",
      "279:\tlearn: 0.1152080\ttotal: 26.5s\tremaining: 1m 8s\n",
      "280:\tlearn: 0.1151511\ttotal: 26.6s\tremaining: 1m 7s\n",
      "281:\tlearn: 0.1151016\ttotal: 26.7s\tremaining: 1m 7s\n",
      "282:\tlearn: 0.1150278\ttotal: 26.8s\tremaining: 1m 7s\n",
      "283:\tlearn: 0.1149628\ttotal: 26.9s\tremaining: 1m 7s\n",
      "284:\tlearn: 0.1149150\ttotal: 27s\tremaining: 1m 7s\n",
      "285:\tlearn: 0.1148546\ttotal: 27.1s\tremaining: 1m 7s\n",
      "286:\tlearn: 0.1147745\ttotal: 27.2s\tremaining: 1m 7s\n",
      "287:\tlearn: 0.1147125\ttotal: 27.3s\tremaining: 1m 7s\n",
      "288:\tlearn: 0.1146376\ttotal: 27.4s\tremaining: 1m 7s\n",
      "289:\tlearn: 0.1145708\ttotal: 27.5s\tremaining: 1m 7s\n",
      "290:\tlearn: 0.1145111\ttotal: 27.6s\tremaining: 1m 7s\n",
      "291:\tlearn: 0.1144546\ttotal: 27.7s\tremaining: 1m 7s\n",
      "292:\tlearn: 0.1144333\ttotal: 27.8s\tremaining: 1m 6s\n",
      "293:\tlearn: 0.1143392\ttotal: 27.9s\tremaining: 1m 6s\n",
      "294:\tlearn: 0.1142644\ttotal: 28s\tremaining: 1m 6s\n",
      "295:\tlearn: 0.1141265\ttotal: 28.1s\tremaining: 1m 6s\n",
      "296:\tlearn: 0.1140634\ttotal: 28.2s\tremaining: 1m 6s\n",
      "297:\tlearn: 0.1139845\ttotal: 28.3s\tremaining: 1m 6s\n",
      "298:\tlearn: 0.1139178\ttotal: 28.4s\tremaining: 1m 6s\n",
      "299:\tlearn: 0.1137370\ttotal: 28.5s\tremaining: 1m 6s\n",
      "300:\tlearn: 0.1136113\ttotal: 28.6s\tremaining: 1m 6s\n",
      "301:\tlearn: 0.1134880\ttotal: 28.7s\tremaining: 1m 6s\n",
      "302:\tlearn: 0.1133748\ttotal: 28.9s\tremaining: 1m 6s\n",
      "303:\tlearn: 0.1133330\ttotal: 29s\tremaining: 1m 6s\n",
      "304:\tlearn: 0.1132257\ttotal: 29.1s\tremaining: 1m 6s\n",
      "305:\tlearn: 0.1131578\ttotal: 29.2s\tremaining: 1m 6s\n",
      "306:\tlearn: 0.1130941\ttotal: 29.3s\tremaining: 1m 6s\n",
      "307:\tlearn: 0.1130344\ttotal: 29.4s\tremaining: 1m 6s\n",
      "308:\tlearn: 0.1129543\ttotal: 29.5s\tremaining: 1m 5s\n",
      "309:\tlearn: 0.1128784\ttotal: 29.6s\tremaining: 1m 5s\n",
      "310:\tlearn: 0.1128084\ttotal: 29.7s\tremaining: 1m 5s\n",
      "311:\tlearn: 0.1126932\ttotal: 29.8s\tremaining: 1m 5s\n",
      "312:\tlearn: 0.1126262\ttotal: 29.9s\tremaining: 1m 5s\n",
      "313:\tlearn: 0.1125723\ttotal: 30s\tremaining: 1m 5s\n",
      "314:\tlearn: 0.1125279\ttotal: 30.1s\tremaining: 1m 5s\n",
      "315:\tlearn: 0.1124599\ttotal: 30.2s\tremaining: 1m 5s\n",
      "316:\tlearn: 0.1124078\ttotal: 30.2s\tremaining: 1m 5s\n",
      "317:\tlearn: 0.1123591\ttotal: 30.3s\tremaining: 1m 5s\n",
      "318:\tlearn: 0.1122762\ttotal: 30.5s\tremaining: 1m 5s\n",
      "319:\tlearn: 0.1122053\ttotal: 30.6s\tremaining: 1m 5s\n",
      "320:\tlearn: 0.1121076\ttotal: 30.7s\tremaining: 1m 4s\n",
      "321:\tlearn: 0.1120445\ttotal: 30.8s\tremaining: 1m 4s\n",
      "322:\tlearn: 0.1120054\ttotal: 30.9s\tremaining: 1m 4s\n",
      "323:\tlearn: 0.1118892\ttotal: 31s\tremaining: 1m 4s\n",
      "324:\tlearn: 0.1118260\ttotal: 31.1s\tremaining: 1m 4s\n",
      "325:\tlearn: 0.1117205\ttotal: 31.2s\tremaining: 1m 4s\n",
      "326:\tlearn: 0.1116737\ttotal: 31.3s\tremaining: 1m 4s\n",
      "327:\tlearn: 0.1116085\ttotal: 31.4s\tremaining: 1m 4s\n",
      "328:\tlearn: 0.1115578\ttotal: 31.4s\tremaining: 1m 4s\n",
      "329:\tlearn: 0.1115093\ttotal: 31.5s\tremaining: 1m 4s\n",
      "330:\tlearn: 0.1114633\ttotal: 31.6s\tremaining: 1m 3s\n",
      "331:\tlearn: 0.1113739\ttotal: 31.7s\tremaining: 1m 3s\n",
      "332:\tlearn: 0.1113105\ttotal: 31.8s\tremaining: 1m 3s\n",
      "333:\tlearn: 0.1112233\ttotal: 31.9s\tremaining: 1m 3s\n",
      "334:\tlearn: 0.1111515\ttotal: 32s\tremaining: 1m 3s\n",
      "335:\tlearn: 0.1110813\ttotal: 32.1s\tremaining: 1m 3s\n",
      "336:\tlearn: 0.1110320\ttotal: 32.2s\tremaining: 1m 3s\n",
      "337:\tlearn: 0.1109610\ttotal: 32.3s\tremaining: 1m 3s\n",
      "338:\tlearn: 0.1108938\ttotal: 32.4s\tremaining: 1m 3s\n",
      "339:\tlearn: 0.1108286\ttotal: 32.5s\tremaining: 1m 3s\n",
      "340:\tlearn: 0.1107233\ttotal: 32.6s\tremaining: 1m 3s\n",
      "341:\tlearn: 0.1106457\ttotal: 32.7s\tremaining: 1m 2s\n",
      "342:\tlearn: 0.1105966\ttotal: 32.8s\tremaining: 1m 2s\n",
      "343:\tlearn: 0.1105263\ttotal: 32.9s\tremaining: 1m 2s\n",
      "344:\tlearn: 0.1104527\ttotal: 33s\tremaining: 1m 2s\n",
      "345:\tlearn: 0.1103891\ttotal: 33.1s\tremaining: 1m 2s\n",
      "346:\tlearn: 0.1103163\ttotal: 33.2s\tremaining: 1m 2s\n",
      "347:\tlearn: 0.1102808\ttotal: 33.3s\tremaining: 1m 2s\n",
      "348:\tlearn: 0.1102423\ttotal: 33.4s\tremaining: 1m 2s\n",
      "349:\tlearn: 0.1101703\ttotal: 33.5s\tremaining: 1m 2s\n",
      "350:\tlearn: 0.1101254\ttotal: 33.7s\tremaining: 1m 2s\n",
      "351:\tlearn: 0.1100886\ttotal: 33.8s\tremaining: 1m 2s\n",
      "352:\tlearn: 0.1100420\ttotal: 33.9s\tremaining: 1m 2s\n",
      "353:\tlearn: 0.1099837\ttotal: 34s\tremaining: 1m 1s\n",
      "354:\tlearn: 0.1099467\ttotal: 34.1s\tremaining: 1m 1s\n",
      "355:\tlearn: 0.1098952\ttotal: 34.1s\tremaining: 1m 1s\n",
      "356:\tlearn: 0.1098048\ttotal: 34.2s\tremaining: 1m 1s\n",
      "357:\tlearn: 0.1097712\ttotal: 34.3s\tremaining: 1m 1s\n",
      "358:\tlearn: 0.1097309\ttotal: 34.4s\tremaining: 1m 1s\n",
      "359:\tlearn: 0.1096630\ttotal: 34.5s\tremaining: 1m 1s\n",
      "360:\tlearn: 0.1096172\ttotal: 34.6s\tremaining: 1m 1s\n",
      "361:\tlearn: 0.1095028\ttotal: 34.7s\tremaining: 1m 1s\n",
      "362:\tlearn: 0.1094185\ttotal: 34.8s\tremaining: 1m 1s\n",
      "363:\tlearn: 0.1093354\ttotal: 34.9s\tremaining: 1m 1s\n",
      "364:\tlearn: 0.1092819\ttotal: 35s\tremaining: 1m\n",
      "365:\tlearn: 0.1092520\ttotal: 35.1s\tremaining: 1m\n",
      "366:\tlearn: 0.1091941\ttotal: 35.2s\tremaining: 1m\n",
      "367:\tlearn: 0.1090997\ttotal: 35.3s\tremaining: 1m\n",
      "368:\tlearn: 0.1090030\ttotal: 35.4s\tremaining: 1m\n",
      "369:\tlearn: 0.1089431\ttotal: 35.5s\tremaining: 1m\n",
      "370:\tlearn: 0.1088689\ttotal: 35.6s\tremaining: 1m\n",
      "371:\tlearn: 0.1088227\ttotal: 35.7s\tremaining: 1m\n",
      "372:\tlearn: 0.1087925\ttotal: 35.8s\tremaining: 1m\n",
      "373:\tlearn: 0.1087366\ttotal: 35.9s\tremaining: 1m\n",
      "374:\tlearn: 0.1086408\ttotal: 36s\tremaining: 1m\n",
      "375:\tlearn: 0.1085843\ttotal: 36.1s\tremaining: 60s\n",
      "376:\tlearn: 0.1085542\ttotal: 36.3s\tremaining: 59.9s\n",
      "377:\tlearn: 0.1085018\ttotal: 36.4s\tremaining: 59.8s\n",
      "378:\tlearn: 0.1084417\ttotal: 36.5s\tremaining: 59.7s\n",
      "379:\tlearn: 0.1084013\ttotal: 36.6s\tremaining: 59.6s\n",
      "380:\tlearn: 0.1083488\ttotal: 36.7s\tremaining: 59.6s\n",
      "381:\tlearn: 0.1082773\ttotal: 36.8s\tremaining: 59.5s\n",
      "382:\tlearn: 0.1082036\ttotal: 36.9s\tremaining: 59.4s\n",
      "383:\tlearn: 0.1081227\ttotal: 37s\tremaining: 59.3s\n",
      "384:\tlearn: 0.1080644\ttotal: 37.1s\tremaining: 59.2s\n",
      "385:\tlearn: 0.1079755\ttotal: 37.2s\tremaining: 59.1s\n",
      "386:\tlearn: 0.1079134\ttotal: 37.3s\tremaining: 59s\n",
      "387:\tlearn: 0.1078013\ttotal: 37.4s\tremaining: 59s\n",
      "388:\tlearn: 0.1077567\ttotal: 37.5s\tremaining: 58.9s\n",
      "389:\tlearn: 0.1077248\ttotal: 37.6s\tremaining: 58.8s\n",
      "390:\tlearn: 0.1076103\ttotal: 37.7s\tremaining: 58.7s\n",
      "391:\tlearn: 0.1075638\ttotal: 37.8s\tremaining: 58.6s\n",
      "392:\tlearn: 0.1075302\ttotal: 37.9s\tremaining: 58.5s\n",
      "393:\tlearn: 0.1074846\ttotal: 38s\tremaining: 58.4s\n",
      "394:\tlearn: 0.1074461\ttotal: 38.1s\tremaining: 58.4s\n",
      "395:\tlearn: 0.1073669\ttotal: 38.2s\tremaining: 58.3s\n",
      "396:\tlearn: 0.1073201\ttotal: 38.4s\tremaining: 58.4s\n",
      "397:\tlearn: 0.1072879\ttotal: 38.5s\tremaining: 58.3s\n",
      "398:\tlearn: 0.1071965\ttotal: 38.6s\tremaining: 58.2s\n",
      "399:\tlearn: 0.1071592\ttotal: 38.8s\tremaining: 58.2s\n",
      "400:\tlearn: 0.1071179\ttotal: 38.9s\tremaining: 58.1s\n",
      "401:\tlearn: 0.1070539\ttotal: 39s\tremaining: 58s\n",
      "402:\tlearn: 0.1069601\ttotal: 39.1s\tremaining: 57.9s\n",
      "403:\tlearn: 0.1068853\ttotal: 39.2s\tremaining: 57.8s\n",
      "404:\tlearn: 0.1068303\ttotal: 39.3s\tremaining: 57.7s\n",
      "405:\tlearn: 0.1067474\ttotal: 39.4s\tremaining: 57.6s\n",
      "406:\tlearn: 0.1066801\ttotal: 39.5s\tremaining: 57.5s\n",
      "407:\tlearn: 0.1066423\ttotal: 39.6s\tremaining: 57.4s\n",
      "408:\tlearn: 0.1066235\ttotal: 39.7s\tremaining: 57.3s\n",
      "409:\tlearn: 0.1065066\ttotal: 39.8s\tremaining: 57.2s\n",
      "410:\tlearn: 0.1064913\ttotal: 39.9s\tremaining: 57.1s\n",
      "411:\tlearn: 0.1064562\ttotal: 40s\tremaining: 57s\n",
      "412:\tlearn: 0.1064314\ttotal: 40s\tremaining: 56.9s\n",
      "413:\tlearn: 0.1063857\ttotal: 40.1s\tremaining: 56.8s\n",
      "414:\tlearn: 0.1063456\ttotal: 40.2s\tremaining: 56.7s\n",
      "415:\tlearn: 0.1062877\ttotal: 40.3s\tremaining: 56.6s\n",
      "416:\tlearn: 0.1062467\ttotal: 40.4s\tremaining: 56.5s\n",
      "417:\tlearn: 0.1061965\ttotal: 40.5s\tremaining: 56.4s\n",
      "418:\tlearn: 0.1061784\ttotal: 40.6s\tremaining: 56.3s\n",
      "419:\tlearn: 0.1060951\ttotal: 40.7s\tremaining: 56.2s\n",
      "420:\tlearn: 0.1060324\ttotal: 40.8s\tremaining: 56.1s\n",
      "421:\tlearn: 0.1059398\ttotal: 40.9s\tremaining: 56s\n",
      "422:\tlearn: 0.1058560\ttotal: 41s\tremaining: 55.9s\n",
      "423:\tlearn: 0.1058071\ttotal: 41.1s\tremaining: 55.8s\n",
      "424:\tlearn: 0.1057371\ttotal: 41.2s\tremaining: 55.7s\n",
      "425:\tlearn: 0.1056639\ttotal: 41.3s\tremaining: 55.6s\n",
      "426:\tlearn: 0.1055931\ttotal: 41.4s\tremaining: 55.6s\n",
      "427:\tlearn: 0.1055291\ttotal: 41.5s\tremaining: 55.5s\n",
      "428:\tlearn: 0.1054750\ttotal: 41.6s\tremaining: 55.4s\n",
      "429:\tlearn: 0.1053635\ttotal: 41.7s\tremaining: 55.3s\n",
      "430:\tlearn: 0.1053158\ttotal: 41.8s\tremaining: 55.2s\n",
      "431:\tlearn: 0.1052378\ttotal: 41.9s\tremaining: 55.1s\n",
      "432:\tlearn: 0.1051864\ttotal: 42s\tremaining: 55s\n",
      "433:\tlearn: 0.1051508\ttotal: 42.1s\tremaining: 54.9s\n",
      "434:\tlearn: 0.1051010\ttotal: 42.2s\tremaining: 54.8s\n",
      "435:\tlearn: 0.1050489\ttotal: 42.3s\tremaining: 54.7s\n",
      "436:\tlearn: 0.1049741\ttotal: 42.4s\tremaining: 54.6s\n",
      "437:\tlearn: 0.1049191\ttotal: 42.5s\tremaining: 54.5s\n",
      "438:\tlearn: 0.1048927\ttotal: 42.6s\tremaining: 54.4s\n",
      "439:\tlearn: 0.1048550\ttotal: 42.7s\tremaining: 54.4s\n",
      "440:\tlearn: 0.1047888\ttotal: 42.8s\tremaining: 54.3s\n",
      "441:\tlearn: 0.1047504\ttotal: 42.9s\tremaining: 54.2s\n",
      "442:\tlearn: 0.1046754\ttotal: 43s\tremaining: 54.1s\n",
      "443:\tlearn: 0.1045911\ttotal: 43.1s\tremaining: 54s\n",
      "444:\tlearn: 0.1045551\ttotal: 43.2s\tremaining: 53.9s\n",
      "445:\tlearn: 0.1044934\ttotal: 43.3s\tremaining: 53.8s\n",
      "446:\tlearn: 0.1044475\ttotal: 43.4s\tremaining: 53.7s\n",
      "447:\tlearn: 0.1043463\ttotal: 43.5s\tremaining: 53.6s\n",
      "448:\tlearn: 0.1042640\ttotal: 43.6s\tremaining: 53.5s\n",
      "449:\tlearn: 0.1042148\ttotal: 43.7s\tremaining: 53.5s\n",
      "450:\tlearn: 0.1041485\ttotal: 43.9s\tremaining: 53.4s\n",
      "451:\tlearn: 0.1040685\ttotal: 44s\tremaining: 53.3s\n",
      "452:\tlearn: 0.1040208\ttotal: 44.1s\tremaining: 53.2s\n",
      "453:\tlearn: 0.1039811\ttotal: 44.2s\tremaining: 53.1s\n",
      "454:\tlearn: 0.1039018\ttotal: 44.3s\tremaining: 53s\n",
      "455:\tlearn: 0.1038572\ttotal: 44.4s\tremaining: 52.9s\n",
      "456:\tlearn: 0.1038081\ttotal: 44.5s\tremaining: 52.8s\n",
      "457:\tlearn: 0.1037531\ttotal: 44.6s\tremaining: 52.7s\n",
      "458:\tlearn: 0.1037260\ttotal: 44.7s\tremaining: 52.6s\n",
      "459:\tlearn: 0.1036971\ttotal: 44.8s\tremaining: 52.5s\n",
      "460:\tlearn: 0.1036528\ttotal: 44.8s\tremaining: 52.4s\n",
      "461:\tlearn: 0.1036320\ttotal: 44.9s\tremaining: 52.3s\n",
      "462:\tlearn: 0.1035635\ttotal: 45s\tremaining: 52.2s\n",
      "463:\tlearn: 0.1035107\ttotal: 45.1s\tremaining: 52.1s\n",
      "464:\tlearn: 0.1034669\ttotal: 45.2s\tremaining: 52s\n",
      "465:\tlearn: 0.1034280\ttotal: 45.3s\tremaining: 51.9s\n",
      "466:\tlearn: 0.1033663\ttotal: 45.4s\tremaining: 51.8s\n",
      "467:\tlearn: 0.1033017\ttotal: 45.5s\tremaining: 51.7s\n",
      "468:\tlearn: 0.1032614\ttotal: 45.6s\tremaining: 51.6s\n",
      "469:\tlearn: 0.1032239\ttotal: 45.7s\tremaining: 51.5s\n",
      "470:\tlearn: 0.1031724\ttotal: 45.8s\tremaining: 51.4s\n",
      "471:\tlearn: 0.1030746\ttotal: 45.9s\tremaining: 51.3s\n",
      "472:\tlearn: 0.1030428\ttotal: 46s\tremaining: 51.2s\n",
      "473:\tlearn: 0.1029877\ttotal: 46.1s\tremaining: 51.1s\n",
      "474:\tlearn: 0.1029318\ttotal: 46.2s\tremaining: 51s\n",
      "475:\tlearn: 0.1028847\ttotal: 46.3s\tremaining: 50.9s\n",
      "476:\tlearn: 0.1028298\ttotal: 46.4s\tremaining: 50.8s\n",
      "477:\tlearn: 0.1027916\ttotal: 46.5s\tremaining: 50.8s\n",
      "478:\tlearn: 0.1027150\ttotal: 46.6s\tremaining: 50.7s\n",
      "479:\tlearn: 0.1026756\ttotal: 46.7s\tremaining: 50.6s\n",
      "480:\tlearn: 0.1025992\ttotal: 46.8s\tremaining: 50.5s\n",
      "481:\tlearn: 0.1025550\ttotal: 46.9s\tremaining: 50.4s\n",
      "482:\tlearn: 0.1025212\ttotal: 47s\tremaining: 50.3s\n",
      "483:\tlearn: 0.1024716\ttotal: 47.1s\tremaining: 50.2s\n",
      "484:\tlearn: 0.1024474\ttotal: 47.2s\tremaining: 50.1s\n",
      "485:\tlearn: 0.1023890\ttotal: 47.3s\tremaining: 50s\n",
      "486:\tlearn: 0.1023603\ttotal: 47.4s\tremaining: 49.9s\n",
      "487:\tlearn: 0.1023297\ttotal: 47.5s\tremaining: 49.8s\n",
      "488:\tlearn: 0.1022846\ttotal: 47.6s\tremaining: 49.7s\n",
      "489:\tlearn: 0.1022208\ttotal: 47.7s\tremaining: 49.6s\n",
      "490:\tlearn: 0.1021503\ttotal: 47.8s\tremaining: 49.5s\n",
      "491:\tlearn: 0.1021112\ttotal: 48s\tremaining: 49.5s\n",
      "492:\tlearn: 0.1020492\ttotal: 48.1s\tremaining: 49.4s\n",
      "493:\tlearn: 0.1019931\ttotal: 48.2s\tremaining: 49.3s\n",
      "494:\tlearn: 0.1019424\ttotal: 48.3s\tremaining: 49.2s\n",
      "495:\tlearn: 0.1019062\ttotal: 48.4s\tremaining: 49.1s\n",
      "496:\tlearn: 0.1018349\ttotal: 48.5s\tremaining: 49s\n",
      "497:\tlearn: 0.1017839\ttotal: 48.6s\tremaining: 48.9s\n",
      "498:\tlearn: 0.1017580\ttotal: 48.6s\tremaining: 48.8s\n",
      "499:\tlearn: 0.1016973\ttotal: 48.7s\tremaining: 48.7s\n",
      "500:\tlearn: 0.1016730\ttotal: 48.8s\tremaining: 48.6s\n",
      "501:\tlearn: 0.1016296\ttotal: 48.9s\tremaining: 48.5s\n",
      "502:\tlearn: 0.1015374\ttotal: 49s\tremaining: 48.4s\n",
      "503:\tlearn: 0.1014544\ttotal: 49.1s\tremaining: 48.3s\n",
      "504:\tlearn: 0.1013913\ttotal: 49.2s\tremaining: 48.2s\n",
      "505:\tlearn: 0.1012892\ttotal: 49.3s\tremaining: 48.1s\n",
      "506:\tlearn: 0.1012481\ttotal: 49.4s\tremaining: 48s\n",
      "507:\tlearn: 0.1012192\ttotal: 49.5s\tremaining: 47.9s\n",
      "508:\tlearn: 0.1011686\ttotal: 49.6s\tremaining: 47.8s\n",
      "509:\tlearn: 0.1011408\ttotal: 49.7s\tremaining: 47.7s\n",
      "510:\tlearn: 0.1010682\ttotal: 49.8s\tremaining: 47.6s\n",
      "511:\tlearn: 0.1010534\ttotal: 49.9s\tremaining: 47.6s\n",
      "512:\tlearn: 0.1010077\ttotal: 50s\tremaining: 47.5s\n",
      "513:\tlearn: 0.1009761\ttotal: 50.1s\tremaining: 47.4s\n",
      "514:\tlearn: 0.1009368\ttotal: 50.2s\tremaining: 47.3s\n",
      "515:\tlearn: 0.1009237\ttotal: 50.3s\tremaining: 47.2s\n",
      "516:\tlearn: 0.1008866\ttotal: 50.4s\tremaining: 47.1s\n",
      "517:\tlearn: 0.1008028\ttotal: 50.5s\tremaining: 47s\n",
      "518:\tlearn: 0.1007303\ttotal: 50.6s\tremaining: 46.9s\n",
      "519:\tlearn: 0.1007040\ttotal: 50.7s\tremaining: 46.8s\n",
      "520:\tlearn: 0.1006621\ttotal: 50.8s\tremaining: 46.7s\n",
      "521:\tlearn: 0.1006128\ttotal: 50.9s\tremaining: 46.6s\n",
      "522:\tlearn: 0.1005540\ttotal: 50.9s\tremaining: 46.5s\n",
      "523:\tlearn: 0.1005105\ttotal: 51s\tremaining: 46.4s\n",
      "524:\tlearn: 0.1004374\ttotal: 51.1s\tremaining: 46.3s\n",
      "525:\tlearn: 0.1004318\ttotal: 51.2s\tremaining: 46.2s\n",
      "526:\tlearn: 0.1003750\ttotal: 51.3s\tremaining: 46.1s\n",
      "527:\tlearn: 0.1003359\ttotal: 51.4s\tremaining: 46s\n",
      "528:\tlearn: 0.1002816\ttotal: 51.5s\tremaining: 45.9s\n",
      "529:\tlearn: 0.1002137\ttotal: 51.6s\tremaining: 45.8s\n",
      "530:\tlearn: 0.1001839\ttotal: 51.7s\tremaining: 45.7s\n",
      "531:\tlearn: 0.1001427\ttotal: 51.8s\tremaining: 45.6s\n",
      "532:\tlearn: 0.1000927\ttotal: 51.9s\tremaining: 45.5s\n",
      "533:\tlearn: 0.0999905\ttotal: 52s\tremaining: 45.4s\n",
      "534:\tlearn: 0.0999561\ttotal: 52.1s\tremaining: 45.3s\n",
      "535:\tlearn: 0.0999040\ttotal: 52.2s\tremaining: 45.2s\n",
      "536:\tlearn: 0.0998768\ttotal: 52.3s\tremaining: 45.1s\n",
      "537:\tlearn: 0.0998195\ttotal: 52.4s\tremaining: 45s\n",
      "538:\tlearn: 0.0997722\ttotal: 52.5s\tremaining: 44.9s\n",
      "539:\tlearn: 0.0997335\ttotal: 52.6s\tremaining: 44.8s\n",
      "540:\tlearn: 0.0997046\ttotal: 52.7s\tremaining: 44.7s\n",
      "541:\tlearn: 0.0996783\ttotal: 52.8s\tremaining: 44.6s\n",
      "542:\tlearn: 0.0996293\ttotal: 52.9s\tremaining: 44.5s\n",
      "543:\tlearn: 0.0995853\ttotal: 53s\tremaining: 44.4s\n",
      "544:\tlearn: 0.0995386\ttotal: 53.1s\tremaining: 44.3s\n",
      "545:\tlearn: 0.0995058\ttotal: 53.2s\tremaining: 44.2s\n",
      "546:\tlearn: 0.0994871\ttotal: 53.3s\tremaining: 44.1s\n",
      "547:\tlearn: 0.0994099\ttotal: 53.4s\tremaining: 44s\n",
      "548:\tlearn: 0.0993441\ttotal: 53.5s\tremaining: 43.9s\n",
      "549:\tlearn: 0.0993177\ttotal: 53.6s\tremaining: 43.8s\n",
      "550:\tlearn: 0.0993028\ttotal: 53.7s\tremaining: 43.7s\n",
      "551:\tlearn: 0.0992690\ttotal: 53.7s\tremaining: 43.6s\n",
      "552:\tlearn: 0.0992325\ttotal: 53.8s\tremaining: 43.5s\n",
      "553:\tlearn: 0.0991663\ttotal: 53.9s\tremaining: 43.4s\n",
      "554:\tlearn: 0.0991475\ttotal: 54s\tremaining: 43.3s\n",
      "555:\tlearn: 0.0990822\ttotal: 54.1s\tremaining: 43.2s\n",
      "556:\tlearn: 0.0990461\ttotal: 54.2s\tremaining: 43.1s\n",
      "557:\tlearn: 0.0989893\ttotal: 54.3s\tremaining: 43s\n",
      "558:\tlearn: 0.0989361\ttotal: 54.4s\tremaining: 42.9s\n",
      "559:\tlearn: 0.0989028\ttotal: 54.5s\tremaining: 42.8s\n",
      "560:\tlearn: 0.0988305\ttotal: 54.6s\tremaining: 42.7s\n",
      "561:\tlearn: 0.0988035\ttotal: 54.7s\tremaining: 42.6s\n",
      "562:\tlearn: 0.0987792\ttotal: 54.8s\tremaining: 42.5s\n",
      "563:\tlearn: 0.0987621\ttotal: 54.9s\tremaining: 42.4s\n",
      "564:\tlearn: 0.0987517\ttotal: 55s\tremaining: 42.3s\n",
      "565:\tlearn: 0.0987091\ttotal: 55.1s\tremaining: 42.2s\n",
      "566:\tlearn: 0.0986804\ttotal: 55.2s\tremaining: 42.1s\n",
      "567:\tlearn: 0.0986537\ttotal: 55.2s\tremaining: 42s\n",
      "568:\tlearn: 0.0986251\ttotal: 55.3s\tremaining: 41.9s\n",
      "569:\tlearn: 0.0985748\ttotal: 55.4s\tremaining: 41.8s\n",
      "570:\tlearn: 0.0985562\ttotal: 55.5s\tremaining: 41.7s\n",
      "571:\tlearn: 0.0984506\ttotal: 55.7s\tremaining: 41.6s\n",
      "572:\tlearn: 0.0984172\ttotal: 55.7s\tremaining: 41.5s\n",
      "573:\tlearn: 0.0983520\ttotal: 55.8s\tremaining: 41.4s\n",
      "574:\tlearn: 0.0983206\ttotal: 55.9s\tremaining: 41.3s\n",
      "575:\tlearn: 0.0982872\ttotal: 56s\tremaining: 41.2s\n",
      "576:\tlearn: 0.0982513\ttotal: 56.1s\tremaining: 41.1s\n",
      "577:\tlearn: 0.0981712\ttotal: 56.2s\tremaining: 41s\n",
      "578:\tlearn: 0.0981257\ttotal: 56.3s\tremaining: 41s\n",
      "579:\tlearn: 0.0981121\ttotal: 56.4s\tremaining: 40.9s\n",
      "580:\tlearn: 0.0980585\ttotal: 56.5s\tremaining: 40.8s\n",
      "581:\tlearn: 0.0980263\ttotal: 56.6s\tremaining: 40.7s\n",
      "582:\tlearn: 0.0979503\ttotal: 56.7s\tremaining: 40.6s\n",
      "583:\tlearn: 0.0979414\ttotal: 56.8s\tremaining: 40.5s\n",
      "584:\tlearn: 0.0978379\ttotal: 56.9s\tremaining: 40.4s\n",
      "585:\tlearn: 0.0977656\ttotal: 57s\tremaining: 40.3s\n",
      "586:\tlearn: 0.0977241\ttotal: 57.1s\tremaining: 40.2s\n",
      "587:\tlearn: 0.0976563\ttotal: 57.2s\tremaining: 40.1s\n",
      "588:\tlearn: 0.0975434\ttotal: 57.4s\tremaining: 40s\n",
      "589:\tlearn: 0.0974875\ttotal: 57.5s\tremaining: 39.9s\n",
      "590:\tlearn: 0.0974629\ttotal: 57.6s\tremaining: 39.9s\n",
      "591:\tlearn: 0.0974390\ttotal: 57.8s\tremaining: 39.8s\n",
      "592:\tlearn: 0.0973693\ttotal: 57.9s\tremaining: 39.7s\n",
      "593:\tlearn: 0.0973420\ttotal: 58s\tremaining: 39.7s\n",
      "594:\tlearn: 0.0972974\ttotal: 58.1s\tremaining: 39.6s\n",
      "595:\tlearn: 0.0972551\ttotal: 58.2s\tremaining: 39.5s\n",
      "596:\tlearn: 0.0972309\ttotal: 58.4s\tremaining: 39.4s\n",
      "597:\tlearn: 0.0971103\ttotal: 58.5s\tremaining: 39.3s\n",
      "598:\tlearn: 0.0970680\ttotal: 58.6s\tremaining: 39.2s\n",
      "599:\tlearn: 0.0970216\ttotal: 58.7s\tremaining: 39.1s\n",
      "600:\tlearn: 0.0969742\ttotal: 58.8s\tremaining: 39s\n",
      "601:\tlearn: 0.0969316\ttotal: 58.9s\tremaining: 39s\n",
      "602:\tlearn: 0.0968708\ttotal: 59s\tremaining: 38.9s\n",
      "603:\tlearn: 0.0968278\ttotal: 59.1s\tremaining: 38.8s\n",
      "604:\tlearn: 0.0967694\ttotal: 59.2s\tremaining: 38.7s\n",
      "605:\tlearn: 0.0966912\ttotal: 59.3s\tremaining: 38.6s\n",
      "606:\tlearn: 0.0966511\ttotal: 59.5s\tremaining: 38.5s\n",
      "607:\tlearn: 0.0965963\ttotal: 59.6s\tremaining: 38.4s\n",
      "608:\tlearn: 0.0965827\ttotal: 59.7s\tremaining: 38.3s\n",
      "609:\tlearn: 0.0965159\ttotal: 59.8s\tremaining: 38.2s\n",
      "610:\tlearn: 0.0965085\ttotal: 59.9s\tremaining: 38.1s\n",
      "611:\tlearn: 0.0964577\ttotal: 1m\tremaining: 38.1s\n",
      "612:\tlearn: 0.0963905\ttotal: 1m\tremaining: 38s\n",
      "613:\tlearn: 0.0963673\ttotal: 1m\tremaining: 37.9s\n",
      "614:\tlearn: 0.0962877\ttotal: 1m\tremaining: 37.8s\n",
      "615:\tlearn: 0.0961573\ttotal: 1m\tremaining: 37.7s\n",
      "616:\tlearn: 0.0961160\ttotal: 1m\tremaining: 37.6s\n",
      "617:\tlearn: 0.0960643\ttotal: 1m\tremaining: 37.5s\n",
      "618:\tlearn: 0.0960372\ttotal: 1m\tremaining: 37.4s\n",
      "619:\tlearn: 0.0960212\ttotal: 1m\tremaining: 37.3s\n",
      "620:\tlearn: 0.0959451\ttotal: 1m 1s\tremaining: 37.2s\n",
      "621:\tlearn: 0.0959269\ttotal: 1m 1s\tremaining: 37.1s\n",
      "622:\tlearn: 0.0958899\ttotal: 1m 1s\tremaining: 37s\n",
      "623:\tlearn: 0.0958619\ttotal: 1m 1s\tremaining: 36.9s\n",
      "624:\tlearn: 0.0958235\ttotal: 1m 1s\tremaining: 36.9s\n",
      "625:\tlearn: 0.0957403\ttotal: 1m 1s\tremaining: 36.8s\n",
      "626:\tlearn: 0.0957176\ttotal: 1m 1s\tremaining: 36.7s\n",
      "627:\tlearn: 0.0956489\ttotal: 1m 1s\tremaining: 36.6s\n",
      "628:\tlearn: 0.0956266\ttotal: 1m 1s\tremaining: 36.5s\n",
      "629:\tlearn: 0.0956038\ttotal: 1m 1s\tremaining: 36.4s\n",
      "630:\tlearn: 0.0955628\ttotal: 1m 2s\tremaining: 36.3s\n",
      "631:\tlearn: 0.0955313\ttotal: 1m 2s\tremaining: 36.2s\n",
      "632:\tlearn: 0.0955062\ttotal: 1m 2s\tremaining: 36.1s\n",
      "633:\tlearn: 0.0954144\ttotal: 1m 2s\tremaining: 36s\n",
      "634:\tlearn: 0.0953805\ttotal: 1m 2s\tremaining: 35.9s\n",
      "635:\tlearn: 0.0953088\ttotal: 1m 2s\tremaining: 35.8s\n",
      "636:\tlearn: 0.0952221\ttotal: 1m 2s\tremaining: 35.7s\n",
      "637:\tlearn: 0.0951921\ttotal: 1m 2s\tremaining: 35.6s\n",
      "638:\tlearn: 0.0951351\ttotal: 1m 2s\tremaining: 35.5s\n",
      "639:\tlearn: 0.0951131\ttotal: 1m 2s\tremaining: 35.4s\n",
      "640:\tlearn: 0.0950508\ttotal: 1m 3s\tremaining: 35.3s\n",
      "641:\tlearn: 0.0950064\ttotal: 1m 3s\tremaining: 35.2s\n",
      "642:\tlearn: 0.0949571\ttotal: 1m 3s\tremaining: 35.1s\n",
      "643:\tlearn: 0.0949087\ttotal: 1m 3s\tremaining: 35s\n",
      "644:\tlearn: 0.0948705\ttotal: 1m 3s\tremaining: 34.9s\n",
      "645:\tlearn: 0.0948489\ttotal: 1m 3s\tremaining: 34.8s\n",
      "646:\tlearn: 0.0948146\ttotal: 1m 3s\tremaining: 34.7s\n",
      "647:\tlearn: 0.0947810\ttotal: 1m 3s\tremaining: 34.6s\n",
      "648:\tlearn: 0.0947070\ttotal: 1m 3s\tremaining: 34.5s\n",
      "649:\tlearn: 0.0946656\ttotal: 1m 3s\tremaining: 34.4s\n",
      "650:\tlearn: 0.0946089\ttotal: 1m 4s\tremaining: 34.4s\n",
      "651:\tlearn: 0.0945647\ttotal: 1m 4s\tremaining: 34.3s\n",
      "652:\tlearn: 0.0945413\ttotal: 1m 4s\tremaining: 34.2s\n",
      "653:\tlearn: 0.0944801\ttotal: 1m 4s\tremaining: 34.1s\n",
      "654:\tlearn: 0.0944455\ttotal: 1m 4s\tremaining: 34s\n",
      "655:\tlearn: 0.0944225\ttotal: 1m 4s\tremaining: 33.9s\n",
      "656:\tlearn: 0.0943631\ttotal: 1m 4s\tremaining: 33.8s\n",
      "657:\tlearn: 0.0942919\ttotal: 1m 4s\tremaining: 33.8s\n",
      "658:\tlearn: 0.0942482\ttotal: 1m 5s\tremaining: 33.7s\n",
      "659:\tlearn: 0.0942140\ttotal: 1m 5s\tremaining: 33.7s\n",
      "660:\tlearn: 0.0941845\ttotal: 1m 5s\tremaining: 33.6s\n",
      "661:\tlearn: 0.0941224\ttotal: 1m 5s\tremaining: 33.5s\n",
      "662:\tlearn: 0.0940840\ttotal: 1m 5s\tremaining: 33.4s\n",
      "663:\tlearn: 0.0940324\ttotal: 1m 5s\tremaining: 33.4s\n",
      "664:\tlearn: 0.0939911\ttotal: 1m 6s\tremaining: 33.3s\n",
      "665:\tlearn: 0.0939438\ttotal: 1m 6s\tremaining: 33.2s\n",
      "666:\tlearn: 0.0938898\ttotal: 1m 6s\tremaining: 33.1s\n",
      "667:\tlearn: 0.0938441\ttotal: 1m 6s\tremaining: 33s\n",
      "668:\tlearn: 0.0937765\ttotal: 1m 6s\tremaining: 32.9s\n",
      "669:\tlearn: 0.0937306\ttotal: 1m 6s\tremaining: 32.8s\n",
      "670:\tlearn: 0.0936673\ttotal: 1m 6s\tremaining: 32.7s\n",
      "671:\tlearn: 0.0936161\ttotal: 1m 6s\tremaining: 32.6s\n",
      "672:\tlearn: 0.0935879\ttotal: 1m 6s\tremaining: 32.5s\n",
      "673:\tlearn: 0.0935363\ttotal: 1m 6s\tremaining: 32.4s\n",
      "674:\tlearn: 0.0934636\ttotal: 1m 7s\tremaining: 32.3s\n",
      "675:\tlearn: 0.0933994\ttotal: 1m 7s\tremaining: 32.3s\n",
      "676:\tlearn: 0.0933838\ttotal: 1m 7s\tremaining: 32.2s\n",
      "677:\tlearn: 0.0933621\ttotal: 1m 7s\tremaining: 32.1s\n",
      "678:\tlearn: 0.0933423\ttotal: 1m 7s\tremaining: 32s\n",
      "679:\tlearn: 0.0933231\ttotal: 1m 7s\tremaining: 31.8s\n",
      "680:\tlearn: 0.0932979\ttotal: 1m 7s\tremaining: 31.7s\n",
      "681:\tlearn: 0.0932553\ttotal: 1m 7s\tremaining: 31.6s\n",
      "682:\tlearn: 0.0932169\ttotal: 1m 7s\tremaining: 31.6s\n",
      "683:\tlearn: 0.0931764\ttotal: 1m 8s\tremaining: 31.5s\n",
      "684:\tlearn: 0.0931129\ttotal: 1m 8s\tremaining: 31.4s\n",
      "685:\tlearn: 0.0930674\ttotal: 1m 8s\tremaining: 31.3s\n",
      "686:\tlearn: 0.0930299\ttotal: 1m 8s\tremaining: 31.2s\n",
      "687:\tlearn: 0.0929974\ttotal: 1m 8s\tremaining: 31.1s\n",
      "688:\tlearn: 0.0929297\ttotal: 1m 8s\tremaining: 31s\n",
      "689:\tlearn: 0.0928640\ttotal: 1m 8s\tremaining: 30.9s\n",
      "690:\tlearn: 0.0927864\ttotal: 1m 8s\tremaining: 30.8s\n",
      "691:\tlearn: 0.0927324\ttotal: 1m 8s\tremaining: 30.7s\n",
      "692:\tlearn: 0.0926281\ttotal: 1m 9s\tremaining: 30.6s\n",
      "693:\tlearn: 0.0925821\ttotal: 1m 9s\tremaining: 30.5s\n",
      "694:\tlearn: 0.0925086\ttotal: 1m 9s\tremaining: 30.4s\n",
      "695:\tlearn: 0.0924994\ttotal: 1m 9s\tremaining: 30.3s\n",
      "696:\tlearn: 0.0924720\ttotal: 1m 9s\tremaining: 30.2s\n",
      "697:\tlearn: 0.0924472\ttotal: 1m 9s\tremaining: 30.1s\n",
      "698:\tlearn: 0.0923819\ttotal: 1m 9s\tremaining: 30s\n",
      "699:\tlearn: 0.0923160\ttotal: 1m 9s\tremaining: 29.9s\n",
      "700:\tlearn: 0.0922752\ttotal: 1m 9s\tremaining: 29.8s\n",
      "701:\tlearn: 0.0922348\ttotal: 1m 10s\tremaining: 29.8s\n",
      "702:\tlearn: 0.0922010\ttotal: 1m 10s\tremaining: 29.7s\n",
      "703:\tlearn: 0.0921669\ttotal: 1m 10s\tremaining: 29.6s\n",
      "704:\tlearn: 0.0921345\ttotal: 1m 10s\tremaining: 29.5s\n",
      "705:\tlearn: 0.0921017\ttotal: 1m 10s\tremaining: 29.4s\n",
      "706:\tlearn: 0.0920149\ttotal: 1m 10s\tremaining: 29.3s\n",
      "707:\tlearn: 0.0919751\ttotal: 1m 10s\tremaining: 29.2s\n",
      "708:\tlearn: 0.0919389\ttotal: 1m 10s\tremaining: 29.1s\n",
      "709:\tlearn: 0.0919046\ttotal: 1m 10s\tremaining: 29s\n",
      "710:\tlearn: 0.0918628\ttotal: 1m 11s\tremaining: 28.9s\n",
      "711:\tlearn: 0.0918104\ttotal: 1m 11s\tremaining: 28.8s\n",
      "712:\tlearn: 0.0917984\ttotal: 1m 11s\tremaining: 28.7s\n",
      "713:\tlearn: 0.0917321\ttotal: 1m 11s\tremaining: 28.6s\n",
      "714:\tlearn: 0.0916685\ttotal: 1m 11s\tremaining: 28.5s\n",
      "715:\tlearn: 0.0916258\ttotal: 1m 11s\tremaining: 28.4s\n",
      "716:\tlearn: 0.0915967\ttotal: 1m 11s\tremaining: 28.3s\n",
      "717:\tlearn: 0.0915568\ttotal: 1m 11s\tremaining: 28.2s\n",
      "718:\tlearn: 0.0915062\ttotal: 1m 11s\tremaining: 28.1s\n",
      "719:\tlearn: 0.0914726\ttotal: 1m 11s\tremaining: 28s\n",
      "720:\tlearn: 0.0914258\ttotal: 1m 12s\tremaining: 27.9s\n",
      "721:\tlearn: 0.0913843\ttotal: 1m 12s\tremaining: 27.8s\n",
      "722:\tlearn: 0.0913567\ttotal: 1m 12s\tremaining: 27.7s\n",
      "723:\tlearn: 0.0913292\ttotal: 1m 12s\tremaining: 27.6s\n",
      "724:\tlearn: 0.0912796\ttotal: 1m 12s\tremaining: 27.5s\n",
      "725:\tlearn: 0.0912382\ttotal: 1m 12s\tremaining: 27.4s\n",
      "726:\tlearn: 0.0911798\ttotal: 1m 12s\tremaining: 27.3s\n",
      "727:\tlearn: 0.0911469\ttotal: 1m 12s\tremaining: 27.2s\n",
      "728:\tlearn: 0.0911090\ttotal: 1m 12s\tremaining: 27.1s\n",
      "729:\tlearn: 0.0910748\ttotal: 1m 12s\tremaining: 27s\n",
      "730:\tlearn: 0.0910558\ttotal: 1m 13s\tremaining: 26.9s\n",
      "731:\tlearn: 0.0909910\ttotal: 1m 13s\tremaining: 26.8s\n",
      "732:\tlearn: 0.0909494\ttotal: 1m 13s\tremaining: 26.7s\n",
      "733:\tlearn: 0.0909179\ttotal: 1m 13s\tremaining: 26.6s\n",
      "734:\tlearn: 0.0908905\ttotal: 1m 13s\tremaining: 26.5s\n",
      "735:\tlearn: 0.0908560\ttotal: 1m 13s\tremaining: 26.4s\n",
      "736:\tlearn: 0.0908359\ttotal: 1m 13s\tremaining: 26.3s\n",
      "737:\tlearn: 0.0907446\ttotal: 1m 13s\tremaining: 26.2s\n",
      "738:\tlearn: 0.0906729\ttotal: 1m 13s\tremaining: 26.1s\n",
      "739:\tlearn: 0.0906044\ttotal: 1m 13s\tremaining: 26s\n",
      "740:\tlearn: 0.0905760\ttotal: 1m 14s\tremaining: 25.9s\n",
      "741:\tlearn: 0.0905101\ttotal: 1m 14s\tremaining: 25.8s\n",
      "742:\tlearn: 0.0904943\ttotal: 1m 14s\tremaining: 25.7s\n",
      "743:\tlearn: 0.0904286\ttotal: 1m 14s\tremaining: 25.6s\n",
      "744:\tlearn: 0.0903989\ttotal: 1m 14s\tremaining: 25.5s\n",
      "745:\tlearn: 0.0903438\ttotal: 1m 14s\tremaining: 25.4s\n",
      "746:\tlearn: 0.0903049\ttotal: 1m 14s\tremaining: 25.3s\n",
      "747:\tlearn: 0.0902721\ttotal: 1m 14s\tremaining: 25.2s\n",
      "748:\tlearn: 0.0902352\ttotal: 1m 14s\tremaining: 25.1s\n",
      "749:\tlearn: 0.0902066\ttotal: 1m 14s\tremaining: 25s\n",
      "750:\tlearn: 0.0901766\ttotal: 1m 15s\tremaining: 24.9s\n",
      "751:\tlearn: 0.0901602\ttotal: 1m 15s\tremaining: 24.8s\n",
      "752:\tlearn: 0.0901231\ttotal: 1m 15s\tremaining: 24.7s\n",
      "753:\tlearn: 0.0900813\ttotal: 1m 15s\tremaining: 24.6s\n",
      "754:\tlearn: 0.0900389\ttotal: 1m 15s\tremaining: 24.5s\n",
      "755:\tlearn: 0.0900041\ttotal: 1m 15s\tremaining: 24.4s\n",
      "756:\tlearn: 0.0899332\ttotal: 1m 15s\tremaining: 24.3s\n",
      "757:\tlearn: 0.0898727\ttotal: 1m 15s\tremaining: 24.2s\n",
      "758:\tlearn: 0.0898283\ttotal: 1m 15s\tremaining: 24.1s\n",
      "759:\tlearn: 0.0897384\ttotal: 1m 16s\tremaining: 24s\n",
      "760:\tlearn: 0.0897060\ttotal: 1m 16s\tremaining: 23.9s\n",
      "761:\tlearn: 0.0896537\ttotal: 1m 16s\tremaining: 23.8s\n",
      "762:\tlearn: 0.0896179\ttotal: 1m 16s\tremaining: 23.7s\n",
      "763:\tlearn: 0.0895797\ttotal: 1m 16s\tremaining: 23.6s\n",
      "764:\tlearn: 0.0895372\ttotal: 1m 16s\tremaining: 23.5s\n",
      "765:\tlearn: 0.0894911\ttotal: 1m 16s\tremaining: 23.4s\n",
      "766:\tlearn: 0.0894464\ttotal: 1m 16s\tremaining: 23.3s\n",
      "767:\tlearn: 0.0893723\ttotal: 1m 16s\tremaining: 23.2s\n",
      "768:\tlearn: 0.0893388\ttotal: 1m 16s\tremaining: 23.1s\n",
      "769:\tlearn: 0.0893195\ttotal: 1m 17s\tremaining: 23s\n",
      "770:\tlearn: 0.0892972\ttotal: 1m 17s\tremaining: 23s\n",
      "771:\tlearn: 0.0892447\ttotal: 1m 17s\tremaining: 22.9s\n",
      "772:\tlearn: 0.0891960\ttotal: 1m 17s\tremaining: 22.8s\n",
      "773:\tlearn: 0.0891510\ttotal: 1m 17s\tremaining: 22.7s\n",
      "774:\tlearn: 0.0891178\ttotal: 1m 17s\tremaining: 22.6s\n",
      "775:\tlearn: 0.0890949\ttotal: 1m 17s\tremaining: 22.5s\n",
      "776:\tlearn: 0.0890297\ttotal: 1m 17s\tremaining: 22.4s\n",
      "777:\tlearn: 0.0890042\ttotal: 1m 18s\tremaining: 22.3s\n",
      "778:\tlearn: 0.0889720\ttotal: 1m 18s\tremaining: 22.2s\n",
      "779:\tlearn: 0.0889279\ttotal: 1m 18s\tremaining: 22.1s\n",
      "780:\tlearn: 0.0889090\ttotal: 1m 18s\tremaining: 22s\n",
      "781:\tlearn: 0.0888743\ttotal: 1m 18s\tremaining: 21.9s\n",
      "782:\tlearn: 0.0888166\ttotal: 1m 18s\tremaining: 21.8s\n",
      "783:\tlearn: 0.0887855\ttotal: 1m 18s\tremaining: 21.7s\n",
      "784:\tlearn: 0.0887002\ttotal: 1m 18s\tremaining: 21.6s\n",
      "785:\tlearn: 0.0886396\ttotal: 1m 18s\tremaining: 21.5s\n",
      "786:\tlearn: 0.0886031\ttotal: 1m 18s\tremaining: 21.4s\n",
      "787:\tlearn: 0.0885483\ttotal: 1m 19s\tremaining: 21.3s\n",
      "788:\tlearn: 0.0885104\ttotal: 1m 19s\tremaining: 21.2s\n",
      "789:\tlearn: 0.0884591\ttotal: 1m 19s\tremaining: 21.1s\n",
      "790:\tlearn: 0.0883924\ttotal: 1m 19s\tremaining: 21s\n",
      "791:\tlearn: 0.0883578\ttotal: 1m 19s\tremaining: 20.9s\n",
      "792:\tlearn: 0.0883255\ttotal: 1m 19s\tremaining: 20.8s\n",
      "793:\tlearn: 0.0882980\ttotal: 1m 19s\tremaining: 20.7s\n",
      "794:\tlearn: 0.0882737\ttotal: 1m 19s\tremaining: 20.6s\n",
      "795:\tlearn: 0.0882487\ttotal: 1m 19s\tremaining: 20.5s\n",
      "796:\tlearn: 0.0882237\ttotal: 1m 19s\tremaining: 20.4s\n",
      "797:\tlearn: 0.0881952\ttotal: 1m 20s\tremaining: 20.3s\n",
      "798:\tlearn: 0.0881393\ttotal: 1m 20s\tremaining: 20.2s\n",
      "799:\tlearn: 0.0881120\ttotal: 1m 20s\tremaining: 20.1s\n",
      "800:\tlearn: 0.0880834\ttotal: 1m 20s\tremaining: 20s\n",
      "801:\tlearn: 0.0880024\ttotal: 1m 20s\tremaining: 19.9s\n",
      "802:\tlearn: 0.0879396\ttotal: 1m 20s\tremaining: 19.8s\n",
      "803:\tlearn: 0.0878938\ttotal: 1m 20s\tremaining: 19.7s\n",
      "804:\tlearn: 0.0878195\ttotal: 1m 20s\tremaining: 19.6s\n",
      "805:\tlearn: 0.0877955\ttotal: 1m 20s\tremaining: 19.5s\n",
      "806:\tlearn: 0.0877662\ttotal: 1m 20s\tremaining: 19.4s\n",
      "807:\tlearn: 0.0877360\ttotal: 1m 21s\tremaining: 19.3s\n",
      "808:\tlearn: 0.0876972\ttotal: 1m 21s\tremaining: 19.1s\n",
      "809:\tlearn: 0.0876571\ttotal: 1m 21s\tremaining: 19.1s\n",
      "810:\tlearn: 0.0876159\ttotal: 1m 21s\tremaining: 19s\n",
      "811:\tlearn: 0.0875820\ttotal: 1m 21s\tremaining: 18.9s\n",
      "812:\tlearn: 0.0875401\ttotal: 1m 21s\tremaining: 18.8s\n",
      "813:\tlearn: 0.0874877\ttotal: 1m 21s\tremaining: 18.7s\n",
      "814:\tlearn: 0.0874424\ttotal: 1m 21s\tremaining: 18.6s\n",
      "815:\tlearn: 0.0873857\ttotal: 1m 21s\tremaining: 18.4s\n",
      "816:\tlearn: 0.0873551\ttotal: 1m 21s\tremaining: 18.4s\n",
      "817:\tlearn: 0.0872936\ttotal: 1m 22s\tremaining: 18.2s\n",
      "818:\tlearn: 0.0872572\ttotal: 1m 22s\tremaining: 18.1s\n",
      "819:\tlearn: 0.0872200\ttotal: 1m 22s\tremaining: 18s\n",
      "820:\tlearn: 0.0871789\ttotal: 1m 22s\tremaining: 17.9s\n",
      "821:\tlearn: 0.0871428\ttotal: 1m 22s\tremaining: 17.8s\n",
      "822:\tlearn: 0.0871019\ttotal: 1m 22s\tremaining: 17.7s\n",
      "823:\tlearn: 0.0870568\ttotal: 1m 22s\tremaining: 17.6s\n",
      "824:\tlearn: 0.0870013\ttotal: 1m 22s\tremaining: 17.5s\n",
      "825:\tlearn: 0.0869530\ttotal: 1m 22s\tremaining: 17.4s\n",
      "826:\tlearn: 0.0869444\ttotal: 1m 22s\tremaining: 17.3s\n",
      "827:\tlearn: 0.0869086\ttotal: 1m 23s\tremaining: 17.2s\n",
      "828:\tlearn: 0.0868362\ttotal: 1m 23s\tremaining: 17.1s\n",
      "829:\tlearn: 0.0868088\ttotal: 1m 23s\tremaining: 17s\n",
      "830:\tlearn: 0.0867672\ttotal: 1m 23s\tremaining: 16.9s\n",
      "831:\tlearn: 0.0867454\ttotal: 1m 23s\tremaining: 16.8s\n",
      "832:\tlearn: 0.0867097\ttotal: 1m 23s\tremaining: 16.7s\n",
      "833:\tlearn: 0.0866451\ttotal: 1m 23s\tremaining: 16.6s\n",
      "834:\tlearn: 0.0866212\ttotal: 1m 23s\tremaining: 16.5s\n",
      "835:\tlearn: 0.0865805\ttotal: 1m 23s\tremaining: 16.4s\n",
      "836:\tlearn: 0.0865529\ttotal: 1m 23s\tremaining: 16.3s\n",
      "837:\tlearn: 0.0865431\ttotal: 1m 24s\tremaining: 16.2s\n",
      "838:\tlearn: 0.0865103\ttotal: 1m 24s\tremaining: 16.1s\n",
      "839:\tlearn: 0.0864270\ttotal: 1m 24s\tremaining: 16s\n",
      "840:\tlearn: 0.0864025\ttotal: 1m 24s\tremaining: 15.9s\n",
      "841:\tlearn: 0.0863457\ttotal: 1m 24s\tremaining: 15.8s\n",
      "842:\tlearn: 0.0863275\ttotal: 1m 24s\tremaining: 15.7s\n",
      "843:\tlearn: 0.0862603\ttotal: 1m 24s\tremaining: 15.6s\n",
      "844:\tlearn: 0.0861880\ttotal: 1m 24s\tremaining: 15.5s\n",
      "845:\tlearn: 0.0861077\ttotal: 1m 24s\tremaining: 15.4s\n",
      "846:\tlearn: 0.0860761\ttotal: 1m 24s\tremaining: 15.3s\n",
      "847:\tlearn: 0.0860405\ttotal: 1m 25s\tremaining: 15.2s\n",
      "848:\tlearn: 0.0859882\ttotal: 1m 25s\tremaining: 15.1s\n",
      "849:\tlearn: 0.0859589\ttotal: 1m 25s\tremaining: 15s\n",
      "850:\tlearn: 0.0859103\ttotal: 1m 25s\tremaining: 14.9s\n",
      "851:\tlearn: 0.0858831\ttotal: 1m 25s\tremaining: 14.8s\n",
      "852:\tlearn: 0.0858224\ttotal: 1m 25s\tremaining: 14.7s\n",
      "853:\tlearn: 0.0857839\ttotal: 1m 25s\tremaining: 14.6s\n",
      "854:\tlearn: 0.0857589\ttotal: 1m 25s\tremaining: 14.5s\n",
      "855:\tlearn: 0.0857267\ttotal: 1m 25s\tremaining: 14.4s\n",
      "856:\tlearn: 0.0857018\ttotal: 1m 25s\tremaining: 14.3s\n",
      "857:\tlearn: 0.0856595\ttotal: 1m 26s\tremaining: 14.2s\n",
      "858:\tlearn: 0.0856181\ttotal: 1m 26s\tremaining: 14.1s\n",
      "859:\tlearn: 0.0856014\ttotal: 1m 26s\tremaining: 14s\n",
      "860:\tlearn: 0.0855674\ttotal: 1m 26s\tremaining: 13.9s\n",
      "861:\tlearn: 0.0855170\ttotal: 1m 26s\tremaining: 13.8s\n",
      "862:\tlearn: 0.0854602\ttotal: 1m 26s\tremaining: 13.7s\n",
      "863:\tlearn: 0.0854427\ttotal: 1m 26s\tremaining: 13.6s\n",
      "864:\tlearn: 0.0854134\ttotal: 1m 26s\tremaining: 13.5s\n",
      "865:\tlearn: 0.0853725\ttotal: 1m 26s\tremaining: 13.4s\n",
      "866:\tlearn: 0.0853364\ttotal: 1m 27s\tremaining: 13.3s\n",
      "867:\tlearn: 0.0852941\ttotal: 1m 27s\tremaining: 13.2s\n",
      "868:\tlearn: 0.0852605\ttotal: 1m 27s\tremaining: 13.1s\n",
      "869:\tlearn: 0.0852235\ttotal: 1m 27s\tremaining: 13s\n",
      "870:\tlearn: 0.0851936\ttotal: 1m 27s\tremaining: 12.9s\n",
      "871:\tlearn: 0.0851690\ttotal: 1m 27s\tremaining: 12.8s\n",
      "872:\tlearn: 0.0851465\ttotal: 1m 27s\tremaining: 12.7s\n",
      "873:\tlearn: 0.0850782\ttotal: 1m 27s\tremaining: 12.6s\n",
      "874:\tlearn: 0.0850567\ttotal: 1m 27s\tremaining: 12.5s\n",
      "875:\tlearn: 0.0849950\ttotal: 1m 27s\tremaining: 12.4s\n",
      "876:\tlearn: 0.0849626\ttotal: 1m 27s\tremaining: 12.3s\n",
      "877:\tlearn: 0.0849561\ttotal: 1m 28s\tremaining: 12.2s\n",
      "878:\tlearn: 0.0849285\ttotal: 1m 28s\tremaining: 12.1s\n",
      "879:\tlearn: 0.0848829\ttotal: 1m 28s\tremaining: 12s\n",
      "880:\tlearn: 0.0848566\ttotal: 1m 28s\tremaining: 11.9s\n",
      "881:\tlearn: 0.0848007\ttotal: 1m 28s\tremaining: 11.8s\n",
      "882:\tlearn: 0.0847570\ttotal: 1m 28s\tremaining: 11.7s\n",
      "883:\tlearn: 0.0846999\ttotal: 1m 28s\tremaining: 11.6s\n",
      "884:\tlearn: 0.0846185\ttotal: 1m 28s\tremaining: 11.5s\n",
      "885:\tlearn: 0.0845726\ttotal: 1m 28s\tremaining: 11.4s\n",
      "886:\tlearn: 0.0844910\ttotal: 1m 29s\tremaining: 11.3s\n",
      "887:\tlearn: 0.0844294\ttotal: 1m 29s\tremaining: 11.2s\n",
      "888:\tlearn: 0.0843752\ttotal: 1m 29s\tremaining: 11.1s\n",
      "889:\tlearn: 0.0843264\ttotal: 1m 29s\tremaining: 11s\n",
      "890:\tlearn: 0.0842859\ttotal: 1m 29s\tremaining: 10.9s\n",
      "891:\tlearn: 0.0842577\ttotal: 1m 29s\tremaining: 10.8s\n",
      "892:\tlearn: 0.0842115\ttotal: 1m 29s\tremaining: 10.7s\n",
      "893:\tlearn: 0.0841797\ttotal: 1m 29s\tremaining: 10.6s\n",
      "894:\tlearn: 0.0841453\ttotal: 1m 29s\tremaining: 10.5s\n",
      "895:\tlearn: 0.0840404\ttotal: 1m 29s\tremaining: 10.4s\n",
      "896:\tlearn: 0.0839890\ttotal: 1m 30s\tremaining: 10.3s\n",
      "897:\tlearn: 0.0839578\ttotal: 1m 30s\tremaining: 10.2s\n",
      "898:\tlearn: 0.0838954\ttotal: 1m 30s\tremaining: 10.1s\n",
      "899:\tlearn: 0.0838740\ttotal: 1m 30s\tremaining: 10s\n",
      "900:\tlearn: 0.0838322\ttotal: 1m 30s\tremaining: 9.95s\n",
      "901:\tlearn: 0.0838022\ttotal: 1m 30s\tremaining: 9.85s\n",
      "902:\tlearn: 0.0837675\ttotal: 1m 30s\tremaining: 9.75s\n",
      "903:\tlearn: 0.0837451\ttotal: 1m 30s\tremaining: 9.65s\n",
      "904:\tlearn: 0.0836728\ttotal: 1m 31s\tremaining: 9.56s\n",
      "905:\tlearn: 0.0836009\ttotal: 1m 31s\tremaining: 9.46s\n",
      "906:\tlearn: 0.0835525\ttotal: 1m 31s\tremaining: 9.36s\n",
      "907:\tlearn: 0.0835280\ttotal: 1m 31s\tremaining: 9.26s\n",
      "908:\tlearn: 0.0835077\ttotal: 1m 31s\tremaining: 9.17s\n",
      "909:\tlearn: 0.0834770\ttotal: 1m 31s\tremaining: 9.07s\n",
      "910:\tlearn: 0.0834229\ttotal: 1m 31s\tremaining: 8.97s\n",
      "911:\tlearn: 0.0833853\ttotal: 1m 31s\tremaining: 8.87s\n",
      "912:\tlearn: 0.0833685\ttotal: 1m 32s\tremaining: 8.78s\n",
      "913:\tlearn: 0.0833314\ttotal: 1m 32s\tremaining: 8.68s\n",
      "914:\tlearn: 0.0833075\ttotal: 1m 32s\tremaining: 8.58s\n",
      "915:\tlearn: 0.0832663\ttotal: 1m 32s\tremaining: 8.48s\n",
      "916:\tlearn: 0.0832285\ttotal: 1m 32s\tremaining: 8.38s\n",
      "917:\tlearn: 0.0832214\ttotal: 1m 32s\tremaining: 8.28s\n",
      "918:\tlearn: 0.0831569\ttotal: 1m 32s\tremaining: 8.19s\n",
      "919:\tlearn: 0.0831292\ttotal: 1m 33s\tremaining: 8.09s\n",
      "920:\tlearn: 0.0830751\ttotal: 1m 33s\tremaining: 7.99s\n",
      "921:\tlearn: 0.0830366\ttotal: 1m 33s\tremaining: 7.89s\n",
      "922:\tlearn: 0.0829828\ttotal: 1m 33s\tremaining: 7.79s\n",
      "923:\tlearn: 0.0829475\ttotal: 1m 33s\tremaining: 7.69s\n",
      "924:\tlearn: 0.0829125\ttotal: 1m 33s\tremaining: 7.59s\n",
      "925:\tlearn: 0.0828846\ttotal: 1m 33s\tremaining: 7.49s\n",
      "926:\tlearn: 0.0828432\ttotal: 1m 33s\tremaining: 7.38s\n",
      "927:\tlearn: 0.0828129\ttotal: 1m 33s\tremaining: 7.28s\n",
      "928:\tlearn: 0.0827569\ttotal: 1m 33s\tremaining: 7.18s\n",
      "929:\tlearn: 0.0827399\ttotal: 1m 34s\tremaining: 7.08s\n",
      "930:\tlearn: 0.0827009\ttotal: 1m 34s\tremaining: 6.98s\n",
      "931:\tlearn: 0.0826543\ttotal: 1m 34s\tremaining: 6.88s\n",
      "932:\tlearn: 0.0825962\ttotal: 1m 34s\tremaining: 6.78s\n",
      "933:\tlearn: 0.0825492\ttotal: 1m 34s\tremaining: 6.67s\n",
      "934:\tlearn: 0.0825237\ttotal: 1m 34s\tremaining: 6.57s\n",
      "935:\tlearn: 0.0824958\ttotal: 1m 34s\tremaining: 6.47s\n",
      "936:\tlearn: 0.0824930\ttotal: 1m 34s\tremaining: 6.37s\n",
      "937:\tlearn: 0.0824412\ttotal: 1m 34s\tremaining: 6.27s\n",
      "938:\tlearn: 0.0824207\ttotal: 1m 34s\tremaining: 6.17s\n",
      "939:\tlearn: 0.0823809\ttotal: 1m 35s\tremaining: 6.07s\n",
      "940:\tlearn: 0.0823627\ttotal: 1m 35s\tremaining: 5.97s\n",
      "941:\tlearn: 0.0823346\ttotal: 1m 35s\tremaining: 5.86s\n",
      "942:\tlearn: 0.0823283\ttotal: 1m 35s\tremaining: 5.76s\n",
      "943:\tlearn: 0.0823137\ttotal: 1m 35s\tremaining: 5.66s\n",
      "944:\tlearn: 0.0822494\ttotal: 1m 35s\tremaining: 5.56s\n",
      "945:\tlearn: 0.0822158\ttotal: 1m 35s\tremaining: 5.46s\n",
      "946:\tlearn: 0.0821810\ttotal: 1m 35s\tremaining: 5.36s\n",
      "947:\tlearn: 0.0820871\ttotal: 1m 35s\tremaining: 5.26s\n",
      "948:\tlearn: 0.0820617\ttotal: 1m 35s\tremaining: 5.16s\n",
      "949:\tlearn: 0.0820359\ttotal: 1m 36s\tremaining: 5.06s\n",
      "950:\tlearn: 0.0820041\ttotal: 1m 36s\tremaining: 4.96s\n",
      "951:\tlearn: 0.0819751\ttotal: 1m 36s\tremaining: 4.86s\n",
      "952:\tlearn: 0.0819550\ttotal: 1m 36s\tremaining: 4.76s\n",
      "953:\tlearn: 0.0819304\ttotal: 1m 36s\tremaining: 4.66s\n",
      "954:\tlearn: 0.0819123\ttotal: 1m 36s\tremaining: 4.55s\n",
      "955:\tlearn: 0.0818755\ttotal: 1m 36s\tremaining: 4.45s\n",
      "956:\tlearn: 0.0818373\ttotal: 1m 36s\tremaining: 4.35s\n",
      "957:\tlearn: 0.0818215\ttotal: 1m 36s\tremaining: 4.25s\n",
      "958:\tlearn: 0.0817790\ttotal: 1m 37s\tremaining: 4.15s\n",
      "959:\tlearn: 0.0817632\ttotal: 1m 37s\tremaining: 4.05s\n",
      "960:\tlearn: 0.0817088\ttotal: 1m 37s\tremaining: 3.95s\n",
      "961:\tlearn: 0.0816807\ttotal: 1m 37s\tremaining: 3.85s\n",
      "962:\tlearn: 0.0816051\ttotal: 1m 37s\tremaining: 3.75s\n",
      "963:\tlearn: 0.0815750\ttotal: 1m 37s\tremaining: 3.64s\n",
      "964:\tlearn: 0.0815597\ttotal: 1m 37s\tremaining: 3.54s\n",
      "965:\tlearn: 0.0815406\ttotal: 1m 37s\tremaining: 3.44s\n",
      "966:\tlearn: 0.0815017\ttotal: 1m 37s\tremaining: 3.34s\n",
      "967:\tlearn: 0.0814761\ttotal: 1m 38s\tremaining: 3.24s\n",
      "968:\tlearn: 0.0814455\ttotal: 1m 38s\tremaining: 3.14s\n",
      "969:\tlearn: 0.0814048\ttotal: 1m 38s\tremaining: 3.04s\n",
      "970:\tlearn: 0.0813825\ttotal: 1m 38s\tremaining: 2.94s\n",
      "971:\tlearn: 0.0813423\ttotal: 1m 38s\tremaining: 2.83s\n",
      "972:\tlearn: 0.0812952\ttotal: 1m 38s\tremaining: 2.73s\n",
      "973:\tlearn: 0.0812902\ttotal: 1m 38s\tremaining: 2.63s\n",
      "974:\tlearn: 0.0812830\ttotal: 1m 38s\tremaining: 2.53s\n",
      "975:\tlearn: 0.0812421\ttotal: 1m 38s\tremaining: 2.43s\n",
      "976:\tlearn: 0.0812048\ttotal: 1m 38s\tremaining: 2.33s\n",
      "977:\tlearn: 0.0811698\ttotal: 1m 38s\tremaining: 2.23s\n",
      "978:\tlearn: 0.0811248\ttotal: 1m 39s\tremaining: 2.13s\n",
      "979:\tlearn: 0.0810942\ttotal: 1m 39s\tremaining: 2.02s\n",
      "980:\tlearn: 0.0810742\ttotal: 1m 39s\tremaining: 1.92s\n",
      "981:\tlearn: 0.0810276\ttotal: 1m 39s\tremaining: 1.82s\n",
      "982:\tlearn: 0.0810020\ttotal: 1m 39s\tremaining: 1.72s\n",
      "983:\tlearn: 0.0809661\ttotal: 1m 39s\tremaining: 1.62s\n",
      "984:\tlearn: 0.0809486\ttotal: 1m 39s\tremaining: 1.52s\n",
      "985:\tlearn: 0.0809220\ttotal: 1m 39s\tremaining: 1.42s\n",
      "986:\tlearn: 0.0808962\ttotal: 1m 39s\tremaining: 1.31s\n",
      "987:\tlearn: 0.0808706\ttotal: 1m 39s\tremaining: 1.21s\n",
      "988:\tlearn: 0.0808463\ttotal: 1m 40s\tremaining: 1.11s\n",
      "989:\tlearn: 0.0808337\ttotal: 1m 40s\tremaining: 1.01s\n",
      "990:\tlearn: 0.0807922\ttotal: 1m 40s\tremaining: 911ms\n",
      "991:\tlearn: 0.0807349\ttotal: 1m 40s\tremaining: 809ms\n",
      "992:\tlearn: 0.0806940\ttotal: 1m 40s\tremaining: 708ms\n",
      "993:\tlearn: 0.0806379\ttotal: 1m 40s\tremaining: 607ms\n",
      "994:\tlearn: 0.0806054\ttotal: 1m 40s\tremaining: 506ms\n",
      "995:\tlearn: 0.0805683\ttotal: 1m 40s\tremaining: 405ms\n",
      "996:\tlearn: 0.0805168\ttotal: 1m 40s\tremaining: 303ms\n",
      "997:\tlearn: 0.0804996\ttotal: 1m 40s\tremaining: 202ms\n",
      "998:\tlearn: 0.0804480\ttotal: 1m 41s\tremaining: 101ms\n",
      "999:\tlearn: 0.0804174\ttotal: 1m 41s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:241: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5268607\ttotal: 109ms\tremaining: 1m 48s\n",
      "1:\tlearn: 0.4269820\ttotal: 221ms\tremaining: 1m 50s\n",
      "2:\tlearn: 0.3681616\ttotal: 318ms\tremaining: 1m 45s\n",
      "3:\tlearn: 0.3268981\ttotal: 417ms\tremaining: 1m 43s\n",
      "4:\tlearn: 0.3002497\ttotal: 527ms\tremaining: 1m 44s\n",
      "5:\tlearn: 0.2817856\ttotal: 623ms\tremaining: 1m 43s\n",
      "6:\tlearn: 0.2573488\ttotal: 725ms\tremaining: 1m 42s\n",
      "7:\tlearn: 0.2462725\ttotal: 814ms\tremaining: 1m 40s\n",
      "8:\tlearn: 0.2380101\ttotal: 906ms\tremaining: 1m 39s\n",
      "9:\tlearn: 0.2280518\ttotal: 1s\tremaining: 1m 39s\n",
      "10:\tlearn: 0.2231006\ttotal: 1.1s\tremaining: 1m 39s\n",
      "11:\tlearn: 0.2143295\ttotal: 1.2s\tremaining: 1m 38s\n",
      "12:\tlearn: 0.2105775\ttotal: 1.29s\tremaining: 1m 38s\n",
      "13:\tlearn: 0.2044587\ttotal: 1.39s\tremaining: 1m 38s\n",
      "14:\tlearn: 0.2017606\ttotal: 1.49s\tremaining: 1m 38s\n",
      "15:\tlearn: 0.1984549\ttotal: 1.59s\tremaining: 1m 38s\n",
      "16:\tlearn: 0.1960099\ttotal: 1.7s\tremaining: 1m 38s\n",
      "17:\tlearn: 0.1939983\ttotal: 1.79s\tremaining: 1m 37s\n",
      "18:\tlearn: 0.1920979\ttotal: 1.89s\tremaining: 1m 37s\n",
      "19:\tlearn: 0.1889975\ttotal: 1.99s\tremaining: 1m 37s\n",
      "20:\tlearn: 0.1862246\ttotal: 2.08s\tremaining: 1m 37s\n",
      "21:\tlearn: 0.1850184\ttotal: 2.18s\tremaining: 1m 37s\n",
      "22:\tlearn: 0.1833773\ttotal: 2.29s\tremaining: 1m 37s\n",
      "23:\tlearn: 0.1822404\ttotal: 2.38s\tremaining: 1m 37s\n",
      "24:\tlearn: 0.1809510\ttotal: 2.48s\tremaining: 1m 36s\n",
      "25:\tlearn: 0.1791770\ttotal: 2.59s\tremaining: 1m 36s\n",
      "26:\tlearn: 0.1780129\ttotal: 2.69s\tremaining: 1m 36s\n",
      "27:\tlearn: 0.1759416\ttotal: 2.79s\tremaining: 1m 36s\n",
      "28:\tlearn: 0.1747437\ttotal: 2.9s\tremaining: 1m 37s\n",
      "29:\tlearn: 0.1736159\ttotal: 3.02s\tremaining: 1m 37s\n",
      "30:\tlearn: 0.1726243\ttotal: 3.11s\tremaining: 1m 37s\n",
      "31:\tlearn: 0.1717540\ttotal: 3.21s\tremaining: 1m 37s\n",
      "32:\tlearn: 0.1707693\ttotal: 3.31s\tremaining: 1m 36s\n",
      "33:\tlearn: 0.1697840\ttotal: 3.41s\tremaining: 1m 36s\n",
      "34:\tlearn: 0.1690638\ttotal: 3.5s\tremaining: 1m 36s\n",
      "35:\tlearn: 0.1680833\ttotal: 3.6s\tremaining: 1m 36s\n",
      "36:\tlearn: 0.1669650\ttotal: 3.71s\tremaining: 1m 36s\n",
      "37:\tlearn: 0.1661950\ttotal: 3.8s\tremaining: 1m 36s\n",
      "38:\tlearn: 0.1648763\ttotal: 3.9s\tremaining: 1m 36s\n",
      "39:\tlearn: 0.1636315\ttotal: 3.98s\tremaining: 1m 35s\n",
      "40:\tlearn: 0.1630486\ttotal: 4.08s\tremaining: 1m 35s\n",
      "41:\tlearn: 0.1624553\ttotal: 4.18s\tremaining: 1m 35s\n",
      "42:\tlearn: 0.1615738\ttotal: 4.29s\tremaining: 1m 35s\n",
      "43:\tlearn: 0.1611372\ttotal: 4.39s\tremaining: 1m 35s\n",
      "44:\tlearn: 0.1604625\ttotal: 4.5s\tremaining: 1m 35s\n",
      "45:\tlearn: 0.1599535\ttotal: 4.59s\tremaining: 1m 35s\n",
      "46:\tlearn: 0.1593361\ttotal: 4.69s\tremaining: 1m 35s\n",
      "47:\tlearn: 0.1588303\ttotal: 4.79s\tremaining: 1m 35s\n",
      "48:\tlearn: 0.1581380\ttotal: 4.89s\tremaining: 1m 35s\n",
      "49:\tlearn: 0.1571699\ttotal: 4.99s\tremaining: 1m 34s\n",
      "50:\tlearn: 0.1567341\ttotal: 5.09s\tremaining: 1m 34s\n",
      "51:\tlearn: 0.1562317\ttotal: 5.19s\tremaining: 1m 34s\n",
      "52:\tlearn: 0.1556617\ttotal: 5.29s\tremaining: 1m 34s\n",
      "53:\tlearn: 0.1550293\ttotal: 5.39s\tremaining: 1m 34s\n",
      "54:\tlearn: 0.1545285\ttotal: 5.49s\tremaining: 1m 34s\n",
      "55:\tlearn: 0.1538878\ttotal: 5.59s\tremaining: 1m 34s\n",
      "56:\tlearn: 0.1534715\ttotal: 5.7s\tremaining: 1m 34s\n",
      "57:\tlearn: 0.1530334\ttotal: 5.81s\tremaining: 1m 34s\n",
      "58:\tlearn: 0.1525355\ttotal: 5.92s\tremaining: 1m 34s\n",
      "59:\tlearn: 0.1520597\ttotal: 6.02s\tremaining: 1m 34s\n",
      "60:\tlearn: 0.1516275\ttotal: 6.12s\tremaining: 1m 34s\n",
      "61:\tlearn: 0.1512854\ttotal: 6.22s\tremaining: 1m 34s\n",
      "62:\tlearn: 0.1506867\ttotal: 6.31s\tremaining: 1m 33s\n",
      "63:\tlearn: 0.1503658\ttotal: 6.41s\tremaining: 1m 33s\n",
      "64:\tlearn: 0.1500679\ttotal: 6.5s\tremaining: 1m 33s\n",
      "65:\tlearn: 0.1498187\ttotal: 6.6s\tremaining: 1m 33s\n",
      "66:\tlearn: 0.1494450\ttotal: 6.69s\tremaining: 1m 33s\n",
      "67:\tlearn: 0.1491698\ttotal: 6.79s\tremaining: 1m 33s\n",
      "68:\tlearn: 0.1489159\ttotal: 6.88s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1485396\ttotal: 6.98s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1481494\ttotal: 7.07s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1478616\ttotal: 7.16s\tremaining: 1m 32s\n",
      "72:\tlearn: 0.1474092\ttotal: 7.26s\tremaining: 1m 32s\n",
      "73:\tlearn: 0.1470613\ttotal: 7.35s\tremaining: 1m 31s\n",
      "74:\tlearn: 0.1466039\ttotal: 7.46s\tremaining: 1m 31s\n",
      "75:\tlearn: 0.1462449\ttotal: 7.57s\tremaining: 1m 32s\n",
      "76:\tlearn: 0.1459978\ttotal: 7.67s\tremaining: 1m 31s\n",
      "77:\tlearn: 0.1455905\ttotal: 7.78s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1452998\ttotal: 7.88s\tremaining: 1m 31s\n",
      "79:\tlearn: 0.1448523\ttotal: 7.98s\tremaining: 1m 31s\n",
      "80:\tlearn: 0.1446336\ttotal: 8.07s\tremaining: 1m 31s\n",
      "81:\tlearn: 0.1443779\ttotal: 8.17s\tremaining: 1m 31s\n",
      "82:\tlearn: 0.1441293\ttotal: 8.26s\tremaining: 1m 31s\n",
      "83:\tlearn: 0.1436414\ttotal: 8.36s\tremaining: 1m 31s\n",
      "84:\tlearn: 0.1431684\ttotal: 8.46s\tremaining: 1m 31s\n",
      "85:\tlearn: 0.1428985\ttotal: 8.57s\tremaining: 1m 31s\n",
      "86:\tlearn: 0.1426856\ttotal: 8.67s\tremaining: 1m 30s\n",
      "87:\tlearn: 0.1424852\ttotal: 8.76s\tremaining: 1m 30s\n",
      "88:\tlearn: 0.1422724\ttotal: 8.87s\tremaining: 1m 30s\n",
      "89:\tlearn: 0.1420304\ttotal: 8.97s\tremaining: 1m 30s\n",
      "90:\tlearn: 0.1416389\ttotal: 9.07s\tremaining: 1m 30s\n",
      "91:\tlearn: 0.1413915\ttotal: 9.17s\tremaining: 1m 30s\n",
      "92:\tlearn: 0.1411444\ttotal: 9.27s\tremaining: 1m 30s\n",
      "93:\tlearn: 0.1409195\ttotal: 9.37s\tremaining: 1m 30s\n",
      "94:\tlearn: 0.1405878\ttotal: 9.46s\tremaining: 1m 30s\n",
      "95:\tlearn: 0.1403854\ttotal: 9.56s\tremaining: 1m 30s\n",
      "96:\tlearn: 0.1400603\ttotal: 9.65s\tremaining: 1m 29s\n",
      "97:\tlearn: 0.1397621\ttotal: 9.75s\tremaining: 1m 29s\n",
      "98:\tlearn: 0.1394985\ttotal: 9.94s\tremaining: 1m 30s\n",
      "99:\tlearn: 0.1391571\ttotal: 10.1s\tremaining: 1m 30s\n",
      "100:\tlearn: 0.1388610\ttotal: 10.2s\tremaining: 1m 30s\n",
      "101:\tlearn: 0.1386445\ttotal: 10.2s\tremaining: 1m 30s\n",
      "102:\tlearn: 0.1384397\ttotal: 10.3s\tremaining: 1m 30s\n",
      "103:\tlearn: 0.1381609\ttotal: 10.4s\tremaining: 1m 29s\n",
      "104:\tlearn: 0.1379193\ttotal: 10.5s\tremaining: 1m 29s\n",
      "105:\tlearn: 0.1376623\ttotal: 10.6s\tremaining: 1m 29s\n",
      "106:\tlearn: 0.1374115\ttotal: 10.7s\tremaining: 1m 29s\n",
      "107:\tlearn: 0.1372035\ttotal: 10.8s\tremaining: 1m 29s\n",
      "108:\tlearn: 0.1369530\ttotal: 10.9s\tremaining: 1m 29s\n",
      "109:\tlearn: 0.1367694\ttotal: 11s\tremaining: 1m 29s\n",
      "110:\tlearn: 0.1365778\ttotal: 11.1s\tremaining: 1m 29s\n",
      "111:\tlearn: 0.1363358\ttotal: 11.2s\tremaining: 1m 29s\n",
      "112:\tlearn: 0.1361752\ttotal: 11.3s\tremaining: 1m 29s\n",
      "113:\tlearn: 0.1359588\ttotal: 11.4s\tremaining: 1m 28s\n",
      "114:\tlearn: 0.1357564\ttotal: 11.5s\tremaining: 1m 28s\n",
      "115:\tlearn: 0.1355401\ttotal: 11.6s\tremaining: 1m 28s\n",
      "116:\tlearn: 0.1353664\ttotal: 11.7s\tremaining: 1m 28s\n",
      "117:\tlearn: 0.1352324\ttotal: 11.8s\tremaining: 1m 28s\n",
      "118:\tlearn: 0.1350380\ttotal: 11.9s\tremaining: 1m 28s\n",
      "119:\tlearn: 0.1348003\ttotal: 12s\tremaining: 1m 28s\n",
      "120:\tlearn: 0.1346391\ttotal: 12.1s\tremaining: 1m 28s\n",
      "121:\tlearn: 0.1344749\ttotal: 12.3s\tremaining: 1m 28s\n",
      "122:\tlearn: 0.1342808\ttotal: 12.4s\tremaining: 1m 28s\n",
      "123:\tlearn: 0.1340956\ttotal: 12.5s\tremaining: 1m 27s\n",
      "124:\tlearn: 0.1337636\ttotal: 12.5s\tremaining: 1m 27s\n",
      "125:\tlearn: 0.1335054\ttotal: 12.6s\tremaining: 1m 27s\n",
      "126:\tlearn: 0.1332535\ttotal: 12.7s\tremaining: 1m 27s\n",
      "127:\tlearn: 0.1331183\ttotal: 12.8s\tremaining: 1m 27s\n",
      "128:\tlearn: 0.1328604\ttotal: 12.9s\tremaining: 1m 27s\n",
      "129:\tlearn: 0.1327031\ttotal: 13s\tremaining: 1m 27s\n",
      "130:\tlearn: 0.1325598\ttotal: 13.1s\tremaining: 1m 27s\n",
      "131:\tlearn: 0.1324171\ttotal: 13.2s\tremaining: 1m 26s\n",
      "132:\tlearn: 0.1322828\ttotal: 13.3s\tremaining: 1m 26s\n",
      "133:\tlearn: 0.1321579\ttotal: 13.4s\tremaining: 1m 26s\n",
      "134:\tlearn: 0.1319771\ttotal: 13.5s\tremaining: 1m 26s\n",
      "135:\tlearn: 0.1318266\ttotal: 13.6s\tremaining: 1m 26s\n",
      "136:\tlearn: 0.1316904\ttotal: 13.7s\tremaining: 1m 26s\n",
      "137:\tlearn: 0.1315639\ttotal: 13.8s\tremaining: 1m 26s\n",
      "138:\tlearn: 0.1314143\ttotal: 13.9s\tremaining: 1m 25s\n",
      "139:\tlearn: 0.1311796\ttotal: 14s\tremaining: 1m 25s\n",
      "140:\tlearn: 0.1310619\ttotal: 14.1s\tremaining: 1m 25s\n",
      "141:\tlearn: 0.1308967\ttotal: 14.2s\tremaining: 1m 25s\n",
      "142:\tlearn: 0.1306634\ttotal: 14.3s\tremaining: 1m 25s\n",
      "143:\tlearn: 0.1305444\ttotal: 14.4s\tremaining: 1m 25s\n",
      "144:\tlearn: 0.1303831\ttotal: 14.5s\tremaining: 1m 25s\n",
      "145:\tlearn: 0.1301791\ttotal: 14.6s\tremaining: 1m 25s\n",
      "146:\tlearn: 0.1300869\ttotal: 14.7s\tremaining: 1m 25s\n",
      "147:\tlearn: 0.1298838\ttotal: 14.8s\tremaining: 1m 25s\n",
      "148:\tlearn: 0.1297737\ttotal: 14.9s\tremaining: 1m 24s\n",
      "149:\tlearn: 0.1295595\ttotal: 15s\tremaining: 1m 24s\n",
      "150:\tlearn: 0.1294348\ttotal: 15.1s\tremaining: 1m 24s\n",
      "151:\tlearn: 0.1292791\ttotal: 15.2s\tremaining: 1m 24s\n",
      "152:\tlearn: 0.1291057\ttotal: 15.3s\tremaining: 1m 24s\n",
      "153:\tlearn: 0.1289654\ttotal: 15.4s\tremaining: 1m 24s\n",
      "154:\tlearn: 0.1288361\ttotal: 15.5s\tremaining: 1m 24s\n",
      "155:\tlearn: 0.1286630\ttotal: 15.6s\tremaining: 1m 24s\n",
      "156:\tlearn: 0.1285243\ttotal: 15.7s\tremaining: 1m 24s\n",
      "157:\tlearn: 0.1283835\ttotal: 15.9s\tremaining: 1m 24s\n",
      "158:\tlearn: 0.1283015\ttotal: 16s\tremaining: 1m 24s\n",
      "159:\tlearn: 0.1281388\ttotal: 16.1s\tremaining: 1m 24s\n",
      "160:\tlearn: 0.1280158\ttotal: 16.2s\tremaining: 1m 24s\n",
      "161:\tlearn: 0.1278931\ttotal: 16.3s\tremaining: 1m 24s\n",
      "162:\tlearn: 0.1277901\ttotal: 16.4s\tremaining: 1m 24s\n",
      "163:\tlearn: 0.1276182\ttotal: 16.5s\tremaining: 1m 24s\n",
      "164:\tlearn: 0.1274463\ttotal: 16.6s\tremaining: 1m 24s\n",
      "165:\tlearn: 0.1273051\ttotal: 16.7s\tremaining: 1m 24s\n",
      "166:\tlearn: 0.1271763\ttotal: 16.8s\tremaining: 1m 23s\n",
      "167:\tlearn: 0.1270587\ttotal: 16.9s\tremaining: 1m 23s\n",
      "168:\tlearn: 0.1269730\ttotal: 17s\tremaining: 1m 23s\n",
      "169:\tlearn: 0.1268438\ttotal: 17.1s\tremaining: 1m 23s\n",
      "170:\tlearn: 0.1267019\ttotal: 17.2s\tremaining: 1m 23s\n",
      "171:\tlearn: 0.1265801\ttotal: 17.3s\tremaining: 1m 23s\n",
      "172:\tlearn: 0.1264997\ttotal: 17.4s\tremaining: 1m 23s\n",
      "173:\tlearn: 0.1263765\ttotal: 17.5s\tremaining: 1m 23s\n",
      "174:\tlearn: 0.1261718\ttotal: 17.6s\tremaining: 1m 23s\n",
      "175:\tlearn: 0.1260292\ttotal: 17.7s\tremaining: 1m 23s\n",
      "176:\tlearn: 0.1258658\ttotal: 17.8s\tremaining: 1m 22s\n",
      "177:\tlearn: 0.1257798\ttotal: 17.9s\tremaining: 1m 22s\n",
      "178:\tlearn: 0.1256956\ttotal: 18s\tremaining: 1m 22s\n",
      "179:\tlearn: 0.1255935\ttotal: 18.2s\tremaining: 1m 22s\n",
      "180:\tlearn: 0.1254653\ttotal: 18.3s\tremaining: 1m 22s\n",
      "181:\tlearn: 0.1252999\ttotal: 18.4s\tremaining: 1m 22s\n",
      "182:\tlearn: 0.1251772\ttotal: 18.5s\tremaining: 1m 22s\n",
      "183:\tlearn: 0.1250018\ttotal: 18.6s\tremaining: 1m 22s\n",
      "184:\tlearn: 0.1248675\ttotal: 18.7s\tremaining: 1m 22s\n",
      "185:\tlearn: 0.1247740\ttotal: 18.8s\tremaining: 1m 22s\n",
      "186:\tlearn: 0.1246235\ttotal: 18.9s\tremaining: 1m 22s\n",
      "187:\tlearn: 0.1245189\ttotal: 19s\tremaining: 1m 22s\n",
      "188:\tlearn: 0.1244232\ttotal: 19.1s\tremaining: 1m 21s\n",
      "189:\tlearn: 0.1243023\ttotal: 19.2s\tremaining: 1m 21s\n",
      "190:\tlearn: 0.1241678\ttotal: 19.3s\tremaining: 1m 21s\n",
      "191:\tlearn: 0.1240883\ttotal: 19.4s\tremaining: 1m 21s\n",
      "192:\tlearn: 0.1239963\ttotal: 19.5s\tremaining: 1m 21s\n",
      "193:\tlearn: 0.1238766\ttotal: 19.6s\tremaining: 1m 21s\n",
      "194:\tlearn: 0.1237634\ttotal: 19.7s\tremaining: 1m 21s\n",
      "195:\tlearn: 0.1236590\ttotal: 19.8s\tremaining: 1m 21s\n",
      "196:\tlearn: 0.1235154\ttotal: 19.9s\tremaining: 1m 21s\n",
      "197:\tlearn: 0.1233674\ttotal: 20.2s\tremaining: 1m 21s\n",
      "198:\tlearn: 0.1232850\ttotal: 20.3s\tremaining: 1m 21s\n",
      "199:\tlearn: 0.1232030\ttotal: 20.4s\tremaining: 1m 21s\n",
      "200:\tlearn: 0.1231343\ttotal: 20.5s\tremaining: 1m 21s\n",
      "201:\tlearn: 0.1230357\ttotal: 20.6s\tremaining: 1m 21s\n",
      "202:\tlearn: 0.1229563\ttotal: 20.7s\tremaining: 1m 21s\n",
      "203:\tlearn: 0.1228525\ttotal: 20.8s\tremaining: 1m 21s\n",
      "204:\tlearn: 0.1227472\ttotal: 20.9s\tremaining: 1m 21s\n",
      "205:\tlearn: 0.1226346\ttotal: 21.1s\tremaining: 1m 21s\n",
      "206:\tlearn: 0.1225470\ttotal: 21.2s\tremaining: 1m 21s\n",
      "207:\tlearn: 0.1224360\ttotal: 21.2s\tremaining: 1m 20s\n",
      "208:\tlearn: 0.1223440\ttotal: 21.3s\tremaining: 1m 20s\n",
      "209:\tlearn: 0.1222354\ttotal: 21.4s\tremaining: 1m 20s\n",
      "210:\tlearn: 0.1221762\ttotal: 21.5s\tremaining: 1m 20s\n",
      "211:\tlearn: 0.1220674\ttotal: 21.6s\tremaining: 1m 20s\n",
      "212:\tlearn: 0.1219843\ttotal: 21.7s\tremaining: 1m 20s\n",
      "213:\tlearn: 0.1218660\ttotal: 21.8s\tremaining: 1m 20s\n",
      "214:\tlearn: 0.1217430\ttotal: 21.9s\tremaining: 1m 20s\n",
      "215:\tlearn: 0.1216662\ttotal: 22s\tremaining: 1m 19s\n",
      "216:\tlearn: 0.1215650\ttotal: 22.1s\tremaining: 1m 19s\n",
      "217:\tlearn: 0.1214617\ttotal: 22.2s\tremaining: 1m 19s\n",
      "218:\tlearn: 0.1213566\ttotal: 22.3s\tremaining: 1m 19s\n",
      "219:\tlearn: 0.1212073\ttotal: 22.4s\tremaining: 1m 19s\n",
      "220:\tlearn: 0.1211008\ttotal: 22.5s\tremaining: 1m 19s\n",
      "221:\tlearn: 0.1209761\ttotal: 22.6s\tremaining: 1m 19s\n",
      "222:\tlearn: 0.1208318\ttotal: 22.7s\tremaining: 1m 19s\n",
      "223:\tlearn: 0.1207367\ttotal: 22.8s\tremaining: 1m 19s\n",
      "224:\tlearn: 0.1206508\ttotal: 22.9s\tremaining: 1m 18s\n",
      "225:\tlearn: 0.1204819\ttotal: 23s\tremaining: 1m 18s\n",
      "226:\tlearn: 0.1204202\ttotal: 23.1s\tremaining: 1m 18s\n",
      "227:\tlearn: 0.1203006\ttotal: 23.2s\tremaining: 1m 18s\n",
      "228:\tlearn: 0.1202015\ttotal: 23.3s\tremaining: 1m 18s\n",
      "229:\tlearn: 0.1200898\ttotal: 23.4s\tremaining: 1m 18s\n",
      "230:\tlearn: 0.1199982\ttotal: 23.5s\tremaining: 1m 18s\n",
      "231:\tlearn: 0.1199072\ttotal: 23.6s\tremaining: 1m 18s\n",
      "232:\tlearn: 0.1196949\ttotal: 23.7s\tremaining: 1m 18s\n",
      "233:\tlearn: 0.1196116\ttotal: 23.8s\tremaining: 1m 18s\n",
      "234:\tlearn: 0.1195102\ttotal: 23.9s\tremaining: 1m 17s\n",
      "235:\tlearn: 0.1194351\ttotal: 24s\tremaining: 1m 17s\n",
      "236:\tlearn: 0.1193870\ttotal: 24.1s\tremaining: 1m 17s\n",
      "237:\tlearn: 0.1192903\ttotal: 24.2s\tremaining: 1m 17s\n",
      "238:\tlearn: 0.1191917\ttotal: 24.3s\tremaining: 1m 17s\n",
      "239:\tlearn: 0.1191190\ttotal: 24.4s\tremaining: 1m 17s\n",
      "240:\tlearn: 0.1190360\ttotal: 24.5s\tremaining: 1m 17s\n",
      "241:\tlearn: 0.1189667\ttotal: 24.6s\tremaining: 1m 17s\n",
      "242:\tlearn: 0.1188091\ttotal: 24.7s\tremaining: 1m 17s\n",
      "243:\tlearn: 0.1186939\ttotal: 24.8s\tremaining: 1m 16s\n",
      "244:\tlearn: 0.1186246\ttotal: 24.9s\tremaining: 1m 16s\n",
      "245:\tlearn: 0.1185713\ttotal: 25s\tremaining: 1m 16s\n",
      "246:\tlearn: 0.1184642\ttotal: 25.2s\tremaining: 1m 16s\n",
      "247:\tlearn: 0.1183165\ttotal: 25.3s\tremaining: 1m 16s\n",
      "248:\tlearn: 0.1181996\ttotal: 25.4s\tremaining: 1m 16s\n",
      "249:\tlearn: 0.1181496\ttotal: 25.5s\tremaining: 1m 16s\n",
      "250:\tlearn: 0.1180268\ttotal: 25.6s\tremaining: 1m 16s\n",
      "251:\tlearn: 0.1179813\ttotal: 25.7s\tremaining: 1m 16s\n",
      "252:\tlearn: 0.1179278\ttotal: 25.8s\tremaining: 1m 16s\n",
      "253:\tlearn: 0.1178308\ttotal: 25.9s\tremaining: 1m 16s\n",
      "254:\tlearn: 0.1177507\ttotal: 26s\tremaining: 1m 15s\n",
      "255:\tlearn: 0.1176785\ttotal: 26.1s\tremaining: 1m 15s\n",
      "256:\tlearn: 0.1176023\ttotal: 26.2s\tremaining: 1m 15s\n",
      "257:\tlearn: 0.1175053\ttotal: 26.3s\tremaining: 1m 15s\n",
      "258:\tlearn: 0.1173955\ttotal: 26.4s\tremaining: 1m 15s\n",
      "259:\tlearn: 0.1172984\ttotal: 26.5s\tremaining: 1m 15s\n",
      "260:\tlearn: 0.1172130\ttotal: 26.6s\tremaining: 1m 15s\n",
      "261:\tlearn: 0.1171593\ttotal: 26.7s\tremaining: 1m 15s\n",
      "262:\tlearn: 0.1171059\ttotal: 26.8s\tremaining: 1m 15s\n",
      "263:\tlearn: 0.1169691\ttotal: 26.9s\tremaining: 1m 14s\n",
      "264:\tlearn: 0.1168836\ttotal: 27s\tremaining: 1m 14s\n",
      "265:\tlearn: 0.1168102\ttotal: 27.1s\tremaining: 1m 14s\n",
      "266:\tlearn: 0.1167102\ttotal: 27.2s\tremaining: 1m 14s\n",
      "267:\tlearn: 0.1166016\ttotal: 27.5s\tremaining: 1m 15s\n",
      "268:\tlearn: 0.1165584\ttotal: 27.6s\tremaining: 1m 14s\n",
      "269:\tlearn: 0.1164041\ttotal: 27.7s\tremaining: 1m 14s\n",
      "270:\tlearn: 0.1163614\ttotal: 27.8s\tremaining: 1m 14s\n",
      "271:\tlearn: 0.1163034\ttotal: 27.9s\tremaining: 1m 14s\n",
      "272:\tlearn: 0.1162619\ttotal: 28s\tremaining: 1m 14s\n",
      "273:\tlearn: 0.1162052\ttotal: 28.1s\tremaining: 1m 14s\n",
      "274:\tlearn: 0.1161460\ttotal: 28.2s\tremaining: 1m 14s\n",
      "275:\tlearn: 0.1160980\ttotal: 28.3s\tremaining: 1m 14s\n",
      "276:\tlearn: 0.1159902\ttotal: 28.4s\tremaining: 1m 14s\n",
      "277:\tlearn: 0.1158668\ttotal: 28.6s\tremaining: 1m 14s\n",
      "278:\tlearn: 0.1158112\ttotal: 28.7s\tremaining: 1m 14s\n",
      "279:\tlearn: 0.1156830\ttotal: 28.8s\tremaining: 1m 14s\n",
      "280:\tlearn: 0.1156241\ttotal: 28.9s\tremaining: 1m 13s\n",
      "281:\tlearn: 0.1155715\ttotal: 29s\tremaining: 1m 13s\n",
      "282:\tlearn: 0.1154883\ttotal: 29.1s\tremaining: 1m 13s\n",
      "283:\tlearn: 0.1154318\ttotal: 29.2s\tremaining: 1m 13s\n",
      "284:\tlearn: 0.1153323\ttotal: 29.3s\tremaining: 1m 13s\n",
      "285:\tlearn: 0.1152554\ttotal: 29.5s\tremaining: 1m 13s\n",
      "286:\tlearn: 0.1151541\ttotal: 29.7s\tremaining: 1m 13s\n",
      "287:\tlearn: 0.1150950\ttotal: 29.8s\tremaining: 1m 13s\n",
      "288:\tlearn: 0.1149792\ttotal: 29.9s\tremaining: 1m 13s\n",
      "289:\tlearn: 0.1149214\ttotal: 30s\tremaining: 1m 13s\n",
      "290:\tlearn: 0.1148017\ttotal: 30.2s\tremaining: 1m 13s\n",
      "291:\tlearn: 0.1147290\ttotal: 30.3s\tremaining: 1m 13s\n",
      "292:\tlearn: 0.1146816\ttotal: 30.4s\tremaining: 1m 13s\n",
      "293:\tlearn: 0.1146140\ttotal: 30.5s\tremaining: 1m 13s\n",
      "294:\tlearn: 0.1145324\ttotal: 30.6s\tremaining: 1m 13s\n",
      "295:\tlearn: 0.1144643\ttotal: 30.7s\tremaining: 1m 13s\n",
      "296:\tlearn: 0.1144124\ttotal: 30.8s\tremaining: 1m 12s\n",
      "297:\tlearn: 0.1143456\ttotal: 30.9s\tremaining: 1m 12s\n",
      "298:\tlearn: 0.1142568\ttotal: 31s\tremaining: 1m 12s\n",
      "299:\tlearn: 0.1142061\ttotal: 31.1s\tremaining: 1m 12s\n",
      "300:\tlearn: 0.1141308\ttotal: 31.2s\tremaining: 1m 12s\n",
      "301:\tlearn: 0.1140720\ttotal: 31.3s\tremaining: 1m 12s\n",
      "302:\tlearn: 0.1139677\ttotal: 31.4s\tremaining: 1m 12s\n",
      "303:\tlearn: 0.1138966\ttotal: 31.5s\tremaining: 1m 12s\n",
      "304:\tlearn: 0.1138539\ttotal: 31.6s\tremaining: 1m 11s\n",
      "305:\tlearn: 0.1137663\ttotal: 31.7s\tremaining: 1m 11s\n",
      "306:\tlearn: 0.1136827\ttotal: 31.8s\tremaining: 1m 11s\n",
      "307:\tlearn: 0.1136191\ttotal: 31.9s\tremaining: 1m 11s\n",
      "308:\tlearn: 0.1135369\ttotal: 32s\tremaining: 1m 11s\n",
      "309:\tlearn: 0.1134610\ttotal: 32.1s\tremaining: 1m 11s\n",
      "310:\tlearn: 0.1133372\ttotal: 32.2s\tremaining: 1m 11s\n",
      "311:\tlearn: 0.1132885\ttotal: 32.3s\tremaining: 1m 11s\n",
      "312:\tlearn: 0.1132097\ttotal: 32.4s\tremaining: 1m 11s\n",
      "313:\tlearn: 0.1131379\ttotal: 32.5s\tremaining: 1m 11s\n",
      "314:\tlearn: 0.1130884\ttotal: 32.6s\tremaining: 1m 10s\n",
      "315:\tlearn: 0.1130160\ttotal: 32.7s\tremaining: 1m 10s\n",
      "316:\tlearn: 0.1129398\ttotal: 32.8s\tremaining: 1m 10s\n",
      "317:\tlearn: 0.1128060\ttotal: 32.9s\tremaining: 1m 10s\n",
      "318:\tlearn: 0.1127520\ttotal: 33s\tremaining: 1m 10s\n",
      "319:\tlearn: 0.1126874\ttotal: 33.1s\tremaining: 1m 10s\n",
      "320:\tlearn: 0.1125919\ttotal: 33.2s\tremaining: 1m 10s\n",
      "321:\tlearn: 0.1125258\ttotal: 33.3s\tremaining: 1m 10s\n",
      "322:\tlearn: 0.1123926\ttotal: 33.4s\tremaining: 1m 10s\n",
      "323:\tlearn: 0.1123490\ttotal: 33.5s\tremaining: 1m 9s\n",
      "324:\tlearn: 0.1122619\ttotal: 33.6s\tremaining: 1m 9s\n",
      "325:\tlearn: 0.1121595\ttotal: 33.8s\tremaining: 1m 9s\n",
      "326:\tlearn: 0.1121187\ttotal: 33.8s\tremaining: 1m 9s\n",
      "327:\tlearn: 0.1120610\ttotal: 33.9s\tremaining: 1m 9s\n",
      "328:\tlearn: 0.1120146\ttotal: 34s\tremaining: 1m 9s\n",
      "329:\tlearn: 0.1119559\ttotal: 34.1s\tremaining: 1m 9s\n",
      "330:\tlearn: 0.1119161\ttotal: 34.2s\tremaining: 1m 9s\n",
      "331:\tlearn: 0.1118606\ttotal: 34.3s\tremaining: 1m 9s\n",
      "332:\tlearn: 0.1118120\ttotal: 34.4s\tremaining: 1m 8s\n",
      "333:\tlearn: 0.1116933\ttotal: 34.5s\tremaining: 1m 8s\n",
      "334:\tlearn: 0.1115994\ttotal: 34.6s\tremaining: 1m 8s\n",
      "335:\tlearn: 0.1115390\ttotal: 34.7s\tremaining: 1m 8s\n",
      "336:\tlearn: 0.1114746\ttotal: 34.8s\tremaining: 1m 8s\n",
      "337:\tlearn: 0.1114000\ttotal: 35s\tremaining: 1m 8s\n",
      "338:\tlearn: 0.1113520\ttotal: 35.1s\tremaining: 1m 8s\n",
      "339:\tlearn: 0.1113132\ttotal: 35.1s\tremaining: 1m 8s\n",
      "340:\tlearn: 0.1112665\ttotal: 35.4s\tremaining: 1m 8s\n",
      "341:\tlearn: 0.1111996\ttotal: 35.5s\tremaining: 1m 8s\n",
      "342:\tlearn: 0.1111083\ttotal: 35.6s\tremaining: 1m 8s\n",
      "343:\tlearn: 0.1109961\ttotal: 35.7s\tremaining: 1m 7s\n",
      "344:\tlearn: 0.1109102\ttotal: 35.7s\tremaining: 1m 7s\n",
      "345:\tlearn: 0.1108340\ttotal: 35.8s\tremaining: 1m 7s\n",
      "346:\tlearn: 0.1107933\ttotal: 35.9s\tremaining: 1m 7s\n",
      "347:\tlearn: 0.1107562\ttotal: 36s\tremaining: 1m 7s\n",
      "348:\tlearn: 0.1107027\ttotal: 36.1s\tremaining: 1m 7s\n",
      "349:\tlearn: 0.1106319\ttotal: 36.2s\tremaining: 1m 7s\n",
      "350:\tlearn: 0.1106017\ttotal: 36.3s\tremaining: 1m 7s\n",
      "351:\tlearn: 0.1105413\ttotal: 36.4s\tremaining: 1m 7s\n",
      "352:\tlearn: 0.1104976\ttotal: 36.5s\tremaining: 1m 6s\n",
      "353:\tlearn: 0.1104406\ttotal: 36.6s\tremaining: 1m 6s\n",
      "354:\tlearn: 0.1103206\ttotal: 36.7s\tremaining: 1m 6s\n",
      "355:\tlearn: 0.1102795\ttotal: 36.8s\tremaining: 1m 6s\n",
      "356:\tlearn: 0.1102216\ttotal: 36.9s\tremaining: 1m 6s\n",
      "357:\tlearn: 0.1101729\ttotal: 37s\tremaining: 1m 6s\n",
      "358:\tlearn: 0.1101206\ttotal: 37.1s\tremaining: 1m 6s\n",
      "359:\tlearn: 0.1100653\ttotal: 37.2s\tremaining: 1m 6s\n",
      "360:\tlearn: 0.1099345\ttotal: 37.3s\tremaining: 1m 5s\n",
      "361:\tlearn: 0.1098405\ttotal: 37.4s\tremaining: 1m 5s\n",
      "362:\tlearn: 0.1097425\ttotal: 37.5s\tremaining: 1m 5s\n",
      "363:\tlearn: 0.1096913\ttotal: 37.6s\tremaining: 1m 5s\n",
      "364:\tlearn: 0.1096190\ttotal: 37.7s\tremaining: 1m 5s\n",
      "365:\tlearn: 0.1095766\ttotal: 37.8s\tremaining: 1m 5s\n",
      "366:\tlearn: 0.1095159\ttotal: 37.9s\tremaining: 1m 5s\n",
      "367:\tlearn: 0.1094287\ttotal: 38s\tremaining: 1m 5s\n",
      "368:\tlearn: 0.1093722\ttotal: 38s\tremaining: 1m 5s\n",
      "369:\tlearn: 0.1092738\ttotal: 38.1s\tremaining: 1m 4s\n",
      "370:\tlearn: 0.1092311\ttotal: 38.2s\tremaining: 1m 4s\n",
      "371:\tlearn: 0.1091775\ttotal: 38.3s\tremaining: 1m 4s\n",
      "372:\tlearn: 0.1091402\ttotal: 38.4s\tremaining: 1m 4s\n",
      "373:\tlearn: 0.1090840\ttotal: 38.5s\tremaining: 1m 4s\n",
      "374:\tlearn: 0.1090522\ttotal: 38.6s\tremaining: 1m 4s\n",
      "375:\tlearn: 0.1089735\ttotal: 38.7s\tremaining: 1m 4s\n",
      "376:\tlearn: 0.1088690\ttotal: 38.8s\tremaining: 1m 4s\n",
      "377:\tlearn: 0.1088153\ttotal: 38.9s\tremaining: 1m 4s\n",
      "378:\tlearn: 0.1087550\ttotal: 39.1s\tremaining: 1m 4s\n",
      "379:\tlearn: 0.1086844\ttotal: 39.2s\tremaining: 1m 3s\n",
      "380:\tlearn: 0.1086224\ttotal: 39.3s\tremaining: 1m 3s\n",
      "381:\tlearn: 0.1085290\ttotal: 39.4s\tremaining: 1m 3s\n",
      "382:\tlearn: 0.1084175\ttotal: 39.5s\tremaining: 1m 3s\n",
      "383:\tlearn: 0.1083730\ttotal: 39.6s\tremaining: 1m 3s\n",
      "384:\tlearn: 0.1083370\ttotal: 39.7s\tremaining: 1m 3s\n",
      "385:\tlearn: 0.1082561\ttotal: 39.8s\tremaining: 1m 3s\n",
      "386:\tlearn: 0.1082050\ttotal: 39.9s\tremaining: 1m 3s\n",
      "387:\tlearn: 0.1081321\ttotal: 40s\tremaining: 1m 3s\n",
      "388:\tlearn: 0.1080594\ttotal: 40s\tremaining: 1m 2s\n",
      "389:\tlearn: 0.1080021\ttotal: 40.1s\tremaining: 1m 2s\n",
      "390:\tlearn: 0.1079459\ttotal: 40.2s\tremaining: 1m 2s\n",
      "391:\tlearn: 0.1078878\ttotal: 40.3s\tremaining: 1m 2s\n",
      "392:\tlearn: 0.1078359\ttotal: 40.4s\tremaining: 1m 2s\n",
      "393:\tlearn: 0.1077565\ttotal: 40.5s\tremaining: 1m 2s\n",
      "394:\tlearn: 0.1076956\ttotal: 40.6s\tremaining: 1m 2s\n",
      "395:\tlearn: 0.1076372\ttotal: 40.7s\tremaining: 1m 2s\n",
      "396:\tlearn: 0.1075912\ttotal: 40.8s\tremaining: 1m 2s\n",
      "397:\tlearn: 0.1075188\ttotal: 40.9s\tremaining: 1m 1s\n",
      "398:\tlearn: 0.1074547\ttotal: 41s\tremaining: 1m 1s\n",
      "399:\tlearn: 0.1074055\ttotal: 41.1s\tremaining: 1m 1s\n",
      "400:\tlearn: 0.1073740\ttotal: 41.2s\tremaining: 1m 1s\n",
      "401:\tlearn: 0.1073353\ttotal: 41.3s\tremaining: 1m 1s\n",
      "402:\tlearn: 0.1072981\ttotal: 41.4s\tremaining: 1m 1s\n",
      "403:\tlearn: 0.1072322\ttotal: 41.5s\tremaining: 1m 1s\n",
      "404:\tlearn: 0.1072032\ttotal: 41.6s\tremaining: 1m 1s\n",
      "405:\tlearn: 0.1070845\ttotal: 41.7s\tremaining: 1m\n",
      "406:\tlearn: 0.1069629\ttotal: 41.8s\tremaining: 1m\n",
      "407:\tlearn: 0.1069156\ttotal: 41.9s\tremaining: 1m\n",
      "408:\tlearn: 0.1068596\ttotal: 42s\tremaining: 1m\n",
      "409:\tlearn: 0.1067911\ttotal: 42s\tremaining: 1m\n",
      "410:\tlearn: 0.1067102\ttotal: 42.1s\tremaining: 1m\n",
      "411:\tlearn: 0.1066133\ttotal: 42.2s\tremaining: 1m\n",
      "412:\tlearn: 0.1065311\ttotal: 42.3s\tremaining: 1m\n",
      "413:\tlearn: 0.1064668\ttotal: 42.4s\tremaining: 1m\n",
      "414:\tlearn: 0.1064219\ttotal: 42.5s\tremaining: 60s\n",
      "415:\tlearn: 0.1063630\ttotal: 42.6s\tremaining: 59.9s\n",
      "416:\tlearn: 0.1063236\ttotal: 42.7s\tremaining: 59.7s\n",
      "417:\tlearn: 0.1062441\ttotal: 42.8s\tremaining: 59.6s\n",
      "418:\tlearn: 0.1060964\ttotal: 42.9s\tremaining: 59.5s\n",
      "419:\tlearn: 0.1060085\ttotal: 43s\tremaining: 59.4s\n",
      "420:\tlearn: 0.1059371\ttotal: 43.1s\tremaining: 59.3s\n",
      "421:\tlearn: 0.1058656\ttotal: 43.2s\tremaining: 59.2s\n",
      "422:\tlearn: 0.1058231\ttotal: 43.3s\tremaining: 59.1s\n",
      "423:\tlearn: 0.1057320\ttotal: 43.4s\tremaining: 59s\n",
      "424:\tlearn: 0.1056566\ttotal: 43.6s\tremaining: 58.9s\n",
      "425:\tlearn: 0.1055982\ttotal: 43.7s\tremaining: 58.8s\n",
      "426:\tlearn: 0.1055493\ttotal: 43.8s\tremaining: 58.7s\n",
      "427:\tlearn: 0.1055111\ttotal: 43.8s\tremaining: 58.6s\n",
      "428:\tlearn: 0.1054509\ttotal: 43.9s\tremaining: 58.5s\n",
      "429:\tlearn: 0.1053899\ttotal: 44s\tremaining: 58.4s\n",
      "430:\tlearn: 0.1053627\ttotal: 44.1s\tremaining: 58.3s\n",
      "431:\tlearn: 0.1052984\ttotal: 44.2s\tremaining: 58.2s\n",
      "432:\tlearn: 0.1052237\ttotal: 44.3s\tremaining: 58s\n",
      "433:\tlearn: 0.1051998\ttotal: 44.4s\tremaining: 57.9s\n",
      "434:\tlearn: 0.1051745\ttotal: 44.5s\tremaining: 57.8s\n",
      "435:\tlearn: 0.1050984\ttotal: 44.6s\tremaining: 57.7s\n",
      "436:\tlearn: 0.1050525\ttotal: 44.7s\tremaining: 57.6s\n",
      "437:\tlearn: 0.1050192\ttotal: 44.8s\tremaining: 57.5s\n",
      "438:\tlearn: 0.1049298\ttotal: 44.9s\tremaining: 57.4s\n",
      "439:\tlearn: 0.1048939\ttotal: 45s\tremaining: 57.3s\n",
      "440:\tlearn: 0.1048248\ttotal: 45.1s\tremaining: 57.2s\n",
      "441:\tlearn: 0.1047913\ttotal: 45.2s\tremaining: 57.1s\n",
      "442:\tlearn: 0.1047529\ttotal: 45.3s\tremaining: 57s\n",
      "443:\tlearn: 0.1046771\ttotal: 45.4s\tremaining: 56.9s\n",
      "444:\tlearn: 0.1046207\ttotal: 45.5s\tremaining: 56.8s\n",
      "445:\tlearn: 0.1044995\ttotal: 45.6s\tremaining: 56.7s\n",
      "446:\tlearn: 0.1044582\ttotal: 45.7s\tremaining: 56.5s\n",
      "447:\tlearn: 0.1044027\ttotal: 45.8s\tremaining: 56.4s\n",
      "448:\tlearn: 0.1043562\ttotal: 45.9s\tremaining: 56.4s\n",
      "449:\tlearn: 0.1042889\ttotal: 46.1s\tremaining: 56.4s\n",
      "450:\tlearn: 0.1042313\ttotal: 46.2s\tremaining: 56.3s\n",
      "451:\tlearn: 0.1041694\ttotal: 46.3s\tremaining: 56.1s\n",
      "452:\tlearn: 0.1040984\ttotal: 46.4s\tremaining: 56s\n",
      "453:\tlearn: 0.1040385\ttotal: 46.5s\tremaining: 55.9s\n",
      "454:\tlearn: 0.1039949\ttotal: 46.6s\tremaining: 55.8s\n",
      "455:\tlearn: 0.1038895\ttotal: 46.7s\tremaining: 55.7s\n",
      "456:\tlearn: 0.1038325\ttotal: 46.8s\tremaining: 55.6s\n",
      "457:\tlearn: 0.1037561\ttotal: 46.9s\tremaining: 55.5s\n",
      "458:\tlearn: 0.1037000\ttotal: 47s\tremaining: 55.4s\n",
      "459:\tlearn: 0.1036289\ttotal: 47.1s\tremaining: 55.3s\n",
      "460:\tlearn: 0.1035799\ttotal: 47.2s\tremaining: 55.2s\n",
      "461:\tlearn: 0.1035177\ttotal: 47.3s\tremaining: 55.1s\n",
      "462:\tlearn: 0.1034864\ttotal: 47.4s\tremaining: 55s\n",
      "463:\tlearn: 0.1034590\ttotal: 47.5s\tremaining: 54.9s\n",
      "464:\tlearn: 0.1033898\ttotal: 47.6s\tremaining: 54.7s\n",
      "465:\tlearn: 0.1033042\ttotal: 47.7s\tremaining: 54.6s\n",
      "466:\tlearn: 0.1032103\ttotal: 47.8s\tremaining: 54.5s\n",
      "467:\tlearn: 0.1031351\ttotal: 47.9s\tremaining: 54.4s\n",
      "468:\tlearn: 0.1030860\ttotal: 48s\tremaining: 54.3s\n",
      "469:\tlearn: 0.1030631\ttotal: 48.1s\tremaining: 54.2s\n",
      "470:\tlearn: 0.1030259\ttotal: 48.2s\tremaining: 54.1s\n",
      "471:\tlearn: 0.1029938\ttotal: 48.3s\tremaining: 54s\n",
      "472:\tlearn: 0.1028833\ttotal: 48.4s\tremaining: 53.9s\n",
      "473:\tlearn: 0.1028097\ttotal: 48.5s\tremaining: 53.8s\n",
      "474:\tlearn: 0.1027862\ttotal: 48.6s\tremaining: 53.7s\n",
      "475:\tlearn: 0.1027162\ttotal: 48.7s\tremaining: 53.6s\n",
      "476:\tlearn: 0.1026755\ttotal: 48.8s\tremaining: 53.5s\n",
      "477:\tlearn: 0.1026549\ttotal: 48.9s\tremaining: 53.4s\n",
      "478:\tlearn: 0.1026251\ttotal: 49s\tremaining: 53.3s\n",
      "479:\tlearn: 0.1025878\ttotal: 49.1s\tremaining: 53.2s\n",
      "480:\tlearn: 0.1025361\ttotal: 49.2s\tremaining: 53.1s\n",
      "481:\tlearn: 0.1024861\ttotal: 49.3s\tremaining: 53s\n",
      "482:\tlearn: 0.1024437\ttotal: 49.4s\tremaining: 52.9s\n",
      "483:\tlearn: 0.1024066\ttotal: 49.5s\tremaining: 52.7s\n",
      "484:\tlearn: 0.1023713\ttotal: 49.6s\tremaining: 52.6s\n",
      "485:\tlearn: 0.1023109\ttotal: 49.7s\tremaining: 52.5s\n",
      "486:\tlearn: 0.1022574\ttotal: 49.8s\tremaining: 52.4s\n",
      "487:\tlearn: 0.1022163\ttotal: 49.9s\tremaining: 52.3s\n",
      "488:\tlearn: 0.1021451\ttotal: 50s\tremaining: 52.2s\n",
      "489:\tlearn: 0.1020817\ttotal: 50.1s\tremaining: 52.1s\n",
      "490:\tlearn: 0.1020182\ttotal: 50.2s\tremaining: 52s\n",
      "491:\tlearn: 0.1019708\ttotal: 50.3s\tremaining: 51.9s\n",
      "492:\tlearn: 0.1019123\ttotal: 50.4s\tremaining: 51.8s\n",
      "493:\tlearn: 0.1018660\ttotal: 50.5s\tremaining: 51.7s\n",
      "494:\tlearn: 0.1017651\ttotal: 50.6s\tremaining: 51.6s\n",
      "495:\tlearn: 0.1017090\ttotal: 50.7s\tremaining: 51.5s\n",
      "496:\tlearn: 0.1016331\ttotal: 50.8s\tremaining: 51.4s\n",
      "497:\tlearn: 0.1015746\ttotal: 50.9s\tremaining: 51.3s\n",
      "498:\tlearn: 0.1014801\ttotal: 51s\tremaining: 51.2s\n",
      "499:\tlearn: 0.1014184\ttotal: 51.1s\tremaining: 51.1s\n",
      "500:\tlearn: 0.1013925\ttotal: 51.2s\tremaining: 51s\n",
      "501:\tlearn: 0.1013701\ttotal: 51.3s\tremaining: 50.9s\n",
      "502:\tlearn: 0.1012575\ttotal: 51.4s\tremaining: 50.8s\n",
      "503:\tlearn: 0.1011925\ttotal: 51.5s\tremaining: 50.7s\n",
      "504:\tlearn: 0.1011324\ttotal: 51.6s\tremaining: 50.6s\n",
      "505:\tlearn: 0.1010925\ttotal: 51.7s\tremaining: 50.5s\n",
      "506:\tlearn: 0.1010333\ttotal: 51.8s\tremaining: 50.3s\n",
      "507:\tlearn: 0.1009471\ttotal: 51.9s\tremaining: 50.3s\n",
      "508:\tlearn: 0.1008977\ttotal: 52s\tremaining: 50.2s\n",
      "509:\tlearn: 0.1008496\ttotal: 52.1s\tremaining: 50.1s\n",
      "510:\tlearn: 0.1007901\ttotal: 52.2s\tremaining: 50s\n",
      "511:\tlearn: 0.1007600\ttotal: 52.3s\tremaining: 49.8s\n",
      "512:\tlearn: 0.1007084\ttotal: 52.4s\tremaining: 49.7s\n",
      "513:\tlearn: 0.1006685\ttotal: 52.5s\tremaining: 49.6s\n",
      "514:\tlearn: 0.1006464\ttotal: 52.6s\tremaining: 49.5s\n",
      "515:\tlearn: 0.1005762\ttotal: 52.7s\tremaining: 49.4s\n",
      "516:\tlearn: 0.1005382\ttotal: 52.8s\tremaining: 49.3s\n",
      "517:\tlearn: 0.1004418\ttotal: 53s\tremaining: 49.3s\n",
      "518:\tlearn: 0.1004000\ttotal: 53.1s\tremaining: 49.2s\n",
      "519:\tlearn: 0.1003716\ttotal: 53.2s\tremaining: 49.1s\n",
      "520:\tlearn: 0.1003344\ttotal: 53.3s\tremaining: 49s\n",
      "521:\tlearn: 0.1002352\ttotal: 53.4s\tremaining: 48.9s\n",
      "522:\tlearn: 0.1001807\ttotal: 53.5s\tremaining: 48.8s\n",
      "523:\tlearn: 0.1001111\ttotal: 53.6s\tremaining: 48.7s\n",
      "524:\tlearn: 0.1000516\ttotal: 53.7s\tremaining: 48.6s\n",
      "525:\tlearn: 0.1000196\ttotal: 53.8s\tremaining: 48.5s\n",
      "526:\tlearn: 0.0999298\ttotal: 53.9s\tremaining: 48.3s\n",
      "527:\tlearn: 0.0998698\ttotal: 54s\tremaining: 48.2s\n",
      "528:\tlearn: 0.0998429\ttotal: 54.1s\tremaining: 48.1s\n",
      "529:\tlearn: 0.0998284\ttotal: 54.2s\tremaining: 48s\n",
      "530:\tlearn: 0.0997813\ttotal: 54.3s\tremaining: 47.9s\n",
      "531:\tlearn: 0.0997484\ttotal: 54.3s\tremaining: 47.8s\n",
      "532:\tlearn: 0.0997146\ttotal: 54.4s\tremaining: 47.7s\n",
      "533:\tlearn: 0.0996414\ttotal: 54.5s\tremaining: 47.6s\n",
      "534:\tlearn: 0.0995815\ttotal: 54.6s\tremaining: 47.5s\n",
      "535:\tlearn: 0.0995382\ttotal: 54.7s\tremaining: 47.4s\n",
      "536:\tlearn: 0.0994989\ttotal: 54.8s\tremaining: 47.3s\n",
      "537:\tlearn: 0.0994539\ttotal: 54.9s\tremaining: 47.2s\n",
      "538:\tlearn: 0.0994183\ttotal: 55s\tremaining: 47.1s\n",
      "539:\tlearn: 0.0993571\ttotal: 55.1s\tremaining: 46.9s\n",
      "540:\tlearn: 0.0993016\ttotal: 55.2s\tremaining: 46.8s\n",
      "541:\tlearn: 0.0992686\ttotal: 55.3s\tremaining: 46.7s\n",
      "542:\tlearn: 0.0992401\ttotal: 55.4s\tremaining: 46.6s\n",
      "543:\tlearn: 0.0992287\ttotal: 55.5s\tremaining: 46.5s\n",
      "544:\tlearn: 0.0991902\ttotal: 55.6s\tremaining: 46.4s\n",
      "545:\tlearn: 0.0991833\ttotal: 55.7s\tremaining: 46.3s\n",
      "546:\tlearn: 0.0991040\ttotal: 55.8s\tremaining: 46.2s\n",
      "547:\tlearn: 0.0990838\ttotal: 55.9s\tremaining: 46.1s\n",
      "548:\tlearn: 0.0990497\ttotal: 55.9s\tremaining: 46s\n",
      "549:\tlearn: 0.0989911\ttotal: 56s\tremaining: 45.8s\n",
      "550:\tlearn: 0.0989180\ttotal: 56.1s\tremaining: 45.7s\n",
      "551:\tlearn: 0.0988747\ttotal: 56.2s\tremaining: 45.6s\n",
      "552:\tlearn: 0.0988080\ttotal: 56.3s\tremaining: 45.5s\n",
      "553:\tlearn: 0.0987348\ttotal: 56.4s\tremaining: 45.4s\n",
      "554:\tlearn: 0.0986941\ttotal: 56.5s\tremaining: 45.3s\n",
      "555:\tlearn: 0.0986555\ttotal: 56.6s\tremaining: 45.2s\n",
      "556:\tlearn: 0.0985957\ttotal: 56.7s\tremaining: 45.1s\n",
      "557:\tlearn: 0.0985564\ttotal: 56.8s\tremaining: 45s\n",
      "558:\tlearn: 0.0984865\ttotal: 56.9s\tremaining: 44.9s\n",
      "559:\tlearn: 0.0984425\ttotal: 57s\tremaining: 44.8s\n",
      "560:\tlearn: 0.0983452\ttotal: 57.1s\tremaining: 44.7s\n",
      "561:\tlearn: 0.0983170\ttotal: 57.2s\tremaining: 44.6s\n",
      "562:\tlearn: 0.0982325\ttotal: 57.3s\tremaining: 44.5s\n",
      "563:\tlearn: 0.0981875\ttotal: 57.4s\tremaining: 44.4s\n",
      "564:\tlearn: 0.0981615\ttotal: 57.5s\tremaining: 44.3s\n",
      "565:\tlearn: 0.0981176\ttotal: 57.6s\tremaining: 44.2s\n",
      "566:\tlearn: 0.0980736\ttotal: 57.7s\tremaining: 44.1s\n",
      "567:\tlearn: 0.0980360\ttotal: 57.8s\tremaining: 44s\n",
      "568:\tlearn: 0.0979939\ttotal: 57.9s\tremaining: 43.9s\n",
      "569:\tlearn: 0.0979679\ttotal: 58s\tremaining: 43.8s\n",
      "570:\tlearn: 0.0978865\ttotal: 58.1s\tremaining: 43.7s\n",
      "571:\tlearn: 0.0978285\ttotal: 58.2s\tremaining: 43.6s\n",
      "572:\tlearn: 0.0977676\ttotal: 58.3s\tremaining: 43.5s\n",
      "573:\tlearn: 0.0977309\ttotal: 58.4s\tremaining: 43.4s\n",
      "574:\tlearn: 0.0977017\ttotal: 58.5s\tremaining: 43.3s\n",
      "575:\tlearn: 0.0976539\ttotal: 58.6s\tremaining: 43.1s\n",
      "576:\tlearn: 0.0976037\ttotal: 58.7s\tremaining: 43s\n",
      "577:\tlearn: 0.0975717\ttotal: 58.8s\tremaining: 42.9s\n",
      "578:\tlearn: 0.0975332\ttotal: 58.9s\tremaining: 42.8s\n",
      "579:\tlearn: 0.0974218\ttotal: 59s\tremaining: 42.7s\n",
      "580:\tlearn: 0.0973910\ttotal: 59.1s\tremaining: 42.6s\n",
      "581:\tlearn: 0.0973398\ttotal: 59.2s\tremaining: 42.5s\n",
      "582:\tlearn: 0.0972992\ttotal: 59.3s\tremaining: 42.4s\n",
      "583:\tlearn: 0.0972528\ttotal: 59.4s\tremaining: 42.3s\n",
      "584:\tlearn: 0.0971898\ttotal: 59.5s\tremaining: 42.2s\n",
      "585:\tlearn: 0.0971315\ttotal: 59.6s\tremaining: 42.1s\n",
      "586:\tlearn: 0.0970943\ttotal: 59.7s\tremaining: 42s\n",
      "587:\tlearn: 0.0970660\ttotal: 59.7s\tremaining: 41.9s\n",
      "588:\tlearn: 0.0969891\ttotal: 59.8s\tremaining: 41.8s\n",
      "589:\tlearn: 0.0969551\ttotal: 59.9s\tremaining: 41.7s\n",
      "590:\tlearn: 0.0968932\ttotal: 1m\tremaining: 41.6s\n",
      "591:\tlearn: 0.0968445\ttotal: 1m\tremaining: 41.4s\n",
      "592:\tlearn: 0.0967952\ttotal: 1m\tremaining: 41.3s\n",
      "593:\tlearn: 0.0967010\ttotal: 1m\tremaining: 41.2s\n",
      "594:\tlearn: 0.0966325\ttotal: 1m\tremaining: 41.1s\n",
      "595:\tlearn: 0.0965786\ttotal: 1m\tremaining: 41s\n",
      "596:\tlearn: 0.0965195\ttotal: 1m\tremaining: 40.9s\n",
      "597:\tlearn: 0.0964636\ttotal: 1m\tremaining: 40.8s\n",
      "598:\tlearn: 0.0964237\ttotal: 1m\tremaining: 40.7s\n",
      "599:\tlearn: 0.0963890\ttotal: 1m\tremaining: 40.6s\n",
      "600:\tlearn: 0.0963449\ttotal: 1m\tremaining: 40.5s\n",
      "601:\tlearn: 0.0962687\ttotal: 1m 1s\tremaining: 40.4s\n",
      "602:\tlearn: 0.0962201\ttotal: 1m 1s\tremaining: 40.3s\n",
      "603:\tlearn: 0.0961486\ttotal: 1m 1s\tremaining: 40.2s\n",
      "604:\tlearn: 0.0960690\ttotal: 1m 1s\tremaining: 40.1s\n",
      "605:\tlearn: 0.0960303\ttotal: 1m 1s\tremaining: 40s\n",
      "606:\tlearn: 0.0959689\ttotal: 1m 1s\tremaining: 39.9s\n",
      "607:\tlearn: 0.0959449\ttotal: 1m 1s\tremaining: 39.8s\n",
      "608:\tlearn: 0.0958971\ttotal: 1m 1s\tremaining: 39.7s\n",
      "609:\tlearn: 0.0958782\ttotal: 1m 1s\tremaining: 39.5s\n",
      "610:\tlearn: 0.0958490\ttotal: 1m 1s\tremaining: 39.4s\n",
      "611:\tlearn: 0.0958011\ttotal: 1m 2s\tremaining: 39.3s\n",
      "612:\tlearn: 0.0957782\ttotal: 1m 2s\tremaining: 39.2s\n",
      "613:\tlearn: 0.0957257\ttotal: 1m 2s\tremaining: 39.1s\n",
      "614:\tlearn: 0.0956810\ttotal: 1m 2s\tremaining: 39s\n",
      "615:\tlearn: 0.0956620\ttotal: 1m 2s\tremaining: 38.9s\n",
      "616:\tlearn: 0.0955953\ttotal: 1m 2s\tremaining: 38.8s\n",
      "617:\tlearn: 0.0955726\ttotal: 1m 2s\tremaining: 38.7s\n",
      "618:\tlearn: 0.0955168\ttotal: 1m 2s\tremaining: 38.6s\n",
      "619:\tlearn: 0.0954812\ttotal: 1m 2s\tremaining: 38.5s\n",
      "620:\tlearn: 0.0954486\ttotal: 1m 2s\tremaining: 38.4s\n",
      "621:\tlearn: 0.0954225\ttotal: 1m 3s\tremaining: 38.3s\n",
      "622:\tlearn: 0.0953873\ttotal: 1m 3s\tremaining: 38.2s\n",
      "623:\tlearn: 0.0953148\ttotal: 1m 3s\tremaining: 38.1s\n",
      "624:\tlearn: 0.0952666\ttotal: 1m 3s\tremaining: 38s\n",
      "625:\tlearn: 0.0952421\ttotal: 1m 3s\tremaining: 37.9s\n",
      "626:\tlearn: 0.0952331\ttotal: 1m 3s\tremaining: 37.8s\n",
      "627:\tlearn: 0.0951834\ttotal: 1m 3s\tremaining: 37.7s\n",
      "628:\tlearn: 0.0951540\ttotal: 1m 3s\tremaining: 37.6s\n",
      "629:\tlearn: 0.0951020\ttotal: 1m 3s\tremaining: 37.5s\n",
      "630:\tlearn: 0.0950829\ttotal: 1m 3s\tremaining: 37.4s\n",
      "631:\tlearn: 0.0950474\ttotal: 1m 4s\tremaining: 37.3s\n",
      "632:\tlearn: 0.0949808\ttotal: 1m 4s\tremaining: 37.2s\n",
      "633:\tlearn: 0.0949523\ttotal: 1m 4s\tremaining: 37.1s\n",
      "634:\tlearn: 0.0948882\ttotal: 1m 4s\tremaining: 37s\n",
      "635:\tlearn: 0.0948299\ttotal: 1m 4s\tremaining: 36.9s\n",
      "636:\tlearn: 0.0947895\ttotal: 1m 4s\tremaining: 36.8s\n",
      "637:\tlearn: 0.0947588\ttotal: 1m 4s\tremaining: 36.7s\n",
      "638:\tlearn: 0.0947142\ttotal: 1m 4s\tremaining: 36.6s\n",
      "639:\tlearn: 0.0946905\ttotal: 1m 4s\tremaining: 36.5s\n",
      "640:\tlearn: 0.0946365\ttotal: 1m 4s\tremaining: 36.4s\n",
      "641:\tlearn: 0.0945863\ttotal: 1m 5s\tremaining: 36.3s\n",
      "642:\tlearn: 0.0945598\ttotal: 1m 5s\tremaining: 36.2s\n",
      "643:\tlearn: 0.0945232\ttotal: 1m 5s\tremaining: 36.1s\n",
      "644:\tlearn: 0.0944666\ttotal: 1m 5s\tremaining: 36s\n",
      "645:\tlearn: 0.0944137\ttotal: 1m 5s\tremaining: 35.9s\n",
      "646:\tlearn: 0.0943834\ttotal: 1m 5s\tremaining: 35.8s\n",
      "647:\tlearn: 0.0943440\ttotal: 1m 5s\tremaining: 35.6s\n",
      "648:\tlearn: 0.0943027\ttotal: 1m 5s\tremaining: 35.5s\n",
      "649:\tlearn: 0.0942741\ttotal: 1m 5s\tremaining: 35.4s\n",
      "650:\tlearn: 0.0942298\ttotal: 1m 5s\tremaining: 35.3s\n",
      "651:\tlearn: 0.0941972\ttotal: 1m 6s\tremaining: 35.2s\n",
      "652:\tlearn: 0.0941660\ttotal: 1m 6s\tremaining: 35.1s\n",
      "653:\tlearn: 0.0940963\ttotal: 1m 6s\tremaining: 35s\n",
      "654:\tlearn: 0.0940655\ttotal: 1m 6s\tremaining: 34.9s\n",
      "655:\tlearn: 0.0940313\ttotal: 1m 6s\tremaining: 34.8s\n",
      "656:\tlearn: 0.0939815\ttotal: 1m 6s\tremaining: 34.7s\n",
      "657:\tlearn: 0.0939295\ttotal: 1m 6s\tremaining: 34.6s\n",
      "658:\tlearn: 0.0938751\ttotal: 1m 6s\tremaining: 34.5s\n",
      "659:\tlearn: 0.0938327\ttotal: 1m 6s\tremaining: 34.4s\n",
      "660:\tlearn: 0.0937807\ttotal: 1m 6s\tremaining: 34.3s\n",
      "661:\tlearn: 0.0937565\ttotal: 1m 6s\tremaining: 34.2s\n",
      "662:\tlearn: 0.0936789\ttotal: 1m 7s\tremaining: 34.1s\n",
      "663:\tlearn: 0.0936515\ttotal: 1m 7s\tremaining: 34s\n",
      "664:\tlearn: 0.0936297\ttotal: 1m 7s\tremaining: 33.9s\n",
      "665:\tlearn: 0.0935856\ttotal: 1m 7s\tremaining: 33.8s\n",
      "666:\tlearn: 0.0935616\ttotal: 1m 7s\tremaining: 33.7s\n",
      "667:\tlearn: 0.0934966\ttotal: 1m 7s\tremaining: 33.6s\n",
      "668:\tlearn: 0.0934587\ttotal: 1m 7s\tremaining: 33.5s\n",
      "669:\tlearn: 0.0934319\ttotal: 1m 7s\tremaining: 33.4s\n",
      "670:\tlearn: 0.0933700\ttotal: 1m 7s\tremaining: 33.3s\n",
      "671:\tlearn: 0.0932815\ttotal: 1m 7s\tremaining: 33.2s\n",
      "672:\tlearn: 0.0932319\ttotal: 1m 8s\tremaining: 33s\n",
      "673:\tlearn: 0.0931783\ttotal: 1m 8s\tremaining: 32.9s\n",
      "674:\tlearn: 0.0931365\ttotal: 1m 8s\tremaining: 32.8s\n",
      "675:\tlearn: 0.0930814\ttotal: 1m 8s\tremaining: 32.7s\n",
      "676:\tlearn: 0.0930375\ttotal: 1m 8s\tremaining: 32.6s\n",
      "677:\tlearn: 0.0929837\ttotal: 1m 8s\tremaining: 32.5s\n",
      "678:\tlearn: 0.0929215\ttotal: 1m 8s\tremaining: 32.4s\n",
      "679:\tlearn: 0.0928851\ttotal: 1m 8s\tremaining: 32.3s\n",
      "680:\tlearn: 0.0928553\ttotal: 1m 8s\tremaining: 32.2s\n",
      "681:\tlearn: 0.0927671\ttotal: 1m 8s\tremaining: 32.1s\n",
      "682:\tlearn: 0.0927436\ttotal: 1m 8s\tremaining: 32s\n",
      "683:\tlearn: 0.0927022\ttotal: 1m 9s\tremaining: 31.9s\n",
      "684:\tlearn: 0.0926832\ttotal: 1m 9s\tremaining: 31.8s\n",
      "685:\tlearn: 0.0926534\ttotal: 1m 9s\tremaining: 31.7s\n",
      "686:\tlearn: 0.0926307\ttotal: 1m 9s\tremaining: 31.6s\n",
      "687:\tlearn: 0.0926029\ttotal: 1m 9s\tremaining: 31.5s\n",
      "688:\tlearn: 0.0925776\ttotal: 1m 9s\tremaining: 31.4s\n",
      "689:\tlearn: 0.0925347\ttotal: 1m 9s\tremaining: 31.3s\n",
      "690:\tlearn: 0.0924938\ttotal: 1m 9s\tremaining: 31.2s\n",
      "691:\tlearn: 0.0924626\ttotal: 1m 9s\tremaining: 31.1s\n",
      "692:\tlearn: 0.0924184\ttotal: 1m 9s\tremaining: 31s\n",
      "693:\tlearn: 0.0923711\ttotal: 1m 10s\tremaining: 30.9s\n",
      "694:\tlearn: 0.0923355\ttotal: 1m 10s\tremaining: 30.8s\n",
      "695:\tlearn: 0.0923022\ttotal: 1m 10s\tremaining: 30.7s\n",
      "696:\tlearn: 0.0922252\ttotal: 1m 10s\tremaining: 30.6s\n",
      "697:\tlearn: 0.0921896\ttotal: 1m 10s\tremaining: 30.5s\n",
      "698:\tlearn: 0.0921362\ttotal: 1m 10s\tremaining: 30.4s\n",
      "699:\tlearn: 0.0920932\ttotal: 1m 10s\tremaining: 30.3s\n",
      "700:\tlearn: 0.0920000\ttotal: 1m 10s\tremaining: 30.2s\n",
      "701:\tlearn: 0.0919572\ttotal: 1m 10s\tremaining: 30.1s\n",
      "702:\tlearn: 0.0918968\ttotal: 1m 10s\tremaining: 29.9s\n",
      "703:\tlearn: 0.0918060\ttotal: 1m 11s\tremaining: 29.9s\n",
      "704:\tlearn: 0.0917554\ttotal: 1m 11s\tremaining: 29.8s\n",
      "705:\tlearn: 0.0917313\ttotal: 1m 11s\tremaining: 29.7s\n",
      "706:\tlearn: 0.0917065\ttotal: 1m 11s\tremaining: 29.6s\n",
      "707:\tlearn: 0.0916832\ttotal: 1m 11s\tremaining: 29.4s\n",
      "708:\tlearn: 0.0916284\ttotal: 1m 11s\tremaining: 29.3s\n",
      "709:\tlearn: 0.0915722\ttotal: 1m 11s\tremaining: 29.2s\n",
      "710:\tlearn: 0.0915156\ttotal: 1m 11s\tremaining: 29.1s\n",
      "711:\tlearn: 0.0914619\ttotal: 1m 11s\tremaining: 29s\n",
      "712:\tlearn: 0.0914208\ttotal: 1m 11s\tremaining: 28.9s\n",
      "713:\tlearn: 0.0913642\ttotal: 1m 11s\tremaining: 28.8s\n",
      "714:\tlearn: 0.0913174\ttotal: 1m 12s\tremaining: 28.7s\n",
      "715:\tlearn: 0.0912747\ttotal: 1m 12s\tremaining: 28.6s\n",
      "716:\tlearn: 0.0912602\ttotal: 1m 12s\tremaining: 28.5s\n",
      "717:\tlearn: 0.0911684\ttotal: 1m 12s\tremaining: 28.4s\n",
      "718:\tlearn: 0.0911295\ttotal: 1m 12s\tremaining: 28.3s\n",
      "719:\tlearn: 0.0910830\ttotal: 1m 12s\tremaining: 28.2s\n",
      "720:\tlearn: 0.0910508\ttotal: 1m 12s\tremaining: 28.1s\n",
      "721:\tlearn: 0.0909968\ttotal: 1m 12s\tremaining: 28s\n",
      "722:\tlearn: 0.0909582\ttotal: 1m 12s\tremaining: 27.9s\n",
      "723:\tlearn: 0.0908864\ttotal: 1m 12s\tremaining: 27.8s\n",
      "724:\tlearn: 0.0908421\ttotal: 1m 13s\tremaining: 27.7s\n",
      "725:\tlearn: 0.0908037\ttotal: 1m 13s\tremaining: 27.6s\n",
      "726:\tlearn: 0.0907607\ttotal: 1m 13s\tremaining: 27.5s\n",
      "727:\tlearn: 0.0907371\ttotal: 1m 13s\tremaining: 27.4s\n",
      "728:\tlearn: 0.0907252\ttotal: 1m 13s\tremaining: 27.3s\n",
      "729:\tlearn: 0.0906981\ttotal: 1m 13s\tremaining: 27.2s\n",
      "730:\tlearn: 0.0906652\ttotal: 1m 13s\tremaining: 27.1s\n",
      "731:\tlearn: 0.0906268\ttotal: 1m 13s\tremaining: 27s\n",
      "732:\tlearn: 0.0906216\ttotal: 1m 13s\tremaining: 26.9s\n",
      "733:\tlearn: 0.0905835\ttotal: 1m 13s\tremaining: 26.8s\n",
      "734:\tlearn: 0.0905330\ttotal: 1m 13s\tremaining: 26.7s\n",
      "735:\tlearn: 0.0904948\ttotal: 1m 14s\tremaining: 26.6s\n",
      "736:\tlearn: 0.0904500\ttotal: 1m 14s\tremaining: 26.5s\n",
      "737:\tlearn: 0.0904026\ttotal: 1m 14s\tremaining: 26.4s\n",
      "738:\tlearn: 0.0903596\ttotal: 1m 14s\tremaining: 26.3s\n",
      "739:\tlearn: 0.0902523\ttotal: 1m 14s\tremaining: 26.2s\n",
      "740:\tlearn: 0.0902030\ttotal: 1m 14s\tremaining: 26.1s\n",
      "741:\tlearn: 0.0901312\ttotal: 1m 14s\tremaining: 26s\n",
      "742:\tlearn: 0.0900960\ttotal: 1m 14s\tremaining: 25.9s\n",
      "743:\tlearn: 0.0900860\ttotal: 1m 14s\tremaining: 25.8s\n",
      "744:\tlearn: 0.0900743\ttotal: 1m 15s\tremaining: 25.7s\n",
      "745:\tlearn: 0.0900147\ttotal: 1m 15s\tremaining: 25.6s\n",
      "746:\tlearn: 0.0899773\ttotal: 1m 15s\tremaining: 25.5s\n",
      "747:\tlearn: 0.0899682\ttotal: 1m 15s\tremaining: 25.4s\n",
      "748:\tlearn: 0.0899191\ttotal: 1m 15s\tremaining: 25.3s\n",
      "749:\tlearn: 0.0898746\ttotal: 1m 15s\tremaining: 25.2s\n",
      "750:\tlearn: 0.0898332\ttotal: 1m 15s\tremaining: 25.1s\n",
      "751:\tlearn: 0.0897862\ttotal: 1m 15s\tremaining: 25s\n",
      "752:\tlearn: 0.0897064\ttotal: 1m 15s\tremaining: 24.9s\n",
      "753:\tlearn: 0.0896686\ttotal: 1m 15s\tremaining: 24.8s\n",
      "754:\tlearn: 0.0896608\ttotal: 1m 15s\tremaining: 24.7s\n",
      "755:\tlearn: 0.0896340\ttotal: 1m 16s\tremaining: 24.6s\n",
      "756:\tlearn: 0.0895661\ttotal: 1m 16s\tremaining: 24.5s\n",
      "757:\tlearn: 0.0895314\ttotal: 1m 16s\tremaining: 24.4s\n",
      "758:\tlearn: 0.0895031\ttotal: 1m 16s\tremaining: 24.3s\n",
      "759:\tlearn: 0.0894616\ttotal: 1m 16s\tremaining: 24.2s\n",
      "760:\tlearn: 0.0894335\ttotal: 1m 16s\tremaining: 24.1s\n",
      "761:\tlearn: 0.0893986\ttotal: 1m 16s\tremaining: 24s\n",
      "762:\tlearn: 0.0893753\ttotal: 1m 16s\tremaining: 23.9s\n",
      "763:\tlearn: 0.0893266\ttotal: 1m 16s\tremaining: 23.8s\n",
      "764:\tlearn: 0.0892692\ttotal: 1m 16s\tremaining: 23.6s\n",
      "765:\tlearn: 0.0892262\ttotal: 1m 17s\tremaining: 23.5s\n",
      "766:\tlearn: 0.0891771\ttotal: 1m 17s\tremaining: 23.4s\n",
      "767:\tlearn: 0.0891509\ttotal: 1m 17s\tremaining: 23.3s\n",
      "768:\tlearn: 0.0891070\ttotal: 1m 17s\tremaining: 23.2s\n",
      "769:\tlearn: 0.0890614\ttotal: 1m 17s\tremaining: 23.1s\n",
      "770:\tlearn: 0.0890159\ttotal: 1m 17s\tremaining: 23s\n",
      "771:\tlearn: 0.0889898\ttotal: 1m 17s\tremaining: 22.9s\n",
      "772:\tlearn: 0.0889361\ttotal: 1m 17s\tremaining: 22.8s\n",
      "773:\tlearn: 0.0889054\ttotal: 1m 17s\tremaining: 22.7s\n",
      "774:\tlearn: 0.0888588\ttotal: 1m 17s\tremaining: 22.6s\n",
      "775:\tlearn: 0.0887957\ttotal: 1m 18s\tremaining: 22.5s\n",
      "776:\tlearn: 0.0887673\ttotal: 1m 18s\tremaining: 22.4s\n",
      "777:\tlearn: 0.0886998\ttotal: 1m 18s\tremaining: 22.3s\n",
      "778:\tlearn: 0.0886917\ttotal: 1m 18s\tremaining: 22.2s\n",
      "779:\tlearn: 0.0886541\ttotal: 1m 18s\tremaining: 22.1s\n",
      "780:\tlearn: 0.0886037\ttotal: 1m 18s\tremaining: 22s\n",
      "781:\tlearn: 0.0885834\ttotal: 1m 18s\tremaining: 21.9s\n",
      "782:\tlearn: 0.0884994\ttotal: 1m 18s\tremaining: 21.8s\n",
      "783:\tlearn: 0.0884850\ttotal: 1m 18s\tremaining: 21.7s\n",
      "784:\tlearn: 0.0884275\ttotal: 1m 18s\tremaining: 21.6s\n",
      "785:\tlearn: 0.0883301\ttotal: 1m 19s\tremaining: 21.5s\n",
      "786:\tlearn: 0.0882605\ttotal: 1m 19s\tremaining: 21.4s\n",
      "787:\tlearn: 0.0882126\ttotal: 1m 19s\tremaining: 21.3s\n",
      "788:\tlearn: 0.0881540\ttotal: 1m 19s\tremaining: 21.2s\n",
      "789:\tlearn: 0.0881212\ttotal: 1m 19s\tremaining: 21.1s\n",
      "790:\tlearn: 0.0880761\ttotal: 1m 19s\tremaining: 21s\n",
      "791:\tlearn: 0.0880448\ttotal: 1m 19s\tremaining: 20.9s\n",
      "792:\tlearn: 0.0880144\ttotal: 1m 19s\tremaining: 20.8s\n",
      "793:\tlearn: 0.0879583\ttotal: 1m 19s\tremaining: 20.7s\n",
      "794:\tlearn: 0.0879391\ttotal: 1m 19s\tremaining: 20.6s\n",
      "795:\tlearn: 0.0879141\ttotal: 1m 20s\tremaining: 20.5s\n",
      "796:\tlearn: 0.0879110\ttotal: 1m 20s\tremaining: 20.4s\n",
      "797:\tlearn: 0.0878602\ttotal: 1m 20s\tremaining: 20.3s\n",
      "798:\tlearn: 0.0878374\ttotal: 1m 20s\tremaining: 20.2s\n",
      "799:\tlearn: 0.0878128\ttotal: 1m 20s\tremaining: 20.1s\n",
      "800:\tlearn: 0.0877718\ttotal: 1m 20s\tremaining: 20s\n",
      "801:\tlearn: 0.0877463\ttotal: 1m 20s\tremaining: 19.9s\n",
      "802:\tlearn: 0.0877013\ttotal: 1m 20s\tremaining: 19.8s\n",
      "803:\tlearn: 0.0876619\ttotal: 1m 20s\tremaining: 19.7s\n",
      "804:\tlearn: 0.0876314\ttotal: 1m 20s\tremaining: 19.6s\n",
      "805:\tlearn: 0.0875443\ttotal: 1m 20s\tremaining: 19.5s\n",
      "806:\tlearn: 0.0875146\ttotal: 1m 21s\tremaining: 19.4s\n",
      "807:\tlearn: 0.0874864\ttotal: 1m 21s\tremaining: 19.3s\n",
      "808:\tlearn: 0.0874413\ttotal: 1m 21s\tremaining: 19.2s\n",
      "809:\tlearn: 0.0873945\ttotal: 1m 21s\tremaining: 19.1s\n",
      "810:\tlearn: 0.0873923\ttotal: 1m 21s\tremaining: 19s\n",
      "811:\tlearn: 0.0873713\ttotal: 1m 21s\tremaining: 18.9s\n",
      "812:\tlearn: 0.0873467\ttotal: 1m 21s\tremaining: 18.8s\n",
      "813:\tlearn: 0.0872789\ttotal: 1m 21s\tremaining: 18.7s\n",
      "814:\tlearn: 0.0872787\ttotal: 1m 21s\tremaining: 18.6s\n",
      "815:\tlearn: 0.0872384\ttotal: 1m 21s\tremaining: 18.5s\n",
      "816:\tlearn: 0.0871977\ttotal: 1m 22s\tremaining: 18.4s\n",
      "817:\tlearn: 0.0871808\ttotal: 1m 22s\tremaining: 18.3s\n",
      "818:\tlearn: 0.0871504\ttotal: 1m 22s\tremaining: 18.2s\n",
      "819:\tlearn: 0.0871009\ttotal: 1m 22s\tremaining: 18.1s\n",
      "820:\tlearn: 0.0870712\ttotal: 1m 22s\tremaining: 18s\n",
      "821:\tlearn: 0.0870203\ttotal: 1m 22s\tremaining: 17.9s\n",
      "822:\tlearn: 0.0869722\ttotal: 1m 22s\tremaining: 17.8s\n",
      "823:\tlearn: 0.0869267\ttotal: 1m 22s\tremaining: 17.7s\n",
      "824:\tlearn: 0.0868910\ttotal: 1m 22s\tremaining: 17.6s\n",
      "825:\tlearn: 0.0868703\ttotal: 1m 22s\tremaining: 17.5s\n",
      "826:\tlearn: 0.0868395\ttotal: 1m 23s\tremaining: 17.4s\n",
      "827:\tlearn: 0.0867417\ttotal: 1m 23s\tremaining: 17.3s\n",
      "828:\tlearn: 0.0866837\ttotal: 1m 23s\tremaining: 17.2s\n",
      "829:\tlearn: 0.0866497\ttotal: 1m 23s\tremaining: 17.1s\n",
      "830:\tlearn: 0.0865959\ttotal: 1m 23s\tremaining: 17s\n",
      "831:\tlearn: 0.0865462\ttotal: 1m 23s\tremaining: 16.9s\n",
      "832:\tlearn: 0.0865130\ttotal: 1m 23s\tremaining: 16.8s\n",
      "833:\tlearn: 0.0865052\ttotal: 1m 23s\tremaining: 16.7s\n",
      "834:\tlearn: 0.0864572\ttotal: 1m 23s\tremaining: 16.6s\n",
      "835:\tlearn: 0.0864219\ttotal: 1m 24s\tremaining: 16.5s\n",
      "836:\tlearn: 0.0863532\ttotal: 1m 24s\tremaining: 16.4s\n",
      "837:\tlearn: 0.0863174\ttotal: 1m 24s\tremaining: 16.3s\n",
      "838:\tlearn: 0.0862952\ttotal: 1m 24s\tremaining: 16.2s\n",
      "839:\tlearn: 0.0862500\ttotal: 1m 24s\tremaining: 16.1s\n",
      "840:\tlearn: 0.0862233\ttotal: 1m 24s\tremaining: 16s\n",
      "841:\tlearn: 0.0861940\ttotal: 1m 24s\tremaining: 15.9s\n",
      "842:\tlearn: 0.0861544\ttotal: 1m 24s\tremaining: 15.8s\n",
      "843:\tlearn: 0.0860868\ttotal: 1m 24s\tremaining: 15.7s\n",
      "844:\tlearn: 0.0860611\ttotal: 1m 24s\tremaining: 15.6s\n",
      "845:\tlearn: 0.0860379\ttotal: 1m 24s\tremaining: 15.5s\n",
      "846:\tlearn: 0.0859970\ttotal: 1m 25s\tremaining: 15.4s\n",
      "847:\tlearn: 0.0859537\ttotal: 1m 25s\tremaining: 15.3s\n",
      "848:\tlearn: 0.0859151\ttotal: 1m 25s\tremaining: 15.2s\n",
      "849:\tlearn: 0.0858897\ttotal: 1m 25s\tremaining: 15.1s\n",
      "850:\tlearn: 0.0858701\ttotal: 1m 25s\tremaining: 15s\n",
      "851:\tlearn: 0.0858469\ttotal: 1m 25s\tremaining: 14.9s\n",
      "852:\tlearn: 0.0858056\ttotal: 1m 25s\tremaining: 14.8s\n",
      "853:\tlearn: 0.0857782\ttotal: 1m 25s\tremaining: 14.7s\n",
      "854:\tlearn: 0.0857331\ttotal: 1m 25s\tremaining: 14.6s\n",
      "855:\tlearn: 0.0857009\ttotal: 1m 25s\tremaining: 14.5s\n",
      "856:\tlearn: 0.0856595\ttotal: 1m 26s\tremaining: 14.4s\n",
      "857:\tlearn: 0.0856253\ttotal: 1m 26s\tremaining: 14.3s\n",
      "858:\tlearn: 0.0855916\ttotal: 1m 26s\tremaining: 14.2s\n",
      "859:\tlearn: 0.0855730\ttotal: 1m 26s\tremaining: 14.1s\n",
      "860:\tlearn: 0.0855232\ttotal: 1m 26s\tremaining: 14s\n",
      "861:\tlearn: 0.0854990\ttotal: 1m 26s\tremaining: 13.9s\n",
      "862:\tlearn: 0.0854636\ttotal: 1m 26s\tremaining: 13.8s\n",
      "863:\tlearn: 0.0854052\ttotal: 1m 26s\tremaining: 13.7s\n",
      "864:\tlearn: 0.0853828\ttotal: 1m 26s\tremaining: 13.6s\n",
      "865:\tlearn: 0.0853762\ttotal: 1m 26s\tremaining: 13.4s\n",
      "866:\tlearn: 0.0853537\ttotal: 1m 27s\tremaining: 13.3s\n",
      "867:\tlearn: 0.0853245\ttotal: 1m 27s\tremaining: 13.2s\n",
      "868:\tlearn: 0.0852981\ttotal: 1m 27s\tremaining: 13.1s\n",
      "869:\tlearn: 0.0852670\ttotal: 1m 27s\tremaining: 13s\n",
      "870:\tlearn: 0.0852255\ttotal: 1m 27s\tremaining: 12.9s\n",
      "871:\tlearn: 0.0852240\ttotal: 1m 27s\tremaining: 12.8s\n",
      "872:\tlearn: 0.0851882\ttotal: 1m 27s\tremaining: 12.7s\n",
      "873:\tlearn: 0.0851758\ttotal: 1m 27s\tremaining: 12.6s\n",
      "874:\tlearn: 0.0851356\ttotal: 1m 27s\tremaining: 12.5s\n",
      "875:\tlearn: 0.0851100\ttotal: 1m 27s\tremaining: 12.4s\n",
      "876:\tlearn: 0.0850813\ttotal: 1m 27s\tremaining: 12.3s\n",
      "877:\tlearn: 0.0850431\ttotal: 1m 28s\tremaining: 12.2s\n",
      "878:\tlearn: 0.0850004\ttotal: 1m 28s\tremaining: 12.1s\n",
      "879:\tlearn: 0.0849878\ttotal: 1m 28s\tremaining: 12s\n",
      "880:\tlearn: 0.0849661\ttotal: 1m 28s\tremaining: 11.9s\n",
      "881:\tlearn: 0.0849547\ttotal: 1m 28s\tremaining: 11.8s\n",
      "882:\tlearn: 0.0848921\ttotal: 1m 28s\tremaining: 11.7s\n",
      "883:\tlearn: 0.0848796\ttotal: 1m 28s\tremaining: 11.6s\n",
      "884:\tlearn: 0.0848440\ttotal: 1m 28s\tremaining: 11.5s\n",
      "885:\tlearn: 0.0848326\ttotal: 1m 28s\tremaining: 11.4s\n",
      "886:\tlearn: 0.0847918\ttotal: 1m 28s\tremaining: 11.3s\n",
      "887:\tlearn: 0.0847584\ttotal: 1m 29s\tremaining: 11.2s\n",
      "888:\tlearn: 0.0847281\ttotal: 1m 29s\tremaining: 11.1s\n",
      "889:\tlearn: 0.0847005\ttotal: 1m 29s\tremaining: 11s\n",
      "890:\tlearn: 0.0846560\ttotal: 1m 29s\tremaining: 10.9s\n",
      "891:\tlearn: 0.0846327\ttotal: 1m 29s\tremaining: 10.8s\n",
      "892:\tlearn: 0.0845780\ttotal: 1m 29s\tremaining: 10.7s\n",
      "893:\tlearn: 0.0845601\ttotal: 1m 29s\tremaining: 10.6s\n",
      "894:\tlearn: 0.0845403\ttotal: 1m 29s\tremaining: 10.5s\n",
      "895:\tlearn: 0.0845190\ttotal: 1m 29s\tremaining: 10.4s\n",
      "896:\tlearn: 0.0844936\ttotal: 1m 29s\tremaining: 10.3s\n",
      "897:\tlearn: 0.0844691\ttotal: 1m 29s\tremaining: 10.2s\n",
      "898:\tlearn: 0.0844366\ttotal: 1m 30s\tremaining: 10.1s\n",
      "899:\tlearn: 0.0844279\ttotal: 1m 30s\tremaining: 10s\n",
      "900:\tlearn: 0.0844003\ttotal: 1m 30s\tremaining: 9.92s\n",
      "901:\tlearn: 0.0843755\ttotal: 1m 30s\tremaining: 9.82s\n",
      "902:\tlearn: 0.0843432\ttotal: 1m 30s\tremaining: 9.72s\n",
      "903:\tlearn: 0.0843358\ttotal: 1m 30s\tremaining: 9.62s\n",
      "904:\tlearn: 0.0842879\ttotal: 1m 30s\tremaining: 9.52s\n",
      "905:\tlearn: 0.0842538\ttotal: 1m 30s\tremaining: 9.42s\n",
      "906:\tlearn: 0.0842134\ttotal: 1m 30s\tremaining: 9.32s\n",
      "907:\tlearn: 0.0841971\ttotal: 1m 31s\tremaining: 9.22s\n",
      "908:\tlearn: 0.0841587\ttotal: 1m 31s\tremaining: 9.12s\n",
      "909:\tlearn: 0.0841127\ttotal: 1m 31s\tremaining: 9.02s\n",
      "910:\tlearn: 0.0840864\ttotal: 1m 31s\tremaining: 8.92s\n",
      "911:\tlearn: 0.0840372\ttotal: 1m 31s\tremaining: 8.83s\n",
      "912:\tlearn: 0.0840005\ttotal: 1m 31s\tremaining: 8.73s\n",
      "913:\tlearn: 0.0839884\ttotal: 1m 31s\tremaining: 8.63s\n",
      "914:\tlearn: 0.0839448\ttotal: 1m 31s\tremaining: 8.53s\n",
      "915:\tlearn: 0.0838899\ttotal: 1m 31s\tremaining: 8.43s\n",
      "916:\tlearn: 0.0838297\ttotal: 1m 32s\tremaining: 8.33s\n",
      "917:\tlearn: 0.0838107\ttotal: 1m 32s\tremaining: 8.23s\n",
      "918:\tlearn: 0.0837399\ttotal: 1m 32s\tremaining: 8.14s\n",
      "919:\tlearn: 0.0837332\ttotal: 1m 32s\tremaining: 8.04s\n",
      "920:\tlearn: 0.0836994\ttotal: 1m 32s\tremaining: 7.94s\n",
      "921:\tlearn: 0.0836678\ttotal: 1m 32s\tremaining: 7.84s\n",
      "922:\tlearn: 0.0836367\ttotal: 1m 32s\tremaining: 7.74s\n",
      "923:\tlearn: 0.0835794\ttotal: 1m 32s\tremaining: 7.64s\n",
      "924:\tlearn: 0.0835337\ttotal: 1m 32s\tremaining: 7.54s\n",
      "925:\tlearn: 0.0834712\ttotal: 1m 33s\tremaining: 7.44s\n",
      "926:\tlearn: 0.0834235\ttotal: 1m 33s\tremaining: 7.34s\n",
      "927:\tlearn: 0.0833784\ttotal: 1m 33s\tremaining: 7.24s\n",
      "928:\tlearn: 0.0833642\ttotal: 1m 33s\tremaining: 7.13s\n",
      "929:\tlearn: 0.0833110\ttotal: 1m 33s\tremaining: 7.03s\n",
      "930:\tlearn: 0.0832857\ttotal: 1m 33s\tremaining: 6.93s\n",
      "931:\tlearn: 0.0832443\ttotal: 1m 33s\tremaining: 6.83s\n",
      "932:\tlearn: 0.0832393\ttotal: 1m 33s\tremaining: 6.73s\n",
      "933:\tlearn: 0.0831671\ttotal: 1m 33s\tremaining: 6.63s\n",
      "934:\tlearn: 0.0831579\ttotal: 1m 33s\tremaining: 6.53s\n",
      "935:\tlearn: 0.0831258\ttotal: 1m 34s\tremaining: 6.43s\n",
      "936:\tlearn: 0.0831033\ttotal: 1m 34s\tremaining: 6.33s\n",
      "937:\tlearn: 0.0830741\ttotal: 1m 34s\tremaining: 6.23s\n",
      "938:\tlearn: 0.0830382\ttotal: 1m 34s\tremaining: 6.13s\n",
      "939:\tlearn: 0.0829956\ttotal: 1m 34s\tremaining: 6.03s\n",
      "940:\tlearn: 0.0829601\ttotal: 1m 34s\tremaining: 5.93s\n",
      "941:\tlearn: 0.0829405\ttotal: 1m 34s\tremaining: 5.83s\n",
      "942:\tlearn: 0.0828888\ttotal: 1m 34s\tremaining: 5.73s\n",
      "943:\tlearn: 0.0828719\ttotal: 1m 34s\tremaining: 5.63s\n",
      "944:\tlearn: 0.0828429\ttotal: 1m 34s\tremaining: 5.53s\n",
      "945:\tlearn: 0.0828020\ttotal: 1m 35s\tremaining: 5.43s\n",
      "946:\tlearn: 0.0827779\ttotal: 1m 35s\tremaining: 5.33s\n",
      "947:\tlearn: 0.0827367\ttotal: 1m 35s\tremaining: 5.23s\n",
      "948:\tlearn: 0.0827218\ttotal: 1m 35s\tremaining: 5.13s\n",
      "949:\tlearn: 0.0826648\ttotal: 1m 35s\tremaining: 5.03s\n",
      "950:\tlearn: 0.0825964\ttotal: 1m 35s\tremaining: 4.92s\n",
      "951:\tlearn: 0.0825402\ttotal: 1m 35s\tremaining: 4.83s\n",
      "952:\tlearn: 0.0825062\ttotal: 1m 35s\tremaining: 4.72s\n",
      "953:\tlearn: 0.0824808\ttotal: 1m 35s\tremaining: 4.62s\n",
      "954:\tlearn: 0.0824547\ttotal: 1m 36s\tremaining: 4.52s\n",
      "955:\tlearn: 0.0824123\ttotal: 1m 36s\tremaining: 4.42s\n",
      "956:\tlearn: 0.0823745\ttotal: 1m 36s\tremaining: 4.32s\n",
      "957:\tlearn: 0.0823275\ttotal: 1m 36s\tremaining: 4.22s\n",
      "958:\tlearn: 0.0823097\ttotal: 1m 36s\tremaining: 4.12s\n",
      "959:\tlearn: 0.0822568\ttotal: 1m 36s\tremaining: 4.02s\n",
      "960:\tlearn: 0.0822325\ttotal: 1m 36s\tremaining: 3.92s\n",
      "961:\tlearn: 0.0821962\ttotal: 1m 36s\tremaining: 3.82s\n",
      "962:\tlearn: 0.0821598\ttotal: 1m 36s\tremaining: 3.72s\n",
      "963:\tlearn: 0.0821322\ttotal: 1m 36s\tremaining: 3.62s\n",
      "964:\tlearn: 0.0820874\ttotal: 1m 37s\tremaining: 3.52s\n",
      "965:\tlearn: 0.0820173\ttotal: 1m 37s\tremaining: 3.42s\n",
      "966:\tlearn: 0.0819875\ttotal: 1m 37s\tremaining: 3.32s\n",
      "967:\tlearn: 0.0819329\ttotal: 1m 37s\tremaining: 3.22s\n",
      "968:\tlearn: 0.0818950\ttotal: 1m 37s\tremaining: 3.12s\n",
      "969:\tlearn: 0.0818568\ttotal: 1m 37s\tremaining: 3.02s\n",
      "970:\tlearn: 0.0818498\ttotal: 1m 37s\tremaining: 2.92s\n",
      "971:\tlearn: 0.0817927\ttotal: 1m 37s\tremaining: 2.82s\n",
      "972:\tlearn: 0.0817556\ttotal: 1m 37s\tremaining: 2.71s\n",
      "973:\tlearn: 0.0817349\ttotal: 1m 37s\tremaining: 2.62s\n",
      "974:\tlearn: 0.0817011\ttotal: 1m 38s\tremaining: 2.51s\n",
      "975:\tlearn: 0.0816690\ttotal: 1m 38s\tremaining: 2.41s\n",
      "976:\tlearn: 0.0816346\ttotal: 1m 38s\tremaining: 2.31s\n",
      "977:\tlearn: 0.0816118\ttotal: 1m 38s\tremaining: 2.21s\n",
      "978:\tlearn: 0.0815759\ttotal: 1m 38s\tremaining: 2.11s\n",
      "979:\tlearn: 0.0815432\ttotal: 1m 38s\tremaining: 2.01s\n",
      "980:\tlearn: 0.0815139\ttotal: 1m 38s\tremaining: 1.91s\n",
      "981:\tlearn: 0.0814732\ttotal: 1m 38s\tremaining: 1.81s\n",
      "982:\tlearn: 0.0814522\ttotal: 1m 38s\tremaining: 1.71s\n",
      "983:\tlearn: 0.0814096\ttotal: 1m 38s\tremaining: 1.61s\n",
      "984:\tlearn: 0.0813404\ttotal: 1m 39s\tremaining: 1.51s\n",
      "985:\tlearn: 0.0812773\ttotal: 1m 39s\tremaining: 1.41s\n",
      "986:\tlearn: 0.0812708\ttotal: 1m 39s\tremaining: 1.31s\n",
      "987:\tlearn: 0.0812477\ttotal: 1m 39s\tremaining: 1.21s\n",
      "988:\tlearn: 0.0811997\ttotal: 1m 39s\tremaining: 1.11s\n",
      "989:\tlearn: 0.0811564\ttotal: 1m 39s\tremaining: 1s\n",
      "990:\tlearn: 0.0811173\ttotal: 1m 39s\tremaining: 905ms\n",
      "991:\tlearn: 0.0810857\ttotal: 1m 39s\tremaining: 805ms\n",
      "992:\tlearn: 0.0810359\ttotal: 1m 39s\tremaining: 704ms\n",
      "993:\tlearn: 0.0810180\ttotal: 1m 39s\tremaining: 604ms\n",
      "994:\tlearn: 0.0809777\ttotal: 1m 40s\tremaining: 503ms\n",
      "995:\tlearn: 0.0809706\ttotal: 1m 40s\tremaining: 403ms\n",
      "996:\tlearn: 0.0809552\ttotal: 1m 40s\tremaining: 302ms\n",
      "997:\tlearn: 0.0809192\ttotal: 1m 40s\tremaining: 202ms\n",
      "998:\tlearn: 0.0808906\ttotal: 1m 40s\tremaining: 101ms\n",
      "999:\tlearn: 0.0808143\ttotal: 1m 40s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[116144,  15911],\n",
       "       [ 12994, 506852]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# On crée la matrice de confusion en passant les classes et les prédictions du modèle, puis on l'affiche\n",
    "y_train_pred_cat = cross_val_predict(catboost_pipeline, X_train, y_train)\n",
    "cm = confusion_matrix(y_train, y_train_pred_cat)\n",
    "display(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff197ee99f0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage avec un plot\n",
    "graph_cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Default\", \"Paid\"])\n",
    "graph_cm.plot(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5258156\ttotal: 94.7ms\tremaining: 1m 34s\n",
      "1:\tlearn: 0.4275702\ttotal: 184ms\tremaining: 1m 31s\n",
      "2:\tlearn: 0.3595619\ttotal: 288ms\tremaining: 1m 35s\n",
      "3:\tlearn: 0.3146526\ttotal: 380ms\tremaining: 1m 34s\n",
      "4:\tlearn: 0.2914315\ttotal: 465ms\tremaining: 1m 32s\n",
      "5:\tlearn: 0.2723684\ttotal: 566ms\tremaining: 1m 33s\n",
      "6:\tlearn: 0.2555552\ttotal: 661ms\tremaining: 1m 33s\n",
      "7:\tlearn: 0.2449120\ttotal: 753ms\tremaining: 1m 33s\n",
      "8:\tlearn: 0.2367658\ttotal: 845ms\tremaining: 1m 33s\n",
      "9:\tlearn: 0.2305051\ttotal: 940ms\tremaining: 1m 33s\n",
      "10:\tlearn: 0.2236396\ttotal: 1.03s\tremaining: 1m 32s\n",
      "11:\tlearn: 0.2149944\ttotal: 1.12s\tremaining: 1m 32s\n",
      "12:\tlearn: 0.2106634\ttotal: 1.22s\tremaining: 1m 32s\n",
      "13:\tlearn: 0.2070814\ttotal: 1.31s\tremaining: 1m 32s\n",
      "14:\tlearn: 0.2032672\ttotal: 1.41s\tremaining: 1m 32s\n",
      "15:\tlearn: 0.2011847\ttotal: 1.5s\tremaining: 1m 32s\n",
      "16:\tlearn: 0.1985610\ttotal: 1.6s\tremaining: 1m 32s\n",
      "17:\tlearn: 0.1964792\ttotal: 1.69s\tremaining: 1m 32s\n",
      "18:\tlearn: 0.1915291\ttotal: 1.78s\tremaining: 1m 32s\n",
      "19:\tlearn: 0.1889605\ttotal: 1.88s\tremaining: 1m 32s\n",
      "20:\tlearn: 0.1872897\ttotal: 1.97s\tremaining: 1m 31s\n",
      "21:\tlearn: 0.1856656\ttotal: 2.06s\tremaining: 1m 31s\n",
      "22:\tlearn: 0.1843362\ttotal: 2.16s\tremaining: 1m 31s\n",
      "23:\tlearn: 0.1832623\ttotal: 2.24s\tremaining: 1m 31s\n",
      "24:\tlearn: 0.1814054\ttotal: 2.33s\tremaining: 1m 31s\n",
      "25:\tlearn: 0.1800819\ttotal: 2.44s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1787554\ttotal: 2.53s\tremaining: 1m 31s\n",
      "27:\tlearn: 0.1776322\ttotal: 2.63s\tremaining: 1m 31s\n",
      "28:\tlearn: 0.1758824\ttotal: 2.72s\tremaining: 1m 31s\n",
      "29:\tlearn: 0.1746993\ttotal: 2.82s\tremaining: 1m 31s\n",
      "30:\tlearn: 0.1738582\ttotal: 2.91s\tremaining: 1m 31s\n",
      "31:\tlearn: 0.1726512\ttotal: 3.01s\tremaining: 1m 31s\n",
      "32:\tlearn: 0.1717344\ttotal: 3.11s\tremaining: 1m 31s\n",
      "33:\tlearn: 0.1711089\ttotal: 3.2s\tremaining: 1m 30s\n",
      "34:\tlearn: 0.1700743\ttotal: 3.29s\tremaining: 1m 30s\n",
      "35:\tlearn: 0.1693322\ttotal: 3.39s\tremaining: 1m 30s\n",
      "36:\tlearn: 0.1680947\ttotal: 3.48s\tremaining: 1m 30s\n",
      "37:\tlearn: 0.1673574\ttotal: 3.58s\tremaining: 1m 30s\n",
      "38:\tlearn: 0.1663959\ttotal: 3.67s\tremaining: 1m 30s\n",
      "39:\tlearn: 0.1654508\ttotal: 3.76s\tremaining: 1m 30s\n",
      "40:\tlearn: 0.1647577\ttotal: 3.86s\tremaining: 1m 30s\n",
      "41:\tlearn: 0.1640215\ttotal: 3.97s\tremaining: 1m 30s\n",
      "42:\tlearn: 0.1634221\ttotal: 4.06s\tremaining: 1m 30s\n",
      "43:\tlearn: 0.1627145\ttotal: 4.2s\tremaining: 1m 31s\n",
      "44:\tlearn: 0.1619436\ttotal: 4.29s\tremaining: 1m 31s\n",
      "45:\tlearn: 0.1610363\ttotal: 4.39s\tremaining: 1m 31s\n",
      "46:\tlearn: 0.1602128\ttotal: 4.5s\tremaining: 1m 31s\n",
      "47:\tlearn: 0.1594734\ttotal: 4.59s\tremaining: 1m 31s\n",
      "48:\tlearn: 0.1588850\ttotal: 4.68s\tremaining: 1m 30s\n",
      "49:\tlearn: 0.1583758\ttotal: 4.79s\tremaining: 1m 30s\n",
      "50:\tlearn: 0.1576319\ttotal: 4.88s\tremaining: 1m 30s\n",
      "51:\tlearn: 0.1572795\ttotal: 4.97s\tremaining: 1m 30s\n",
      "52:\tlearn: 0.1568771\ttotal: 5.07s\tremaining: 1m 30s\n",
      "53:\tlearn: 0.1563854\ttotal: 5.17s\tremaining: 1m 30s\n",
      "54:\tlearn: 0.1556445\ttotal: 5.26s\tremaining: 1m 30s\n",
      "55:\tlearn: 0.1550236\ttotal: 5.36s\tremaining: 1m 30s\n",
      "56:\tlearn: 0.1545078\ttotal: 5.45s\tremaining: 1m 30s\n",
      "57:\tlearn: 0.1541929\ttotal: 5.54s\tremaining: 1m 30s\n",
      "58:\tlearn: 0.1537567\ttotal: 5.64s\tremaining: 1m 30s\n",
      "59:\tlearn: 0.1531768\ttotal: 5.74s\tremaining: 1m 29s\n",
      "60:\tlearn: 0.1526770\ttotal: 5.83s\tremaining: 1m 29s\n",
      "61:\tlearn: 0.1523053\ttotal: 5.93s\tremaining: 1m 29s\n",
      "62:\tlearn: 0.1519712\ttotal: 6.02s\tremaining: 1m 29s\n",
      "63:\tlearn: 0.1514593\ttotal: 6.12s\tremaining: 1m 29s\n",
      "64:\tlearn: 0.1508909\ttotal: 6.21s\tremaining: 1m 29s\n",
      "65:\tlearn: 0.1505376\ttotal: 6.31s\tremaining: 1m 29s\n",
      "66:\tlearn: 0.1502094\ttotal: 6.41s\tremaining: 1m 29s\n",
      "67:\tlearn: 0.1497804\ttotal: 6.5s\tremaining: 1m 29s\n",
      "68:\tlearn: 0.1494810\ttotal: 6.59s\tremaining: 1m 28s\n",
      "69:\tlearn: 0.1490864\ttotal: 6.69s\tremaining: 1m 28s\n",
      "70:\tlearn: 0.1487769\ttotal: 6.79s\tremaining: 1m 28s\n",
      "71:\tlearn: 0.1485032\ttotal: 6.88s\tremaining: 1m 28s\n",
      "72:\tlearn: 0.1481410\ttotal: 6.97s\tremaining: 1m 28s\n",
      "73:\tlearn: 0.1478073\ttotal: 7.07s\tremaining: 1m 28s\n",
      "74:\tlearn: 0.1475518\ttotal: 7.17s\tremaining: 1m 28s\n",
      "75:\tlearn: 0.1472674\ttotal: 7.26s\tremaining: 1m 28s\n",
      "76:\tlearn: 0.1465009\ttotal: 7.35s\tremaining: 1m 28s\n",
      "77:\tlearn: 0.1461867\ttotal: 7.44s\tremaining: 1m 27s\n",
      "78:\tlearn: 0.1459362\ttotal: 7.53s\tremaining: 1m 27s\n",
      "79:\tlearn: 0.1455664\ttotal: 7.62s\tremaining: 1m 27s\n",
      "80:\tlearn: 0.1453026\ttotal: 7.72s\tremaining: 1m 27s\n",
      "81:\tlearn: 0.1448449\ttotal: 7.82s\tremaining: 1m 27s\n",
      "82:\tlearn: 0.1446525\ttotal: 7.91s\tremaining: 1m 27s\n",
      "83:\tlearn: 0.1444388\ttotal: 8.01s\tremaining: 1m 27s\n",
      "84:\tlearn: 0.1442546\ttotal: 8.1s\tremaining: 1m 27s\n",
      "85:\tlearn: 0.1439770\ttotal: 8.2s\tremaining: 1m 27s\n",
      "86:\tlearn: 0.1436316\ttotal: 8.29s\tremaining: 1m 26s\n",
      "87:\tlearn: 0.1433885\ttotal: 8.38s\tremaining: 1m 26s\n",
      "88:\tlearn: 0.1431889\ttotal: 8.48s\tremaining: 1m 26s\n",
      "89:\tlearn: 0.1428542\ttotal: 8.57s\tremaining: 1m 26s\n",
      "90:\tlearn: 0.1424933\ttotal: 8.66s\tremaining: 1m 26s\n",
      "91:\tlearn: 0.1422926\ttotal: 8.76s\tremaining: 1m 26s\n",
      "92:\tlearn: 0.1420365\ttotal: 8.85s\tremaining: 1m 26s\n",
      "93:\tlearn: 0.1418552\ttotal: 8.94s\tremaining: 1m 26s\n",
      "94:\tlearn: 0.1415318\ttotal: 9.04s\tremaining: 1m 26s\n",
      "95:\tlearn: 0.1411421\ttotal: 9.13s\tremaining: 1m 25s\n",
      "96:\tlearn: 0.1408045\ttotal: 9.22s\tremaining: 1m 25s\n",
      "97:\tlearn: 0.1405540\ttotal: 9.32s\tremaining: 1m 25s\n",
      "98:\tlearn: 0.1402427\ttotal: 9.41s\tremaining: 1m 25s\n",
      "99:\tlearn: 0.1398887\ttotal: 9.51s\tremaining: 1m 25s\n",
      "100:\tlearn: 0.1397203\ttotal: 9.61s\tremaining: 1m 25s\n",
      "101:\tlearn: 0.1394975\ttotal: 9.7s\tremaining: 1m 25s\n",
      "102:\tlearn: 0.1392873\ttotal: 9.79s\tremaining: 1m 25s\n",
      "103:\tlearn: 0.1389902\ttotal: 9.89s\tremaining: 1m 25s\n",
      "104:\tlearn: 0.1387788\ttotal: 9.98s\tremaining: 1m 25s\n",
      "105:\tlearn: 0.1384857\ttotal: 10.1s\tremaining: 1m 24s\n",
      "106:\tlearn: 0.1382386\ttotal: 10.2s\tremaining: 1m 24s\n",
      "107:\tlearn: 0.1380589\ttotal: 10.3s\tremaining: 1m 24s\n",
      "108:\tlearn: 0.1378432\ttotal: 10.4s\tremaining: 1m 24s\n",
      "109:\tlearn: 0.1376637\ttotal: 10.5s\tremaining: 1m 24s\n",
      "110:\tlearn: 0.1374519\ttotal: 10.6s\tremaining: 1m 24s\n",
      "111:\tlearn: 0.1372601\ttotal: 10.7s\tremaining: 1m 24s\n",
      "112:\tlearn: 0.1370462\ttotal: 10.8s\tremaining: 1m 24s\n",
      "113:\tlearn: 0.1368055\ttotal: 10.9s\tremaining: 1m 24s\n",
      "114:\tlearn: 0.1366886\ttotal: 11s\tremaining: 1m 24s\n",
      "115:\tlearn: 0.1365339\ttotal: 11.1s\tremaining: 1m 24s\n",
      "116:\tlearn: 0.1361956\ttotal: 11.2s\tremaining: 1m 24s\n",
      "117:\tlearn: 0.1360213\ttotal: 11.3s\tremaining: 1m 24s\n",
      "118:\tlearn: 0.1357781\ttotal: 11.5s\tremaining: 1m 24s\n",
      "119:\tlearn: 0.1355307\ttotal: 11.6s\tremaining: 1m 24s\n",
      "120:\tlearn: 0.1353288\ttotal: 11.7s\tremaining: 1m 24s\n",
      "121:\tlearn: 0.1351510\ttotal: 11.8s\tremaining: 1m 24s\n",
      "122:\tlearn: 0.1350057\ttotal: 11.9s\tremaining: 1m 24s\n",
      "123:\tlearn: 0.1347995\ttotal: 12s\tremaining: 1m 24s\n",
      "124:\tlearn: 0.1345595\ttotal: 12.1s\tremaining: 1m 24s\n",
      "125:\tlearn: 0.1344063\ttotal: 12.2s\tremaining: 1m 24s\n",
      "126:\tlearn: 0.1342242\ttotal: 12.3s\tremaining: 1m 24s\n",
      "127:\tlearn: 0.1340336\ttotal: 12.4s\tremaining: 1m 24s\n",
      "128:\tlearn: 0.1337690\ttotal: 12.5s\tremaining: 1m 24s\n",
      "129:\tlearn: 0.1335338\ttotal: 12.6s\tremaining: 1m 24s\n",
      "130:\tlearn: 0.1333892\ttotal: 12.7s\tremaining: 1m 24s\n",
      "131:\tlearn: 0.1332723\ttotal: 12.8s\tremaining: 1m 24s\n",
      "132:\tlearn: 0.1331502\ttotal: 12.9s\tremaining: 1m 23s\n",
      "133:\tlearn: 0.1330121\ttotal: 13s\tremaining: 1m 23s\n",
      "134:\tlearn: 0.1329061\ttotal: 13.1s\tremaining: 1m 23s\n",
      "135:\tlearn: 0.1327149\ttotal: 13.2s\tremaining: 1m 23s\n",
      "136:\tlearn: 0.1325452\ttotal: 13.3s\tremaining: 1m 23s\n",
      "137:\tlearn: 0.1323956\ttotal: 13.4s\tremaining: 1m 23s\n",
      "138:\tlearn: 0.1322082\ttotal: 13.4s\tremaining: 1m 23s\n",
      "139:\tlearn: 0.1320507\ttotal: 13.5s\tremaining: 1m 23s\n",
      "140:\tlearn: 0.1319117\ttotal: 13.6s\tremaining: 1m 23s\n",
      "141:\tlearn: 0.1317541\ttotal: 13.7s\tremaining: 1m 22s\n",
      "142:\tlearn: 0.1315899\ttotal: 13.8s\tremaining: 1m 22s\n",
      "143:\tlearn: 0.1313998\ttotal: 13.9s\tremaining: 1m 22s\n",
      "144:\tlearn: 0.1312715\ttotal: 14s\tremaining: 1m 22s\n",
      "145:\tlearn: 0.1311112\ttotal: 14.1s\tremaining: 1m 22s\n",
      "146:\tlearn: 0.1308869\ttotal: 14.2s\tremaining: 1m 22s\n",
      "147:\tlearn: 0.1306621\ttotal: 14.3s\tremaining: 1m 22s\n",
      "148:\tlearn: 0.1305288\ttotal: 14.4s\tremaining: 1m 22s\n",
      "149:\tlearn: 0.1304038\ttotal: 14.5s\tremaining: 1m 22s\n",
      "150:\tlearn: 0.1302783\ttotal: 14.7s\tremaining: 1m 22s\n",
      "151:\tlearn: 0.1301604\ttotal: 14.8s\tremaining: 1m 22s\n",
      "152:\tlearn: 0.1300270\ttotal: 14.9s\tremaining: 1m 22s\n",
      "153:\tlearn: 0.1299273\ttotal: 15s\tremaining: 1m 22s\n",
      "154:\tlearn: 0.1298137\ttotal: 15.1s\tremaining: 1m 22s\n",
      "155:\tlearn: 0.1297211\ttotal: 15.2s\tremaining: 1m 22s\n",
      "156:\tlearn: 0.1296072\ttotal: 15.3s\tremaining: 1m 22s\n",
      "157:\tlearn: 0.1295105\ttotal: 15.4s\tremaining: 1m 22s\n",
      "158:\tlearn: 0.1294048\ttotal: 15.5s\tremaining: 1m 22s\n",
      "159:\tlearn: 0.1292461\ttotal: 15.7s\tremaining: 1m 22s\n",
      "160:\tlearn: 0.1291276\ttotal: 15.8s\tremaining: 1m 22s\n",
      "161:\tlearn: 0.1289962\ttotal: 15.9s\tremaining: 1m 22s\n",
      "162:\tlearn: 0.1288591\ttotal: 16s\tremaining: 1m 22s\n",
      "163:\tlearn: 0.1287675\ttotal: 16.1s\tremaining: 1m 22s\n",
      "164:\tlearn: 0.1285956\ttotal: 16.2s\tremaining: 1m 22s\n",
      "165:\tlearn: 0.1284192\ttotal: 16.3s\tremaining: 1m 22s\n",
      "166:\tlearn: 0.1283228\ttotal: 16.4s\tremaining: 1m 21s\n",
      "167:\tlearn: 0.1281271\ttotal: 16.5s\tremaining: 1m 21s\n",
      "168:\tlearn: 0.1279557\ttotal: 16.6s\tremaining: 1m 21s\n",
      "169:\tlearn: 0.1278561\ttotal: 16.7s\tremaining: 1m 21s\n",
      "170:\tlearn: 0.1276495\ttotal: 16.8s\tremaining: 1m 21s\n",
      "171:\tlearn: 0.1275104\ttotal: 16.9s\tremaining: 1m 21s\n",
      "172:\tlearn: 0.1273983\ttotal: 17s\tremaining: 1m 21s\n",
      "173:\tlearn: 0.1272570\ttotal: 17.1s\tremaining: 1m 21s\n",
      "174:\tlearn: 0.1271269\ttotal: 17.2s\tremaining: 1m 21s\n",
      "175:\tlearn: 0.1270277\ttotal: 17.3s\tremaining: 1m 21s\n",
      "176:\tlearn: 0.1269040\ttotal: 17.4s\tremaining: 1m 20s\n",
      "177:\tlearn: 0.1267990\ttotal: 17.5s\tremaining: 1m 20s\n",
      "178:\tlearn: 0.1267441\ttotal: 17.6s\tremaining: 1m 20s\n",
      "179:\tlearn: 0.1265583\ttotal: 17.7s\tremaining: 1m 20s\n",
      "180:\tlearn: 0.1263702\ttotal: 17.8s\tremaining: 1m 20s\n",
      "181:\tlearn: 0.1262566\ttotal: 17.9s\tremaining: 1m 20s\n",
      "182:\tlearn: 0.1260798\ttotal: 18s\tremaining: 1m 20s\n",
      "183:\tlearn: 0.1259582\ttotal: 18.1s\tremaining: 1m 20s\n",
      "184:\tlearn: 0.1258243\ttotal: 18.2s\tremaining: 1m 20s\n",
      "185:\tlearn: 0.1257476\ttotal: 18.3s\tremaining: 1m 20s\n",
      "186:\tlearn: 0.1256174\ttotal: 18.4s\tremaining: 1m 20s\n",
      "187:\tlearn: 0.1254876\ttotal: 18.5s\tremaining: 1m 19s\n",
      "188:\tlearn: 0.1254090\ttotal: 18.6s\tremaining: 1m 19s\n",
      "189:\tlearn: 0.1253168\ttotal: 18.7s\tremaining: 1m 19s\n",
      "190:\tlearn: 0.1252451\ttotal: 18.8s\tremaining: 1m 19s\n",
      "191:\tlearn: 0.1251441\ttotal: 18.9s\tremaining: 1m 19s\n",
      "192:\tlearn: 0.1250396\ttotal: 19s\tremaining: 1m 19s\n",
      "193:\tlearn: 0.1249147\ttotal: 19.1s\tremaining: 1m 19s\n",
      "194:\tlearn: 0.1247739\ttotal: 19.1s\tremaining: 1m 19s\n",
      "195:\tlearn: 0.1245966\ttotal: 19.2s\tremaining: 1m 18s\n",
      "196:\tlearn: 0.1245060\ttotal: 19.3s\tremaining: 1m 18s\n",
      "197:\tlearn: 0.1243858\ttotal: 19.5s\tremaining: 1m 18s\n",
      "198:\tlearn: 0.1242927\ttotal: 19.6s\tremaining: 1m 18s\n",
      "199:\tlearn: 0.1241966\ttotal: 19.7s\tremaining: 1m 18s\n",
      "200:\tlearn: 0.1241192\ttotal: 19.8s\tremaining: 1m 18s\n",
      "201:\tlearn: 0.1239571\ttotal: 19.9s\tremaining: 1m 18s\n",
      "202:\tlearn: 0.1238523\ttotal: 19.9s\tremaining: 1m 18s\n",
      "203:\tlearn: 0.1237112\ttotal: 20s\tremaining: 1m 18s\n",
      "204:\tlearn: 0.1235913\ttotal: 20.1s\tremaining: 1m 18s\n",
      "205:\tlearn: 0.1235183\ttotal: 20.2s\tremaining: 1m 18s\n",
      "206:\tlearn: 0.1233837\ttotal: 20.4s\tremaining: 1m 18s\n",
      "207:\tlearn: 0.1233208\ttotal: 20.5s\tremaining: 1m 18s\n",
      "208:\tlearn: 0.1232357\ttotal: 20.6s\tremaining: 1m 18s\n",
      "209:\tlearn: 0.1231225\ttotal: 20.7s\tremaining: 1m 17s\n",
      "210:\tlearn: 0.1230434\ttotal: 20.8s\tremaining: 1m 17s\n",
      "211:\tlearn: 0.1229484\ttotal: 20.9s\tremaining: 1m 17s\n",
      "212:\tlearn: 0.1228616\ttotal: 21s\tremaining: 1m 17s\n",
      "213:\tlearn: 0.1227769\ttotal: 21.1s\tremaining: 1m 17s\n",
      "214:\tlearn: 0.1226802\ttotal: 21.2s\tremaining: 1m 17s\n",
      "215:\tlearn: 0.1226135\ttotal: 21.3s\tremaining: 1m 17s\n",
      "216:\tlearn: 0.1225393\ttotal: 21.4s\tremaining: 1m 17s\n",
      "217:\tlearn: 0.1223897\ttotal: 21.5s\tremaining: 1m 17s\n",
      "218:\tlearn: 0.1222908\ttotal: 21.6s\tremaining: 1m 17s\n",
      "219:\tlearn: 0.1221805\ttotal: 21.7s\tremaining: 1m 16s\n",
      "220:\tlearn: 0.1220267\ttotal: 21.8s\tremaining: 1m 16s\n",
      "221:\tlearn: 0.1219132\ttotal: 21.9s\tremaining: 1m 16s\n",
      "222:\tlearn: 0.1218110\ttotal: 22s\tremaining: 1m 16s\n",
      "223:\tlearn: 0.1217200\ttotal: 22.1s\tremaining: 1m 16s\n",
      "224:\tlearn: 0.1215757\ttotal: 22.2s\tremaining: 1m 16s\n",
      "225:\tlearn: 0.1215034\ttotal: 22.3s\tremaining: 1m 16s\n",
      "226:\tlearn: 0.1213970\ttotal: 22.4s\tremaining: 1m 16s\n",
      "227:\tlearn: 0.1212845\ttotal: 22.5s\tremaining: 1m 16s\n",
      "228:\tlearn: 0.1211862\ttotal: 22.6s\tremaining: 1m 15s\n",
      "229:\tlearn: 0.1210999\ttotal: 22.7s\tremaining: 1m 15s\n",
      "230:\tlearn: 0.1210223\ttotal: 22.8s\tremaining: 1m 15s\n",
      "231:\tlearn: 0.1209428\ttotal: 22.9s\tremaining: 1m 15s\n",
      "232:\tlearn: 0.1208081\ttotal: 23s\tremaining: 1m 15s\n",
      "233:\tlearn: 0.1206547\ttotal: 23.1s\tremaining: 1m 15s\n",
      "234:\tlearn: 0.1205656\ttotal: 23.2s\tremaining: 1m 15s\n",
      "235:\tlearn: 0.1204955\ttotal: 23.2s\tremaining: 1m 15s\n",
      "236:\tlearn: 0.1203941\ttotal: 23.3s\tremaining: 1m 15s\n",
      "237:\tlearn: 0.1202786\ttotal: 23.4s\tremaining: 1m 15s\n",
      "238:\tlearn: 0.1201887\ttotal: 23.5s\tremaining: 1m 14s\n",
      "239:\tlearn: 0.1200797\ttotal: 23.6s\tremaining: 1m 14s\n",
      "240:\tlearn: 0.1200221\ttotal: 23.7s\tremaining: 1m 14s\n",
      "241:\tlearn: 0.1199261\ttotal: 23.8s\tremaining: 1m 14s\n",
      "242:\tlearn: 0.1198521\ttotal: 23.9s\tremaining: 1m 14s\n",
      "243:\tlearn: 0.1196802\ttotal: 24s\tremaining: 1m 14s\n",
      "244:\tlearn: 0.1196184\ttotal: 24.1s\tremaining: 1m 14s\n",
      "245:\tlearn: 0.1195320\ttotal: 24.2s\tremaining: 1m 14s\n",
      "246:\tlearn: 0.1194098\ttotal: 24.3s\tremaining: 1m 14s\n",
      "247:\tlearn: 0.1193075\ttotal: 24.4s\tremaining: 1m 14s\n",
      "248:\tlearn: 0.1192447\ttotal: 24.5s\tremaining: 1m 14s\n",
      "249:\tlearn: 0.1191508\ttotal: 24.6s\tremaining: 1m 13s\n",
      "250:\tlearn: 0.1190514\ttotal: 24.8s\tremaining: 1m 13s\n",
      "251:\tlearn: 0.1189760\ttotal: 24.9s\tremaining: 1m 13s\n",
      "252:\tlearn: 0.1187875\ttotal: 25s\tremaining: 1m 13s\n",
      "253:\tlearn: 0.1186761\ttotal: 25.1s\tremaining: 1m 13s\n",
      "254:\tlearn: 0.1186180\ttotal: 25.2s\tremaining: 1m 13s\n",
      "255:\tlearn: 0.1184723\ttotal: 25.3s\tremaining: 1m 13s\n",
      "256:\tlearn: 0.1183526\ttotal: 25.4s\tremaining: 1m 13s\n",
      "257:\tlearn: 0.1182457\ttotal: 25.5s\tremaining: 1m 13s\n",
      "258:\tlearn: 0.1181412\ttotal: 25.6s\tremaining: 1m 13s\n",
      "259:\tlearn: 0.1180766\ttotal: 25.7s\tremaining: 1m 13s\n",
      "260:\tlearn: 0.1179667\ttotal: 25.8s\tremaining: 1m 13s\n",
      "261:\tlearn: 0.1178890\ttotal: 25.9s\tremaining: 1m 12s\n",
      "262:\tlearn: 0.1177935\ttotal: 26s\tremaining: 1m 12s\n",
      "263:\tlearn: 0.1177041\ttotal: 26.1s\tremaining: 1m 12s\n",
      "264:\tlearn: 0.1176498\ttotal: 26.2s\tremaining: 1m 12s\n",
      "265:\tlearn: 0.1175866\ttotal: 26.3s\tremaining: 1m 12s\n",
      "266:\tlearn: 0.1175166\ttotal: 26.4s\tremaining: 1m 12s\n",
      "267:\tlearn: 0.1173913\ttotal: 26.5s\tremaining: 1m 12s\n",
      "268:\tlearn: 0.1173162\ttotal: 26.6s\tremaining: 1m 12s\n",
      "269:\tlearn: 0.1172632\ttotal: 26.7s\tremaining: 1m 12s\n",
      "270:\tlearn: 0.1171676\ttotal: 26.8s\tremaining: 1m 12s\n",
      "271:\tlearn: 0.1170599\ttotal: 26.9s\tremaining: 1m 11s\n",
      "272:\tlearn: 0.1169683\ttotal: 27s\tremaining: 1m 11s\n",
      "273:\tlearn: 0.1169000\ttotal: 27.2s\tremaining: 1m 11s\n",
      "274:\tlearn: 0.1168447\ttotal: 27.3s\tremaining: 1m 11s\n",
      "275:\tlearn: 0.1167840\ttotal: 27.4s\tremaining: 1m 11s\n",
      "276:\tlearn: 0.1166810\ttotal: 27.5s\tremaining: 1m 11s\n",
      "277:\tlearn: 0.1165916\ttotal: 27.6s\tremaining: 1m 11s\n",
      "278:\tlearn: 0.1165465\ttotal: 27.7s\tremaining: 1m 11s\n",
      "279:\tlearn: 0.1164246\ttotal: 27.8s\tremaining: 1m 11s\n",
      "280:\tlearn: 0.1163223\ttotal: 27.9s\tremaining: 1m 11s\n",
      "281:\tlearn: 0.1162536\ttotal: 28s\tremaining: 1m 11s\n",
      "282:\tlearn: 0.1161693\ttotal: 28.1s\tremaining: 1m 11s\n",
      "283:\tlearn: 0.1160696\ttotal: 28.2s\tremaining: 1m 11s\n",
      "284:\tlearn: 0.1159481\ttotal: 28.3s\tremaining: 1m 10s\n",
      "285:\tlearn: 0.1158446\ttotal: 28.4s\tremaining: 1m 10s\n",
      "286:\tlearn: 0.1158064\ttotal: 28.5s\tremaining: 1m 10s\n",
      "287:\tlearn: 0.1157317\ttotal: 28.6s\tremaining: 1m 10s\n",
      "288:\tlearn: 0.1156252\ttotal: 28.7s\tremaining: 1m 10s\n",
      "289:\tlearn: 0.1155437\ttotal: 28.8s\tremaining: 1m 10s\n",
      "290:\tlearn: 0.1154923\ttotal: 28.9s\tremaining: 1m 10s\n",
      "291:\tlearn: 0.1154226\ttotal: 29s\tremaining: 1m 10s\n",
      "292:\tlearn: 0.1153639\ttotal: 29.1s\tremaining: 1m 10s\n",
      "293:\tlearn: 0.1152655\ttotal: 29.2s\tremaining: 1m 10s\n",
      "294:\tlearn: 0.1152095\ttotal: 29.3s\tremaining: 1m 9s\n",
      "295:\tlearn: 0.1151149\ttotal: 29.4s\tremaining: 1m 9s\n",
      "296:\tlearn: 0.1150489\ttotal: 29.5s\tremaining: 1m 9s\n",
      "297:\tlearn: 0.1149891\ttotal: 29.6s\tremaining: 1m 9s\n",
      "298:\tlearn: 0.1148956\ttotal: 29.7s\tremaining: 1m 9s\n",
      "299:\tlearn: 0.1148263\ttotal: 29.8s\tremaining: 1m 9s\n",
      "300:\tlearn: 0.1147254\ttotal: 29.9s\tremaining: 1m 9s\n",
      "301:\tlearn: 0.1146407\ttotal: 30s\tremaining: 1m 9s\n",
      "302:\tlearn: 0.1145758\ttotal: 30.1s\tremaining: 1m 9s\n",
      "303:\tlearn: 0.1145075\ttotal: 30.2s\tremaining: 1m 9s\n",
      "304:\tlearn: 0.1144093\ttotal: 30.3s\tremaining: 1m 9s\n",
      "305:\tlearn: 0.1142887\ttotal: 30.4s\tremaining: 1m 8s\n",
      "306:\tlearn: 0.1141871\ttotal: 30.5s\tremaining: 1m 8s\n",
      "307:\tlearn: 0.1141307\ttotal: 30.6s\tremaining: 1m 8s\n",
      "308:\tlearn: 0.1140627\ttotal: 30.7s\tremaining: 1m 8s\n",
      "309:\tlearn: 0.1140052\ttotal: 30.8s\tremaining: 1m 8s\n",
      "310:\tlearn: 0.1139314\ttotal: 30.9s\tremaining: 1m 8s\n",
      "311:\tlearn: 0.1138762\ttotal: 31s\tremaining: 1m 8s\n",
      "312:\tlearn: 0.1138072\ttotal: 31.1s\tremaining: 1m 8s\n",
      "313:\tlearn: 0.1137323\ttotal: 31.2s\tremaining: 1m 8s\n",
      "314:\tlearn: 0.1136684\ttotal: 31.4s\tremaining: 1m 8s\n",
      "315:\tlearn: 0.1136112\ttotal: 31.5s\tremaining: 1m 8s\n",
      "316:\tlearn: 0.1135566\ttotal: 31.6s\tremaining: 1m 8s\n",
      "317:\tlearn: 0.1135028\ttotal: 31.7s\tremaining: 1m 7s\n",
      "318:\tlearn: 0.1134374\ttotal: 31.8s\tremaining: 1m 7s\n",
      "319:\tlearn: 0.1133728\ttotal: 31.9s\tremaining: 1m 7s\n",
      "320:\tlearn: 0.1132364\ttotal: 32.1s\tremaining: 1m 7s\n",
      "321:\tlearn: 0.1131448\ttotal: 32.2s\tremaining: 1m 7s\n",
      "322:\tlearn: 0.1130902\ttotal: 32.3s\tremaining: 1m 7s\n",
      "323:\tlearn: 0.1129988\ttotal: 32.4s\tremaining: 1m 7s\n",
      "324:\tlearn: 0.1129015\ttotal: 32.5s\tremaining: 1m 7s\n",
      "325:\tlearn: 0.1128635\ttotal: 32.6s\tremaining: 1m 7s\n",
      "326:\tlearn: 0.1128169\ttotal: 32.7s\tremaining: 1m 7s\n",
      "327:\tlearn: 0.1127460\ttotal: 32.8s\tremaining: 1m 7s\n",
      "328:\tlearn: 0.1126309\ttotal: 32.9s\tremaining: 1m 7s\n",
      "329:\tlearn: 0.1125885\ttotal: 33s\tremaining: 1m 7s\n",
      "330:\tlearn: 0.1125097\ttotal: 33.1s\tremaining: 1m 6s\n",
      "331:\tlearn: 0.1124096\ttotal: 33.2s\tremaining: 1m 6s\n",
      "332:\tlearn: 0.1123089\ttotal: 33.3s\tremaining: 1m 6s\n",
      "333:\tlearn: 0.1122214\ttotal: 33.4s\tremaining: 1m 6s\n",
      "334:\tlearn: 0.1121619\ttotal: 33.5s\tremaining: 1m 6s\n",
      "335:\tlearn: 0.1120752\ttotal: 33.6s\tremaining: 1m 6s\n",
      "336:\tlearn: 0.1119766\ttotal: 33.7s\tremaining: 1m 6s\n",
      "337:\tlearn: 0.1119360\ttotal: 33.8s\tremaining: 1m 6s\n",
      "338:\tlearn: 0.1118691\ttotal: 33.9s\tremaining: 1m 6s\n",
      "339:\tlearn: 0.1117882\ttotal: 34s\tremaining: 1m 5s\n",
      "340:\tlearn: 0.1117318\ttotal: 34.1s\tremaining: 1m 5s\n",
      "341:\tlearn: 0.1116664\ttotal: 34.2s\tremaining: 1m 5s\n",
      "342:\tlearn: 0.1116071\ttotal: 34.3s\tremaining: 1m 5s\n",
      "343:\tlearn: 0.1115561\ttotal: 34.4s\tremaining: 1m 5s\n",
      "344:\tlearn: 0.1114846\ttotal: 34.5s\tremaining: 1m 5s\n",
      "345:\tlearn: 0.1113984\ttotal: 34.6s\tremaining: 1m 5s\n",
      "346:\tlearn: 0.1113329\ttotal: 34.7s\tremaining: 1m 5s\n",
      "347:\tlearn: 0.1112619\ttotal: 34.9s\tremaining: 1m 5s\n",
      "348:\tlearn: 0.1112077\ttotal: 35s\tremaining: 1m 5s\n",
      "349:\tlearn: 0.1111519\ttotal: 35.1s\tremaining: 1m 5s\n",
      "350:\tlearn: 0.1111156\ttotal: 35.2s\tremaining: 1m 5s\n",
      "351:\tlearn: 0.1110531\ttotal: 35.3s\tremaining: 1m 4s\n",
      "352:\tlearn: 0.1109741\ttotal: 35.4s\tremaining: 1m 4s\n",
      "353:\tlearn: 0.1109330\ttotal: 35.5s\tremaining: 1m 4s\n",
      "354:\tlearn: 0.1108743\ttotal: 35.6s\tremaining: 1m 4s\n",
      "355:\tlearn: 0.1108165\ttotal: 35.7s\tremaining: 1m 4s\n",
      "356:\tlearn: 0.1107436\ttotal: 35.8s\tremaining: 1m 4s\n",
      "357:\tlearn: 0.1106966\ttotal: 35.9s\tremaining: 1m 4s\n",
      "358:\tlearn: 0.1106627\ttotal: 36s\tremaining: 1m 4s\n",
      "359:\tlearn: 0.1105864\ttotal: 36.1s\tremaining: 1m 4s\n",
      "360:\tlearn: 0.1105304\ttotal: 36.2s\tremaining: 1m 4s\n",
      "361:\tlearn: 0.1104290\ttotal: 36.3s\tremaining: 1m 4s\n",
      "362:\tlearn: 0.1103410\ttotal: 36.4s\tremaining: 1m 3s\n",
      "363:\tlearn: 0.1102466\ttotal: 36.5s\tremaining: 1m 3s\n",
      "364:\tlearn: 0.1102104\ttotal: 36.6s\tremaining: 1m 3s\n",
      "365:\tlearn: 0.1101551\ttotal: 36.7s\tremaining: 1m 3s\n",
      "366:\tlearn: 0.1101030\ttotal: 36.8s\tremaining: 1m 3s\n",
      "367:\tlearn: 0.1100458\ttotal: 36.9s\tremaining: 1m 3s\n",
      "368:\tlearn: 0.1099845\ttotal: 37s\tremaining: 1m 3s\n",
      "369:\tlearn: 0.1099090\ttotal: 37.1s\tremaining: 1m 3s\n",
      "370:\tlearn: 0.1098376\ttotal: 37.2s\tremaining: 1m 3s\n",
      "371:\tlearn: 0.1097221\ttotal: 37.3s\tremaining: 1m 2s\n",
      "372:\tlearn: 0.1096605\ttotal: 37.4s\tremaining: 1m 2s\n",
      "373:\tlearn: 0.1095837\ttotal: 37.5s\tremaining: 1m 2s\n",
      "374:\tlearn: 0.1095105\ttotal: 37.6s\tremaining: 1m 2s\n",
      "375:\tlearn: 0.1094312\ttotal: 37.7s\tremaining: 1m 2s\n",
      "376:\tlearn: 0.1093434\ttotal: 37.8s\tremaining: 1m 2s\n",
      "377:\tlearn: 0.1093040\ttotal: 37.8s\tremaining: 1m 2s\n",
      "378:\tlearn: 0.1092519\ttotal: 37.9s\tremaining: 1m 2s\n",
      "379:\tlearn: 0.1091940\ttotal: 38s\tremaining: 1m 2s\n",
      "380:\tlearn: 0.1091011\ttotal: 38.1s\tremaining: 1m 1s\n",
      "381:\tlearn: 0.1090519\ttotal: 38.3s\tremaining: 1m 2s\n",
      "382:\tlearn: 0.1089955\ttotal: 38.4s\tremaining: 1m 1s\n",
      "383:\tlearn: 0.1089547\ttotal: 38.5s\tremaining: 1m 1s\n",
      "384:\tlearn: 0.1089092\ttotal: 38.6s\tremaining: 1m 1s\n",
      "385:\tlearn: 0.1088552\ttotal: 38.7s\tremaining: 1m 1s\n",
      "386:\tlearn: 0.1088072\ttotal: 38.8s\tremaining: 1m 1s\n",
      "387:\tlearn: 0.1087356\ttotal: 38.9s\tremaining: 1m 1s\n",
      "388:\tlearn: 0.1086892\ttotal: 39s\tremaining: 1m 1s\n",
      "389:\tlearn: 0.1086363\ttotal: 39.1s\tremaining: 1m 1s\n",
      "390:\tlearn: 0.1085912\ttotal: 39.2s\tremaining: 1m 1s\n",
      "391:\tlearn: 0.1085322\ttotal: 39.3s\tremaining: 1m\n",
      "392:\tlearn: 0.1084876\ttotal: 39.4s\tremaining: 1m\n",
      "393:\tlearn: 0.1084022\ttotal: 39.5s\tremaining: 1m\n",
      "394:\tlearn: 0.1083265\ttotal: 39.6s\tremaining: 1m\n",
      "395:\tlearn: 0.1082353\ttotal: 39.7s\tremaining: 1m\n",
      "396:\tlearn: 0.1081728\ttotal: 39.8s\tremaining: 1m\n",
      "397:\tlearn: 0.1080952\ttotal: 39.9s\tremaining: 1m\n",
      "398:\tlearn: 0.1080419\ttotal: 40s\tremaining: 1m\n",
      "399:\tlearn: 0.1079772\ttotal: 40.1s\tremaining: 1m\n",
      "400:\tlearn: 0.1079106\ttotal: 40.1s\tremaining: 60s\n",
      "401:\tlearn: 0.1078669\ttotal: 40.2s\tremaining: 59.9s\n",
      "402:\tlearn: 0.1078238\ttotal: 40.3s\tremaining: 59.8s\n",
      "403:\tlearn: 0.1077651\ttotal: 40.4s\tremaining: 59.6s\n",
      "404:\tlearn: 0.1077040\ttotal: 40.5s\tremaining: 59.5s\n",
      "405:\tlearn: 0.1076433\ttotal: 40.6s\tremaining: 59.4s\n",
      "406:\tlearn: 0.1075618\ttotal: 40.7s\tremaining: 59.3s\n",
      "407:\tlearn: 0.1074988\ttotal: 40.8s\tremaining: 59.2s\n",
      "408:\tlearn: 0.1074053\ttotal: 40.9s\tremaining: 59.1s\n",
      "409:\tlearn: 0.1073068\ttotal: 41s\tremaining: 59s\n",
      "410:\tlearn: 0.1072657\ttotal: 41.1s\tremaining: 58.9s\n",
      "411:\tlearn: 0.1072315\ttotal: 41.2s\tremaining: 58.8s\n",
      "412:\tlearn: 0.1071882\ttotal: 41.3s\tremaining: 58.7s\n",
      "413:\tlearn: 0.1071472\ttotal: 41.4s\tremaining: 58.6s\n",
      "414:\tlearn: 0.1070933\ttotal: 41.5s\tremaining: 58.5s\n",
      "415:\tlearn: 0.1070432\ttotal: 41.6s\tremaining: 58.4s\n",
      "416:\tlearn: 0.1069935\ttotal: 41.7s\tremaining: 58.3s\n",
      "417:\tlearn: 0.1069595\ttotal: 41.8s\tremaining: 58.1s\n",
      "418:\tlearn: 0.1069269\ttotal: 41.9s\tremaining: 58s\n",
      "419:\tlearn: 0.1068526\ttotal: 41.9s\tremaining: 57.9s\n",
      "420:\tlearn: 0.1067973\ttotal: 42s\tremaining: 57.8s\n",
      "421:\tlearn: 0.1067752\ttotal: 42.1s\tremaining: 57.7s\n",
      "422:\tlearn: 0.1067318\ttotal: 42.2s\tremaining: 57.6s\n",
      "423:\tlearn: 0.1066570\ttotal: 42.3s\tremaining: 57.5s\n",
      "424:\tlearn: 0.1066336\ttotal: 42.4s\tremaining: 57.4s\n",
      "425:\tlearn: 0.1065610\ttotal: 42.5s\tremaining: 57.3s\n",
      "426:\tlearn: 0.1064957\ttotal: 42.6s\tremaining: 57.2s\n",
      "427:\tlearn: 0.1064288\ttotal: 42.7s\tremaining: 57.1s\n",
      "428:\tlearn: 0.1063885\ttotal: 42.8s\tremaining: 57s\n",
      "429:\tlearn: 0.1063123\ttotal: 42.9s\tremaining: 56.9s\n",
      "430:\tlearn: 0.1062341\ttotal: 43s\tremaining: 56.7s\n",
      "431:\tlearn: 0.1061859\ttotal: 43.1s\tremaining: 56.6s\n",
      "432:\tlearn: 0.1061336\ttotal: 43.2s\tremaining: 56.5s\n",
      "433:\tlearn: 0.1060379\ttotal: 43.3s\tremaining: 56.4s\n",
      "434:\tlearn: 0.1060070\ttotal: 43.4s\tremaining: 56.3s\n",
      "435:\tlearn: 0.1059620\ttotal: 43.4s\tremaining: 56.2s\n",
      "436:\tlearn: 0.1059318\ttotal: 43.5s\tremaining: 56.1s\n",
      "437:\tlearn: 0.1058861\ttotal: 43.6s\tremaining: 56s\n",
      "438:\tlearn: 0.1058443\ttotal: 43.7s\tremaining: 55.9s\n",
      "439:\tlearn: 0.1057875\ttotal: 43.8s\tremaining: 55.8s\n",
      "440:\tlearn: 0.1057515\ttotal: 43.9s\tremaining: 55.7s\n",
      "441:\tlearn: 0.1057092\ttotal: 44s\tremaining: 55.6s\n",
      "442:\tlearn: 0.1056275\ttotal: 44.1s\tremaining: 55.5s\n",
      "443:\tlearn: 0.1055677\ttotal: 44.2s\tremaining: 55.4s\n",
      "444:\tlearn: 0.1055492\ttotal: 44.3s\tremaining: 55.3s\n",
      "445:\tlearn: 0.1055123\ttotal: 44.4s\tremaining: 55.2s\n",
      "446:\tlearn: 0.1054823\ttotal: 44.5s\tremaining: 55.1s\n",
      "447:\tlearn: 0.1054491\ttotal: 44.6s\tremaining: 55s\n",
      "448:\tlearn: 0.1053506\ttotal: 44.7s\tremaining: 54.9s\n",
      "449:\tlearn: 0.1052901\ttotal: 44.8s\tremaining: 54.8s\n",
      "450:\tlearn: 0.1052523\ttotal: 44.9s\tremaining: 54.6s\n",
      "451:\tlearn: 0.1051563\ttotal: 45s\tremaining: 54.5s\n",
      "452:\tlearn: 0.1050992\ttotal: 45.1s\tremaining: 54.4s\n",
      "453:\tlearn: 0.1050467\ttotal: 45.2s\tremaining: 54.3s\n",
      "454:\tlearn: 0.1049697\ttotal: 45.3s\tremaining: 54.2s\n",
      "455:\tlearn: 0.1049340\ttotal: 45.4s\tremaining: 54.1s\n",
      "456:\tlearn: 0.1048861\ttotal: 45.5s\tremaining: 54s\n",
      "457:\tlearn: 0.1048527\ttotal: 45.6s\tremaining: 53.9s\n",
      "458:\tlearn: 0.1047802\ttotal: 45.7s\tremaining: 53.8s\n",
      "459:\tlearn: 0.1047423\ttotal: 45.7s\tremaining: 53.7s\n",
      "460:\tlearn: 0.1046720\ttotal: 45.8s\tremaining: 53.6s\n",
      "461:\tlearn: 0.1046226\ttotal: 45.9s\tremaining: 53.5s\n",
      "462:\tlearn: 0.1045529\ttotal: 46s\tremaining: 53.4s\n",
      "463:\tlearn: 0.1044806\ttotal: 46.1s\tremaining: 53.3s\n",
      "464:\tlearn: 0.1043959\ttotal: 46.2s\tremaining: 53.2s\n",
      "465:\tlearn: 0.1042882\ttotal: 46.3s\tremaining: 53.1s\n",
      "466:\tlearn: 0.1042310\ttotal: 46.4s\tremaining: 53s\n",
      "467:\tlearn: 0.1041251\ttotal: 46.5s\tremaining: 52.9s\n",
      "468:\tlearn: 0.1040247\ttotal: 46.6s\tremaining: 52.8s\n",
      "469:\tlearn: 0.1039779\ttotal: 46.7s\tremaining: 52.7s\n",
      "470:\tlearn: 0.1039398\ttotal: 46.8s\tremaining: 52.6s\n",
      "471:\tlearn: 0.1038936\ttotal: 46.9s\tremaining: 52.5s\n",
      "472:\tlearn: 0.1038063\ttotal: 47s\tremaining: 52.4s\n",
      "473:\tlearn: 0.1037577\ttotal: 47.1s\tremaining: 52.3s\n",
      "474:\tlearn: 0.1037177\ttotal: 47.2s\tremaining: 52.1s\n",
      "475:\tlearn: 0.1036616\ttotal: 47.3s\tremaining: 52s\n",
      "476:\tlearn: 0.1036321\ttotal: 47.4s\tremaining: 51.9s\n",
      "477:\tlearn: 0.1035682\ttotal: 47.5s\tremaining: 51.8s\n",
      "478:\tlearn: 0.1035216\ttotal: 47.6s\tremaining: 51.7s\n",
      "479:\tlearn: 0.1034574\ttotal: 47.7s\tremaining: 51.6s\n",
      "480:\tlearn: 0.1034118\ttotal: 47.7s\tremaining: 51.5s\n",
      "481:\tlearn: 0.1033725\ttotal: 47.8s\tremaining: 51.4s\n",
      "482:\tlearn: 0.1033055\ttotal: 47.9s\tremaining: 51.3s\n",
      "483:\tlearn: 0.1032163\ttotal: 48s\tremaining: 51.2s\n",
      "484:\tlearn: 0.1031632\ttotal: 48.1s\tremaining: 51.1s\n",
      "485:\tlearn: 0.1031129\ttotal: 48.2s\tremaining: 51s\n",
      "486:\tlearn: 0.1030485\ttotal: 48.3s\tremaining: 50.9s\n",
      "487:\tlearn: 0.1029897\ttotal: 48.4s\tremaining: 50.8s\n",
      "488:\tlearn: 0.1029513\ttotal: 48.5s\tremaining: 50.7s\n",
      "489:\tlearn: 0.1028164\ttotal: 48.6s\tremaining: 50.6s\n",
      "490:\tlearn: 0.1027818\ttotal: 48.7s\tremaining: 50.5s\n",
      "491:\tlearn: 0.1027290\ttotal: 48.9s\tremaining: 50.5s\n",
      "492:\tlearn: 0.1026487\ttotal: 49s\tremaining: 50.4s\n",
      "493:\tlearn: 0.1026069\ttotal: 49.1s\tremaining: 50.3s\n",
      "494:\tlearn: 0.1025419\ttotal: 49.2s\tremaining: 50.2s\n",
      "495:\tlearn: 0.1024861\ttotal: 49.3s\tremaining: 50.1s\n",
      "496:\tlearn: 0.1024461\ttotal: 49.4s\tremaining: 50s\n",
      "497:\tlearn: 0.1023815\ttotal: 49.5s\tremaining: 49.9s\n",
      "498:\tlearn: 0.1023297\ttotal: 49.6s\tremaining: 49.8s\n",
      "499:\tlearn: 0.1022593\ttotal: 49.7s\tremaining: 49.7s\n",
      "500:\tlearn: 0.1022286\ttotal: 49.8s\tremaining: 49.6s\n",
      "501:\tlearn: 0.1021687\ttotal: 49.9s\tremaining: 49.5s\n",
      "502:\tlearn: 0.1021339\ttotal: 49.9s\tremaining: 49.4s\n",
      "503:\tlearn: 0.1020858\ttotal: 50s\tremaining: 49.2s\n",
      "504:\tlearn: 0.1020536\ttotal: 50.1s\tremaining: 49.1s\n",
      "505:\tlearn: 0.1020113\ttotal: 50.2s\tremaining: 49s\n",
      "506:\tlearn: 0.1019590\ttotal: 50.3s\tremaining: 48.9s\n",
      "507:\tlearn: 0.1018748\ttotal: 50.4s\tremaining: 48.8s\n",
      "508:\tlearn: 0.1018161\ttotal: 50.5s\tremaining: 48.7s\n",
      "509:\tlearn: 0.1017445\ttotal: 50.6s\tremaining: 48.6s\n",
      "510:\tlearn: 0.1016810\ttotal: 50.7s\tremaining: 48.5s\n",
      "511:\tlearn: 0.1016089\ttotal: 50.8s\tremaining: 48.4s\n",
      "512:\tlearn: 0.1015713\ttotal: 50.9s\tremaining: 48.3s\n",
      "513:\tlearn: 0.1015362\ttotal: 51s\tremaining: 48.2s\n",
      "514:\tlearn: 0.1014996\ttotal: 51.1s\tremaining: 48.1s\n",
      "515:\tlearn: 0.1014597\ttotal: 51.2s\tremaining: 48s\n",
      "516:\tlearn: 0.1014275\ttotal: 51.3s\tremaining: 47.9s\n",
      "517:\tlearn: 0.1014170\ttotal: 51.4s\tremaining: 47.8s\n",
      "518:\tlearn: 0.1013824\ttotal: 51.5s\tremaining: 47.7s\n",
      "519:\tlearn: 0.1013222\ttotal: 51.6s\tremaining: 47.6s\n",
      "520:\tlearn: 0.1012755\ttotal: 51.7s\tremaining: 47.5s\n",
      "521:\tlearn: 0.1011839\ttotal: 51.8s\tremaining: 47.4s\n",
      "522:\tlearn: 0.1011128\ttotal: 51.8s\tremaining: 47.3s\n",
      "523:\tlearn: 0.1010742\ttotal: 51.9s\tremaining: 47.2s\n",
      "524:\tlearn: 0.1010050\ttotal: 52s\tremaining: 47.1s\n",
      "525:\tlearn: 0.1009357\ttotal: 52.1s\tremaining: 47s\n",
      "526:\tlearn: 0.1008995\ttotal: 52.2s\tremaining: 46.9s\n",
      "527:\tlearn: 0.1008382\ttotal: 52.3s\tremaining: 46.8s\n",
      "528:\tlearn: 0.1007782\ttotal: 52.4s\tremaining: 46.7s\n",
      "529:\tlearn: 0.1007341\ttotal: 52.5s\tremaining: 46.6s\n",
      "530:\tlearn: 0.1006717\ttotal: 52.6s\tremaining: 46.5s\n",
      "531:\tlearn: 0.1006349\ttotal: 52.7s\tremaining: 46.4s\n",
      "532:\tlearn: 0.1005962\ttotal: 52.8s\tremaining: 46.3s\n",
      "533:\tlearn: 0.1005529\ttotal: 52.9s\tremaining: 46.2s\n",
      "534:\tlearn: 0.1005067\ttotal: 53s\tremaining: 46s\n",
      "535:\tlearn: 0.1004736\ttotal: 53.1s\tremaining: 45.9s\n",
      "536:\tlearn: 0.1004253\ttotal: 53.2s\tremaining: 45.8s\n",
      "537:\tlearn: 0.1003952\ttotal: 53.3s\tremaining: 45.7s\n",
      "538:\tlearn: 0.1003680\ttotal: 53.4s\tremaining: 45.6s\n",
      "539:\tlearn: 0.1003096\ttotal: 53.4s\tremaining: 45.5s\n",
      "540:\tlearn: 0.1002832\ttotal: 53.5s\tremaining: 45.4s\n",
      "541:\tlearn: 0.1002196\ttotal: 53.6s\tremaining: 45.3s\n",
      "542:\tlearn: 0.1001721\ttotal: 53.7s\tremaining: 45.2s\n",
      "543:\tlearn: 0.1001298\ttotal: 53.8s\tremaining: 45.1s\n",
      "544:\tlearn: 0.1001006\ttotal: 53.9s\tremaining: 45s\n",
      "545:\tlearn: 0.1000774\ttotal: 54s\tremaining: 44.9s\n",
      "546:\tlearn: 0.1000347\ttotal: 54.1s\tremaining: 44.8s\n",
      "547:\tlearn: 0.1000184\ttotal: 54.2s\tremaining: 44.7s\n",
      "548:\tlearn: 0.0999327\ttotal: 54.3s\tremaining: 44.6s\n",
      "549:\tlearn: 0.0998931\ttotal: 54.4s\tremaining: 44.5s\n",
      "550:\tlearn: 0.0998526\ttotal: 54.5s\tremaining: 44.4s\n",
      "551:\tlearn: 0.0997918\ttotal: 54.6s\tremaining: 44.3s\n",
      "552:\tlearn: 0.0997313\ttotal: 54.7s\tremaining: 44.2s\n",
      "553:\tlearn: 0.0996972\ttotal: 54.8s\tremaining: 44.1s\n",
      "554:\tlearn: 0.0996578\ttotal: 54.9s\tremaining: 44s\n",
      "555:\tlearn: 0.0995861\ttotal: 55s\tremaining: 43.9s\n",
      "556:\tlearn: 0.0995600\ttotal: 55.1s\tremaining: 43.8s\n",
      "557:\tlearn: 0.0995316\ttotal: 55.1s\tremaining: 43.7s\n",
      "558:\tlearn: 0.0994770\ttotal: 55.2s\tremaining: 43.6s\n",
      "559:\tlearn: 0.0994026\ttotal: 55.3s\tremaining: 43.5s\n",
      "560:\tlearn: 0.0993559\ttotal: 55.4s\tremaining: 43.4s\n",
      "561:\tlearn: 0.0993146\ttotal: 55.5s\tremaining: 43.3s\n",
      "562:\tlearn: 0.0992730\ttotal: 55.6s\tremaining: 43.2s\n",
      "563:\tlearn: 0.0992537\ttotal: 55.7s\tremaining: 43.1s\n",
      "564:\tlearn: 0.0992030\ttotal: 55.8s\tremaining: 43s\n",
      "565:\tlearn: 0.0991652\ttotal: 55.9s\tremaining: 42.9s\n",
      "566:\tlearn: 0.0991110\ttotal: 56s\tremaining: 42.8s\n",
      "567:\tlearn: 0.0990691\ttotal: 56.1s\tremaining: 42.7s\n",
      "568:\tlearn: 0.0990244\ttotal: 56.2s\tremaining: 42.6s\n",
      "569:\tlearn: 0.0989606\ttotal: 56.3s\tremaining: 42.5s\n",
      "570:\tlearn: 0.0989046\ttotal: 56.4s\tremaining: 42.4s\n",
      "571:\tlearn: 0.0988791\ttotal: 56.5s\tremaining: 42.2s\n",
      "572:\tlearn: 0.0987839\ttotal: 56.6s\tremaining: 42.1s\n",
      "573:\tlearn: 0.0987060\ttotal: 56.6s\tremaining: 42s\n",
      "574:\tlearn: 0.0987045\ttotal: 56.7s\tremaining: 41.9s\n",
      "575:\tlearn: 0.0986104\ttotal: 56.8s\tremaining: 41.8s\n",
      "576:\tlearn: 0.0985745\ttotal: 56.9s\tremaining: 41.7s\n",
      "577:\tlearn: 0.0985396\ttotal: 57s\tremaining: 41.6s\n",
      "578:\tlearn: 0.0984767\ttotal: 57.1s\tremaining: 41.5s\n",
      "579:\tlearn: 0.0984393\ttotal: 57.2s\tremaining: 41.4s\n",
      "580:\tlearn: 0.0984321\ttotal: 57.3s\tremaining: 41.3s\n",
      "581:\tlearn: 0.0983673\ttotal: 57.4s\tremaining: 41.2s\n",
      "582:\tlearn: 0.0982753\ttotal: 57.5s\tremaining: 41.1s\n",
      "583:\tlearn: 0.0982333\ttotal: 57.6s\tremaining: 41s\n",
      "584:\tlearn: 0.0982015\ttotal: 57.7s\tremaining: 40.9s\n",
      "585:\tlearn: 0.0981375\ttotal: 57.8s\tremaining: 40.8s\n",
      "586:\tlearn: 0.0980923\ttotal: 57.9s\tremaining: 40.7s\n",
      "587:\tlearn: 0.0979703\ttotal: 58s\tremaining: 40.6s\n",
      "588:\tlearn: 0.0979366\ttotal: 58.1s\tremaining: 40.5s\n",
      "589:\tlearn: 0.0979099\ttotal: 58.2s\tremaining: 40.4s\n",
      "590:\tlearn: 0.0978675\ttotal: 58.3s\tremaining: 40.3s\n",
      "591:\tlearn: 0.0978423\ttotal: 58.4s\tremaining: 40.2s\n",
      "592:\tlearn: 0.0977861\ttotal: 58.5s\tremaining: 40.1s\n",
      "593:\tlearn: 0.0977394\ttotal: 58.5s\tremaining: 40s\n",
      "594:\tlearn: 0.0976892\ttotal: 58.6s\tremaining: 39.9s\n",
      "595:\tlearn: 0.0976653\ttotal: 58.7s\tremaining: 39.8s\n",
      "596:\tlearn: 0.0976280\ttotal: 58.8s\tremaining: 39.7s\n",
      "597:\tlearn: 0.0975611\ttotal: 58.9s\tremaining: 39.6s\n",
      "598:\tlearn: 0.0974966\ttotal: 59s\tremaining: 39.5s\n",
      "599:\tlearn: 0.0974552\ttotal: 59.1s\tremaining: 39.4s\n",
      "600:\tlearn: 0.0973875\ttotal: 59.2s\tremaining: 39.3s\n",
      "601:\tlearn: 0.0973265\ttotal: 59.4s\tremaining: 39.3s\n",
      "602:\tlearn: 0.0972902\ttotal: 59.5s\tremaining: 39.2s\n",
      "603:\tlearn: 0.0972698\ttotal: 59.6s\tremaining: 39.1s\n",
      "604:\tlearn: 0.0972168\ttotal: 59.7s\tremaining: 39s\n",
      "605:\tlearn: 0.0971434\ttotal: 59.8s\tremaining: 38.9s\n",
      "606:\tlearn: 0.0971099\ttotal: 59.9s\tremaining: 38.8s\n",
      "607:\tlearn: 0.0970572\ttotal: 60s\tremaining: 38.7s\n",
      "608:\tlearn: 0.0970238\ttotal: 1m\tremaining: 38.6s\n",
      "609:\tlearn: 0.0970195\ttotal: 1m\tremaining: 38.5s\n",
      "610:\tlearn: 0.0969825\ttotal: 1m\tremaining: 38.4s\n",
      "611:\tlearn: 0.0969221\ttotal: 1m\tremaining: 38.3s\n",
      "612:\tlearn: 0.0968616\ttotal: 1m\tremaining: 38.2s\n",
      "613:\tlearn: 0.0968193\ttotal: 1m\tremaining: 38.1s\n",
      "614:\tlearn: 0.0967837\ttotal: 1m\tremaining: 38s\n",
      "615:\tlearn: 0.0967517\ttotal: 1m\tremaining: 37.9s\n",
      "616:\tlearn: 0.0967063\ttotal: 1m\tremaining: 37.8s\n",
      "617:\tlearn: 0.0966762\ttotal: 1m\tremaining: 37.7s\n",
      "618:\tlearn: 0.0966525\ttotal: 1m 1s\tremaining: 37.6s\n",
      "619:\tlearn: 0.0965922\ttotal: 1m 1s\tremaining: 37.5s\n",
      "620:\tlearn: 0.0965531\ttotal: 1m 1s\tremaining: 37.4s\n",
      "621:\tlearn: 0.0965160\ttotal: 1m 1s\tremaining: 37.3s\n",
      "622:\tlearn: 0.0964863\ttotal: 1m 1s\tremaining: 37.2s\n",
      "623:\tlearn: 0.0964282\ttotal: 1m 1s\tremaining: 37.1s\n",
      "624:\tlearn: 0.0963706\ttotal: 1m 1s\tremaining: 37s\n",
      "625:\tlearn: 0.0962877\ttotal: 1m 1s\tremaining: 36.9s\n",
      "626:\tlearn: 0.0962467\ttotal: 1m 1s\tremaining: 36.8s\n",
      "627:\tlearn: 0.0961915\ttotal: 1m 1s\tremaining: 36.7s\n",
      "628:\tlearn: 0.0961580\ttotal: 1m 1s\tremaining: 36.6s\n",
      "629:\tlearn: 0.0961169\ttotal: 1m 2s\tremaining: 36.5s\n",
      "630:\tlearn: 0.0960518\ttotal: 1m 2s\tremaining: 36.4s\n",
      "631:\tlearn: 0.0960081\ttotal: 1m 2s\tremaining: 36.3s\n",
      "632:\tlearn: 0.0959750\ttotal: 1m 2s\tremaining: 36.2s\n",
      "633:\tlearn: 0.0959415\ttotal: 1m 2s\tremaining: 36.1s\n",
      "634:\tlearn: 0.0958993\ttotal: 1m 2s\tremaining: 36s\n",
      "635:\tlearn: 0.0958602\ttotal: 1m 2s\tremaining: 35.8s\n",
      "636:\tlearn: 0.0958526\ttotal: 1m 2s\tremaining: 35.7s\n",
      "637:\tlearn: 0.0958146\ttotal: 1m 2s\tremaining: 35.6s\n",
      "638:\tlearn: 0.0957812\ttotal: 1m 2s\tremaining: 35.5s\n",
      "639:\tlearn: 0.0957375\ttotal: 1m 3s\tremaining: 35.4s\n",
      "640:\tlearn: 0.0956979\ttotal: 1m 3s\tremaining: 35.3s\n",
      "641:\tlearn: 0.0956417\ttotal: 1m 3s\tremaining: 35.2s\n",
      "642:\tlearn: 0.0956193\ttotal: 1m 3s\tremaining: 35.1s\n",
      "643:\tlearn: 0.0954900\ttotal: 1m 3s\tremaining: 35s\n",
      "644:\tlearn: 0.0954614\ttotal: 1m 3s\tremaining: 34.9s\n",
      "645:\tlearn: 0.0954090\ttotal: 1m 3s\tremaining: 34.8s\n",
      "646:\tlearn: 0.0953562\ttotal: 1m 3s\tremaining: 34.7s\n",
      "647:\tlearn: 0.0952877\ttotal: 1m 3s\tremaining: 34.6s\n",
      "648:\tlearn: 0.0952360\ttotal: 1m 3s\tremaining: 34.5s\n",
      "649:\tlearn: 0.0951742\ttotal: 1m 3s\tremaining: 34.4s\n",
      "650:\tlearn: 0.0951245\ttotal: 1m 4s\tremaining: 34.3s\n",
      "651:\tlearn: 0.0950830\ttotal: 1m 4s\tremaining: 34.2s\n",
      "652:\tlearn: 0.0950629\ttotal: 1m 4s\tremaining: 34.1s\n",
      "653:\tlearn: 0.0950083\ttotal: 1m 4s\tremaining: 34s\n",
      "654:\tlearn: 0.0949639\ttotal: 1m 4s\tremaining: 33.9s\n",
      "655:\tlearn: 0.0949304\ttotal: 1m 4s\tremaining: 33.8s\n",
      "656:\tlearn: 0.0948993\ttotal: 1m 4s\tremaining: 33.7s\n",
      "657:\tlearn: 0.0948420\ttotal: 1m 4s\tremaining: 33.6s\n",
      "658:\tlearn: 0.0947979\ttotal: 1m 4s\tremaining: 33.5s\n",
      "659:\tlearn: 0.0947978\ttotal: 1m 4s\tremaining: 33.4s\n",
      "660:\tlearn: 0.0947904\ttotal: 1m 4s\tremaining: 33.3s\n",
      "661:\tlearn: 0.0947683\ttotal: 1m 5s\tremaining: 33.2s\n",
      "662:\tlearn: 0.0947166\ttotal: 1m 5s\tremaining: 33.1s\n",
      "663:\tlearn: 0.0946892\ttotal: 1m 5s\tremaining: 33s\n",
      "664:\tlearn: 0.0946406\ttotal: 1m 5s\tremaining: 32.9s\n",
      "665:\tlearn: 0.0945721\ttotal: 1m 5s\tremaining: 32.8s\n",
      "666:\tlearn: 0.0945175\ttotal: 1m 5s\tremaining: 32.7s\n",
      "667:\tlearn: 0.0944683\ttotal: 1m 5s\tremaining: 32.6s\n",
      "668:\tlearn: 0.0944467\ttotal: 1m 5s\tremaining: 32.5s\n",
      "669:\tlearn: 0.0943961\ttotal: 1m 5s\tremaining: 32.4s\n",
      "670:\tlearn: 0.0943504\ttotal: 1m 5s\tremaining: 32.3s\n",
      "671:\tlearn: 0.0942559\ttotal: 1m 6s\tremaining: 32.2s\n",
      "672:\tlearn: 0.0942184\ttotal: 1m 6s\tremaining: 32.1s\n",
      "673:\tlearn: 0.0941736\ttotal: 1m 6s\tremaining: 32s\n",
      "674:\tlearn: 0.0940708\ttotal: 1m 6s\tremaining: 31.9s\n",
      "675:\tlearn: 0.0940202\ttotal: 1m 6s\tremaining: 31.8s\n",
      "676:\tlearn: 0.0939885\ttotal: 1m 6s\tremaining: 31.7s\n",
      "677:\tlearn: 0.0939372\ttotal: 1m 6s\tremaining: 31.6s\n",
      "678:\tlearn: 0.0939008\ttotal: 1m 6s\tremaining: 31.5s\n",
      "679:\tlearn: 0.0938618\ttotal: 1m 6s\tremaining: 31.4s\n",
      "680:\tlearn: 0.0937987\ttotal: 1m 6s\tremaining: 31.3s\n",
      "681:\tlearn: 0.0937435\ttotal: 1m 6s\tremaining: 31.2s\n",
      "682:\tlearn: 0.0937242\ttotal: 1m 7s\tremaining: 31.1s\n",
      "683:\tlearn: 0.0936932\ttotal: 1m 7s\tremaining: 31s\n",
      "684:\tlearn: 0.0936387\ttotal: 1m 7s\tremaining: 30.9s\n",
      "685:\tlearn: 0.0935697\ttotal: 1m 7s\tremaining: 30.8s\n",
      "686:\tlearn: 0.0935264\ttotal: 1m 7s\tremaining: 30.7s\n",
      "687:\tlearn: 0.0934636\ttotal: 1m 7s\tremaining: 30.6s\n",
      "688:\tlearn: 0.0934377\ttotal: 1m 7s\tremaining: 30.5s\n",
      "689:\tlearn: 0.0933903\ttotal: 1m 7s\tremaining: 30.4s\n",
      "690:\tlearn: 0.0933404\ttotal: 1m 7s\tremaining: 30.3s\n",
      "691:\tlearn: 0.0933071\ttotal: 1m 7s\tremaining: 30.2s\n",
      "692:\tlearn: 0.0932768\ttotal: 1m 8s\tremaining: 30.1s\n",
      "693:\tlearn: 0.0932143\ttotal: 1m 8s\tremaining: 30s\n",
      "694:\tlearn: 0.0931650\ttotal: 1m 8s\tremaining: 29.9s\n",
      "695:\tlearn: 0.0931434\ttotal: 1m 8s\tremaining: 29.8s\n",
      "696:\tlearn: 0.0930959\ttotal: 1m 8s\tremaining: 29.7s\n",
      "697:\tlearn: 0.0929880\ttotal: 1m 8s\tremaining: 29.6s\n",
      "698:\tlearn: 0.0929511\ttotal: 1m 8s\tremaining: 29.5s\n",
      "699:\tlearn: 0.0929026\ttotal: 1m 8s\tremaining: 29.4s\n",
      "700:\tlearn: 0.0928536\ttotal: 1m 8s\tremaining: 29.3s\n",
      "701:\tlearn: 0.0928037\ttotal: 1m 8s\tremaining: 29.2s\n",
      "702:\tlearn: 0.0927661\ttotal: 1m 8s\tremaining: 29.1s\n",
      "703:\tlearn: 0.0927254\ttotal: 1m 9s\tremaining: 29s\n",
      "704:\tlearn: 0.0927027\ttotal: 1m 9s\tremaining: 28.9s\n",
      "705:\tlearn: 0.0926579\ttotal: 1m 9s\tremaining: 28.9s\n",
      "706:\tlearn: 0.0925911\ttotal: 1m 9s\tremaining: 28.8s\n",
      "707:\tlearn: 0.0925088\ttotal: 1m 9s\tremaining: 28.7s\n",
      "708:\tlearn: 0.0924422\ttotal: 1m 9s\tremaining: 28.6s\n",
      "709:\tlearn: 0.0923786\ttotal: 1m 9s\tremaining: 28.5s\n",
      "710:\tlearn: 0.0923524\ttotal: 1m 9s\tremaining: 28.4s\n",
      "711:\tlearn: 0.0923065\ttotal: 1m 9s\tremaining: 28.3s\n",
      "712:\tlearn: 0.0922176\ttotal: 1m 10s\tremaining: 28.2s\n",
      "713:\tlearn: 0.0921936\ttotal: 1m 10s\tremaining: 28.1s\n",
      "714:\tlearn: 0.0921441\ttotal: 1m 10s\tremaining: 28s\n",
      "715:\tlearn: 0.0920978\ttotal: 1m 10s\tremaining: 27.9s\n",
      "716:\tlearn: 0.0920327\ttotal: 1m 10s\tremaining: 27.8s\n",
      "717:\tlearn: 0.0919831\ttotal: 1m 10s\tremaining: 27.7s\n",
      "718:\tlearn: 0.0919415\ttotal: 1m 10s\tremaining: 27.6s\n",
      "719:\tlearn: 0.0918931\ttotal: 1m 10s\tremaining: 27.5s\n",
      "720:\tlearn: 0.0918540\ttotal: 1m 10s\tremaining: 27.4s\n",
      "721:\tlearn: 0.0918127\ttotal: 1m 10s\tremaining: 27.3s\n",
      "722:\tlearn: 0.0917840\ttotal: 1m 11s\tremaining: 27.2s\n",
      "723:\tlearn: 0.0917657\ttotal: 1m 11s\tremaining: 27.1s\n",
      "724:\tlearn: 0.0917396\ttotal: 1m 11s\tremaining: 27s\n",
      "725:\tlearn: 0.0917157\ttotal: 1m 11s\tremaining: 26.9s\n",
      "726:\tlearn: 0.0916484\ttotal: 1m 11s\tremaining: 26.8s\n",
      "727:\tlearn: 0.0916033\ttotal: 1m 11s\tremaining: 26.7s\n",
      "728:\tlearn: 0.0915751\ttotal: 1m 11s\tremaining: 26.6s\n",
      "729:\tlearn: 0.0915510\ttotal: 1m 11s\tremaining: 26.5s\n",
      "730:\tlearn: 0.0915024\ttotal: 1m 11s\tremaining: 26.4s\n",
      "731:\tlearn: 0.0914671\ttotal: 1m 11s\tremaining: 26.3s\n",
      "732:\tlearn: 0.0914259\ttotal: 1m 11s\tremaining: 26.2s\n",
      "733:\tlearn: 0.0914209\ttotal: 1m 12s\tremaining: 26.1s\n",
      "734:\tlearn: 0.0913872\ttotal: 1m 12s\tremaining: 26s\n",
      "735:\tlearn: 0.0913615\ttotal: 1m 12s\tremaining: 25.9s\n",
      "736:\tlearn: 0.0913355\ttotal: 1m 12s\tremaining: 25.8s\n",
      "737:\tlearn: 0.0913114\ttotal: 1m 12s\tremaining: 25.7s\n",
      "738:\tlearn: 0.0912754\ttotal: 1m 12s\tremaining: 25.6s\n",
      "739:\tlearn: 0.0912499\ttotal: 1m 12s\tremaining: 25.5s\n",
      "740:\tlearn: 0.0912071\ttotal: 1m 12s\tremaining: 25.4s\n",
      "741:\tlearn: 0.0911646\ttotal: 1m 12s\tremaining: 25.3s\n",
      "742:\tlearn: 0.0911263\ttotal: 1m 12s\tremaining: 25.2s\n",
      "743:\tlearn: 0.0910837\ttotal: 1m 12s\tremaining: 25.1s\n",
      "744:\tlearn: 0.0910316\ttotal: 1m 13s\tremaining: 25s\n",
      "745:\tlearn: 0.0909896\ttotal: 1m 13s\tremaining: 24.9s\n",
      "746:\tlearn: 0.0909725\ttotal: 1m 13s\tremaining: 24.8s\n",
      "747:\tlearn: 0.0909478\ttotal: 1m 13s\tremaining: 24.7s\n",
      "748:\tlearn: 0.0909221\ttotal: 1m 13s\tremaining: 24.6s\n",
      "749:\tlearn: 0.0908869\ttotal: 1m 13s\tremaining: 24.5s\n",
      "750:\tlearn: 0.0908449\ttotal: 1m 13s\tremaining: 24.4s\n",
      "751:\tlearn: 0.0908228\ttotal: 1m 13s\tremaining: 24.3s\n",
      "752:\tlearn: 0.0907795\ttotal: 1m 13s\tremaining: 24.2s\n",
      "753:\tlearn: 0.0907704\ttotal: 1m 13s\tremaining: 24.1s\n",
      "754:\tlearn: 0.0907184\ttotal: 1m 14s\tremaining: 24s\n",
      "755:\tlearn: 0.0906947\ttotal: 1m 14s\tremaining: 23.9s\n",
      "756:\tlearn: 0.0906486\ttotal: 1m 14s\tremaining: 23.9s\n",
      "757:\tlearn: 0.0906171\ttotal: 1m 14s\tremaining: 23.8s\n",
      "758:\tlearn: 0.0905605\ttotal: 1m 14s\tremaining: 23.7s\n",
      "759:\tlearn: 0.0904890\ttotal: 1m 14s\tremaining: 23.6s\n",
      "760:\tlearn: 0.0904203\ttotal: 1m 14s\tremaining: 23.5s\n",
      "761:\tlearn: 0.0903949\ttotal: 1m 14s\tremaining: 23.4s\n",
      "762:\tlearn: 0.0903148\ttotal: 1m 14s\tremaining: 23.3s\n",
      "763:\tlearn: 0.0902233\ttotal: 1m 14s\tremaining: 23.2s\n",
      "764:\tlearn: 0.0901904\ttotal: 1m 15s\tremaining: 23.1s\n",
      "765:\tlearn: 0.0901655\ttotal: 1m 15s\tremaining: 23s\n",
      "766:\tlearn: 0.0901331\ttotal: 1m 15s\tremaining: 22.9s\n",
      "767:\tlearn: 0.0900723\ttotal: 1m 15s\tremaining: 22.8s\n",
      "768:\tlearn: 0.0900385\ttotal: 1m 15s\tremaining: 22.7s\n",
      "769:\tlearn: 0.0899908\ttotal: 1m 15s\tremaining: 22.6s\n",
      "770:\tlearn: 0.0899223\ttotal: 1m 15s\tremaining: 22.5s\n",
      "771:\tlearn: 0.0898896\ttotal: 1m 15s\tremaining: 22.4s\n",
      "772:\tlearn: 0.0898337\ttotal: 1m 15s\tremaining: 22.3s\n",
      "773:\tlearn: 0.0898015\ttotal: 1m 15s\tremaining: 22.2s\n",
      "774:\tlearn: 0.0897681\ttotal: 1m 16s\tremaining: 22.1s\n",
      "775:\tlearn: 0.0897276\ttotal: 1m 16s\tremaining: 22s\n",
      "776:\tlearn: 0.0896881\ttotal: 1m 16s\tremaining: 21.9s\n",
      "777:\tlearn: 0.0896332\ttotal: 1m 16s\tremaining: 21.8s\n",
      "778:\tlearn: 0.0895695\ttotal: 1m 16s\tremaining: 21.7s\n",
      "779:\tlearn: 0.0895234\ttotal: 1m 16s\tremaining: 21.6s\n",
      "780:\tlearn: 0.0894866\ttotal: 1m 16s\tremaining: 21.5s\n",
      "781:\tlearn: 0.0894560\ttotal: 1m 16s\tremaining: 21.4s\n",
      "782:\tlearn: 0.0894273\ttotal: 1m 16s\tremaining: 21.3s\n",
      "783:\tlearn: 0.0893932\ttotal: 1m 16s\tremaining: 21.2s\n",
      "784:\tlearn: 0.0893508\ttotal: 1m 17s\tremaining: 21.1s\n",
      "785:\tlearn: 0.0893131\ttotal: 1m 17s\tremaining: 21s\n",
      "786:\tlearn: 0.0892929\ttotal: 1m 17s\tremaining: 20.9s\n",
      "787:\tlearn: 0.0892542\ttotal: 1m 17s\tremaining: 20.8s\n",
      "788:\tlearn: 0.0892388\ttotal: 1m 17s\tremaining: 20.7s\n",
      "789:\tlearn: 0.0892172\ttotal: 1m 17s\tremaining: 20.6s\n",
      "790:\tlearn: 0.0891676\ttotal: 1m 17s\tremaining: 20.5s\n",
      "791:\tlearn: 0.0891521\ttotal: 1m 17s\tremaining: 20.4s\n",
      "792:\tlearn: 0.0891108\ttotal: 1m 17s\tremaining: 20.3s\n",
      "793:\tlearn: 0.0890796\ttotal: 1m 17s\tremaining: 20.2s\n",
      "794:\tlearn: 0.0890341\ttotal: 1m 17s\tremaining: 20.1s\n",
      "795:\tlearn: 0.0889726\ttotal: 1m 18s\tremaining: 20s\n",
      "796:\tlearn: 0.0889252\ttotal: 1m 18s\tremaining: 19.9s\n",
      "797:\tlearn: 0.0888970\ttotal: 1m 18s\tremaining: 19.8s\n",
      "798:\tlearn: 0.0888452\ttotal: 1m 18s\tremaining: 19.7s\n",
      "799:\tlearn: 0.0887619\ttotal: 1m 18s\tremaining: 19.6s\n",
      "800:\tlearn: 0.0887527\ttotal: 1m 18s\tremaining: 19.5s\n",
      "801:\tlearn: 0.0886950\ttotal: 1m 18s\tremaining: 19.4s\n",
      "802:\tlearn: 0.0886582\ttotal: 1m 18s\tremaining: 19.3s\n",
      "803:\tlearn: 0.0886219\ttotal: 1m 18s\tremaining: 19.2s\n",
      "804:\tlearn: 0.0885604\ttotal: 1m 18s\tremaining: 19.1s\n",
      "805:\tlearn: 0.0885246\ttotal: 1m 19s\tremaining: 19s\n",
      "806:\tlearn: 0.0884702\ttotal: 1m 19s\tremaining: 18.9s\n",
      "807:\tlearn: 0.0884335\ttotal: 1m 19s\tremaining: 18.8s\n",
      "808:\tlearn: 0.0883859\ttotal: 1m 19s\tremaining: 18.7s\n",
      "809:\tlearn: 0.0883515\ttotal: 1m 19s\tremaining: 18.6s\n",
      "810:\tlearn: 0.0883279\ttotal: 1m 19s\tremaining: 18.5s\n",
      "811:\tlearn: 0.0883022\ttotal: 1m 19s\tremaining: 18.4s\n",
      "812:\tlearn: 0.0882558\ttotal: 1m 19s\tremaining: 18.3s\n",
      "813:\tlearn: 0.0882056\ttotal: 1m 19s\tremaining: 18.2s\n",
      "814:\tlearn: 0.0881872\ttotal: 1m 19s\tremaining: 18.1s\n",
      "815:\tlearn: 0.0881719\ttotal: 1m 19s\tremaining: 18s\n",
      "816:\tlearn: 0.0881365\ttotal: 1m 20s\tremaining: 17.9s\n",
      "817:\tlearn: 0.0880918\ttotal: 1m 20s\tremaining: 17.9s\n",
      "818:\tlearn: 0.0880654\ttotal: 1m 20s\tremaining: 17.8s\n",
      "819:\tlearn: 0.0880242\ttotal: 1m 20s\tremaining: 17.7s\n",
      "820:\tlearn: 0.0879757\ttotal: 1m 20s\tremaining: 17.6s\n",
      "821:\tlearn: 0.0879375\ttotal: 1m 20s\tremaining: 17.5s\n",
      "822:\tlearn: 0.0878922\ttotal: 1m 20s\tremaining: 17.4s\n",
      "823:\tlearn: 0.0878533\ttotal: 1m 20s\tremaining: 17.3s\n",
      "824:\tlearn: 0.0878129\ttotal: 1m 20s\tremaining: 17.2s\n",
      "825:\tlearn: 0.0877690\ttotal: 1m 21s\tremaining: 17.1s\n",
      "826:\tlearn: 0.0877247\ttotal: 1m 21s\tremaining: 17s\n",
      "827:\tlearn: 0.0876759\ttotal: 1m 21s\tremaining: 16.9s\n",
      "828:\tlearn: 0.0876048\ttotal: 1m 21s\tremaining: 16.8s\n",
      "829:\tlearn: 0.0875982\ttotal: 1m 21s\tremaining: 16.7s\n",
      "830:\tlearn: 0.0875385\ttotal: 1m 21s\tremaining: 16.6s\n",
      "831:\tlearn: 0.0875069\ttotal: 1m 21s\tremaining: 16.5s\n",
      "832:\tlearn: 0.0874996\ttotal: 1m 21s\tremaining: 16.4s\n",
      "833:\tlearn: 0.0874893\ttotal: 1m 21s\tremaining: 16.3s\n",
      "834:\tlearn: 0.0874466\ttotal: 1m 21s\tremaining: 16.2s\n",
      "835:\tlearn: 0.0874111\ttotal: 1m 22s\tremaining: 16.1s\n",
      "836:\tlearn: 0.0873855\ttotal: 1m 22s\tremaining: 16s\n",
      "837:\tlearn: 0.0873502\ttotal: 1m 22s\tremaining: 15.9s\n",
      "838:\tlearn: 0.0873031\ttotal: 1m 22s\tremaining: 15.8s\n",
      "839:\tlearn: 0.0872634\ttotal: 1m 22s\tremaining: 15.7s\n",
      "840:\tlearn: 0.0871989\ttotal: 1m 22s\tremaining: 15.6s\n",
      "841:\tlearn: 0.0871655\ttotal: 1m 22s\tremaining: 15.5s\n",
      "842:\tlearn: 0.0871489\ttotal: 1m 22s\tremaining: 15.4s\n",
      "843:\tlearn: 0.0870871\ttotal: 1m 22s\tremaining: 15.3s\n",
      "844:\tlearn: 0.0870428\ttotal: 1m 22s\tremaining: 15.2s\n",
      "845:\tlearn: 0.0870028\ttotal: 1m 22s\tremaining: 15.1s\n",
      "846:\tlearn: 0.0869636\ttotal: 1m 23s\tremaining: 15s\n",
      "847:\tlearn: 0.0869377\ttotal: 1m 23s\tremaining: 14.9s\n",
      "848:\tlearn: 0.0869139\ttotal: 1m 23s\tremaining: 14.8s\n",
      "849:\tlearn: 0.0869065\ttotal: 1m 23s\tremaining: 14.7s\n",
      "850:\tlearn: 0.0868707\ttotal: 1m 23s\tremaining: 14.6s\n",
      "851:\tlearn: 0.0868534\ttotal: 1m 23s\tremaining: 14.5s\n",
      "852:\tlearn: 0.0867982\ttotal: 1m 23s\tremaining: 14.4s\n",
      "853:\tlearn: 0.0867629\ttotal: 1m 23s\tremaining: 14.3s\n",
      "854:\tlearn: 0.0867342\ttotal: 1m 23s\tremaining: 14.2s\n",
      "855:\tlearn: 0.0866834\ttotal: 1m 23s\tremaining: 14.1s\n",
      "856:\tlearn: 0.0866574\ttotal: 1m 24s\tremaining: 14s\n",
      "857:\tlearn: 0.0866377\ttotal: 1m 24s\tremaining: 13.9s\n",
      "858:\tlearn: 0.0866003\ttotal: 1m 24s\tremaining: 13.8s\n",
      "859:\tlearn: 0.0865713\ttotal: 1m 24s\tremaining: 13.7s\n",
      "860:\tlearn: 0.0865280\ttotal: 1m 24s\tremaining: 13.6s\n",
      "861:\tlearn: 0.0864837\ttotal: 1m 24s\tremaining: 13.5s\n",
      "862:\tlearn: 0.0864522\ttotal: 1m 24s\tremaining: 13.4s\n",
      "863:\tlearn: 0.0864126\ttotal: 1m 24s\tremaining: 13.3s\n",
      "864:\tlearn: 0.0863769\ttotal: 1m 24s\tremaining: 13.2s\n",
      "865:\tlearn: 0.0863151\ttotal: 1m 24s\tremaining: 13.1s\n",
      "866:\tlearn: 0.0862606\ttotal: 1m 24s\tremaining: 13s\n",
      "867:\tlearn: 0.0862213\ttotal: 1m 25s\tremaining: 12.9s\n",
      "868:\tlearn: 0.0862024\ttotal: 1m 25s\tremaining: 12.8s\n",
      "869:\tlearn: 0.0861501\ttotal: 1m 25s\tremaining: 12.7s\n",
      "870:\tlearn: 0.0861123\ttotal: 1m 25s\tremaining: 12.7s\n",
      "871:\tlearn: 0.0860864\ttotal: 1m 25s\tremaining: 12.6s\n",
      "872:\tlearn: 0.0860367\ttotal: 1m 25s\tremaining: 12.5s\n",
      "873:\tlearn: 0.0859837\ttotal: 1m 25s\tremaining: 12.4s\n",
      "874:\tlearn: 0.0859636\ttotal: 1m 25s\tremaining: 12.3s\n",
      "875:\tlearn: 0.0859343\ttotal: 1m 26s\tremaining: 12.2s\n",
      "876:\tlearn: 0.0858571\ttotal: 1m 26s\tremaining: 12.1s\n",
      "877:\tlearn: 0.0858371\ttotal: 1m 26s\tremaining: 12s\n",
      "878:\tlearn: 0.0857947\ttotal: 1m 26s\tremaining: 11.9s\n",
      "879:\tlearn: 0.0857689\ttotal: 1m 26s\tremaining: 11.8s\n",
      "880:\tlearn: 0.0857270\ttotal: 1m 26s\tremaining: 11.7s\n",
      "881:\tlearn: 0.0857049\ttotal: 1m 26s\tremaining: 11.6s\n",
      "882:\tlearn: 0.0856735\ttotal: 1m 26s\tremaining: 11.5s\n",
      "883:\tlearn: 0.0856233\ttotal: 1m 27s\tremaining: 11.4s\n",
      "884:\tlearn: 0.0855643\ttotal: 1m 27s\tremaining: 11.3s\n",
      "885:\tlearn: 0.0855360\ttotal: 1m 27s\tremaining: 11.2s\n",
      "886:\tlearn: 0.0855098\ttotal: 1m 27s\tremaining: 11.1s\n",
      "887:\tlearn: 0.0854904\ttotal: 1m 27s\tremaining: 11s\n",
      "888:\tlearn: 0.0854593\ttotal: 1m 27s\tremaining: 10.9s\n",
      "889:\tlearn: 0.0854361\ttotal: 1m 27s\tremaining: 10.8s\n",
      "890:\tlearn: 0.0853891\ttotal: 1m 27s\tremaining: 10.7s\n",
      "891:\tlearn: 0.0853455\ttotal: 1m 27s\tremaining: 10.6s\n",
      "892:\tlearn: 0.0852821\ttotal: 1m 28s\tremaining: 10.5s\n",
      "893:\tlearn: 0.0852635\ttotal: 1m 28s\tremaining: 10.4s\n",
      "894:\tlearn: 0.0852087\ttotal: 1m 28s\tremaining: 10.4s\n",
      "895:\tlearn: 0.0851357\ttotal: 1m 28s\tremaining: 10.3s\n",
      "896:\tlearn: 0.0850698\ttotal: 1m 28s\tremaining: 10.2s\n",
      "897:\tlearn: 0.0850258\ttotal: 1m 28s\tremaining: 10.1s\n",
      "898:\tlearn: 0.0849860\ttotal: 1m 28s\tremaining: 9.96s\n",
      "899:\tlearn: 0.0849355\ttotal: 1m 28s\tremaining: 9.86s\n",
      "900:\tlearn: 0.0848915\ttotal: 1m 28s\tremaining: 9.76s\n",
      "901:\tlearn: 0.0848527\ttotal: 1m 28s\tremaining: 9.66s\n",
      "902:\tlearn: 0.0848169\ttotal: 1m 29s\tremaining: 9.56s\n",
      "903:\tlearn: 0.0847838\ttotal: 1m 29s\tremaining: 9.46s\n",
      "904:\tlearn: 0.0847368\ttotal: 1m 29s\tremaining: 9.36s\n",
      "905:\tlearn: 0.0846891\ttotal: 1m 29s\tremaining: 9.27s\n",
      "906:\tlearn: 0.0846436\ttotal: 1m 29s\tremaining: 9.17s\n",
      "907:\tlearn: 0.0846121\ttotal: 1m 29s\tremaining: 9.08s\n",
      "908:\tlearn: 0.0845931\ttotal: 1m 29s\tremaining: 8.98s\n",
      "909:\tlearn: 0.0845158\ttotal: 1m 29s\tremaining: 8.88s\n",
      "910:\tlearn: 0.0844811\ttotal: 1m 29s\tremaining: 8.78s\n",
      "911:\tlearn: 0.0844382\ttotal: 1m 30s\tremaining: 8.69s\n",
      "912:\tlearn: 0.0844285\ttotal: 1m 30s\tremaining: 8.59s\n",
      "913:\tlearn: 0.0843971\ttotal: 1m 30s\tremaining: 8.49s\n",
      "914:\tlearn: 0.0843494\ttotal: 1m 30s\tremaining: 8.39s\n",
      "915:\tlearn: 0.0843141\ttotal: 1m 30s\tremaining: 8.29s\n",
      "916:\tlearn: 0.0842547\ttotal: 1m 30s\tremaining: 8.19s\n",
      "917:\tlearn: 0.0842140\ttotal: 1m 30s\tremaining: 8.1s\n",
      "918:\tlearn: 0.0841420\ttotal: 1m 30s\tremaining: 8s\n",
      "919:\tlearn: 0.0841131\ttotal: 1m 30s\tremaining: 7.9s\n",
      "920:\tlearn: 0.0840436\ttotal: 1m 30s\tremaining: 7.8s\n",
      "921:\tlearn: 0.0840373\ttotal: 1m 31s\tremaining: 7.7s\n",
      "922:\tlearn: 0.0840040\ttotal: 1m 31s\tremaining: 7.6s\n",
      "923:\tlearn: 0.0839728\ttotal: 1m 31s\tremaining: 7.5s\n",
      "924:\tlearn: 0.0839494\ttotal: 1m 31s\tremaining: 7.4s\n",
      "925:\tlearn: 0.0839308\ttotal: 1m 31s\tremaining: 7.3s\n",
      "926:\tlearn: 0.0838985\ttotal: 1m 31s\tremaining: 7.2s\n",
      "927:\tlearn: 0.0838924\ttotal: 1m 31s\tremaining: 7.1s\n",
      "928:\tlearn: 0.0838326\ttotal: 1m 31s\tremaining: 7s\n",
      "929:\tlearn: 0.0837899\ttotal: 1m 31s\tremaining: 6.91s\n",
      "930:\tlearn: 0.0837534\ttotal: 1m 31s\tremaining: 6.81s\n",
      "931:\tlearn: 0.0837308\ttotal: 1m 31s\tremaining: 6.71s\n",
      "932:\tlearn: 0.0836582\ttotal: 1m 32s\tremaining: 6.61s\n",
      "933:\tlearn: 0.0836322\ttotal: 1m 32s\tremaining: 6.52s\n",
      "934:\tlearn: 0.0836091\ttotal: 1m 32s\tremaining: 6.42s\n",
      "935:\tlearn: 0.0835631\ttotal: 1m 32s\tremaining: 6.32s\n",
      "936:\tlearn: 0.0835241\ttotal: 1m 32s\tremaining: 6.22s\n",
      "937:\tlearn: 0.0835185\ttotal: 1m 32s\tremaining: 6.13s\n",
      "938:\tlearn: 0.0834941\ttotal: 1m 32s\tremaining: 6.03s\n",
      "939:\tlearn: 0.0834744\ttotal: 1m 32s\tremaining: 5.93s\n",
      "940:\tlearn: 0.0834348\ttotal: 1m 32s\tremaining: 5.83s\n",
      "941:\tlearn: 0.0834199\ttotal: 1m 33s\tremaining: 5.73s\n",
      "942:\tlearn: 0.0833884\ttotal: 1m 33s\tremaining: 5.63s\n",
      "943:\tlearn: 0.0833762\ttotal: 1m 33s\tremaining: 5.53s\n",
      "944:\tlearn: 0.0833449\ttotal: 1m 33s\tremaining: 5.43s\n",
      "945:\tlearn: 0.0832930\ttotal: 1m 33s\tremaining: 5.33s\n",
      "946:\tlearn: 0.0832396\ttotal: 1m 33s\tremaining: 5.23s\n",
      "947:\tlearn: 0.0832043\ttotal: 1m 33s\tremaining: 5.13s\n",
      "948:\tlearn: 0.0831565\ttotal: 1m 33s\tremaining: 5.03s\n",
      "949:\tlearn: 0.0831350\ttotal: 1m 33s\tremaining: 4.93s\n",
      "950:\tlearn: 0.0831013\ttotal: 1m 33s\tremaining: 4.83s\n",
      "951:\tlearn: 0.0830370\ttotal: 1m 33s\tremaining: 4.74s\n",
      "952:\tlearn: 0.0829900\ttotal: 1m 34s\tremaining: 4.64s\n",
      "953:\tlearn: 0.0829587\ttotal: 1m 34s\tremaining: 4.54s\n",
      "954:\tlearn: 0.0829541\ttotal: 1m 34s\tremaining: 4.44s\n",
      "955:\tlearn: 0.0829360\ttotal: 1m 34s\tremaining: 4.34s\n",
      "956:\tlearn: 0.0828946\ttotal: 1m 34s\tremaining: 4.24s\n",
      "957:\tlearn: 0.0828632\ttotal: 1m 34s\tremaining: 4.14s\n",
      "958:\tlearn: 0.0828385\ttotal: 1m 34s\tremaining: 4.04s\n",
      "959:\tlearn: 0.0828076\ttotal: 1m 34s\tremaining: 3.94s\n",
      "960:\tlearn: 0.0827560\ttotal: 1m 34s\tremaining: 3.85s\n",
      "961:\tlearn: 0.0827265\ttotal: 1m 34s\tremaining: 3.75s\n",
      "962:\tlearn: 0.0826841\ttotal: 1m 34s\tremaining: 3.65s\n",
      "963:\tlearn: 0.0826125\ttotal: 1m 35s\tremaining: 3.55s\n",
      "964:\tlearn: 0.0825614\ttotal: 1m 35s\tremaining: 3.45s\n",
      "965:\tlearn: 0.0825101\ttotal: 1m 35s\tremaining: 3.35s\n",
      "966:\tlearn: 0.0824345\ttotal: 1m 35s\tremaining: 3.25s\n",
      "967:\tlearn: 0.0824062\ttotal: 1m 35s\tremaining: 3.15s\n",
      "968:\tlearn: 0.0823855\ttotal: 1m 35s\tremaining: 3.06s\n",
      "969:\tlearn: 0.0823408\ttotal: 1m 35s\tremaining: 2.96s\n",
      "970:\tlearn: 0.0822999\ttotal: 1m 35s\tremaining: 2.86s\n",
      "971:\tlearn: 0.0822614\ttotal: 1m 35s\tremaining: 2.76s\n",
      "972:\tlearn: 0.0822224\ttotal: 1m 35s\tremaining: 2.66s\n",
      "973:\tlearn: 0.0821842\ttotal: 1m 35s\tremaining: 2.56s\n",
      "974:\tlearn: 0.0821246\ttotal: 1m 36s\tremaining: 2.46s\n",
      "975:\tlearn: 0.0821012\ttotal: 1m 36s\tremaining: 2.36s\n",
      "976:\tlearn: 0.0820600\ttotal: 1m 36s\tremaining: 2.27s\n",
      "977:\tlearn: 0.0820165\ttotal: 1m 36s\tremaining: 2.17s\n",
      "978:\tlearn: 0.0819582\ttotal: 1m 36s\tremaining: 2.07s\n",
      "979:\tlearn: 0.0819294\ttotal: 1m 36s\tremaining: 1.97s\n",
      "980:\tlearn: 0.0819049\ttotal: 1m 36s\tremaining: 1.87s\n",
      "981:\tlearn: 0.0818808\ttotal: 1m 36s\tremaining: 1.77s\n",
      "982:\tlearn: 0.0818464\ttotal: 1m 36s\tremaining: 1.67s\n",
      "983:\tlearn: 0.0818416\ttotal: 1m 36s\tremaining: 1.57s\n",
      "984:\tlearn: 0.0818161\ttotal: 1m 36s\tremaining: 1.48s\n",
      "985:\tlearn: 0.0817881\ttotal: 1m 37s\tremaining: 1.38s\n",
      "986:\tlearn: 0.0817652\ttotal: 1m 37s\tremaining: 1.28s\n",
      "987:\tlearn: 0.0817244\ttotal: 1m 37s\tremaining: 1.18s\n",
      "988:\tlearn: 0.0817086\ttotal: 1m 37s\tremaining: 1.08s\n",
      "989:\tlearn: 0.0816760\ttotal: 1m 37s\tremaining: 984ms\n",
      "990:\tlearn: 0.0816094\ttotal: 1m 37s\tremaining: 886ms\n",
      "991:\tlearn: 0.0815574\ttotal: 1m 37s\tremaining: 787ms\n",
      "992:\tlearn: 0.0815480\ttotal: 1m 37s\tremaining: 689ms\n",
      "993:\tlearn: 0.0814742\ttotal: 1m 37s\tremaining: 590ms\n",
      "994:\tlearn: 0.0814373\ttotal: 1m 37s\tremaining: 492ms\n",
      "995:\tlearn: 0.0814186\ttotal: 1m 37s\tremaining: 394ms\n",
      "996:\tlearn: 0.0813767\ttotal: 1m 38s\tremaining: 295ms\n",
      "997:\tlearn: 0.0813466\ttotal: 1m 38s\tremaining: 197ms\n",
      "998:\tlearn: 0.0813239\ttotal: 1m 38s\tremaining: 98.4ms\n",
      "999:\tlearn: 0.0812905\ttotal: 1m 38s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5247142\ttotal: 92.4ms\tremaining: 1m 32s\n",
      "1:\tlearn: 0.4290319\ttotal: 189ms\tremaining: 1m 34s\n",
      "2:\tlearn: 0.3662552\ttotal: 282ms\tremaining: 1m 33s\n",
      "3:\tlearn: 0.3285835\ttotal: 372ms\tremaining: 1m 32s\n",
      "4:\tlearn: 0.2969836\ttotal: 479ms\tremaining: 1m 35s\n",
      "5:\tlearn: 0.2747354\ttotal: 571ms\tremaining: 1m 34s\n",
      "6:\tlearn: 0.2611399\ttotal: 660ms\tremaining: 1m 33s\n",
      "7:\tlearn: 0.2520892\ttotal: 745ms\tremaining: 1m 32s\n",
      "8:\tlearn: 0.2410174\ttotal: 838ms\tremaining: 1m 32s\n",
      "9:\tlearn: 0.2324716\ttotal: 926ms\tremaining: 1m 31s\n",
      "10:\tlearn: 0.2217792\ttotal: 1.02s\tremaining: 1m 31s\n",
      "11:\tlearn: 0.2160220\ttotal: 1.12s\tremaining: 1m 31s\n",
      "12:\tlearn: 0.2115084\ttotal: 1.21s\tremaining: 1m 31s\n",
      "13:\tlearn: 0.2060661\ttotal: 1.31s\tremaining: 1m 32s\n",
      "14:\tlearn: 0.2027406\ttotal: 1.39s\tremaining: 1m 31s\n",
      "15:\tlearn: 0.2006642\ttotal: 1.52s\tremaining: 1m 33s\n",
      "16:\tlearn: 0.1979357\ttotal: 1.67s\tremaining: 1m 36s\n",
      "17:\tlearn: 0.1952674\ttotal: 1.78s\tremaining: 1m 37s\n",
      "18:\tlearn: 0.1926978\ttotal: 1.87s\tremaining: 1m 36s\n",
      "19:\tlearn: 0.1907816\ttotal: 1.96s\tremaining: 1m 36s\n",
      "20:\tlearn: 0.1880369\ttotal: 2.05s\tremaining: 1m 35s\n",
      "21:\tlearn: 0.1866016\ttotal: 2.15s\tremaining: 1m 35s\n",
      "22:\tlearn: 0.1846220\ttotal: 2.26s\tremaining: 1m 36s\n",
      "23:\tlearn: 0.1823096\ttotal: 2.37s\tremaining: 1m 36s\n",
      "24:\tlearn: 0.1804778\ttotal: 2.47s\tremaining: 1m 36s\n",
      "25:\tlearn: 0.1793183\ttotal: 2.57s\tremaining: 1m 36s\n",
      "26:\tlearn: 0.1777770\ttotal: 2.66s\tremaining: 1m 35s\n",
      "27:\tlearn: 0.1764437\ttotal: 2.77s\tremaining: 1m 36s\n",
      "28:\tlearn: 0.1752544\ttotal: 2.86s\tremaining: 1m 35s\n",
      "29:\tlearn: 0.1742391\ttotal: 2.95s\tremaining: 1m 35s\n",
      "30:\tlearn: 0.1729539\ttotal: 3.06s\tremaining: 1m 35s\n",
      "31:\tlearn: 0.1712130\ttotal: 3.17s\tremaining: 1m 35s\n",
      "32:\tlearn: 0.1700782\ttotal: 3.27s\tremaining: 1m 35s\n",
      "33:\tlearn: 0.1691012\ttotal: 3.38s\tremaining: 1m 36s\n",
      "34:\tlearn: 0.1682603\ttotal: 3.48s\tremaining: 1m 36s\n",
      "35:\tlearn: 0.1673643\ttotal: 3.6s\tremaining: 1m 36s\n",
      "36:\tlearn: 0.1665535\ttotal: 3.7s\tremaining: 1m 36s\n",
      "37:\tlearn: 0.1656842\ttotal: 3.8s\tremaining: 1m 36s\n",
      "38:\tlearn: 0.1648665\ttotal: 3.89s\tremaining: 1m 35s\n",
      "39:\tlearn: 0.1633246\ttotal: 4.01s\tremaining: 1m 36s\n",
      "40:\tlearn: 0.1625337\ttotal: 4.1s\tremaining: 1m 35s\n",
      "41:\tlearn: 0.1618969\ttotal: 4.2s\tremaining: 1m 35s\n",
      "42:\tlearn: 0.1614084\ttotal: 4.29s\tremaining: 1m 35s\n",
      "43:\tlearn: 0.1609816\ttotal: 4.38s\tremaining: 1m 35s\n",
      "44:\tlearn: 0.1605900\ttotal: 4.46s\tremaining: 1m 34s\n",
      "45:\tlearn: 0.1600429\ttotal: 4.56s\tremaining: 1m 34s\n",
      "46:\tlearn: 0.1593968\ttotal: 4.66s\tremaining: 1m 34s\n",
      "47:\tlearn: 0.1589328\ttotal: 4.75s\tremaining: 1m 34s\n",
      "48:\tlearn: 0.1580936\ttotal: 4.84s\tremaining: 1m 33s\n",
      "49:\tlearn: 0.1572806\ttotal: 4.94s\tremaining: 1m 33s\n",
      "50:\tlearn: 0.1566601\ttotal: 5.03s\tremaining: 1m 33s\n",
      "51:\tlearn: 0.1559921\ttotal: 5.15s\tremaining: 1m 33s\n",
      "52:\tlearn: 0.1554978\ttotal: 5.26s\tremaining: 1m 34s\n",
      "53:\tlearn: 0.1549274\ttotal: 5.38s\tremaining: 1m 34s\n",
      "54:\tlearn: 0.1544377\ttotal: 5.49s\tremaining: 1m 34s\n",
      "55:\tlearn: 0.1537820\ttotal: 5.6s\tremaining: 1m 34s\n",
      "56:\tlearn: 0.1532621\ttotal: 5.7s\tremaining: 1m 34s\n",
      "57:\tlearn: 0.1529369\ttotal: 5.79s\tremaining: 1m 34s\n",
      "58:\tlearn: 0.1524606\ttotal: 5.9s\tremaining: 1m 34s\n",
      "59:\tlearn: 0.1521997\ttotal: 5.99s\tremaining: 1m 33s\n",
      "60:\tlearn: 0.1517807\ttotal: 6.09s\tremaining: 1m 33s\n",
      "61:\tlearn: 0.1512709\ttotal: 6.17s\tremaining: 1m 33s\n",
      "62:\tlearn: 0.1507872\ttotal: 6.27s\tremaining: 1m 33s\n",
      "63:\tlearn: 0.1502769\ttotal: 6.36s\tremaining: 1m 33s\n",
      "64:\tlearn: 0.1497003\ttotal: 6.48s\tremaining: 1m 33s\n",
      "65:\tlearn: 0.1493667\ttotal: 6.58s\tremaining: 1m 33s\n",
      "66:\tlearn: 0.1490641\ttotal: 6.68s\tremaining: 1m 33s\n",
      "67:\tlearn: 0.1487439\ttotal: 6.78s\tremaining: 1m 32s\n",
      "68:\tlearn: 0.1482392\ttotal: 6.89s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1478802\ttotal: 6.99s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1474656\ttotal: 7.08s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1472309\ttotal: 7.18s\tremaining: 1m 32s\n",
      "72:\tlearn: 0.1469294\ttotal: 7.28s\tremaining: 1m 32s\n",
      "73:\tlearn: 0.1466011\ttotal: 7.38s\tremaining: 1m 32s\n",
      "74:\tlearn: 0.1462527\ttotal: 7.47s\tremaining: 1m 32s\n",
      "75:\tlearn: 0.1459718\ttotal: 7.57s\tremaining: 1m 32s\n",
      "76:\tlearn: 0.1456238\ttotal: 7.67s\tremaining: 1m 31s\n",
      "77:\tlearn: 0.1453612\ttotal: 7.77s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1450347\ttotal: 7.86s\tremaining: 1m 31s\n",
      "79:\tlearn: 0.1447586\ttotal: 7.95s\tremaining: 1m 31s\n",
      "80:\tlearn: 0.1445564\ttotal: 8.04s\tremaining: 1m 31s\n",
      "81:\tlearn: 0.1442456\ttotal: 8.13s\tremaining: 1m 31s\n",
      "82:\tlearn: 0.1440459\ttotal: 8.23s\tremaining: 1m 30s\n",
      "83:\tlearn: 0.1436258\ttotal: 8.33s\tremaining: 1m 30s\n",
      "84:\tlearn: 0.1433357\ttotal: 8.43s\tremaining: 1m 30s\n",
      "85:\tlearn: 0.1430034\ttotal: 8.53s\tremaining: 1m 30s\n",
      "86:\tlearn: 0.1428288\ttotal: 8.63s\tremaining: 1m 30s\n",
      "87:\tlearn: 0.1425692\ttotal: 8.72s\tremaining: 1m 30s\n",
      "88:\tlearn: 0.1423890\ttotal: 8.8s\tremaining: 1m 30s\n",
      "89:\tlearn: 0.1421052\ttotal: 8.89s\tremaining: 1m 29s\n",
      "90:\tlearn: 0.1418699\ttotal: 8.99s\tremaining: 1m 29s\n",
      "91:\tlearn: 0.1415642\ttotal: 9.09s\tremaining: 1m 29s\n",
      "92:\tlearn: 0.1413346\ttotal: 9.19s\tremaining: 1m 29s\n",
      "93:\tlearn: 0.1411255\ttotal: 9.3s\tremaining: 1m 29s\n",
      "94:\tlearn: 0.1408647\ttotal: 9.45s\tremaining: 1m 30s\n",
      "95:\tlearn: 0.1406386\ttotal: 9.55s\tremaining: 1m 29s\n",
      "96:\tlearn: 0.1404526\ttotal: 9.64s\tremaining: 1m 29s\n",
      "97:\tlearn: 0.1403146\ttotal: 9.82s\tremaining: 1m 30s\n",
      "98:\tlearn: 0.1399154\ttotal: 9.95s\tremaining: 1m 30s\n",
      "99:\tlearn: 0.1396737\ttotal: 10s\tremaining: 1m 30s\n",
      "100:\tlearn: 0.1393900\ttotal: 10.1s\tremaining: 1m 30s\n",
      "101:\tlearn: 0.1390799\ttotal: 10.2s\tremaining: 1m 30s\n",
      "102:\tlearn: 0.1388730\ttotal: 10.4s\tremaining: 1m 30s\n",
      "103:\tlearn: 0.1386459\ttotal: 10.5s\tremaining: 1m 30s\n",
      "104:\tlearn: 0.1382075\ttotal: 10.6s\tremaining: 1m 30s\n",
      "105:\tlearn: 0.1380331\ttotal: 10.7s\tremaining: 1m 30s\n",
      "106:\tlearn: 0.1378826\ttotal: 10.8s\tremaining: 1m 29s\n",
      "107:\tlearn: 0.1376540\ttotal: 10.9s\tremaining: 1m 29s\n",
      "108:\tlearn: 0.1374066\ttotal: 11s\tremaining: 1m 29s\n",
      "109:\tlearn: 0.1371449\ttotal: 11.1s\tremaining: 1m 29s\n",
      "110:\tlearn: 0.1368167\ttotal: 11.2s\tremaining: 1m 29s\n",
      "111:\tlearn: 0.1366158\ttotal: 11.2s\tremaining: 1m 29s\n",
      "112:\tlearn: 0.1363906\ttotal: 11.3s\tremaining: 1m 29s\n",
      "113:\tlearn: 0.1362661\ttotal: 11.4s\tremaining: 1m 28s\n",
      "114:\tlearn: 0.1359261\ttotal: 11.5s\tremaining: 1m 28s\n",
      "115:\tlearn: 0.1356938\ttotal: 11.6s\tremaining: 1m 28s\n",
      "116:\tlearn: 0.1355054\ttotal: 11.7s\tremaining: 1m 28s\n",
      "117:\tlearn: 0.1352392\ttotal: 11.8s\tremaining: 1m 28s\n",
      "118:\tlearn: 0.1350467\ttotal: 11.9s\tremaining: 1m 27s\n",
      "119:\tlearn: 0.1348777\ttotal: 12s\tremaining: 1m 27s\n",
      "120:\tlearn: 0.1347154\ttotal: 12.1s\tremaining: 1m 27s\n",
      "121:\tlearn: 0.1345329\ttotal: 12.2s\tremaining: 1m 27s\n",
      "122:\tlearn: 0.1342786\ttotal: 12.3s\tremaining: 1m 27s\n",
      "123:\tlearn: 0.1341378\ttotal: 12.4s\tremaining: 1m 27s\n",
      "124:\tlearn: 0.1339675\ttotal: 12.5s\tremaining: 1m 27s\n",
      "125:\tlearn: 0.1337047\ttotal: 12.6s\tremaining: 1m 27s\n",
      "126:\tlearn: 0.1334960\ttotal: 12.7s\tremaining: 1m 27s\n",
      "127:\tlearn: 0.1332800\ttotal: 12.8s\tremaining: 1m 27s\n",
      "128:\tlearn: 0.1330483\ttotal: 12.9s\tremaining: 1m 27s\n",
      "129:\tlearn: 0.1327978\ttotal: 13s\tremaining: 1m 27s\n",
      "130:\tlearn: 0.1326519\ttotal: 13.1s\tremaining: 1m 27s\n",
      "131:\tlearn: 0.1324372\ttotal: 13.2s\tremaining: 1m 26s\n",
      "132:\tlearn: 0.1322280\ttotal: 13.3s\tremaining: 1m 26s\n",
      "133:\tlearn: 0.1320908\ttotal: 13.4s\tremaining: 1m 26s\n",
      "134:\tlearn: 0.1319449\ttotal: 13.5s\tremaining: 1m 26s\n",
      "135:\tlearn: 0.1317987\ttotal: 13.6s\tremaining: 1m 26s\n",
      "136:\tlearn: 0.1315724\ttotal: 13.7s\tremaining: 1m 26s\n",
      "137:\tlearn: 0.1313807\ttotal: 13.8s\tremaining: 1m 26s\n",
      "138:\tlearn: 0.1312960\ttotal: 13.9s\tremaining: 1m 26s\n",
      "139:\tlearn: 0.1310957\ttotal: 14.1s\tremaining: 1m 26s\n",
      "140:\tlearn: 0.1309753\ttotal: 14.1s\tremaining: 1m 26s\n",
      "141:\tlearn: 0.1307997\ttotal: 14.2s\tremaining: 1m 26s\n",
      "142:\tlearn: 0.1306800\ttotal: 14.3s\tremaining: 1m 25s\n",
      "143:\tlearn: 0.1305669\ttotal: 14.4s\tremaining: 1m 25s\n",
      "144:\tlearn: 0.1304110\ttotal: 14.5s\tremaining: 1m 25s\n",
      "145:\tlearn: 0.1302813\ttotal: 14.6s\tremaining: 1m 25s\n",
      "146:\tlearn: 0.1300555\ttotal: 14.7s\tremaining: 1m 25s\n",
      "147:\tlearn: 0.1299303\ttotal: 14.8s\tremaining: 1m 25s\n",
      "148:\tlearn: 0.1297095\ttotal: 14.9s\tremaining: 1m 25s\n",
      "149:\tlearn: 0.1295947\ttotal: 15s\tremaining: 1m 25s\n",
      "150:\tlearn: 0.1294462\ttotal: 15.1s\tremaining: 1m 25s\n",
      "151:\tlearn: 0.1292897\ttotal: 15.2s\tremaining: 1m 24s\n",
      "152:\tlearn: 0.1291531\ttotal: 15.3s\tremaining: 1m 24s\n",
      "153:\tlearn: 0.1289635\ttotal: 15.4s\tremaining: 1m 24s\n",
      "154:\tlearn: 0.1287964\ttotal: 15.5s\tremaining: 1m 24s\n",
      "155:\tlearn: 0.1286321\ttotal: 15.6s\tremaining: 1m 24s\n",
      "156:\tlearn: 0.1285308\ttotal: 15.7s\tremaining: 1m 24s\n",
      "157:\tlearn: 0.1284044\ttotal: 15.8s\tremaining: 1m 24s\n",
      "158:\tlearn: 0.1282614\ttotal: 15.9s\tremaining: 1m 24s\n",
      "159:\tlearn: 0.1281640\ttotal: 16.1s\tremaining: 1m 24s\n",
      "160:\tlearn: 0.1278987\ttotal: 16.2s\tremaining: 1m 24s\n",
      "161:\tlearn: 0.1277917\ttotal: 16.3s\tremaining: 1m 24s\n",
      "162:\tlearn: 0.1276562\ttotal: 16.4s\tremaining: 1m 24s\n",
      "163:\tlearn: 0.1275424\ttotal: 16.5s\tremaining: 1m 24s\n",
      "164:\tlearn: 0.1273804\ttotal: 16.6s\tremaining: 1m 23s\n",
      "165:\tlearn: 0.1272900\ttotal: 16.7s\tremaining: 1m 23s\n",
      "166:\tlearn: 0.1271990\ttotal: 16.8s\tremaining: 1m 23s\n",
      "167:\tlearn: 0.1270662\ttotal: 16.9s\tremaining: 1m 23s\n",
      "168:\tlearn: 0.1269807\ttotal: 17s\tremaining: 1m 23s\n",
      "169:\tlearn: 0.1268347\ttotal: 17.1s\tremaining: 1m 23s\n",
      "170:\tlearn: 0.1267478\ttotal: 17.2s\tremaining: 1m 23s\n",
      "171:\tlearn: 0.1265774\ttotal: 17.3s\tremaining: 1m 23s\n",
      "172:\tlearn: 0.1264081\ttotal: 17.4s\tremaining: 1m 23s\n",
      "173:\tlearn: 0.1262483\ttotal: 17.5s\tremaining: 1m 22s\n",
      "174:\tlearn: 0.1261476\ttotal: 17.6s\tremaining: 1m 22s\n",
      "175:\tlearn: 0.1259788\ttotal: 17.7s\tremaining: 1m 22s\n",
      "176:\tlearn: 0.1257798\ttotal: 17.8s\tremaining: 1m 22s\n",
      "177:\tlearn: 0.1256822\ttotal: 17.9s\tremaining: 1m 22s\n",
      "178:\tlearn: 0.1255589\ttotal: 18s\tremaining: 1m 22s\n",
      "179:\tlearn: 0.1254087\ttotal: 18.1s\tremaining: 1m 22s\n",
      "180:\tlearn: 0.1253234\ttotal: 18.2s\tremaining: 1m 22s\n",
      "181:\tlearn: 0.1251857\ttotal: 18.3s\tremaining: 1m 22s\n",
      "182:\tlearn: 0.1249929\ttotal: 18.4s\tremaining: 1m 22s\n",
      "183:\tlearn: 0.1248791\ttotal: 18.5s\tremaining: 1m 22s\n",
      "184:\tlearn: 0.1247574\ttotal: 18.6s\tremaining: 1m 22s\n",
      "185:\tlearn: 0.1246110\ttotal: 18.7s\tremaining: 1m 21s\n",
      "186:\tlearn: 0.1244564\ttotal: 18.8s\tremaining: 1m 21s\n",
      "187:\tlearn: 0.1243859\ttotal: 18.9s\tremaining: 1m 21s\n",
      "188:\tlearn: 0.1242768\ttotal: 19s\tremaining: 1m 21s\n",
      "189:\tlearn: 0.1241866\ttotal: 19.1s\tremaining: 1m 21s\n",
      "190:\tlearn: 0.1240816\ttotal: 19.2s\tremaining: 1m 21s\n",
      "191:\tlearn: 0.1239623\ttotal: 19.4s\tremaining: 1m 21s\n",
      "192:\tlearn: 0.1238748\ttotal: 19.5s\tremaining: 1m 21s\n",
      "193:\tlearn: 0.1237862\ttotal: 19.6s\tremaining: 1m 21s\n",
      "194:\tlearn: 0.1236979\ttotal: 19.7s\tremaining: 1m 21s\n",
      "195:\tlearn: 0.1235935\ttotal: 19.8s\tremaining: 1m 21s\n",
      "196:\tlearn: 0.1233832\ttotal: 19.9s\tremaining: 1m 21s\n",
      "197:\tlearn: 0.1232541\ttotal: 20s\tremaining: 1m 21s\n",
      "198:\tlearn: 0.1230974\ttotal: 20.1s\tremaining: 1m 20s\n",
      "199:\tlearn: 0.1230000\ttotal: 20.2s\tremaining: 1m 20s\n",
      "200:\tlearn: 0.1228879\ttotal: 20.3s\tremaining: 1m 20s\n",
      "201:\tlearn: 0.1227618\ttotal: 20.4s\tremaining: 1m 20s\n",
      "202:\tlearn: 0.1226634\ttotal: 20.5s\tremaining: 1m 20s\n",
      "203:\tlearn: 0.1225659\ttotal: 20.6s\tremaining: 1m 20s\n",
      "204:\tlearn: 0.1224876\ttotal: 20.7s\tremaining: 1m 20s\n",
      "205:\tlearn: 0.1224038\ttotal: 20.9s\tremaining: 1m 20s\n",
      "206:\tlearn: 0.1222986\ttotal: 21s\tremaining: 1m 20s\n",
      "207:\tlearn: 0.1221665\ttotal: 21.1s\tremaining: 1m 20s\n",
      "208:\tlearn: 0.1220021\ttotal: 21.2s\tremaining: 1m 20s\n",
      "209:\tlearn: 0.1218890\ttotal: 21.3s\tremaining: 1m 19s\n",
      "210:\tlearn: 0.1218059\ttotal: 21.3s\tremaining: 1m 19s\n",
      "211:\tlearn: 0.1217227\ttotal: 21.5s\tremaining: 1m 19s\n",
      "212:\tlearn: 0.1216273\ttotal: 21.6s\tremaining: 1m 19s\n",
      "213:\tlearn: 0.1214810\ttotal: 21.7s\tremaining: 1m 19s\n",
      "214:\tlearn: 0.1213770\ttotal: 21.9s\tremaining: 1m 19s\n",
      "215:\tlearn: 0.1212872\ttotal: 22s\tremaining: 1m 19s\n",
      "216:\tlearn: 0.1212118\ttotal: 22.1s\tremaining: 1m 19s\n",
      "217:\tlearn: 0.1211335\ttotal: 22.2s\tremaining: 1m 19s\n",
      "218:\tlearn: 0.1210574\ttotal: 22.3s\tremaining: 1m 19s\n",
      "219:\tlearn: 0.1208791\ttotal: 22.4s\tremaining: 1m 19s\n",
      "220:\tlearn: 0.1207165\ttotal: 22.5s\tremaining: 1m 19s\n",
      "221:\tlearn: 0.1206384\ttotal: 22.6s\tremaining: 1m 19s\n",
      "222:\tlearn: 0.1205391\ttotal: 22.7s\tremaining: 1m 19s\n",
      "223:\tlearn: 0.1204514\ttotal: 22.8s\tremaining: 1m 19s\n",
      "224:\tlearn: 0.1203252\ttotal: 22.9s\tremaining: 1m 18s\n",
      "225:\tlearn: 0.1202140\ttotal: 23s\tremaining: 1m 18s\n",
      "226:\tlearn: 0.1201345\ttotal: 23.1s\tremaining: 1m 18s\n",
      "227:\tlearn: 0.1200165\ttotal: 23.2s\tremaining: 1m 18s\n",
      "228:\tlearn: 0.1199505\ttotal: 23.3s\tremaining: 1m 18s\n",
      "229:\tlearn: 0.1198620\ttotal: 23.4s\tremaining: 1m 18s\n",
      "230:\tlearn: 0.1197694\ttotal: 23.5s\tremaining: 1m 18s\n",
      "231:\tlearn: 0.1196419\ttotal: 23.6s\tremaining: 1m 18s\n",
      "232:\tlearn: 0.1195453\ttotal: 23.7s\tremaining: 1m 18s\n",
      "233:\tlearn: 0.1194415\ttotal: 23.8s\tremaining: 1m 17s\n",
      "234:\tlearn: 0.1193641\ttotal: 23.9s\tremaining: 1m 17s\n",
      "235:\tlearn: 0.1192952\ttotal: 24s\tremaining: 1m 17s\n",
      "236:\tlearn: 0.1191535\ttotal: 24.1s\tremaining: 1m 17s\n",
      "237:\tlearn: 0.1190850\ttotal: 24.2s\tremaining: 1m 17s\n",
      "238:\tlearn: 0.1189564\ttotal: 24.3s\tremaining: 1m 17s\n",
      "239:\tlearn: 0.1188672\ttotal: 24.4s\tremaining: 1m 17s\n",
      "240:\tlearn: 0.1188024\ttotal: 24.5s\tremaining: 1m 17s\n",
      "241:\tlearn: 0.1186929\ttotal: 24.6s\tremaining: 1m 16s\n",
      "242:\tlearn: 0.1186097\ttotal: 24.7s\tremaining: 1m 16s\n",
      "243:\tlearn: 0.1185232\ttotal: 24.8s\tremaining: 1m 16s\n",
      "244:\tlearn: 0.1183942\ttotal: 24.9s\tremaining: 1m 16s\n",
      "245:\tlearn: 0.1183201\ttotal: 24.9s\tremaining: 1m 16s\n",
      "246:\tlearn: 0.1182528\ttotal: 25s\tremaining: 1m 16s\n",
      "247:\tlearn: 0.1181763\ttotal: 25.1s\tremaining: 1m 16s\n",
      "248:\tlearn: 0.1181012\ttotal: 25.2s\tremaining: 1m 16s\n",
      "249:\tlearn: 0.1180398\ttotal: 25.3s\tremaining: 1m 15s\n",
      "250:\tlearn: 0.1179590\ttotal: 25.4s\tremaining: 1m 15s\n",
      "251:\tlearn: 0.1178490\ttotal: 25.5s\tremaining: 1m 15s\n",
      "252:\tlearn: 0.1177588\ttotal: 25.6s\tremaining: 1m 15s\n",
      "253:\tlearn: 0.1176626\ttotal: 25.7s\tremaining: 1m 15s\n",
      "254:\tlearn: 0.1175720\ttotal: 25.8s\tremaining: 1m 15s\n",
      "255:\tlearn: 0.1174761\ttotal: 25.9s\tremaining: 1m 15s\n",
      "256:\tlearn: 0.1173988\ttotal: 25.9s\tremaining: 1m 15s\n",
      "257:\tlearn: 0.1173485\ttotal: 26.1s\tremaining: 1m 14s\n",
      "258:\tlearn: 0.1173083\ttotal: 26.2s\tremaining: 1m 14s\n",
      "259:\tlearn: 0.1172119\ttotal: 26.2s\tremaining: 1m 14s\n",
      "260:\tlearn: 0.1170693\ttotal: 26.3s\tremaining: 1m 14s\n",
      "261:\tlearn: 0.1170081\ttotal: 26.4s\tremaining: 1m 14s\n",
      "262:\tlearn: 0.1169300\ttotal: 26.5s\tremaining: 1m 14s\n",
      "263:\tlearn: 0.1168331\ttotal: 26.6s\tremaining: 1m 14s\n",
      "264:\tlearn: 0.1167321\ttotal: 26.7s\tremaining: 1m 14s\n",
      "265:\tlearn: 0.1166709\ttotal: 26.8s\tremaining: 1m 13s\n",
      "266:\tlearn: 0.1165897\ttotal: 26.9s\tremaining: 1m 13s\n",
      "267:\tlearn: 0.1164707\ttotal: 27s\tremaining: 1m 13s\n",
      "268:\tlearn: 0.1163407\ttotal: 27.1s\tremaining: 1m 13s\n",
      "269:\tlearn: 0.1162691\ttotal: 27.2s\tremaining: 1m 13s\n",
      "270:\tlearn: 0.1161864\ttotal: 27.3s\tremaining: 1m 13s\n",
      "271:\tlearn: 0.1161165\ttotal: 27.3s\tremaining: 1m 13s\n",
      "272:\tlearn: 0.1160044\ttotal: 27.4s\tremaining: 1m 13s\n",
      "273:\tlearn: 0.1158990\ttotal: 27.5s\tremaining: 1m 12s\n",
      "274:\tlearn: 0.1158360\ttotal: 27.6s\tremaining: 1m 12s\n",
      "275:\tlearn: 0.1157836\ttotal: 27.7s\tremaining: 1m 12s\n",
      "276:\tlearn: 0.1156688\ttotal: 27.8s\tremaining: 1m 12s\n",
      "277:\tlearn: 0.1155881\ttotal: 27.9s\tremaining: 1m 12s\n",
      "278:\tlearn: 0.1154832\ttotal: 28s\tremaining: 1m 12s\n",
      "279:\tlearn: 0.1153797\ttotal: 28.1s\tremaining: 1m 12s\n",
      "280:\tlearn: 0.1152845\ttotal: 28.2s\tremaining: 1m 12s\n",
      "281:\tlearn: 0.1152215\ttotal: 28.3s\tremaining: 1m 12s\n",
      "282:\tlearn: 0.1151648\ttotal: 28.4s\tremaining: 1m 11s\n",
      "283:\tlearn: 0.1150675\ttotal: 28.5s\tremaining: 1m 11s\n",
      "284:\tlearn: 0.1149638\ttotal: 28.6s\tremaining: 1m 11s\n",
      "285:\tlearn: 0.1148778\ttotal: 28.7s\tremaining: 1m 11s\n",
      "286:\tlearn: 0.1148189\ttotal: 28.8s\tremaining: 1m 11s\n",
      "287:\tlearn: 0.1147735\ttotal: 28.9s\tremaining: 1m 11s\n",
      "288:\tlearn: 0.1147135\ttotal: 29s\tremaining: 1m 11s\n",
      "289:\tlearn: 0.1146410\ttotal: 29.1s\tremaining: 1m 11s\n",
      "290:\tlearn: 0.1145926\ttotal: 29.2s\tremaining: 1m 11s\n",
      "291:\tlearn: 0.1145223\ttotal: 29.3s\tremaining: 1m 10s\n",
      "292:\tlearn: 0.1144283\ttotal: 29.4s\tremaining: 1m 10s\n",
      "293:\tlearn: 0.1143778\ttotal: 29.4s\tremaining: 1m 10s\n",
      "294:\tlearn: 0.1142480\ttotal: 29.5s\tremaining: 1m 10s\n",
      "295:\tlearn: 0.1142257\ttotal: 29.6s\tremaining: 1m 10s\n",
      "296:\tlearn: 0.1141463\ttotal: 29.7s\tremaining: 1m 10s\n",
      "297:\tlearn: 0.1140559\ttotal: 29.8s\tremaining: 1m 10s\n",
      "298:\tlearn: 0.1140196\ttotal: 30s\tremaining: 1m 10s\n",
      "299:\tlearn: 0.1139539\ttotal: 30.1s\tremaining: 1m 10s\n",
      "300:\tlearn: 0.1138379\ttotal: 30.2s\tremaining: 1m 10s\n",
      "301:\tlearn: 0.1137363\ttotal: 30.3s\tremaining: 1m 10s\n",
      "302:\tlearn: 0.1136611\ttotal: 30.4s\tremaining: 1m 9s\n",
      "303:\tlearn: 0.1135326\ttotal: 30.5s\tremaining: 1m 9s\n",
      "304:\tlearn: 0.1134628\ttotal: 30.6s\tremaining: 1m 9s\n",
      "305:\tlearn: 0.1134153\ttotal: 30.7s\tremaining: 1m 9s\n",
      "306:\tlearn: 0.1133590\ttotal: 30.8s\tremaining: 1m 9s\n",
      "307:\tlearn: 0.1132577\ttotal: 30.9s\tremaining: 1m 9s\n",
      "308:\tlearn: 0.1131683\ttotal: 31s\tremaining: 1m 9s\n",
      "309:\tlearn: 0.1131015\ttotal: 31.1s\tremaining: 1m 9s\n",
      "310:\tlearn: 0.1130395\ttotal: 31.2s\tremaining: 1m 9s\n",
      "311:\tlearn: 0.1129564\ttotal: 31.3s\tremaining: 1m 8s\n",
      "312:\tlearn: 0.1128990\ttotal: 31.4s\tremaining: 1m 8s\n",
      "313:\tlearn: 0.1128224\ttotal: 31.4s\tremaining: 1m 8s\n",
      "314:\tlearn: 0.1127848\ttotal: 31.5s\tremaining: 1m 8s\n",
      "315:\tlearn: 0.1127165\ttotal: 31.6s\tremaining: 1m 8s\n",
      "316:\tlearn: 0.1126764\ttotal: 31.7s\tremaining: 1m 8s\n",
      "317:\tlearn: 0.1126074\ttotal: 31.8s\tremaining: 1m 8s\n",
      "318:\tlearn: 0.1125427\ttotal: 31.9s\tremaining: 1m 8s\n",
      "319:\tlearn: 0.1125312\ttotal: 32s\tremaining: 1m 8s\n",
      "320:\tlearn: 0.1124177\ttotal: 32.2s\tremaining: 1m 8s\n",
      "321:\tlearn: 0.1123354\ttotal: 32.3s\tremaining: 1m 7s\n",
      "322:\tlearn: 0.1122505\ttotal: 32.4s\tremaining: 1m 7s\n",
      "323:\tlearn: 0.1121877\ttotal: 32.5s\tremaining: 1m 7s\n",
      "324:\tlearn: 0.1121278\ttotal: 32.6s\tremaining: 1m 7s\n",
      "325:\tlearn: 0.1120654\ttotal: 32.7s\tremaining: 1m 7s\n",
      "326:\tlearn: 0.1119621\ttotal: 32.9s\tremaining: 1m 7s\n",
      "327:\tlearn: 0.1118909\ttotal: 32.9s\tremaining: 1m 7s\n",
      "328:\tlearn: 0.1117945\ttotal: 33s\tremaining: 1m 7s\n",
      "329:\tlearn: 0.1117305\ttotal: 33.1s\tremaining: 1m 7s\n",
      "330:\tlearn: 0.1116406\ttotal: 33.2s\tremaining: 1m 7s\n",
      "331:\tlearn: 0.1115625\ttotal: 33.3s\tremaining: 1m 7s\n",
      "332:\tlearn: 0.1114836\ttotal: 33.4s\tremaining: 1m 6s\n",
      "333:\tlearn: 0.1113890\ttotal: 33.5s\tremaining: 1m 6s\n",
      "334:\tlearn: 0.1113620\ttotal: 33.6s\tremaining: 1m 6s\n",
      "335:\tlearn: 0.1113127\ttotal: 33.7s\tremaining: 1m 6s\n",
      "336:\tlearn: 0.1112471\ttotal: 33.8s\tremaining: 1m 6s\n",
      "337:\tlearn: 0.1111673\ttotal: 33.9s\tremaining: 1m 6s\n",
      "338:\tlearn: 0.1111143\ttotal: 34s\tremaining: 1m 6s\n",
      "339:\tlearn: 0.1110427\ttotal: 34.1s\tremaining: 1m 6s\n",
      "340:\tlearn: 0.1109699\ttotal: 34.1s\tremaining: 1m 5s\n",
      "341:\tlearn: 0.1109105\ttotal: 34.2s\tremaining: 1m 5s\n",
      "342:\tlearn: 0.1108035\ttotal: 34.3s\tremaining: 1m 5s\n",
      "343:\tlearn: 0.1107678\ttotal: 34.4s\tremaining: 1m 5s\n",
      "344:\tlearn: 0.1107121\ttotal: 34.5s\tremaining: 1m 5s\n",
      "345:\tlearn: 0.1106216\ttotal: 34.6s\tremaining: 1m 5s\n",
      "346:\tlearn: 0.1105299\ttotal: 34.7s\tremaining: 1m 5s\n",
      "347:\tlearn: 0.1104339\ttotal: 34.8s\tremaining: 1m 5s\n",
      "348:\tlearn: 0.1103471\ttotal: 34.9s\tremaining: 1m 5s\n",
      "349:\tlearn: 0.1102855\ttotal: 35s\tremaining: 1m 4s\n",
      "350:\tlearn: 0.1102040\ttotal: 35.1s\tremaining: 1m 4s\n",
      "351:\tlearn: 0.1101308\ttotal: 35.2s\tremaining: 1m 4s\n",
      "352:\tlearn: 0.1100712\ttotal: 35.3s\tremaining: 1m 4s\n",
      "353:\tlearn: 0.1099928\ttotal: 35.4s\tremaining: 1m 4s\n",
      "354:\tlearn: 0.1099539\ttotal: 35.4s\tremaining: 1m 4s\n",
      "355:\tlearn: 0.1099014\ttotal: 35.5s\tremaining: 1m 4s\n",
      "356:\tlearn: 0.1098632\ttotal: 35.6s\tremaining: 1m 4s\n",
      "357:\tlearn: 0.1097862\ttotal: 35.7s\tremaining: 1m 4s\n",
      "358:\tlearn: 0.1097106\ttotal: 35.8s\tremaining: 1m 3s\n",
      "359:\tlearn: 0.1096077\ttotal: 35.9s\tremaining: 1m 3s\n",
      "360:\tlearn: 0.1095293\ttotal: 36s\tremaining: 1m 3s\n",
      "361:\tlearn: 0.1094603\ttotal: 36.1s\tremaining: 1m 3s\n",
      "362:\tlearn: 0.1093909\ttotal: 36.2s\tremaining: 1m 3s\n",
      "363:\tlearn: 0.1093445\ttotal: 36.3s\tremaining: 1m 3s\n",
      "364:\tlearn: 0.1092544\ttotal: 36.4s\tremaining: 1m 3s\n",
      "365:\tlearn: 0.1091960\ttotal: 36.4s\tremaining: 1m 3s\n",
      "366:\tlearn: 0.1091193\ttotal: 36.5s\tremaining: 1m 3s\n",
      "367:\tlearn: 0.1090714\ttotal: 36.6s\tremaining: 1m 2s\n",
      "368:\tlearn: 0.1090255\ttotal: 36.7s\tremaining: 1m 2s\n",
      "369:\tlearn: 0.1089614\ttotal: 36.8s\tremaining: 1m 2s\n",
      "370:\tlearn: 0.1088988\ttotal: 36.9s\tremaining: 1m 2s\n",
      "371:\tlearn: 0.1088537\ttotal: 37s\tremaining: 1m 2s\n",
      "372:\tlearn: 0.1088012\ttotal: 37.1s\tremaining: 1m 2s\n",
      "373:\tlearn: 0.1087195\ttotal: 37.2s\tremaining: 1m 2s\n",
      "374:\tlearn: 0.1086712\ttotal: 37.3s\tremaining: 1m 2s\n",
      "375:\tlearn: 0.1085692\ttotal: 37.4s\tremaining: 1m 2s\n",
      "376:\tlearn: 0.1085141\ttotal: 37.5s\tremaining: 1m 1s\n",
      "377:\tlearn: 0.1084217\ttotal: 37.5s\tremaining: 1m 1s\n",
      "378:\tlearn: 0.1083925\ttotal: 37.6s\tremaining: 1m 1s\n",
      "379:\tlearn: 0.1083423\ttotal: 37.7s\tremaining: 1m 1s\n",
      "380:\tlearn: 0.1082509\ttotal: 37.8s\tremaining: 1m 1s\n",
      "381:\tlearn: 0.1081810\ttotal: 37.9s\tremaining: 1m 1s\n",
      "382:\tlearn: 0.1080899\ttotal: 38s\tremaining: 1m 1s\n",
      "383:\tlearn: 0.1080243\ttotal: 38.1s\tremaining: 1m 1s\n",
      "384:\tlearn: 0.1079494\ttotal: 38.2s\tremaining: 1m 1s\n",
      "385:\tlearn: 0.1079197\ttotal: 38.3s\tremaining: 1m\n",
      "386:\tlearn: 0.1078795\ttotal: 38.4s\tremaining: 1m\n",
      "387:\tlearn: 0.1078409\ttotal: 38.5s\tremaining: 1m\n",
      "388:\tlearn: 0.1077796\ttotal: 38.6s\tremaining: 1m\n",
      "389:\tlearn: 0.1077103\ttotal: 38.7s\tremaining: 1m\n",
      "390:\tlearn: 0.1076629\ttotal: 38.7s\tremaining: 1m\n",
      "391:\tlearn: 0.1076156\ttotal: 38.8s\tremaining: 1m\n",
      "392:\tlearn: 0.1075478\ttotal: 38.9s\tremaining: 1m\n",
      "393:\tlearn: 0.1074825\ttotal: 39s\tremaining: 1m\n",
      "394:\tlearn: 0.1074023\ttotal: 39.2s\tremaining: 60s\n",
      "395:\tlearn: 0.1073141\ttotal: 39.3s\tremaining: 59.9s\n",
      "396:\tlearn: 0.1071882\ttotal: 39.4s\tremaining: 59.8s\n",
      "397:\tlearn: 0.1071273\ttotal: 39.5s\tremaining: 59.7s\n",
      "398:\tlearn: 0.1070407\ttotal: 39.6s\tremaining: 59.6s\n",
      "399:\tlearn: 0.1069970\ttotal: 39.7s\tremaining: 59.5s\n",
      "400:\tlearn: 0.1069517\ttotal: 39.8s\tremaining: 59.4s\n",
      "401:\tlearn: 0.1069176\ttotal: 39.9s\tremaining: 59.3s\n",
      "402:\tlearn: 0.1068584\ttotal: 40s\tremaining: 59.2s\n",
      "403:\tlearn: 0.1068030\ttotal: 40.1s\tremaining: 59.2s\n",
      "404:\tlearn: 0.1067088\ttotal: 40.3s\tremaining: 59.1s\n",
      "405:\tlearn: 0.1066504\ttotal: 40.4s\tremaining: 59s\n",
      "406:\tlearn: 0.1066023\ttotal: 40.5s\tremaining: 58.9s\n",
      "407:\tlearn: 0.1065658\ttotal: 40.5s\tremaining: 58.8s\n",
      "408:\tlearn: 0.1065242\ttotal: 40.6s\tremaining: 58.7s\n",
      "409:\tlearn: 0.1064198\ttotal: 40.7s\tremaining: 58.6s\n",
      "410:\tlearn: 0.1063658\ttotal: 40.8s\tremaining: 58.5s\n",
      "411:\tlearn: 0.1063027\ttotal: 40.9s\tremaining: 58.4s\n",
      "412:\tlearn: 0.1062231\ttotal: 41s\tremaining: 58.3s\n",
      "413:\tlearn: 0.1061911\ttotal: 41.1s\tremaining: 58.2s\n",
      "414:\tlearn: 0.1061737\ttotal: 41.2s\tremaining: 58s\n",
      "415:\tlearn: 0.1061040\ttotal: 41.3s\tremaining: 57.9s\n",
      "416:\tlearn: 0.1060320\ttotal: 41.4s\tremaining: 57.8s\n",
      "417:\tlearn: 0.1059529\ttotal: 41.5s\tremaining: 57.7s\n",
      "418:\tlearn: 0.1058797\ttotal: 41.6s\tremaining: 57.6s\n",
      "419:\tlearn: 0.1058025\ttotal: 41.7s\tremaining: 57.5s\n",
      "420:\tlearn: 0.1057431\ttotal: 41.7s\tremaining: 57.4s\n",
      "421:\tlearn: 0.1056825\ttotal: 41.8s\tremaining: 57.3s\n",
      "422:\tlearn: 0.1056206\ttotal: 41.9s\tremaining: 57.2s\n",
      "423:\tlearn: 0.1055255\ttotal: 42s\tremaining: 57.1s\n",
      "424:\tlearn: 0.1054663\ttotal: 42.1s\tremaining: 57s\n",
      "425:\tlearn: 0.1054177\ttotal: 42.2s\tremaining: 56.9s\n",
      "426:\tlearn: 0.1053717\ttotal: 42.3s\tremaining: 56.8s\n",
      "427:\tlearn: 0.1053123\ttotal: 42.4s\tremaining: 56.7s\n",
      "428:\tlearn: 0.1052497\ttotal: 42.5s\tremaining: 56.6s\n",
      "429:\tlearn: 0.1052099\ttotal: 42.6s\tremaining: 56.5s\n",
      "430:\tlearn: 0.1051328\ttotal: 42.7s\tremaining: 56.4s\n",
      "431:\tlearn: 0.1050592\ttotal: 42.8s\tremaining: 56.3s\n",
      "432:\tlearn: 0.1050008\ttotal: 42.9s\tremaining: 56.2s\n",
      "433:\tlearn: 0.1049483\ttotal: 43s\tremaining: 56.1s\n",
      "434:\tlearn: 0.1049195\ttotal: 43.1s\tremaining: 56s\n",
      "435:\tlearn: 0.1048727\ttotal: 43.2s\tremaining: 55.9s\n",
      "436:\tlearn: 0.1047717\ttotal: 43.3s\tremaining: 55.8s\n",
      "437:\tlearn: 0.1047148\ttotal: 43.4s\tremaining: 55.7s\n",
      "438:\tlearn: 0.1046330\ttotal: 43.5s\tremaining: 55.6s\n",
      "439:\tlearn: 0.1045799\ttotal: 43.6s\tremaining: 55.5s\n",
      "440:\tlearn: 0.1045062\ttotal: 43.7s\tremaining: 55.4s\n",
      "441:\tlearn: 0.1044288\ttotal: 43.8s\tremaining: 55.3s\n",
      "442:\tlearn: 0.1043689\ttotal: 43.9s\tremaining: 55.2s\n",
      "443:\tlearn: 0.1042701\ttotal: 44s\tremaining: 55.1s\n",
      "444:\tlearn: 0.1041864\ttotal: 44.1s\tremaining: 55s\n",
      "445:\tlearn: 0.1041280\ttotal: 44.2s\tremaining: 54.9s\n",
      "446:\tlearn: 0.1040582\ttotal: 44.3s\tremaining: 54.8s\n",
      "447:\tlearn: 0.1039699\ttotal: 44.4s\tremaining: 54.6s\n",
      "448:\tlearn: 0.1038992\ttotal: 44.4s\tremaining: 54.5s\n",
      "449:\tlearn: 0.1038498\ttotal: 44.5s\tremaining: 54.4s\n",
      "450:\tlearn: 0.1037764\ttotal: 44.6s\tremaining: 54.3s\n",
      "451:\tlearn: 0.1037288\ttotal: 44.7s\tremaining: 54.2s\n",
      "452:\tlearn: 0.1036958\ttotal: 44.8s\tremaining: 54.1s\n",
      "453:\tlearn: 0.1036384\ttotal: 44.9s\tremaining: 54s\n",
      "454:\tlearn: 0.1035788\ttotal: 45s\tremaining: 53.9s\n",
      "455:\tlearn: 0.1035553\ttotal: 45.1s\tremaining: 53.8s\n",
      "456:\tlearn: 0.1034935\ttotal: 45.2s\tremaining: 53.7s\n",
      "457:\tlearn: 0.1034194\ttotal: 45.3s\tremaining: 53.6s\n",
      "458:\tlearn: 0.1033821\ttotal: 45.4s\tremaining: 53.5s\n",
      "459:\tlearn: 0.1033191\ttotal: 45.5s\tremaining: 53.4s\n",
      "460:\tlearn: 0.1032763\ttotal: 45.6s\tremaining: 53.3s\n",
      "461:\tlearn: 0.1032393\ttotal: 45.7s\tremaining: 53.2s\n",
      "462:\tlearn: 0.1031833\ttotal: 45.8s\tremaining: 53.1s\n",
      "463:\tlearn: 0.1030917\ttotal: 45.9s\tremaining: 53s\n",
      "464:\tlearn: 0.1030507\ttotal: 45.9s\tremaining: 52.9s\n",
      "465:\tlearn: 0.1029780\ttotal: 46s\tremaining: 52.8s\n",
      "466:\tlearn: 0.1029087\ttotal: 46.1s\tremaining: 52.7s\n",
      "467:\tlearn: 0.1028589\ttotal: 46.2s\tremaining: 52.5s\n",
      "468:\tlearn: 0.1027918\ttotal: 46.3s\tremaining: 52.5s\n",
      "469:\tlearn: 0.1027470\ttotal: 46.4s\tremaining: 52.4s\n",
      "470:\tlearn: 0.1026799\ttotal: 46.5s\tremaining: 52.3s\n",
      "471:\tlearn: 0.1026214\ttotal: 46.6s\tremaining: 52.2s\n",
      "472:\tlearn: 0.1025731\ttotal: 46.7s\tremaining: 52.1s\n",
      "473:\tlearn: 0.1024982\ttotal: 46.8s\tremaining: 52s\n",
      "474:\tlearn: 0.1024458\ttotal: 46.9s\tremaining: 51.9s\n",
      "475:\tlearn: 0.1024106\ttotal: 47s\tremaining: 51.8s\n",
      "476:\tlearn: 0.1023227\ttotal: 47.1s\tremaining: 51.7s\n",
      "477:\tlearn: 0.1022996\ttotal: 47.2s\tremaining: 51.6s\n",
      "478:\tlearn: 0.1021949\ttotal: 47.3s\tremaining: 51.5s\n",
      "479:\tlearn: 0.1021555\ttotal: 47.4s\tremaining: 51.4s\n",
      "480:\tlearn: 0.1020973\ttotal: 47.5s\tremaining: 51.3s\n",
      "481:\tlearn: 0.1020433\ttotal: 47.6s\tremaining: 51.2s\n",
      "482:\tlearn: 0.1020022\ttotal: 47.7s\tremaining: 51s\n",
      "483:\tlearn: 0.1019250\ttotal: 47.8s\tremaining: 51s\n",
      "484:\tlearn: 0.1018330\ttotal: 47.9s\tremaining: 50.9s\n",
      "485:\tlearn: 0.1017642\ttotal: 48s\tremaining: 50.8s\n",
      "486:\tlearn: 0.1017150\ttotal: 48.1s\tremaining: 50.7s\n",
      "487:\tlearn: 0.1016704\ttotal: 48.2s\tremaining: 50.6s\n",
      "488:\tlearn: 0.1016317\ttotal: 48.3s\tremaining: 50.5s\n",
      "489:\tlearn: 0.1015952\ttotal: 48.4s\tremaining: 50.4s\n",
      "490:\tlearn: 0.1015046\ttotal: 48.5s\tremaining: 50.3s\n",
      "491:\tlearn: 0.1014442\ttotal: 48.6s\tremaining: 50.2s\n",
      "492:\tlearn: 0.1013940\ttotal: 48.7s\tremaining: 50.1s\n",
      "493:\tlearn: 0.1013620\ttotal: 48.8s\tremaining: 50s\n",
      "494:\tlearn: 0.1013268\ttotal: 48.9s\tremaining: 49.9s\n",
      "495:\tlearn: 0.1012717\ttotal: 49s\tremaining: 49.8s\n",
      "496:\tlearn: 0.1012335\ttotal: 49.1s\tremaining: 49.7s\n",
      "497:\tlearn: 0.1011966\ttotal: 49.2s\tremaining: 49.6s\n",
      "498:\tlearn: 0.1011366\ttotal: 49.3s\tremaining: 49.5s\n",
      "499:\tlearn: 0.1010900\ttotal: 49.4s\tremaining: 49.4s\n",
      "500:\tlearn: 0.1010434\ttotal: 49.4s\tremaining: 49.3s\n",
      "501:\tlearn: 0.1010235\ttotal: 49.5s\tremaining: 49.1s\n",
      "502:\tlearn: 0.1010025\ttotal: 49.6s\tremaining: 49s\n",
      "503:\tlearn: 0.1009616\ttotal: 49.7s\tremaining: 48.9s\n",
      "504:\tlearn: 0.1008572\ttotal: 49.8s\tremaining: 48.8s\n",
      "505:\tlearn: 0.1007294\ttotal: 49.9s\tremaining: 48.7s\n",
      "506:\tlearn: 0.1006762\ttotal: 50s\tremaining: 48.6s\n",
      "507:\tlearn: 0.1005986\ttotal: 50.2s\tremaining: 48.6s\n",
      "508:\tlearn: 0.1005551\ttotal: 50.4s\tremaining: 48.6s\n",
      "509:\tlearn: 0.1005271\ttotal: 50.5s\tremaining: 48.5s\n",
      "510:\tlearn: 0.1004580\ttotal: 50.6s\tremaining: 48.4s\n",
      "511:\tlearn: 0.1004165\ttotal: 50.7s\tremaining: 48.3s\n",
      "512:\tlearn: 0.1003703\ttotal: 50.8s\tremaining: 48.2s\n",
      "513:\tlearn: 0.1003258\ttotal: 50.9s\tremaining: 48.1s\n",
      "514:\tlearn: 0.1002892\ttotal: 51s\tremaining: 48s\n",
      "515:\tlearn: 0.1002356\ttotal: 51.1s\tremaining: 47.9s\n",
      "516:\tlearn: 0.1001919\ttotal: 51.2s\tremaining: 47.8s\n",
      "517:\tlearn: 0.1001565\ttotal: 51.3s\tremaining: 47.7s\n",
      "518:\tlearn: 0.1001263\ttotal: 51.3s\tremaining: 47.6s\n",
      "519:\tlearn: 0.1000407\ttotal: 51.4s\tremaining: 47.5s\n",
      "520:\tlearn: 0.0999781\ttotal: 51.5s\tremaining: 47.4s\n",
      "521:\tlearn: 0.0999309\ttotal: 51.6s\tremaining: 47.3s\n",
      "522:\tlearn: 0.0998879\ttotal: 51.7s\tremaining: 47.2s\n",
      "523:\tlearn: 0.0998279\ttotal: 51.8s\tremaining: 47.1s\n",
      "524:\tlearn: 0.0997863\ttotal: 51.9s\tremaining: 47s\n",
      "525:\tlearn: 0.0997450\ttotal: 52s\tremaining: 46.9s\n",
      "526:\tlearn: 0.0996984\ttotal: 52.1s\tremaining: 46.8s\n",
      "527:\tlearn: 0.0996361\ttotal: 52.2s\tremaining: 46.7s\n",
      "528:\tlearn: 0.0995630\ttotal: 52.3s\tremaining: 46.6s\n",
      "529:\tlearn: 0.0994694\ttotal: 52.4s\tremaining: 46.5s\n",
      "530:\tlearn: 0.0994002\ttotal: 52.5s\tremaining: 46.4s\n",
      "531:\tlearn: 0.0993757\ttotal: 52.6s\tremaining: 46.3s\n",
      "532:\tlearn: 0.0993106\ttotal: 52.7s\tremaining: 46.2s\n",
      "533:\tlearn: 0.0992626\ttotal: 52.8s\tremaining: 46.1s\n",
      "534:\tlearn: 0.0992287\ttotal: 52.9s\tremaining: 46s\n",
      "535:\tlearn: 0.0991991\ttotal: 53s\tremaining: 45.9s\n",
      "536:\tlearn: 0.0991473\ttotal: 53.1s\tremaining: 45.8s\n",
      "537:\tlearn: 0.0990854\ttotal: 53.2s\tremaining: 45.7s\n",
      "538:\tlearn: 0.0990240\ttotal: 53.3s\tremaining: 45.6s\n",
      "539:\tlearn: 0.0989039\ttotal: 53.4s\tremaining: 45.5s\n",
      "540:\tlearn: 0.0988664\ttotal: 53.5s\tremaining: 45.4s\n",
      "541:\tlearn: 0.0988102\ttotal: 53.6s\tremaining: 45.3s\n",
      "542:\tlearn: 0.0987354\ttotal: 53.6s\tremaining: 45.1s\n",
      "543:\tlearn: 0.0986677\ttotal: 53.7s\tremaining: 45s\n",
      "544:\tlearn: 0.0986036\ttotal: 53.8s\tremaining: 44.9s\n",
      "545:\tlearn: 0.0985366\ttotal: 53.9s\tremaining: 44.8s\n",
      "546:\tlearn: 0.0984925\ttotal: 54s\tremaining: 44.7s\n",
      "547:\tlearn: 0.0984245\ttotal: 54.1s\tremaining: 44.6s\n",
      "548:\tlearn: 0.0983782\ttotal: 54.2s\tremaining: 44.5s\n",
      "549:\tlearn: 0.0983185\ttotal: 54.3s\tremaining: 44.4s\n",
      "550:\tlearn: 0.0982978\ttotal: 54.4s\tremaining: 44.3s\n",
      "551:\tlearn: 0.0982428\ttotal: 54.5s\tremaining: 44.2s\n",
      "552:\tlearn: 0.0982013\ttotal: 54.6s\tremaining: 44.1s\n",
      "553:\tlearn: 0.0981329\ttotal: 54.7s\tremaining: 44s\n",
      "554:\tlearn: 0.0980611\ttotal: 54.8s\tremaining: 43.9s\n",
      "555:\tlearn: 0.0980184\ttotal: 54.9s\tremaining: 43.8s\n",
      "556:\tlearn: 0.0979936\ttotal: 55s\tremaining: 43.7s\n",
      "557:\tlearn: 0.0979407\ttotal: 55.1s\tremaining: 43.6s\n",
      "558:\tlearn: 0.0979112\ttotal: 55.2s\tremaining: 43.5s\n",
      "559:\tlearn: 0.0978388\ttotal: 55.3s\tremaining: 43.4s\n",
      "560:\tlearn: 0.0977724\ttotal: 55.4s\tremaining: 43.3s\n",
      "561:\tlearn: 0.0977483\ttotal: 55.5s\tremaining: 43.2s\n",
      "562:\tlearn: 0.0977087\ttotal: 55.6s\tremaining: 43.1s\n",
      "563:\tlearn: 0.0976697\ttotal: 55.6s\tremaining: 43s\n",
      "564:\tlearn: 0.0976197\ttotal: 55.7s\tremaining: 42.9s\n",
      "565:\tlearn: 0.0975644\ttotal: 55.8s\tremaining: 42.8s\n",
      "566:\tlearn: 0.0975348\ttotal: 56s\tremaining: 42.7s\n",
      "567:\tlearn: 0.0975029\ttotal: 56s\tremaining: 42.6s\n",
      "568:\tlearn: 0.0974182\ttotal: 56.1s\tremaining: 42.5s\n",
      "569:\tlearn: 0.0973887\ttotal: 56.3s\tremaining: 42.4s\n",
      "570:\tlearn: 0.0973501\ttotal: 56.4s\tremaining: 42.3s\n",
      "571:\tlearn: 0.0973217\ttotal: 56.4s\tremaining: 42.2s\n",
      "572:\tlearn: 0.0972851\ttotal: 56.5s\tremaining: 42.1s\n",
      "573:\tlearn: 0.0972322\ttotal: 56.6s\tremaining: 42s\n",
      "574:\tlearn: 0.0971787\ttotal: 56.7s\tremaining: 41.9s\n",
      "575:\tlearn: 0.0971393\ttotal: 56.8s\tremaining: 41.8s\n",
      "576:\tlearn: 0.0971052\ttotal: 56.9s\tremaining: 41.7s\n",
      "577:\tlearn: 0.0970532\ttotal: 57s\tremaining: 41.6s\n",
      "578:\tlearn: 0.0970136\ttotal: 57.1s\tremaining: 41.5s\n",
      "579:\tlearn: 0.0970010\ttotal: 57.2s\tremaining: 41.4s\n",
      "580:\tlearn: 0.0969718\ttotal: 57.3s\tremaining: 41.3s\n",
      "581:\tlearn: 0.0969182\ttotal: 57.4s\tremaining: 41.2s\n",
      "582:\tlearn: 0.0968720\ttotal: 57.5s\tremaining: 41.1s\n",
      "583:\tlearn: 0.0968000\ttotal: 57.6s\tremaining: 41s\n",
      "584:\tlearn: 0.0967489\ttotal: 57.7s\tremaining: 40.9s\n",
      "585:\tlearn: 0.0966840\ttotal: 57.8s\tremaining: 40.8s\n",
      "586:\tlearn: 0.0966337\ttotal: 57.9s\tremaining: 40.7s\n",
      "587:\tlearn: 0.0965517\ttotal: 58s\tremaining: 40.6s\n",
      "588:\tlearn: 0.0965223\ttotal: 58.1s\tremaining: 40.5s\n",
      "589:\tlearn: 0.0964838\ttotal: 58.2s\tremaining: 40.4s\n",
      "590:\tlearn: 0.0964383\ttotal: 58.3s\tremaining: 40.3s\n",
      "591:\tlearn: 0.0964028\ttotal: 58.4s\tremaining: 40.2s\n",
      "592:\tlearn: 0.0963440\ttotal: 58.5s\tremaining: 40.1s\n",
      "593:\tlearn: 0.0963153\ttotal: 58.6s\tremaining: 40.1s\n",
      "594:\tlearn: 0.0962773\ttotal: 58.8s\tremaining: 40s\n",
      "595:\tlearn: 0.0962070\ttotal: 58.9s\tremaining: 39.9s\n",
      "596:\tlearn: 0.0961200\ttotal: 59s\tremaining: 39.8s\n",
      "597:\tlearn: 0.0960626\ttotal: 59.1s\tremaining: 39.7s\n",
      "598:\tlearn: 0.0960301\ttotal: 59.1s\tremaining: 39.6s\n",
      "599:\tlearn: 0.0960038\ttotal: 59.2s\tremaining: 39.5s\n",
      "600:\tlearn: 0.0959707\ttotal: 59.3s\tremaining: 39.4s\n",
      "601:\tlearn: 0.0959057\ttotal: 59.4s\tremaining: 39.3s\n",
      "602:\tlearn: 0.0958773\ttotal: 59.5s\tremaining: 39.2s\n",
      "603:\tlearn: 0.0958132\ttotal: 59.6s\tremaining: 39.1s\n",
      "604:\tlearn: 0.0957860\ttotal: 59.7s\tremaining: 39s\n",
      "605:\tlearn: 0.0957418\ttotal: 59.8s\tremaining: 38.9s\n",
      "606:\tlearn: 0.0956878\ttotal: 59.9s\tremaining: 38.8s\n",
      "607:\tlearn: 0.0956250\ttotal: 60s\tremaining: 38.7s\n",
      "608:\tlearn: 0.0955911\ttotal: 1m\tremaining: 38.6s\n",
      "609:\tlearn: 0.0955369\ttotal: 1m\tremaining: 38.5s\n",
      "610:\tlearn: 0.0954880\ttotal: 1m\tremaining: 38.4s\n",
      "611:\tlearn: 0.0954033\ttotal: 1m\tremaining: 38.3s\n",
      "612:\tlearn: 0.0953683\ttotal: 1m\tremaining: 38.2s\n",
      "613:\tlearn: 0.0953139\ttotal: 1m\tremaining: 38.1s\n",
      "614:\tlearn: 0.0952857\ttotal: 1m\tremaining: 37.9s\n",
      "615:\tlearn: 0.0952583\ttotal: 1m\tremaining: 37.8s\n",
      "616:\tlearn: 0.0951996\ttotal: 1m\tremaining: 37.7s\n",
      "617:\tlearn: 0.0951709\ttotal: 1m\tremaining: 37.6s\n",
      "618:\tlearn: 0.0951247\ttotal: 1m\tremaining: 37.5s\n",
      "619:\tlearn: 0.0950662\ttotal: 1m 1s\tremaining: 37.4s\n",
      "620:\tlearn: 0.0950134\ttotal: 1m 1s\tremaining: 37.3s\n",
      "621:\tlearn: 0.0949668\ttotal: 1m 1s\tremaining: 37.2s\n",
      "622:\tlearn: 0.0948745\ttotal: 1m 1s\tremaining: 37.1s\n",
      "623:\tlearn: 0.0948320\ttotal: 1m 1s\tremaining: 37s\n",
      "624:\tlearn: 0.0947919\ttotal: 1m 1s\tremaining: 36.9s\n",
      "625:\tlearn: 0.0947517\ttotal: 1m 1s\tremaining: 36.8s\n",
      "626:\tlearn: 0.0946994\ttotal: 1m 1s\tremaining: 36.7s\n",
      "627:\tlearn: 0.0946438\ttotal: 1m 1s\tremaining: 36.6s\n",
      "628:\tlearn: 0.0946123\ttotal: 1m 1s\tremaining: 36.5s\n",
      "629:\tlearn: 0.0945857\ttotal: 1m 1s\tremaining: 36.4s\n",
      "630:\tlearn: 0.0945242\ttotal: 1m 2s\tremaining: 36.3s\n",
      "631:\tlearn: 0.0944519\ttotal: 1m 2s\tremaining: 36.2s\n",
      "632:\tlearn: 0.0943896\ttotal: 1m 2s\tremaining: 36.1s\n",
      "633:\tlearn: 0.0943680\ttotal: 1m 2s\tremaining: 36s\n",
      "634:\tlearn: 0.0943149\ttotal: 1m 2s\tremaining: 35.9s\n",
      "635:\tlearn: 0.0942781\ttotal: 1m 2s\tremaining: 35.8s\n",
      "636:\tlearn: 0.0942227\ttotal: 1m 2s\tremaining: 35.7s\n",
      "637:\tlearn: 0.0942062\ttotal: 1m 2s\tremaining: 35.6s\n",
      "638:\tlearn: 0.0941511\ttotal: 1m 2s\tremaining: 35.5s\n",
      "639:\tlearn: 0.0940965\ttotal: 1m 2s\tremaining: 35.4s\n",
      "640:\tlearn: 0.0939992\ttotal: 1m 3s\tremaining: 35.3s\n",
      "641:\tlearn: 0.0939727\ttotal: 1m 3s\tremaining: 35.2s\n",
      "642:\tlearn: 0.0938989\ttotal: 1m 3s\tremaining: 35.1s\n",
      "643:\tlearn: 0.0938695\ttotal: 1m 3s\tremaining: 35s\n",
      "644:\tlearn: 0.0938430\ttotal: 1m 3s\tremaining: 34.9s\n",
      "645:\tlearn: 0.0937603\ttotal: 1m 3s\tremaining: 34.8s\n",
      "646:\tlearn: 0.0937297\ttotal: 1m 3s\tremaining: 34.7s\n",
      "647:\tlearn: 0.0936739\ttotal: 1m 3s\tremaining: 34.6s\n",
      "648:\tlearn: 0.0935922\ttotal: 1m 3s\tremaining: 34.5s\n",
      "649:\tlearn: 0.0935634\ttotal: 1m 3s\tremaining: 34.4s\n",
      "650:\tlearn: 0.0935174\ttotal: 1m 3s\tremaining: 34.3s\n",
      "651:\tlearn: 0.0934939\ttotal: 1m 4s\tremaining: 34.2s\n",
      "652:\tlearn: 0.0934586\ttotal: 1m 4s\tremaining: 34.1s\n",
      "653:\tlearn: 0.0934270\ttotal: 1m 4s\tremaining: 34s\n",
      "654:\tlearn: 0.0933917\ttotal: 1m 4s\tremaining: 33.9s\n",
      "655:\tlearn: 0.0933731\ttotal: 1m 4s\tremaining: 33.8s\n",
      "656:\tlearn: 0.0933062\ttotal: 1m 4s\tremaining: 33.7s\n",
      "657:\tlearn: 0.0932697\ttotal: 1m 4s\tremaining: 33.6s\n",
      "658:\tlearn: 0.0931930\ttotal: 1m 4s\tremaining: 33.5s\n",
      "659:\tlearn: 0.0931703\ttotal: 1m 4s\tremaining: 33.4s\n",
      "660:\tlearn: 0.0931057\ttotal: 1m 4s\tremaining: 33.3s\n",
      "661:\tlearn: 0.0930534\ttotal: 1m 4s\tremaining: 33.2s\n",
      "662:\tlearn: 0.0929970\ttotal: 1m 5s\tremaining: 33.1s\n",
      "663:\tlearn: 0.0929380\ttotal: 1m 5s\tremaining: 33s\n",
      "664:\tlearn: 0.0929184\ttotal: 1m 5s\tremaining: 32.9s\n",
      "665:\tlearn: 0.0928565\ttotal: 1m 5s\tremaining: 32.8s\n",
      "666:\tlearn: 0.0928416\ttotal: 1m 5s\tremaining: 32.7s\n",
      "667:\tlearn: 0.0928150\ttotal: 1m 5s\tremaining: 32.6s\n",
      "668:\tlearn: 0.0927774\ttotal: 1m 5s\tremaining: 32.5s\n",
      "669:\tlearn: 0.0927526\ttotal: 1m 5s\tremaining: 32.4s\n",
      "670:\tlearn: 0.0927269\ttotal: 1m 5s\tremaining: 32.3s\n",
      "671:\tlearn: 0.0927023\ttotal: 1m 5s\tremaining: 32.2s\n",
      "672:\tlearn: 0.0926683\ttotal: 1m 5s\tremaining: 32.1s\n",
      "673:\tlearn: 0.0926427\ttotal: 1m 6s\tremaining: 32s\n",
      "674:\tlearn: 0.0925956\ttotal: 1m 6s\tremaining: 31.9s\n",
      "675:\tlearn: 0.0925817\ttotal: 1m 6s\tremaining: 31.8s\n",
      "676:\tlearn: 0.0925528\ttotal: 1m 6s\tremaining: 31.7s\n",
      "677:\tlearn: 0.0925189\ttotal: 1m 6s\tremaining: 31.5s\n",
      "678:\tlearn: 0.0924958\ttotal: 1m 6s\tremaining: 31.4s\n",
      "679:\tlearn: 0.0924574\ttotal: 1m 6s\tremaining: 31.3s\n",
      "680:\tlearn: 0.0924115\ttotal: 1m 6s\tremaining: 31.2s\n",
      "681:\tlearn: 0.0923862\ttotal: 1m 6s\tremaining: 31.1s\n",
      "682:\tlearn: 0.0923623\ttotal: 1m 6s\tremaining: 31s\n",
      "683:\tlearn: 0.0923244\ttotal: 1m 6s\tremaining: 30.9s\n",
      "684:\tlearn: 0.0922825\ttotal: 1m 7s\tremaining: 30.8s\n",
      "685:\tlearn: 0.0922624\ttotal: 1m 7s\tremaining: 30.7s\n",
      "686:\tlearn: 0.0922021\ttotal: 1m 7s\tremaining: 30.6s\n",
      "687:\tlearn: 0.0921669\ttotal: 1m 7s\tremaining: 30.5s\n",
      "688:\tlearn: 0.0921166\ttotal: 1m 7s\tremaining: 30.4s\n",
      "689:\tlearn: 0.0920537\ttotal: 1m 7s\tremaining: 30.3s\n",
      "690:\tlearn: 0.0920110\ttotal: 1m 7s\tremaining: 30.2s\n",
      "691:\tlearn: 0.0919552\ttotal: 1m 7s\tremaining: 30.1s\n",
      "692:\tlearn: 0.0919221\ttotal: 1m 7s\tremaining: 30s\n",
      "693:\tlearn: 0.0918869\ttotal: 1m 7s\tremaining: 29.9s\n",
      "694:\tlearn: 0.0918613\ttotal: 1m 7s\tremaining: 29.8s\n",
      "695:\tlearn: 0.0918101\ttotal: 1m 8s\tremaining: 29.7s\n",
      "696:\tlearn: 0.0917783\ttotal: 1m 8s\tremaining: 29.6s\n",
      "697:\tlearn: 0.0917265\ttotal: 1m 8s\tremaining: 29.5s\n",
      "698:\tlearn: 0.0916624\ttotal: 1m 8s\tremaining: 29.4s\n",
      "699:\tlearn: 0.0916257\ttotal: 1m 8s\tremaining: 29.3s\n",
      "700:\tlearn: 0.0915839\ttotal: 1m 8s\tremaining: 29.2s\n",
      "701:\tlearn: 0.0915477\ttotal: 1m 8s\tremaining: 29.1s\n",
      "702:\tlearn: 0.0915072\ttotal: 1m 8s\tremaining: 29s\n",
      "703:\tlearn: 0.0914788\ttotal: 1m 8s\tremaining: 28.9s\n",
      "704:\tlearn: 0.0914121\ttotal: 1m 8s\tremaining: 28.8s\n",
      "705:\tlearn: 0.0913583\ttotal: 1m 9s\tremaining: 28.7s\n",
      "706:\tlearn: 0.0912947\ttotal: 1m 9s\tremaining: 28.6s\n",
      "707:\tlearn: 0.0912508\ttotal: 1m 9s\tremaining: 28.5s\n",
      "708:\tlearn: 0.0912033\ttotal: 1m 9s\tremaining: 28.4s\n",
      "709:\tlearn: 0.0911463\ttotal: 1m 9s\tremaining: 28.3s\n",
      "710:\tlearn: 0.0911169\ttotal: 1m 9s\tremaining: 28.2s\n",
      "711:\tlearn: 0.0910756\ttotal: 1m 9s\tremaining: 28.1s\n",
      "712:\tlearn: 0.0910300\ttotal: 1m 9s\tremaining: 28.1s\n",
      "713:\tlearn: 0.0909858\ttotal: 1m 9s\tremaining: 28s\n",
      "714:\tlearn: 0.0909662\ttotal: 1m 9s\tremaining: 27.9s\n",
      "715:\tlearn: 0.0909228\ttotal: 1m 10s\tremaining: 27.8s\n",
      "716:\tlearn: 0.0908939\ttotal: 1m 10s\tremaining: 27.7s\n",
      "717:\tlearn: 0.0908362\ttotal: 1m 10s\tremaining: 27.6s\n",
      "718:\tlearn: 0.0907940\ttotal: 1m 10s\tremaining: 27.5s\n",
      "719:\tlearn: 0.0907799\ttotal: 1m 10s\tremaining: 27.4s\n",
      "720:\tlearn: 0.0907379\ttotal: 1m 10s\tremaining: 27.3s\n",
      "721:\tlearn: 0.0907042\ttotal: 1m 10s\tremaining: 27.2s\n",
      "722:\tlearn: 0.0906373\ttotal: 1m 10s\tremaining: 27.1s\n",
      "723:\tlearn: 0.0905966\ttotal: 1m 10s\tremaining: 27s\n",
      "724:\tlearn: 0.0905850\ttotal: 1m 10s\tremaining: 26.9s\n",
      "725:\tlearn: 0.0905540\ttotal: 1m 10s\tremaining: 26.8s\n",
      "726:\tlearn: 0.0905265\ttotal: 1m 11s\tremaining: 26.7s\n",
      "727:\tlearn: 0.0904739\ttotal: 1m 11s\tremaining: 26.6s\n",
      "728:\tlearn: 0.0904352\ttotal: 1m 11s\tremaining: 26.5s\n",
      "729:\tlearn: 0.0904147\ttotal: 1m 11s\tremaining: 26.4s\n",
      "730:\tlearn: 0.0903831\ttotal: 1m 11s\tremaining: 26.3s\n",
      "731:\tlearn: 0.0903431\ttotal: 1m 11s\tremaining: 26.2s\n",
      "732:\tlearn: 0.0902671\ttotal: 1m 11s\tremaining: 26.1s\n",
      "733:\tlearn: 0.0902373\ttotal: 1m 11s\tremaining: 26s\n",
      "734:\tlearn: 0.0902039\ttotal: 1m 11s\tremaining: 25.9s\n",
      "735:\tlearn: 0.0901708\ttotal: 1m 11s\tremaining: 25.8s\n",
      "736:\tlearn: 0.0901340\ttotal: 1m 11s\tremaining: 25.7s\n",
      "737:\tlearn: 0.0901062\ttotal: 1m 12s\tremaining: 25.6s\n",
      "738:\tlearn: 0.0900809\ttotal: 1m 12s\tremaining: 25.5s\n",
      "739:\tlearn: 0.0900472\ttotal: 1m 12s\tremaining: 25.4s\n",
      "740:\tlearn: 0.0900023\ttotal: 1m 12s\tremaining: 25.3s\n",
      "741:\tlearn: 0.0899825\ttotal: 1m 12s\tremaining: 25.2s\n",
      "742:\tlearn: 0.0899528\ttotal: 1m 12s\tremaining: 25.1s\n",
      "743:\tlearn: 0.0898788\ttotal: 1m 12s\tremaining: 25s\n",
      "744:\tlearn: 0.0898182\ttotal: 1m 12s\tremaining: 24.9s\n",
      "745:\tlearn: 0.0897689\ttotal: 1m 12s\tremaining: 24.8s\n",
      "746:\tlearn: 0.0897516\ttotal: 1m 12s\tremaining: 24.7s\n",
      "747:\tlearn: 0.0896990\ttotal: 1m 12s\tremaining: 24.6s\n",
      "748:\tlearn: 0.0896552\ttotal: 1m 13s\tremaining: 24.5s\n",
      "749:\tlearn: 0.0895864\ttotal: 1m 13s\tremaining: 24.4s\n",
      "750:\tlearn: 0.0895579\ttotal: 1m 13s\tremaining: 24.3s\n",
      "751:\tlearn: 0.0895101\ttotal: 1m 13s\tremaining: 24.2s\n",
      "752:\tlearn: 0.0894651\ttotal: 1m 13s\tremaining: 24.1s\n",
      "753:\tlearn: 0.0894205\ttotal: 1m 13s\tremaining: 24s\n",
      "754:\tlearn: 0.0893906\ttotal: 1m 13s\tremaining: 23.9s\n",
      "755:\tlearn: 0.0893472\ttotal: 1m 13s\tremaining: 23.8s\n",
      "756:\tlearn: 0.0893120\ttotal: 1m 13s\tremaining: 23.7s\n",
      "757:\tlearn: 0.0892566\ttotal: 1m 13s\tremaining: 23.6s\n",
      "758:\tlearn: 0.0891749\ttotal: 1m 13s\tremaining: 23.5s\n",
      "759:\tlearn: 0.0891449\ttotal: 1m 14s\tremaining: 23.4s\n",
      "760:\tlearn: 0.0891251\ttotal: 1m 14s\tremaining: 23.3s\n",
      "761:\tlearn: 0.0890641\ttotal: 1m 14s\tremaining: 23.2s\n",
      "762:\tlearn: 0.0890540\ttotal: 1m 14s\tremaining: 23.1s\n",
      "763:\tlearn: 0.0890391\ttotal: 1m 14s\tremaining: 23s\n",
      "764:\tlearn: 0.0890096\ttotal: 1m 14s\tremaining: 22.9s\n",
      "765:\tlearn: 0.0889963\ttotal: 1m 14s\tremaining: 22.8s\n",
      "766:\tlearn: 0.0889511\ttotal: 1m 14s\tremaining: 22.7s\n",
      "767:\tlearn: 0.0889281\ttotal: 1m 14s\tremaining: 22.6s\n",
      "768:\tlearn: 0.0888707\ttotal: 1m 14s\tremaining: 22.5s\n",
      "769:\tlearn: 0.0888094\ttotal: 1m 15s\tremaining: 22.4s\n",
      "770:\tlearn: 0.0887553\ttotal: 1m 15s\tremaining: 22.3s\n",
      "771:\tlearn: 0.0887528\ttotal: 1m 15s\tremaining: 22.2s\n",
      "772:\tlearn: 0.0887100\ttotal: 1m 15s\tremaining: 22.1s\n",
      "773:\tlearn: 0.0886850\ttotal: 1m 15s\tremaining: 22s\n",
      "774:\tlearn: 0.0886306\ttotal: 1m 15s\tremaining: 21.9s\n",
      "775:\tlearn: 0.0885762\ttotal: 1m 15s\tremaining: 21.8s\n",
      "776:\tlearn: 0.0885551\ttotal: 1m 15s\tremaining: 21.7s\n",
      "777:\tlearn: 0.0884973\ttotal: 1m 15s\tremaining: 21.6s\n",
      "778:\tlearn: 0.0884624\ttotal: 1m 15s\tremaining: 21.5s\n",
      "779:\tlearn: 0.0884329\ttotal: 1m 15s\tremaining: 21.4s\n",
      "780:\tlearn: 0.0883726\ttotal: 1m 16s\tremaining: 21.3s\n",
      "781:\tlearn: 0.0883501\ttotal: 1m 16s\tremaining: 21.2s\n",
      "782:\tlearn: 0.0883023\ttotal: 1m 16s\tremaining: 21.1s\n",
      "783:\tlearn: 0.0882461\ttotal: 1m 16s\tremaining: 21s\n",
      "784:\tlearn: 0.0882135\ttotal: 1m 16s\tremaining: 20.9s\n",
      "785:\tlearn: 0.0881875\ttotal: 1m 16s\tremaining: 20.8s\n",
      "786:\tlearn: 0.0881398\ttotal: 1m 16s\tremaining: 20.7s\n",
      "787:\tlearn: 0.0881252\ttotal: 1m 16s\tremaining: 20.6s\n",
      "788:\tlearn: 0.0881102\ttotal: 1m 16s\tremaining: 20.5s\n",
      "789:\tlearn: 0.0880761\ttotal: 1m 16s\tremaining: 20.4s\n",
      "790:\tlearn: 0.0880679\ttotal: 1m 16s\tremaining: 20.3s\n",
      "791:\tlearn: 0.0880031\ttotal: 1m 17s\tremaining: 20.2s\n",
      "792:\tlearn: 0.0879792\ttotal: 1m 17s\tremaining: 20.1s\n",
      "793:\tlearn: 0.0879500\ttotal: 1m 17s\tremaining: 20s\n",
      "794:\tlearn: 0.0879113\ttotal: 1m 17s\tremaining: 19.9s\n",
      "795:\tlearn: 0.0878496\ttotal: 1m 17s\tremaining: 19.9s\n",
      "796:\tlearn: 0.0878167\ttotal: 1m 17s\tremaining: 19.8s\n",
      "797:\tlearn: 0.0877706\ttotal: 1m 17s\tremaining: 19.7s\n",
      "798:\tlearn: 0.0877317\ttotal: 1m 17s\tremaining: 19.6s\n",
      "799:\tlearn: 0.0876804\ttotal: 1m 17s\tremaining: 19.5s\n",
      "800:\tlearn: 0.0875981\ttotal: 1m 17s\tremaining: 19.4s\n",
      "801:\tlearn: 0.0875397\ttotal: 1m 18s\tremaining: 19.3s\n",
      "802:\tlearn: 0.0875099\ttotal: 1m 18s\tremaining: 19.2s\n",
      "803:\tlearn: 0.0874639\ttotal: 1m 18s\tremaining: 19.1s\n",
      "804:\tlearn: 0.0874333\ttotal: 1m 18s\tremaining: 19s\n",
      "805:\tlearn: 0.0873800\ttotal: 1m 18s\tremaining: 18.9s\n",
      "806:\tlearn: 0.0873603\ttotal: 1m 18s\tremaining: 18.8s\n",
      "807:\tlearn: 0.0873175\ttotal: 1m 18s\tremaining: 18.7s\n",
      "808:\tlearn: 0.0872650\ttotal: 1m 18s\tremaining: 18.6s\n",
      "809:\tlearn: 0.0872146\ttotal: 1m 18s\tremaining: 18.5s\n",
      "810:\tlearn: 0.0871979\ttotal: 1m 18s\tremaining: 18.4s\n",
      "811:\tlearn: 0.0871585\ttotal: 1m 18s\tremaining: 18.3s\n",
      "812:\tlearn: 0.0871138\ttotal: 1m 19s\tremaining: 18.2s\n",
      "813:\tlearn: 0.0870591\ttotal: 1m 19s\tremaining: 18.1s\n",
      "814:\tlearn: 0.0869760\ttotal: 1m 19s\tremaining: 18s\n",
      "815:\tlearn: 0.0868909\ttotal: 1m 19s\tremaining: 17.9s\n",
      "816:\tlearn: 0.0868353\ttotal: 1m 19s\tremaining: 17.8s\n",
      "817:\tlearn: 0.0868092\ttotal: 1m 19s\tremaining: 17.7s\n",
      "818:\tlearn: 0.0867720\ttotal: 1m 19s\tremaining: 17.6s\n",
      "819:\tlearn: 0.0867048\ttotal: 1m 19s\tremaining: 17.5s\n",
      "820:\tlearn: 0.0866692\ttotal: 1m 19s\tremaining: 17.4s\n",
      "821:\tlearn: 0.0866411\ttotal: 1m 19s\tremaining: 17.3s\n",
      "822:\tlearn: 0.0866114\ttotal: 1m 20s\tremaining: 17.2s\n",
      "823:\tlearn: 0.0865458\ttotal: 1m 20s\tremaining: 17.1s\n",
      "824:\tlearn: 0.0865330\ttotal: 1m 20s\tremaining: 17s\n",
      "825:\tlearn: 0.0865094\ttotal: 1m 20s\tremaining: 16.9s\n",
      "826:\tlearn: 0.0864632\ttotal: 1m 20s\tremaining: 16.8s\n",
      "827:\tlearn: 0.0864417\ttotal: 1m 20s\tremaining: 16.7s\n",
      "828:\tlearn: 0.0863862\ttotal: 1m 20s\tremaining: 16.6s\n",
      "829:\tlearn: 0.0863553\ttotal: 1m 20s\tremaining: 16.5s\n",
      "830:\tlearn: 0.0863196\ttotal: 1m 20s\tremaining: 16.4s\n",
      "831:\tlearn: 0.0862798\ttotal: 1m 20s\tremaining: 16.3s\n",
      "832:\tlearn: 0.0862313\ttotal: 1m 21s\tremaining: 16.2s\n",
      "833:\tlearn: 0.0862040\ttotal: 1m 21s\tremaining: 16.1s\n",
      "834:\tlearn: 0.0861628\ttotal: 1m 21s\tremaining: 16.1s\n",
      "835:\tlearn: 0.0861286\ttotal: 1m 21s\tremaining: 16s\n",
      "836:\tlearn: 0.0860798\ttotal: 1m 21s\tremaining: 15.9s\n",
      "837:\tlearn: 0.0860447\ttotal: 1m 21s\tremaining: 15.8s\n",
      "838:\tlearn: 0.0860167\ttotal: 1m 21s\tremaining: 15.7s\n",
      "839:\tlearn: 0.0859995\ttotal: 1m 21s\tremaining: 15.6s\n",
      "840:\tlearn: 0.0859493\ttotal: 1m 21s\tremaining: 15.5s\n",
      "841:\tlearn: 0.0859006\ttotal: 1m 21s\tremaining: 15.4s\n",
      "842:\tlearn: 0.0858559\ttotal: 1m 21s\tremaining: 15.3s\n",
      "843:\tlearn: 0.0858183\ttotal: 1m 22s\tremaining: 15.2s\n",
      "844:\tlearn: 0.0857898\ttotal: 1m 22s\tremaining: 15.1s\n",
      "845:\tlearn: 0.0857495\ttotal: 1m 22s\tremaining: 15s\n",
      "846:\tlearn: 0.0857337\ttotal: 1m 22s\tremaining: 14.9s\n",
      "847:\tlearn: 0.0856939\ttotal: 1m 22s\tremaining: 14.8s\n",
      "848:\tlearn: 0.0856560\ttotal: 1m 22s\tremaining: 14.7s\n",
      "849:\tlearn: 0.0856257\ttotal: 1m 22s\tremaining: 14.6s\n",
      "850:\tlearn: 0.0855721\ttotal: 1m 22s\tremaining: 14.5s\n",
      "851:\tlearn: 0.0855038\ttotal: 1m 22s\tremaining: 14.4s\n",
      "852:\tlearn: 0.0854352\ttotal: 1m 22s\tremaining: 14.3s\n",
      "853:\tlearn: 0.0854168\ttotal: 1m 22s\tremaining: 14.2s\n",
      "854:\tlearn: 0.0853916\ttotal: 1m 23s\tremaining: 14.1s\n",
      "855:\tlearn: 0.0853702\ttotal: 1m 23s\tremaining: 14s\n",
      "856:\tlearn: 0.0853265\ttotal: 1m 23s\tremaining: 13.9s\n",
      "857:\tlearn: 0.0852338\ttotal: 1m 23s\tremaining: 13.8s\n",
      "858:\tlearn: 0.0852052\ttotal: 1m 23s\tremaining: 13.7s\n",
      "859:\tlearn: 0.0851692\ttotal: 1m 23s\tremaining: 13.6s\n",
      "860:\tlearn: 0.0851256\ttotal: 1m 23s\tremaining: 13.5s\n",
      "861:\tlearn: 0.0850633\ttotal: 1m 23s\tremaining: 13.4s\n",
      "862:\tlearn: 0.0850110\ttotal: 1m 23s\tremaining: 13.3s\n",
      "863:\tlearn: 0.0849932\ttotal: 1m 23s\tremaining: 13.2s\n",
      "864:\tlearn: 0.0849428\ttotal: 1m 23s\tremaining: 13.1s\n",
      "865:\tlearn: 0.0848883\ttotal: 1m 24s\tremaining: 13s\n",
      "866:\tlearn: 0.0848320\ttotal: 1m 24s\tremaining: 12.9s\n",
      "867:\tlearn: 0.0847992\ttotal: 1m 24s\tremaining: 12.8s\n",
      "868:\tlearn: 0.0847429\ttotal: 1m 24s\tremaining: 12.7s\n",
      "869:\tlearn: 0.0847029\ttotal: 1m 24s\tremaining: 12.6s\n",
      "870:\tlearn: 0.0846868\ttotal: 1m 24s\tremaining: 12.5s\n",
      "871:\tlearn: 0.0846543\ttotal: 1m 24s\tremaining: 12.4s\n",
      "872:\tlearn: 0.0846193\ttotal: 1m 24s\tremaining: 12.3s\n",
      "873:\tlearn: 0.0845928\ttotal: 1m 24s\tremaining: 12.2s\n",
      "874:\tlearn: 0.0845448\ttotal: 1m 24s\tremaining: 12.1s\n",
      "875:\tlearn: 0.0845223\ttotal: 1m 25s\tremaining: 12s\n",
      "876:\tlearn: 0.0844718\ttotal: 1m 25s\tremaining: 11.9s\n",
      "877:\tlearn: 0.0844383\ttotal: 1m 25s\tremaining: 11.8s\n",
      "878:\tlearn: 0.0844146\ttotal: 1m 25s\tremaining: 11.7s\n",
      "879:\tlearn: 0.0843985\ttotal: 1m 25s\tremaining: 11.6s\n",
      "880:\tlearn: 0.0843768\ttotal: 1m 25s\tremaining: 11.5s\n",
      "881:\tlearn: 0.0843109\ttotal: 1m 25s\tremaining: 11.4s\n",
      "882:\tlearn: 0.0842590\ttotal: 1m 25s\tremaining: 11.4s\n",
      "883:\tlearn: 0.0842166\ttotal: 1m 25s\tremaining: 11.3s\n",
      "884:\tlearn: 0.0841655\ttotal: 1m 25s\tremaining: 11.2s\n",
      "885:\tlearn: 0.0841257\ttotal: 1m 25s\tremaining: 11.1s\n",
      "886:\tlearn: 0.0840654\ttotal: 1m 26s\tremaining: 11s\n",
      "887:\tlearn: 0.0840329\ttotal: 1m 26s\tremaining: 10.9s\n",
      "888:\tlearn: 0.0840329\ttotal: 1m 26s\tremaining: 10.8s\n",
      "889:\tlearn: 0.0839770\ttotal: 1m 26s\tremaining: 10.7s\n",
      "890:\tlearn: 0.0839545\ttotal: 1m 26s\tremaining: 10.6s\n",
      "891:\tlearn: 0.0839030\ttotal: 1m 26s\tremaining: 10.5s\n",
      "892:\tlearn: 0.0838786\ttotal: 1m 26s\tremaining: 10.4s\n",
      "893:\tlearn: 0.0838238\ttotal: 1m 26s\tremaining: 10.3s\n",
      "894:\tlearn: 0.0838125\ttotal: 1m 26s\tremaining: 10.2s\n",
      "895:\tlearn: 0.0837976\ttotal: 1m 26s\tremaining: 10.1s\n",
      "896:\tlearn: 0.0837728\ttotal: 1m 26s\tremaining: 9.98s\n",
      "897:\tlearn: 0.0837276\ttotal: 1m 27s\tremaining: 9.89s\n",
      "898:\tlearn: 0.0836914\ttotal: 1m 27s\tremaining: 9.79s\n",
      "899:\tlearn: 0.0836363\ttotal: 1m 27s\tremaining: 9.69s\n",
      "900:\tlearn: 0.0836126\ttotal: 1m 27s\tremaining: 9.59s\n",
      "901:\tlearn: 0.0835736\ttotal: 1m 27s\tremaining: 9.5s\n",
      "902:\tlearn: 0.0835612\ttotal: 1m 27s\tremaining: 9.4s\n",
      "903:\tlearn: 0.0835279\ttotal: 1m 27s\tremaining: 9.3s\n",
      "904:\tlearn: 0.0834971\ttotal: 1m 27s\tremaining: 9.2s\n",
      "905:\tlearn: 0.0834487\ttotal: 1m 27s\tremaining: 9.11s\n",
      "906:\tlearn: 0.0834079\ttotal: 1m 27s\tremaining: 9.01s\n",
      "907:\tlearn: 0.0833896\ttotal: 1m 27s\tremaining: 8.91s\n",
      "908:\tlearn: 0.0833366\ttotal: 1m 28s\tremaining: 8.81s\n",
      "909:\tlearn: 0.0832746\ttotal: 1m 28s\tremaining: 8.72s\n",
      "910:\tlearn: 0.0832493\ttotal: 1m 28s\tremaining: 8.62s\n",
      "911:\tlearn: 0.0832261\ttotal: 1m 28s\tremaining: 8.52s\n",
      "912:\tlearn: 0.0831808\ttotal: 1m 28s\tremaining: 8.42s\n",
      "913:\tlearn: 0.0831697\ttotal: 1m 28s\tremaining: 8.33s\n",
      "914:\tlearn: 0.0831285\ttotal: 1m 28s\tremaining: 8.23s\n",
      "915:\tlearn: 0.0831185\ttotal: 1m 28s\tremaining: 8.13s\n",
      "916:\tlearn: 0.0830861\ttotal: 1m 28s\tremaining: 8.03s\n",
      "917:\tlearn: 0.0830597\ttotal: 1m 28s\tremaining: 7.94s\n",
      "918:\tlearn: 0.0830202\ttotal: 1m 28s\tremaining: 7.84s\n",
      "919:\tlearn: 0.0829947\ttotal: 1m 29s\tremaining: 7.74s\n",
      "920:\tlearn: 0.0829801\ttotal: 1m 29s\tremaining: 7.64s\n",
      "921:\tlearn: 0.0829598\ttotal: 1m 29s\tremaining: 7.55s\n",
      "922:\tlearn: 0.0829211\ttotal: 1m 29s\tremaining: 7.46s\n",
      "923:\tlearn: 0.0828566\ttotal: 1m 29s\tremaining: 7.37s\n",
      "924:\tlearn: 0.0828198\ttotal: 1m 29s\tremaining: 7.27s\n",
      "925:\tlearn: 0.0827822\ttotal: 1m 29s\tremaining: 7.17s\n",
      "926:\tlearn: 0.0827541\ttotal: 1m 29s\tremaining: 7.07s\n",
      "927:\tlearn: 0.0827390\ttotal: 1m 29s\tremaining: 6.97s\n",
      "928:\tlearn: 0.0826856\ttotal: 1m 30s\tremaining: 6.88s\n",
      "929:\tlearn: 0.0826460\ttotal: 1m 30s\tremaining: 6.78s\n",
      "930:\tlearn: 0.0825962\ttotal: 1m 30s\tremaining: 6.68s\n",
      "931:\tlearn: 0.0825322\ttotal: 1m 30s\tremaining: 6.59s\n",
      "932:\tlearn: 0.0824746\ttotal: 1m 30s\tremaining: 6.49s\n",
      "933:\tlearn: 0.0824422\ttotal: 1m 30s\tremaining: 6.39s\n",
      "934:\tlearn: 0.0823927\ttotal: 1m 30s\tremaining: 6.3s\n",
      "935:\tlearn: 0.0823597\ttotal: 1m 30s\tremaining: 6.2s\n",
      "936:\tlearn: 0.0823478\ttotal: 1m 30s\tremaining: 6.1s\n",
      "937:\tlearn: 0.0823014\ttotal: 1m 30s\tremaining: 6s\n",
      "938:\tlearn: 0.0822396\ttotal: 1m 30s\tremaining: 5.91s\n",
      "939:\tlearn: 0.0821560\ttotal: 1m 31s\tremaining: 5.81s\n",
      "940:\tlearn: 0.0821245\ttotal: 1m 31s\tremaining: 5.71s\n",
      "941:\tlearn: 0.0820913\ttotal: 1m 31s\tremaining: 5.62s\n",
      "942:\tlearn: 0.0820449\ttotal: 1m 31s\tremaining: 5.52s\n",
      "943:\tlearn: 0.0820052\ttotal: 1m 31s\tremaining: 5.42s\n",
      "944:\tlearn: 0.0819635\ttotal: 1m 31s\tremaining: 5.33s\n",
      "945:\tlearn: 0.0819127\ttotal: 1m 31s\tremaining: 5.23s\n",
      "946:\tlearn: 0.0818708\ttotal: 1m 31s\tremaining: 5.13s\n",
      "947:\tlearn: 0.0818362\ttotal: 1m 31s\tremaining: 5.03s\n",
      "948:\tlearn: 0.0818216\ttotal: 1m 31s\tremaining: 4.94s\n",
      "949:\tlearn: 0.0817785\ttotal: 1m 31s\tremaining: 4.84s\n",
      "950:\tlearn: 0.0817456\ttotal: 1m 32s\tremaining: 4.74s\n",
      "951:\tlearn: 0.0817131\ttotal: 1m 32s\tremaining: 4.64s\n",
      "952:\tlearn: 0.0816924\ttotal: 1m 32s\tremaining: 4.55s\n",
      "953:\tlearn: 0.0816717\ttotal: 1m 32s\tremaining: 4.45s\n",
      "954:\tlearn: 0.0816484\ttotal: 1m 32s\tremaining: 4.35s\n",
      "955:\tlearn: 0.0815842\ttotal: 1m 32s\tremaining: 4.26s\n",
      "956:\tlearn: 0.0815408\ttotal: 1m 32s\tremaining: 4.16s\n",
      "957:\tlearn: 0.0815180\ttotal: 1m 32s\tremaining: 4.06s\n",
      "958:\tlearn: 0.0814931\ttotal: 1m 32s\tremaining: 3.97s\n",
      "959:\tlearn: 0.0814700\ttotal: 1m 32s\tremaining: 3.87s\n",
      "960:\tlearn: 0.0814452\ttotal: 1m 32s\tremaining: 3.77s\n",
      "961:\tlearn: 0.0813897\ttotal: 1m 33s\tremaining: 3.67s\n",
      "962:\tlearn: 0.0813728\ttotal: 1m 33s\tremaining: 3.58s\n",
      "963:\tlearn: 0.0813506\ttotal: 1m 33s\tremaining: 3.48s\n",
      "964:\tlearn: 0.0813274\ttotal: 1m 33s\tremaining: 3.38s\n",
      "965:\tlearn: 0.0813172\ttotal: 1m 33s\tremaining: 3.29s\n",
      "966:\tlearn: 0.0812696\ttotal: 1m 33s\tremaining: 3.19s\n",
      "967:\tlearn: 0.0812535\ttotal: 1m 33s\tremaining: 3.09s\n",
      "968:\tlearn: 0.0812192\ttotal: 1m 33s\tremaining: 3s\n",
      "969:\tlearn: 0.0811859\ttotal: 1m 33s\tremaining: 2.9s\n",
      "970:\tlearn: 0.0811561\ttotal: 1m 33s\tremaining: 2.8s\n",
      "971:\tlearn: 0.0811038\ttotal: 1m 33s\tremaining: 2.71s\n",
      "972:\tlearn: 0.0810386\ttotal: 1m 34s\tremaining: 2.61s\n",
      "973:\tlearn: 0.0810154\ttotal: 1m 34s\tremaining: 2.51s\n",
      "974:\tlearn: 0.0809596\ttotal: 1m 34s\tremaining: 2.42s\n",
      "975:\tlearn: 0.0809224\ttotal: 1m 34s\tremaining: 2.32s\n",
      "976:\tlearn: 0.0808828\ttotal: 1m 34s\tremaining: 2.22s\n",
      "977:\tlearn: 0.0808228\ttotal: 1m 34s\tremaining: 2.13s\n",
      "978:\tlearn: 0.0807972\ttotal: 1m 34s\tremaining: 2.03s\n",
      "979:\tlearn: 0.0807559\ttotal: 1m 34s\tremaining: 1.93s\n",
      "980:\tlearn: 0.0807050\ttotal: 1m 34s\tremaining: 1.84s\n",
      "981:\tlearn: 0.0806688\ttotal: 1m 34s\tremaining: 1.74s\n",
      "982:\tlearn: 0.0806022\ttotal: 1m 35s\tremaining: 1.64s\n",
      "983:\tlearn: 0.0805649\ttotal: 1m 35s\tremaining: 1.55s\n",
      "984:\tlearn: 0.0805124\ttotal: 1m 35s\tremaining: 1.45s\n",
      "985:\tlearn: 0.0804644\ttotal: 1m 35s\tremaining: 1.35s\n",
      "986:\tlearn: 0.0804398\ttotal: 1m 35s\tremaining: 1.26s\n",
      "987:\tlearn: 0.0803963\ttotal: 1m 35s\tremaining: 1.16s\n",
      "988:\tlearn: 0.0803384\ttotal: 1m 35s\tremaining: 1.06s\n",
      "989:\tlearn: 0.0802596\ttotal: 1m 35s\tremaining: 967ms\n",
      "990:\tlearn: 0.0802293\ttotal: 1m 35s\tremaining: 870ms\n",
      "991:\tlearn: 0.0802097\ttotal: 1m 35s\tremaining: 774ms\n",
      "992:\tlearn: 0.0801766\ttotal: 1m 36s\tremaining: 677ms\n",
      "993:\tlearn: 0.0801464\ttotal: 1m 36s\tremaining: 580ms\n",
      "994:\tlearn: 0.0801145\ttotal: 1m 36s\tremaining: 483ms\n",
      "995:\tlearn: 0.0800984\ttotal: 1m 36s\tremaining: 387ms\n",
      "996:\tlearn: 0.0800487\ttotal: 1m 36s\tremaining: 290ms\n",
      "997:\tlearn: 0.0800181\ttotal: 1m 36s\tremaining: 193ms\n",
      "998:\tlearn: 0.0799809\ttotal: 1m 36s\tremaining: 96.7ms\n",
      "999:\tlearn: 0.0799424\ttotal: 1m 36s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5202879\ttotal: 89.9ms\tremaining: 1m 29s\n",
      "1:\tlearn: 0.4233036\ttotal: 183ms\tremaining: 1m 31s\n",
      "2:\tlearn: 0.3584844\ttotal: 294ms\tremaining: 1m 37s\n",
      "3:\tlearn: 0.3223838\ttotal: 430ms\tremaining: 1m 46s\n",
      "4:\tlearn: 0.2949049\ttotal: 527ms\tremaining: 1m 44s\n",
      "5:\tlearn: 0.2779582\ttotal: 619ms\tremaining: 1m 42s\n",
      "6:\tlearn: 0.2574987\ttotal: 714ms\tremaining: 1m 41s\n",
      "7:\tlearn: 0.2471196\ttotal: 817ms\tremaining: 1m 41s\n",
      "8:\tlearn: 0.2386418\ttotal: 916ms\tremaining: 1m 40s\n",
      "9:\tlearn: 0.2260424\ttotal: 1.02s\tremaining: 1m 40s\n",
      "10:\tlearn: 0.2174175\ttotal: 1.11s\tremaining: 1m 40s\n",
      "11:\tlearn: 0.2134280\ttotal: 1.22s\tremaining: 1m 40s\n",
      "12:\tlearn: 0.2101899\ttotal: 1.32s\tremaining: 1m 40s\n",
      "13:\tlearn: 0.2047555\ttotal: 1.42s\tremaining: 1m 40s\n",
      "14:\tlearn: 0.2024618\ttotal: 1.53s\tremaining: 1m 40s\n",
      "15:\tlearn: 0.2004523\ttotal: 1.63s\tremaining: 1m 40s\n",
      "16:\tlearn: 0.1979402\ttotal: 1.73s\tremaining: 1m 40s\n",
      "17:\tlearn: 0.1941637\ttotal: 1.82s\tremaining: 1m 39s\n",
      "18:\tlearn: 0.1920484\ttotal: 1.92s\tremaining: 1m 39s\n",
      "19:\tlearn: 0.1900425\ttotal: 2.02s\tremaining: 1m 38s\n",
      "20:\tlearn: 0.1873963\ttotal: 2.12s\tremaining: 1m 38s\n",
      "21:\tlearn: 0.1861834\ttotal: 2.24s\tremaining: 1m 39s\n",
      "22:\tlearn: 0.1846827\ttotal: 2.34s\tremaining: 1m 39s\n",
      "23:\tlearn: 0.1824957\ttotal: 2.44s\tremaining: 1m 39s\n",
      "24:\tlearn: 0.1809935\ttotal: 2.54s\tremaining: 1m 39s\n",
      "25:\tlearn: 0.1796033\ttotal: 2.65s\tremaining: 1m 39s\n",
      "26:\tlearn: 0.1781335\ttotal: 2.75s\tremaining: 1m 38s\n",
      "27:\tlearn: 0.1757106\ttotal: 2.84s\tremaining: 1m 38s\n",
      "28:\tlearn: 0.1745203\ttotal: 2.95s\tremaining: 1m 38s\n",
      "29:\tlearn: 0.1732687\ttotal: 3.05s\tremaining: 1m 38s\n",
      "30:\tlearn: 0.1717565\ttotal: 3.14s\tremaining: 1m 38s\n",
      "31:\tlearn: 0.1710146\ttotal: 3.24s\tremaining: 1m 37s\n",
      "32:\tlearn: 0.1699484\ttotal: 3.33s\tremaining: 1m 37s\n",
      "33:\tlearn: 0.1686906\ttotal: 3.43s\tremaining: 1m 37s\n",
      "34:\tlearn: 0.1680522\ttotal: 3.52s\tremaining: 1m 37s\n",
      "35:\tlearn: 0.1672166\ttotal: 3.62s\tremaining: 1m 36s\n",
      "36:\tlearn: 0.1664976\ttotal: 3.76s\tremaining: 1m 37s\n",
      "37:\tlearn: 0.1654004\ttotal: 3.93s\tremaining: 1m 39s\n",
      "38:\tlearn: 0.1646917\ttotal: 4.04s\tremaining: 1m 39s\n",
      "39:\tlearn: 0.1634072\ttotal: 4.13s\tremaining: 1m 39s\n",
      "40:\tlearn: 0.1625236\ttotal: 4.22s\tremaining: 1m 38s\n",
      "41:\tlearn: 0.1618867\ttotal: 4.31s\tremaining: 1m 38s\n",
      "42:\tlearn: 0.1612370\ttotal: 4.41s\tremaining: 1m 38s\n",
      "43:\tlearn: 0.1605362\ttotal: 4.51s\tremaining: 1m 38s\n",
      "44:\tlearn: 0.1599859\ttotal: 4.61s\tremaining: 1m 37s\n",
      "45:\tlearn: 0.1592201\ttotal: 4.71s\tremaining: 1m 37s\n",
      "46:\tlearn: 0.1587677\ttotal: 4.8s\tremaining: 1m 37s\n",
      "47:\tlearn: 0.1581722\ttotal: 4.9s\tremaining: 1m 37s\n",
      "48:\tlearn: 0.1573938\ttotal: 5s\tremaining: 1m 37s\n",
      "49:\tlearn: 0.1566059\ttotal: 5.09s\tremaining: 1m 36s\n",
      "50:\tlearn: 0.1557287\ttotal: 5.19s\tremaining: 1m 36s\n",
      "51:\tlearn: 0.1551930\ttotal: 5.28s\tremaining: 1m 36s\n",
      "52:\tlearn: 0.1546719\ttotal: 5.37s\tremaining: 1m 35s\n",
      "53:\tlearn: 0.1542653\ttotal: 5.46s\tremaining: 1m 35s\n",
      "54:\tlearn: 0.1538120\ttotal: 5.55s\tremaining: 1m 35s\n",
      "55:\tlearn: 0.1534961\ttotal: 5.65s\tremaining: 1m 35s\n",
      "56:\tlearn: 0.1527439\ttotal: 5.74s\tremaining: 1m 34s\n",
      "57:\tlearn: 0.1520798\ttotal: 5.83s\tremaining: 1m 34s\n",
      "58:\tlearn: 0.1517715\ttotal: 5.92s\tremaining: 1m 34s\n",
      "59:\tlearn: 0.1514190\ttotal: 6.01s\tremaining: 1m 34s\n",
      "60:\tlearn: 0.1510939\ttotal: 6.1s\tremaining: 1m 33s\n",
      "61:\tlearn: 0.1507517\ttotal: 6.19s\tremaining: 1m 33s\n",
      "62:\tlearn: 0.1504627\ttotal: 6.28s\tremaining: 1m 33s\n",
      "63:\tlearn: 0.1501569\ttotal: 6.37s\tremaining: 1m 33s\n",
      "64:\tlearn: 0.1497491\ttotal: 6.46s\tremaining: 1m 32s\n",
      "65:\tlearn: 0.1493173\ttotal: 6.58s\tremaining: 1m 33s\n",
      "66:\tlearn: 0.1488112\ttotal: 6.67s\tremaining: 1m 32s\n",
      "67:\tlearn: 0.1485700\ttotal: 6.77s\tremaining: 1m 32s\n",
      "68:\tlearn: 0.1482102\ttotal: 6.86s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1476959\ttotal: 6.94s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1471820\ttotal: 7.03s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1468548\ttotal: 7.13s\tremaining: 1m 31s\n",
      "72:\tlearn: 0.1463526\ttotal: 7.22s\tremaining: 1m 31s\n",
      "73:\tlearn: 0.1460063\ttotal: 7.33s\tremaining: 1m 31s\n",
      "74:\tlearn: 0.1456899\ttotal: 7.42s\tremaining: 1m 31s\n",
      "75:\tlearn: 0.1453766\ttotal: 7.52s\tremaining: 1m 31s\n",
      "76:\tlearn: 0.1450181\ttotal: 7.61s\tremaining: 1m 31s\n",
      "77:\tlearn: 0.1447886\ttotal: 7.71s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1444716\ttotal: 7.8s\tremaining: 1m 30s\n",
      "79:\tlearn: 0.1442052\ttotal: 7.88s\tremaining: 1m 30s\n",
      "80:\tlearn: 0.1438696\ttotal: 7.98s\tremaining: 1m 30s\n",
      "81:\tlearn: 0.1436511\ttotal: 8.07s\tremaining: 1m 30s\n",
      "82:\tlearn: 0.1433436\ttotal: 8.16s\tremaining: 1m 30s\n",
      "83:\tlearn: 0.1429985\ttotal: 8.25s\tremaining: 1m 29s\n",
      "84:\tlearn: 0.1427558\ttotal: 8.34s\tremaining: 1m 29s\n",
      "85:\tlearn: 0.1424134\ttotal: 8.43s\tremaining: 1m 29s\n",
      "86:\tlearn: 0.1422554\ttotal: 8.52s\tremaining: 1m 29s\n",
      "87:\tlearn: 0.1419037\ttotal: 8.61s\tremaining: 1m 29s\n",
      "88:\tlearn: 0.1416295\ttotal: 8.7s\tremaining: 1m 29s\n",
      "89:\tlearn: 0.1414106\ttotal: 8.79s\tremaining: 1m 28s\n",
      "90:\tlearn: 0.1410792\ttotal: 8.88s\tremaining: 1m 28s\n",
      "91:\tlearn: 0.1408227\ttotal: 8.97s\tremaining: 1m 28s\n",
      "92:\tlearn: 0.1405724\ttotal: 9.07s\tremaining: 1m 28s\n",
      "93:\tlearn: 0.1404036\ttotal: 9.17s\tremaining: 1m 28s\n",
      "94:\tlearn: 0.1400766\ttotal: 9.26s\tremaining: 1m 28s\n",
      "95:\tlearn: 0.1397773\ttotal: 9.36s\tremaining: 1m 28s\n",
      "96:\tlearn: 0.1393887\ttotal: 9.47s\tremaining: 1m 28s\n",
      "97:\tlearn: 0.1392482\ttotal: 9.56s\tremaining: 1m 27s\n",
      "98:\tlearn: 0.1388778\ttotal: 9.65s\tremaining: 1m 27s\n",
      "99:\tlearn: 0.1386668\ttotal: 9.75s\tremaining: 1m 27s\n",
      "100:\tlearn: 0.1383983\ttotal: 9.85s\tremaining: 1m 27s\n",
      "101:\tlearn: 0.1381434\ttotal: 9.94s\tremaining: 1m 27s\n",
      "102:\tlearn: 0.1377997\ttotal: 10s\tremaining: 1m 27s\n",
      "103:\tlearn: 0.1375880\ttotal: 10.1s\tremaining: 1m 27s\n",
      "104:\tlearn: 0.1374422\ttotal: 10.2s\tremaining: 1m 27s\n",
      "105:\tlearn: 0.1372619\ttotal: 10.3s\tremaining: 1m 27s\n",
      "106:\tlearn: 0.1369503\ttotal: 10.4s\tremaining: 1m 27s\n",
      "107:\tlearn: 0.1366777\ttotal: 10.5s\tremaining: 1m 26s\n",
      "108:\tlearn: 0.1363947\ttotal: 10.6s\tremaining: 1m 26s\n",
      "109:\tlearn: 0.1362098\ttotal: 10.7s\tremaining: 1m 26s\n",
      "110:\tlearn: 0.1359351\ttotal: 10.8s\tremaining: 1m 26s\n",
      "111:\tlearn: 0.1357820\ttotal: 10.9s\tremaining: 1m 26s\n",
      "112:\tlearn: 0.1355856\ttotal: 11s\tremaining: 1m 26s\n",
      "113:\tlearn: 0.1354284\ttotal: 11.1s\tremaining: 1m 26s\n",
      "114:\tlearn: 0.1352133\ttotal: 11.2s\tremaining: 1m 26s\n",
      "115:\tlearn: 0.1349720\ttotal: 11.3s\tremaining: 1m 26s\n",
      "116:\tlearn: 0.1347276\ttotal: 11.4s\tremaining: 1m 26s\n",
      "117:\tlearn: 0.1345699\ttotal: 11.5s\tremaining: 1m 25s\n",
      "118:\tlearn: 0.1344476\ttotal: 11.6s\tremaining: 1m 25s\n",
      "119:\tlearn: 0.1342577\ttotal: 11.7s\tremaining: 1m 25s\n",
      "120:\tlearn: 0.1341392\ttotal: 11.8s\tremaining: 1m 25s\n",
      "121:\tlearn: 0.1339629\ttotal: 11.9s\tremaining: 1m 25s\n",
      "122:\tlearn: 0.1337657\ttotal: 12s\tremaining: 1m 25s\n",
      "123:\tlearn: 0.1335919\ttotal: 12.1s\tremaining: 1m 25s\n",
      "124:\tlearn: 0.1334004\ttotal: 12.2s\tremaining: 1m 25s\n",
      "125:\tlearn: 0.1332171\ttotal: 12.2s\tremaining: 1m 24s\n",
      "126:\tlearn: 0.1330186\ttotal: 12.3s\tremaining: 1m 24s\n",
      "127:\tlearn: 0.1328003\ttotal: 12.4s\tremaining: 1m 24s\n",
      "128:\tlearn: 0.1326064\ttotal: 12.5s\tremaining: 1m 24s\n",
      "129:\tlearn: 0.1324480\ttotal: 12.6s\tremaining: 1m 24s\n",
      "130:\tlearn: 0.1323101\ttotal: 12.7s\tremaining: 1m 24s\n",
      "131:\tlearn: 0.1322080\ttotal: 12.8s\tremaining: 1m 24s\n",
      "132:\tlearn: 0.1319428\ttotal: 12.9s\tremaining: 1m 23s\n",
      "133:\tlearn: 0.1318032\ttotal: 13s\tremaining: 1m 23s\n",
      "134:\tlearn: 0.1316087\ttotal: 13.1s\tremaining: 1m 23s\n",
      "135:\tlearn: 0.1314734\ttotal: 13.2s\tremaining: 1m 23s\n",
      "136:\tlearn: 0.1313295\ttotal: 13.3s\tremaining: 1m 23s\n",
      "137:\tlearn: 0.1311329\ttotal: 13.3s\tremaining: 1m 23s\n",
      "138:\tlearn: 0.1309771\ttotal: 13.4s\tremaining: 1m 23s\n",
      "139:\tlearn: 0.1307498\ttotal: 13.5s\tremaining: 1m 23s\n",
      "140:\tlearn: 0.1304703\ttotal: 13.6s\tremaining: 1m 22s\n",
      "141:\tlearn: 0.1303216\ttotal: 13.7s\tremaining: 1m 22s\n",
      "142:\tlearn: 0.1301026\ttotal: 13.8s\tremaining: 1m 22s\n",
      "143:\tlearn: 0.1298645\ttotal: 13.9s\tremaining: 1m 22s\n",
      "144:\tlearn: 0.1296306\ttotal: 14s\tremaining: 1m 22s\n",
      "145:\tlearn: 0.1295015\ttotal: 14.1s\tremaining: 1m 22s\n",
      "146:\tlearn: 0.1293417\ttotal: 14.2s\tremaining: 1m 22s\n",
      "147:\tlearn: 0.1291925\ttotal: 14.3s\tremaining: 1m 22s\n",
      "148:\tlearn: 0.1290488\ttotal: 14.4s\tremaining: 1m 22s\n",
      "149:\tlearn: 0.1288731\ttotal: 14.5s\tremaining: 1m 22s\n",
      "150:\tlearn: 0.1287745\ttotal: 14.6s\tremaining: 1m 22s\n",
      "151:\tlearn: 0.1286384\ttotal: 14.7s\tremaining: 1m 21s\n",
      "152:\tlearn: 0.1285143\ttotal: 14.8s\tremaining: 1m 21s\n",
      "153:\tlearn: 0.1283022\ttotal: 14.9s\tremaining: 1m 21s\n",
      "154:\tlearn: 0.1280641\ttotal: 15s\tremaining: 1m 21s\n",
      "155:\tlearn: 0.1279204\ttotal: 15.1s\tremaining: 1m 21s\n",
      "156:\tlearn: 0.1277804\ttotal: 15.1s\tremaining: 1m 21s\n",
      "157:\tlearn: 0.1276370\ttotal: 15.2s\tremaining: 1m 21s\n",
      "158:\tlearn: 0.1275417\ttotal: 15.3s\tremaining: 1m 21s\n",
      "159:\tlearn: 0.1273958\ttotal: 15.4s\tremaining: 1m 20s\n",
      "160:\tlearn: 0.1272892\ttotal: 15.5s\tremaining: 1m 20s\n",
      "161:\tlearn: 0.1271030\ttotal: 15.6s\tremaining: 1m 20s\n",
      "162:\tlearn: 0.1270077\ttotal: 15.7s\tremaining: 1m 20s\n",
      "163:\tlearn: 0.1268669\ttotal: 15.8s\tremaining: 1m 20s\n",
      "164:\tlearn: 0.1267244\ttotal: 15.9s\tremaining: 1m 20s\n",
      "165:\tlearn: 0.1266211\ttotal: 16s\tremaining: 1m 20s\n",
      "166:\tlearn: 0.1264340\ttotal: 16.1s\tremaining: 1m 20s\n",
      "167:\tlearn: 0.1262935\ttotal: 16.2s\tremaining: 1m 20s\n",
      "168:\tlearn: 0.1261899\ttotal: 16.3s\tremaining: 1m 20s\n",
      "169:\tlearn: 0.1260848\ttotal: 16.4s\tremaining: 1m 20s\n",
      "170:\tlearn: 0.1259992\ttotal: 16.5s\tremaining: 1m 20s\n",
      "171:\tlearn: 0.1258176\ttotal: 16.6s\tremaining: 1m 19s\n",
      "172:\tlearn: 0.1256927\ttotal: 16.7s\tremaining: 1m 19s\n",
      "173:\tlearn: 0.1255513\ttotal: 16.8s\tremaining: 1m 19s\n",
      "174:\tlearn: 0.1254354\ttotal: 16.9s\tremaining: 1m 19s\n",
      "175:\tlearn: 0.1252639\ttotal: 17s\tremaining: 1m 19s\n",
      "176:\tlearn: 0.1251941\ttotal: 17.1s\tremaining: 1m 19s\n",
      "177:\tlearn: 0.1251105\ttotal: 17.3s\tremaining: 1m 19s\n",
      "178:\tlearn: 0.1249704\ttotal: 17.4s\tremaining: 1m 19s\n",
      "179:\tlearn: 0.1248085\ttotal: 17.5s\tremaining: 1m 19s\n",
      "180:\tlearn: 0.1246514\ttotal: 17.6s\tremaining: 1m 19s\n",
      "181:\tlearn: 0.1245686\ttotal: 17.6s\tremaining: 1m 19s\n",
      "182:\tlearn: 0.1244722\ttotal: 17.7s\tremaining: 1m 19s\n",
      "183:\tlearn: 0.1243482\ttotal: 17.8s\tremaining: 1m 19s\n",
      "184:\tlearn: 0.1242375\ttotal: 17.9s\tremaining: 1m 18s\n",
      "185:\tlearn: 0.1241323\ttotal: 18s\tremaining: 1m 18s\n",
      "186:\tlearn: 0.1239512\ttotal: 18.1s\tremaining: 1m 18s\n",
      "187:\tlearn: 0.1238090\ttotal: 18.2s\tremaining: 1m 18s\n",
      "188:\tlearn: 0.1236879\ttotal: 18.3s\tremaining: 1m 18s\n",
      "189:\tlearn: 0.1235623\ttotal: 18.4s\tremaining: 1m 18s\n",
      "190:\tlearn: 0.1234524\ttotal: 18.5s\tremaining: 1m 18s\n",
      "191:\tlearn: 0.1233731\ttotal: 18.6s\tremaining: 1m 18s\n",
      "192:\tlearn: 0.1232854\ttotal: 18.6s\tremaining: 1m 17s\n",
      "193:\tlearn: 0.1232227\ttotal: 18.7s\tremaining: 1m 17s\n",
      "194:\tlearn: 0.1231062\ttotal: 18.8s\tremaining: 1m 17s\n",
      "195:\tlearn: 0.1229690\ttotal: 18.9s\tremaining: 1m 17s\n",
      "196:\tlearn: 0.1228751\ttotal: 19s\tremaining: 1m 17s\n",
      "197:\tlearn: 0.1227930\ttotal: 19.1s\tremaining: 1m 17s\n",
      "198:\tlearn: 0.1227288\ttotal: 19.2s\tremaining: 1m 17s\n",
      "199:\tlearn: 0.1225978\ttotal: 19.3s\tremaining: 1m 17s\n",
      "200:\tlearn: 0.1224208\ttotal: 19.4s\tremaining: 1m 16s\n",
      "201:\tlearn: 0.1223269\ttotal: 19.5s\tremaining: 1m 16s\n",
      "202:\tlearn: 0.1222359\ttotal: 19.6s\tremaining: 1m 16s\n",
      "203:\tlearn: 0.1221697\ttotal: 19.7s\tremaining: 1m 16s\n",
      "204:\tlearn: 0.1220919\ttotal: 19.7s\tremaining: 1m 16s\n",
      "205:\tlearn: 0.1220124\ttotal: 19.8s\tremaining: 1m 16s\n",
      "206:\tlearn: 0.1219422\ttotal: 19.9s\tremaining: 1m 16s\n",
      "207:\tlearn: 0.1218490\ttotal: 20s\tremaining: 1m 16s\n",
      "208:\tlearn: 0.1217777\ttotal: 20.1s\tremaining: 1m 16s\n",
      "209:\tlearn: 0.1216703\ttotal: 20.2s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1215809\ttotal: 20.3s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1215300\ttotal: 20.4s\tremaining: 1m 15s\n",
      "212:\tlearn: 0.1214200\ttotal: 20.5s\tremaining: 1m 15s\n",
      "213:\tlearn: 0.1213492\ttotal: 20.6s\tremaining: 1m 15s\n",
      "214:\tlearn: 0.1212171\ttotal: 20.7s\tremaining: 1m 15s\n",
      "215:\tlearn: 0.1211333\ttotal: 20.8s\tremaining: 1m 15s\n",
      "216:\tlearn: 0.1209849\ttotal: 20.9s\tremaining: 1m 15s\n",
      "217:\tlearn: 0.1208524\ttotal: 21s\tremaining: 1m 15s\n",
      "218:\tlearn: 0.1207570\ttotal: 21.1s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1206696\ttotal: 21.2s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1205814\ttotal: 21.3s\tremaining: 1m 14s\n",
      "221:\tlearn: 0.1205084\ttotal: 21.4s\tremaining: 1m 14s\n",
      "222:\tlearn: 0.1204526\ttotal: 21.5s\tremaining: 1m 14s\n",
      "223:\tlearn: 0.1203039\ttotal: 21.6s\tremaining: 1m 14s\n",
      "224:\tlearn: 0.1201944\ttotal: 21.7s\tremaining: 1m 14s\n",
      "225:\tlearn: 0.1201260\ttotal: 21.8s\tremaining: 1m 14s\n",
      "226:\tlearn: 0.1200675\ttotal: 21.9s\tremaining: 1m 14s\n",
      "227:\tlearn: 0.1199782\ttotal: 22s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1198787\ttotal: 22.1s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1197470\ttotal: 22.2s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1195795\ttotal: 22.3s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1195441\ttotal: 22.4s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1194321\ttotal: 22.5s\tremaining: 1m 13s\n",
      "233:\tlearn: 0.1193388\ttotal: 22.6s\tremaining: 1m 13s\n",
      "234:\tlearn: 0.1192196\ttotal: 22.7s\tremaining: 1m 13s\n",
      "235:\tlearn: 0.1191617\ttotal: 22.8s\tremaining: 1m 13s\n",
      "236:\tlearn: 0.1190994\ttotal: 22.9s\tremaining: 1m 13s\n",
      "237:\tlearn: 0.1190339\ttotal: 23.1s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1189300\ttotal: 23.2s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1188731\ttotal: 23.4s\tremaining: 1m 14s\n",
      "240:\tlearn: 0.1187465\ttotal: 23.5s\tremaining: 1m 14s\n",
      "241:\tlearn: 0.1185917\ttotal: 23.6s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1184509\ttotal: 23.7s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1183171\ttotal: 23.8s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1182474\ttotal: 24s\tremaining: 1m 13s\n",
      "245:\tlearn: 0.1181438\ttotal: 24.1s\tremaining: 1m 13s\n",
      "246:\tlearn: 0.1180360\ttotal: 24.3s\tremaining: 1m 14s\n",
      "247:\tlearn: 0.1179527\ttotal: 24.5s\tremaining: 1m 14s\n",
      "248:\tlearn: 0.1178847\ttotal: 24.6s\tremaining: 1m 14s\n",
      "249:\tlearn: 0.1177990\ttotal: 24.7s\tremaining: 1m 14s\n",
      "250:\tlearn: 0.1177112\ttotal: 24.8s\tremaining: 1m 13s\n",
      "251:\tlearn: 0.1176156\ttotal: 24.9s\tremaining: 1m 13s\n",
      "252:\tlearn: 0.1175522\ttotal: 25s\tremaining: 1m 13s\n",
      "253:\tlearn: 0.1174925\ttotal: 25.1s\tremaining: 1m 13s\n",
      "254:\tlearn: 0.1173682\ttotal: 25.2s\tremaining: 1m 13s\n",
      "255:\tlearn: 0.1172844\ttotal: 25.3s\tremaining: 1m 13s\n",
      "256:\tlearn: 0.1172253\ttotal: 25.4s\tremaining: 1m 13s\n",
      "257:\tlearn: 0.1171145\ttotal: 25.5s\tremaining: 1m 13s\n",
      "258:\tlearn: 0.1169486\ttotal: 25.6s\tremaining: 1m 13s\n",
      "259:\tlearn: 0.1168518\ttotal: 25.7s\tremaining: 1m 13s\n",
      "260:\tlearn: 0.1167141\ttotal: 25.8s\tremaining: 1m 13s\n",
      "261:\tlearn: 0.1166056\ttotal: 25.9s\tremaining: 1m 12s\n",
      "262:\tlearn: 0.1165067\ttotal: 26s\tremaining: 1m 12s\n",
      "263:\tlearn: 0.1163971\ttotal: 26.1s\tremaining: 1m 12s\n",
      "264:\tlearn: 0.1163305\ttotal: 26.2s\tremaining: 1m 12s\n",
      "265:\tlearn: 0.1162906\ttotal: 26.3s\tremaining: 1m 12s\n",
      "266:\tlearn: 0.1161653\ttotal: 26.4s\tremaining: 1m 12s\n",
      "267:\tlearn: 0.1161059\ttotal: 26.5s\tremaining: 1m 12s\n",
      "268:\tlearn: 0.1160543\ttotal: 26.6s\tremaining: 1m 12s\n",
      "269:\tlearn: 0.1159719\ttotal: 26.7s\tremaining: 1m 12s\n",
      "270:\tlearn: 0.1159439\ttotal: 26.8s\tremaining: 1m 12s\n",
      "271:\tlearn: 0.1158694\ttotal: 26.9s\tremaining: 1m 12s\n",
      "272:\tlearn: 0.1157780\ttotal: 27s\tremaining: 1m 11s\n",
      "273:\tlearn: 0.1156777\ttotal: 27.1s\tremaining: 1m 11s\n",
      "274:\tlearn: 0.1155581\ttotal: 27.2s\tremaining: 1m 11s\n",
      "275:\tlearn: 0.1154564\ttotal: 27.3s\tremaining: 1m 11s\n",
      "276:\tlearn: 0.1153883\ttotal: 27.4s\tremaining: 1m 11s\n",
      "277:\tlearn: 0.1153035\ttotal: 27.5s\tremaining: 1m 11s\n",
      "278:\tlearn: 0.1152216\ttotal: 27.6s\tremaining: 1m 11s\n",
      "279:\tlearn: 0.1151446\ttotal: 27.7s\tremaining: 1m 11s\n",
      "280:\tlearn: 0.1150910\ttotal: 27.8s\tremaining: 1m 11s\n",
      "281:\tlearn: 0.1149703\ttotal: 27.9s\tremaining: 1m 11s\n",
      "282:\tlearn: 0.1149283\ttotal: 28.1s\tremaining: 1m 11s\n",
      "283:\tlearn: 0.1148753\ttotal: 28.2s\tremaining: 1m 11s\n",
      "284:\tlearn: 0.1148054\ttotal: 28.3s\tremaining: 1m 11s\n",
      "285:\tlearn: 0.1147372\ttotal: 28.4s\tremaining: 1m 10s\n",
      "286:\tlearn: 0.1146632\ttotal: 28.5s\tremaining: 1m 10s\n",
      "287:\tlearn: 0.1145697\ttotal: 28.6s\tremaining: 1m 10s\n",
      "288:\tlearn: 0.1144803\ttotal: 28.7s\tremaining: 1m 10s\n",
      "289:\tlearn: 0.1143799\ttotal: 28.8s\tremaining: 1m 10s\n",
      "290:\tlearn: 0.1142951\ttotal: 28.9s\tremaining: 1m 10s\n",
      "291:\tlearn: 0.1141846\ttotal: 29s\tremaining: 1m 10s\n",
      "292:\tlearn: 0.1141160\ttotal: 29.1s\tremaining: 1m 10s\n",
      "293:\tlearn: 0.1140380\ttotal: 29.2s\tremaining: 1m 10s\n",
      "294:\tlearn: 0.1139657\ttotal: 29.3s\tremaining: 1m 9s\n",
      "295:\tlearn: 0.1139092\ttotal: 29.4s\tremaining: 1m 9s\n",
      "296:\tlearn: 0.1138104\ttotal: 29.5s\tremaining: 1m 9s\n",
      "297:\tlearn: 0.1137226\ttotal: 29.6s\tremaining: 1m 9s\n",
      "298:\tlearn: 0.1136778\ttotal: 29.7s\tremaining: 1m 9s\n",
      "299:\tlearn: 0.1136180\ttotal: 29.8s\tremaining: 1m 9s\n",
      "300:\tlearn: 0.1135102\ttotal: 29.9s\tremaining: 1m 9s\n",
      "301:\tlearn: 0.1134408\ttotal: 30s\tremaining: 1m 9s\n",
      "302:\tlearn: 0.1133739\ttotal: 30s\tremaining: 1m 9s\n",
      "303:\tlearn: 0.1132882\ttotal: 30.1s\tremaining: 1m 9s\n",
      "304:\tlearn: 0.1132080\ttotal: 30.2s\tremaining: 1m 8s\n",
      "305:\tlearn: 0.1131298\ttotal: 30.3s\tremaining: 1m 8s\n",
      "306:\tlearn: 0.1130438\ttotal: 30.4s\tremaining: 1m 8s\n",
      "307:\tlearn: 0.1129823\ttotal: 30.5s\tremaining: 1m 8s\n",
      "308:\tlearn: 0.1129176\ttotal: 30.6s\tremaining: 1m 8s\n",
      "309:\tlearn: 0.1128365\ttotal: 30.7s\tremaining: 1m 8s\n",
      "310:\tlearn: 0.1127855\ttotal: 30.8s\tremaining: 1m 8s\n",
      "311:\tlearn: 0.1127301\ttotal: 30.9s\tremaining: 1m 8s\n",
      "312:\tlearn: 0.1126855\ttotal: 31s\tremaining: 1m 8s\n",
      "313:\tlearn: 0.1126111\ttotal: 31.1s\tremaining: 1m 7s\n",
      "314:\tlearn: 0.1125252\ttotal: 31.2s\tremaining: 1m 7s\n",
      "315:\tlearn: 0.1124428\ttotal: 31.3s\tremaining: 1m 7s\n",
      "316:\tlearn: 0.1123970\ttotal: 31.4s\tremaining: 1m 7s\n",
      "317:\tlearn: 0.1122772\ttotal: 31.5s\tremaining: 1m 7s\n",
      "318:\tlearn: 0.1121640\ttotal: 31.6s\tremaining: 1m 7s\n",
      "319:\tlearn: 0.1120998\ttotal: 31.7s\tremaining: 1m 7s\n",
      "320:\tlearn: 0.1120326\ttotal: 31.8s\tremaining: 1m 7s\n",
      "321:\tlearn: 0.1119446\ttotal: 31.9s\tremaining: 1m 7s\n",
      "322:\tlearn: 0.1118592\ttotal: 31.9s\tremaining: 1m 6s\n",
      "323:\tlearn: 0.1118031\ttotal: 32s\tremaining: 1m 6s\n",
      "324:\tlearn: 0.1117431\ttotal: 32.1s\tremaining: 1m 6s\n",
      "325:\tlearn: 0.1116729\ttotal: 32.2s\tremaining: 1m 6s\n",
      "326:\tlearn: 0.1115677\ttotal: 32.3s\tremaining: 1m 6s\n",
      "327:\tlearn: 0.1115249\ttotal: 32.4s\tremaining: 1m 6s\n",
      "328:\tlearn: 0.1114121\ttotal: 32.5s\tremaining: 1m 6s\n",
      "329:\tlearn: 0.1113030\ttotal: 32.6s\tremaining: 1m 6s\n",
      "330:\tlearn: 0.1112434\ttotal: 32.7s\tremaining: 1m 6s\n",
      "331:\tlearn: 0.1111463\ttotal: 32.8s\tremaining: 1m 5s\n",
      "332:\tlearn: 0.1111192\ttotal: 32.9s\tremaining: 1m 5s\n",
      "333:\tlearn: 0.1110348\ttotal: 33s\tremaining: 1m 5s\n",
      "334:\tlearn: 0.1109454\ttotal: 33.1s\tremaining: 1m 5s\n",
      "335:\tlearn: 0.1108782\ttotal: 33.2s\tremaining: 1m 5s\n",
      "336:\tlearn: 0.1107875\ttotal: 33.3s\tremaining: 1m 5s\n",
      "337:\tlearn: 0.1106851\ttotal: 33.4s\tremaining: 1m 5s\n",
      "338:\tlearn: 0.1105861\ttotal: 33.4s\tremaining: 1m 5s\n",
      "339:\tlearn: 0.1105106\ttotal: 33.5s\tremaining: 1m 5s\n",
      "340:\tlearn: 0.1104357\ttotal: 33.6s\tremaining: 1m 5s\n",
      "341:\tlearn: 0.1103875\ttotal: 33.7s\tremaining: 1m 4s\n",
      "342:\tlearn: 0.1103168\ttotal: 33.8s\tremaining: 1m 4s\n",
      "343:\tlearn: 0.1102018\ttotal: 33.9s\tremaining: 1m 4s\n",
      "344:\tlearn: 0.1101294\ttotal: 34s\tremaining: 1m 4s\n",
      "345:\tlearn: 0.1100577\ttotal: 34.1s\tremaining: 1m 4s\n",
      "346:\tlearn: 0.1099968\ttotal: 34.2s\tremaining: 1m 4s\n",
      "347:\tlearn: 0.1099362\ttotal: 34.3s\tremaining: 1m 4s\n",
      "348:\tlearn: 0.1098737\ttotal: 34.4s\tremaining: 1m 4s\n",
      "349:\tlearn: 0.1098092\ttotal: 34.5s\tremaining: 1m 4s\n",
      "350:\tlearn: 0.1097635\ttotal: 34.6s\tremaining: 1m 3s\n",
      "351:\tlearn: 0.1097189\ttotal: 34.7s\tremaining: 1m 3s\n",
      "352:\tlearn: 0.1096532\ttotal: 34.8s\tremaining: 1m 3s\n",
      "353:\tlearn: 0.1095847\ttotal: 34.9s\tremaining: 1m 3s\n",
      "354:\tlearn: 0.1095200\ttotal: 34.9s\tremaining: 1m 3s\n",
      "355:\tlearn: 0.1094191\ttotal: 35s\tremaining: 1m 3s\n",
      "356:\tlearn: 0.1093657\ttotal: 35.1s\tremaining: 1m 3s\n",
      "357:\tlearn: 0.1093055\ttotal: 35.2s\tremaining: 1m 3s\n",
      "358:\tlearn: 0.1092443\ttotal: 35.3s\tremaining: 1m 3s\n",
      "359:\tlearn: 0.1092005\ttotal: 35.4s\tremaining: 1m 2s\n",
      "360:\tlearn: 0.1091248\ttotal: 35.5s\tremaining: 1m 2s\n",
      "361:\tlearn: 0.1090493\ttotal: 35.6s\tremaining: 1m 2s\n",
      "362:\tlearn: 0.1090259\ttotal: 35.7s\tremaining: 1m 2s\n",
      "363:\tlearn: 0.1089916\ttotal: 35.8s\tremaining: 1m 2s\n",
      "364:\tlearn: 0.1088839\ttotal: 35.9s\tremaining: 1m 2s\n",
      "365:\tlearn: 0.1087817\ttotal: 36s\tremaining: 1m 2s\n",
      "366:\tlearn: 0.1087012\ttotal: 36.1s\tremaining: 1m 2s\n",
      "367:\tlearn: 0.1086499\ttotal: 36.3s\tremaining: 1m 2s\n",
      "368:\tlearn: 0.1085936\ttotal: 36.4s\tremaining: 1m 2s\n",
      "369:\tlearn: 0.1085467\ttotal: 36.5s\tremaining: 1m 2s\n",
      "370:\tlearn: 0.1084983\ttotal: 36.6s\tremaining: 1m 1s\n",
      "371:\tlearn: 0.1084301\ttotal: 36.7s\tremaining: 1m 1s\n",
      "372:\tlearn: 0.1083620\ttotal: 36.8s\tremaining: 1m 1s\n",
      "373:\tlearn: 0.1083142\ttotal: 36.9s\tremaining: 1m 1s\n",
      "374:\tlearn: 0.1082587\ttotal: 36.9s\tremaining: 1m 1s\n",
      "375:\tlearn: 0.1082202\ttotal: 37s\tremaining: 1m 1s\n",
      "376:\tlearn: 0.1081757\ttotal: 37.1s\tremaining: 1m 1s\n",
      "377:\tlearn: 0.1081284\ttotal: 37.2s\tremaining: 1m 1s\n",
      "378:\tlearn: 0.1080837\ttotal: 37.3s\tremaining: 1m 1s\n",
      "379:\tlearn: 0.1080105\ttotal: 37.4s\tremaining: 1m 1s\n",
      "380:\tlearn: 0.1079709\ttotal: 37.5s\tremaining: 1m\n",
      "381:\tlearn: 0.1078845\ttotal: 37.6s\tremaining: 1m\n",
      "382:\tlearn: 0.1078084\ttotal: 37.7s\tremaining: 1m\n",
      "383:\tlearn: 0.1077160\ttotal: 37.8s\tremaining: 1m\n",
      "384:\tlearn: 0.1076457\ttotal: 37.9s\tremaining: 1m\n",
      "385:\tlearn: 0.1075944\ttotal: 38s\tremaining: 1m\n",
      "386:\tlearn: 0.1075337\ttotal: 38.1s\tremaining: 1m\n",
      "387:\tlearn: 0.1074716\ttotal: 38.2s\tremaining: 1m\n",
      "388:\tlearn: 0.1074126\ttotal: 38.3s\tremaining: 1m\n",
      "389:\tlearn: 0.1073613\ttotal: 38.4s\tremaining: 1m\n",
      "390:\tlearn: 0.1072946\ttotal: 38.5s\tremaining: 59.9s\n",
      "391:\tlearn: 0.1071978\ttotal: 38.6s\tremaining: 59.8s\n",
      "392:\tlearn: 0.1071514\ttotal: 38.7s\tremaining: 59.7s\n",
      "393:\tlearn: 0.1070677\ttotal: 38.8s\tremaining: 59.6s\n",
      "394:\tlearn: 0.1069788\ttotal: 38.9s\tremaining: 59.5s\n",
      "395:\tlearn: 0.1069103\ttotal: 39s\tremaining: 59.4s\n",
      "396:\tlearn: 0.1068366\ttotal: 39.1s\tremaining: 59.3s\n",
      "397:\tlearn: 0.1067630\ttotal: 39.2s\tremaining: 59.2s\n",
      "398:\tlearn: 0.1066849\ttotal: 39.2s\tremaining: 59.1s\n",
      "399:\tlearn: 0.1066019\ttotal: 39.3s\tremaining: 59s\n",
      "400:\tlearn: 0.1065090\ttotal: 39.4s\tremaining: 58.9s\n",
      "401:\tlearn: 0.1064396\ttotal: 39.5s\tremaining: 58.8s\n",
      "402:\tlearn: 0.1063973\ttotal: 39.6s\tremaining: 58.7s\n",
      "403:\tlearn: 0.1063481\ttotal: 39.7s\tremaining: 58.6s\n",
      "404:\tlearn: 0.1062770\ttotal: 39.8s\tremaining: 58.5s\n",
      "405:\tlearn: 0.1062303\ttotal: 39.9s\tremaining: 58.4s\n",
      "406:\tlearn: 0.1061704\ttotal: 40s\tremaining: 58.3s\n",
      "407:\tlearn: 0.1061245\ttotal: 40.1s\tremaining: 58.2s\n",
      "408:\tlearn: 0.1060988\ttotal: 40.2s\tremaining: 58.1s\n",
      "409:\tlearn: 0.1060247\ttotal: 40.3s\tremaining: 58s\n",
      "410:\tlearn: 0.1059980\ttotal: 40.4s\tremaining: 57.9s\n",
      "411:\tlearn: 0.1059226\ttotal: 40.5s\tremaining: 57.8s\n",
      "412:\tlearn: 0.1058734\ttotal: 40.6s\tremaining: 57.7s\n",
      "413:\tlearn: 0.1058276\ttotal: 40.7s\tremaining: 57.6s\n",
      "414:\tlearn: 0.1057688\ttotal: 40.8s\tremaining: 57.5s\n",
      "415:\tlearn: 0.1057226\ttotal: 40.9s\tremaining: 57.5s\n",
      "416:\tlearn: 0.1056749\ttotal: 41s\tremaining: 57.3s\n",
      "417:\tlearn: 0.1056240\ttotal: 41.1s\tremaining: 57.2s\n",
      "418:\tlearn: 0.1055933\ttotal: 41.2s\tremaining: 57.1s\n",
      "419:\tlearn: 0.1055730\ttotal: 41.3s\tremaining: 57s\n",
      "420:\tlearn: 0.1055468\ttotal: 41.4s\tremaining: 57s\n",
      "421:\tlearn: 0.1054895\ttotal: 41.5s\tremaining: 56.9s\n",
      "422:\tlearn: 0.1054299\ttotal: 41.6s\tremaining: 56.8s\n",
      "423:\tlearn: 0.1053506\ttotal: 41.7s\tremaining: 56.7s\n",
      "424:\tlearn: 0.1052914\ttotal: 41.8s\tremaining: 56.5s\n",
      "425:\tlearn: 0.1052305\ttotal: 41.9s\tremaining: 56.4s\n",
      "426:\tlearn: 0.1051949\ttotal: 42s\tremaining: 56.3s\n",
      "427:\tlearn: 0.1051494\ttotal: 42.1s\tremaining: 56.2s\n",
      "428:\tlearn: 0.1051117\ttotal: 42.2s\tremaining: 56.1s\n",
      "429:\tlearn: 0.1050474\ttotal: 42.3s\tremaining: 56s\n",
      "430:\tlearn: 0.1049814\ttotal: 42.4s\tremaining: 55.9s\n",
      "431:\tlearn: 0.1049244\ttotal: 42.5s\tremaining: 55.8s\n",
      "432:\tlearn: 0.1048423\ttotal: 42.6s\tremaining: 55.7s\n",
      "433:\tlearn: 0.1047374\ttotal: 42.7s\tremaining: 55.6s\n",
      "434:\tlearn: 0.1046912\ttotal: 42.8s\tremaining: 55.5s\n",
      "435:\tlearn: 0.1046606\ttotal: 42.8s\tremaining: 55.4s\n",
      "436:\tlearn: 0.1046121\ttotal: 42.9s\tremaining: 55.3s\n",
      "437:\tlearn: 0.1045662\ttotal: 43s\tremaining: 55.2s\n",
      "438:\tlearn: 0.1045301\ttotal: 43.1s\tremaining: 55.1s\n",
      "439:\tlearn: 0.1044950\ttotal: 43.2s\tremaining: 55s\n",
      "440:\tlearn: 0.1044465\ttotal: 43.3s\tremaining: 54.9s\n",
      "441:\tlearn: 0.1043102\ttotal: 43.4s\tremaining: 54.8s\n",
      "442:\tlearn: 0.1042005\ttotal: 43.5s\tremaining: 54.7s\n",
      "443:\tlearn: 0.1041277\ttotal: 43.6s\tremaining: 54.6s\n",
      "444:\tlearn: 0.1040635\ttotal: 43.7s\tremaining: 54.5s\n",
      "445:\tlearn: 0.1039909\ttotal: 43.8s\tremaining: 54.4s\n",
      "446:\tlearn: 0.1039576\ttotal: 43.9s\tremaining: 54.3s\n",
      "447:\tlearn: 0.1039084\ttotal: 44s\tremaining: 54.2s\n",
      "448:\tlearn: 0.1038744\ttotal: 44.1s\tremaining: 54.1s\n",
      "449:\tlearn: 0.1038494\ttotal: 44.2s\tremaining: 54s\n",
      "450:\tlearn: 0.1037860\ttotal: 44.3s\tremaining: 53.9s\n",
      "451:\tlearn: 0.1037561\ttotal: 44.4s\tremaining: 53.8s\n",
      "452:\tlearn: 0.1037093\ttotal: 44.5s\tremaining: 53.7s\n",
      "453:\tlearn: 0.1036297\ttotal: 44.6s\tremaining: 53.6s\n",
      "454:\tlearn: 0.1035982\ttotal: 44.6s\tremaining: 53.5s\n",
      "455:\tlearn: 0.1035144\ttotal: 44.7s\tremaining: 53.4s\n",
      "456:\tlearn: 0.1034706\ttotal: 44.8s\tremaining: 53.3s\n",
      "457:\tlearn: 0.1034171\ttotal: 44.9s\tremaining: 53.2s\n",
      "458:\tlearn: 0.1033626\ttotal: 45s\tremaining: 53.1s\n",
      "459:\tlearn: 0.1032723\ttotal: 45.1s\tremaining: 53s\n",
      "460:\tlearn: 0.1031601\ttotal: 45.2s\tremaining: 52.9s\n",
      "461:\tlearn: 0.1031226\ttotal: 45.3s\tremaining: 52.8s\n",
      "462:\tlearn: 0.1030834\ttotal: 45.4s\tremaining: 52.7s\n",
      "463:\tlearn: 0.1030446\ttotal: 45.5s\tremaining: 52.6s\n",
      "464:\tlearn: 0.1029850\ttotal: 45.6s\tremaining: 52.5s\n",
      "465:\tlearn: 0.1028897\ttotal: 45.7s\tremaining: 52.4s\n",
      "466:\tlearn: 0.1028639\ttotal: 45.8s\tremaining: 52.3s\n",
      "467:\tlearn: 0.1028516\ttotal: 45.9s\tremaining: 52.2s\n",
      "468:\tlearn: 0.1028094\ttotal: 46s\tremaining: 52.1s\n",
      "469:\tlearn: 0.1027897\ttotal: 46.1s\tremaining: 52s\n",
      "470:\tlearn: 0.1027187\ttotal: 46.2s\tremaining: 51.9s\n",
      "471:\tlearn: 0.1026495\ttotal: 46.3s\tremaining: 51.8s\n",
      "472:\tlearn: 0.1025851\ttotal: 46.4s\tremaining: 51.7s\n",
      "473:\tlearn: 0.1025098\ttotal: 46.5s\tremaining: 51.6s\n",
      "474:\tlearn: 0.1024712\ttotal: 46.7s\tremaining: 51.6s\n",
      "475:\tlearn: 0.1024305\ttotal: 46.8s\tremaining: 51.5s\n",
      "476:\tlearn: 0.1024087\ttotal: 46.9s\tremaining: 51.4s\n",
      "477:\tlearn: 0.1023255\ttotal: 47s\tremaining: 51.3s\n",
      "478:\tlearn: 0.1022617\ttotal: 47.1s\tremaining: 51.2s\n",
      "479:\tlearn: 0.1022078\ttotal: 47.2s\tremaining: 51.1s\n",
      "480:\tlearn: 0.1021759\ttotal: 47.3s\tremaining: 51s\n",
      "481:\tlearn: 0.1021272\ttotal: 47.4s\tremaining: 51s\n",
      "482:\tlearn: 0.1020570\ttotal: 47.5s\tremaining: 50.9s\n",
      "483:\tlearn: 0.1020120\ttotal: 47.6s\tremaining: 50.8s\n",
      "484:\tlearn: 0.1019934\ttotal: 47.7s\tremaining: 50.7s\n",
      "485:\tlearn: 0.1019420\ttotal: 47.8s\tremaining: 50.6s\n",
      "486:\tlearn: 0.1018721\ttotal: 47.9s\tremaining: 50.5s\n",
      "487:\tlearn: 0.1018246\ttotal: 48s\tremaining: 50.4s\n",
      "488:\tlearn: 0.1017632\ttotal: 48.1s\tremaining: 50.3s\n",
      "489:\tlearn: 0.1016866\ttotal: 48.2s\tremaining: 50.2s\n",
      "490:\tlearn: 0.1016198\ttotal: 48.3s\tremaining: 50.1s\n",
      "491:\tlearn: 0.1015667\ttotal: 48.4s\tremaining: 49.9s\n",
      "492:\tlearn: 0.1015109\ttotal: 48.5s\tremaining: 49.8s\n",
      "493:\tlearn: 0.1014447\ttotal: 48.6s\tremaining: 49.8s\n",
      "494:\tlearn: 0.1013670\ttotal: 48.7s\tremaining: 49.6s\n",
      "495:\tlearn: 0.1013284\ttotal: 48.8s\tremaining: 49.5s\n",
      "496:\tlearn: 0.1012985\ttotal: 48.8s\tremaining: 49.4s\n",
      "497:\tlearn: 0.1012371\ttotal: 48.9s\tremaining: 49.3s\n",
      "498:\tlearn: 0.1011709\ttotal: 49s\tremaining: 49.2s\n",
      "499:\tlearn: 0.1011129\ttotal: 49.1s\tremaining: 49.1s\n",
      "500:\tlearn: 0.1010682\ttotal: 49.2s\tremaining: 49s\n",
      "501:\tlearn: 0.1010568\ttotal: 49.3s\tremaining: 48.9s\n",
      "502:\tlearn: 0.1009981\ttotal: 49.4s\tremaining: 48.8s\n",
      "503:\tlearn: 0.1009505\ttotal: 49.5s\tremaining: 48.7s\n",
      "504:\tlearn: 0.1009093\ttotal: 49.6s\tremaining: 48.6s\n",
      "505:\tlearn: 0.1008739\ttotal: 49.7s\tremaining: 48.5s\n",
      "506:\tlearn: 0.1008095\ttotal: 49.8s\tremaining: 48.4s\n",
      "507:\tlearn: 0.1007630\ttotal: 49.9s\tremaining: 48.3s\n",
      "508:\tlearn: 0.1007177\ttotal: 50s\tremaining: 48.2s\n",
      "509:\tlearn: 0.1006971\ttotal: 50.1s\tremaining: 48.1s\n",
      "510:\tlearn: 0.1006391\ttotal: 50.2s\tremaining: 48s\n",
      "511:\tlearn: 0.1006089\ttotal: 50.3s\tremaining: 47.9s\n",
      "512:\tlearn: 0.1005440\ttotal: 50.4s\tremaining: 47.8s\n",
      "513:\tlearn: 0.1005097\ttotal: 50.5s\tremaining: 47.7s\n",
      "514:\tlearn: 0.1004431\ttotal: 50.5s\tremaining: 47.6s\n",
      "515:\tlearn: 0.1004166\ttotal: 50.6s\tremaining: 47.5s\n",
      "516:\tlearn: 0.1003514\ttotal: 50.7s\tremaining: 47.4s\n",
      "517:\tlearn: 0.1002334\ttotal: 50.8s\tremaining: 47.3s\n",
      "518:\tlearn: 0.1001722\ttotal: 50.9s\tremaining: 47.2s\n",
      "519:\tlearn: 0.1001560\ttotal: 51s\tremaining: 47.1s\n",
      "520:\tlearn: 0.1001316\ttotal: 51.1s\tremaining: 47s\n",
      "521:\tlearn: 0.1000732\ttotal: 51.2s\tremaining: 46.9s\n",
      "522:\tlearn: 0.0999689\ttotal: 51.3s\tremaining: 46.8s\n",
      "523:\tlearn: 0.0999286\ttotal: 51.4s\tremaining: 46.7s\n",
      "524:\tlearn: 0.0998779\ttotal: 51.5s\tremaining: 46.6s\n",
      "525:\tlearn: 0.0998419\ttotal: 51.6s\tremaining: 46.5s\n",
      "526:\tlearn: 0.0998039\ttotal: 51.7s\tremaining: 46.4s\n",
      "527:\tlearn: 0.0997711\ttotal: 51.8s\tremaining: 46.3s\n",
      "528:\tlearn: 0.0997213\ttotal: 51.9s\tremaining: 46.2s\n",
      "529:\tlearn: 0.0996611\ttotal: 52s\tremaining: 46.1s\n",
      "530:\tlearn: 0.0996231\ttotal: 52.1s\tremaining: 46s\n",
      "531:\tlearn: 0.0995576\ttotal: 52.2s\tremaining: 45.9s\n",
      "532:\tlearn: 0.0994983\ttotal: 52.3s\tremaining: 45.8s\n",
      "533:\tlearn: 0.0994420\ttotal: 52.4s\tremaining: 45.7s\n",
      "534:\tlearn: 0.0993978\ttotal: 52.5s\tremaining: 45.6s\n",
      "535:\tlearn: 0.0993676\ttotal: 52.6s\tremaining: 45.5s\n",
      "536:\tlearn: 0.0993292\ttotal: 52.7s\tremaining: 45.4s\n",
      "537:\tlearn: 0.0992897\ttotal: 52.8s\tremaining: 45.3s\n",
      "538:\tlearn: 0.0992303\ttotal: 52.9s\tremaining: 45.2s\n",
      "539:\tlearn: 0.0991846\ttotal: 53s\tremaining: 45.1s\n",
      "540:\tlearn: 0.0991548\ttotal: 53s\tremaining: 45s\n",
      "541:\tlearn: 0.0990973\ttotal: 53.1s\tremaining: 44.9s\n",
      "542:\tlearn: 0.0990487\ttotal: 53.2s\tremaining: 44.8s\n",
      "543:\tlearn: 0.0990165\ttotal: 53.3s\tremaining: 44.7s\n",
      "544:\tlearn: 0.0989818\ttotal: 53.4s\tremaining: 44.6s\n",
      "545:\tlearn: 0.0989544\ttotal: 53.5s\tremaining: 44.5s\n",
      "546:\tlearn: 0.0989144\ttotal: 53.6s\tremaining: 44.4s\n",
      "547:\tlearn: 0.0988441\ttotal: 53.7s\tremaining: 44.3s\n",
      "548:\tlearn: 0.0988188\ttotal: 53.8s\tremaining: 44.2s\n",
      "549:\tlearn: 0.0987599\ttotal: 53.9s\tremaining: 44.1s\n",
      "550:\tlearn: 0.0987347\ttotal: 54s\tremaining: 44s\n",
      "551:\tlearn: 0.0986112\ttotal: 54.1s\tremaining: 43.9s\n",
      "552:\tlearn: 0.0985955\ttotal: 54.2s\tremaining: 43.8s\n",
      "553:\tlearn: 0.0985364\ttotal: 54.3s\tremaining: 43.7s\n",
      "554:\tlearn: 0.0984992\ttotal: 54.4s\tremaining: 43.6s\n",
      "555:\tlearn: 0.0984520\ttotal: 54.5s\tremaining: 43.5s\n",
      "556:\tlearn: 0.0984267\ttotal: 54.6s\tremaining: 43.4s\n",
      "557:\tlearn: 0.0983984\ttotal: 54.6s\tremaining: 43.3s\n",
      "558:\tlearn: 0.0983543\ttotal: 54.7s\tremaining: 43.2s\n",
      "559:\tlearn: 0.0982917\ttotal: 54.8s\tremaining: 43.1s\n",
      "560:\tlearn: 0.0982316\ttotal: 54.9s\tremaining: 43s\n",
      "561:\tlearn: 0.0982021\ttotal: 55s\tremaining: 42.9s\n",
      "562:\tlearn: 0.0981403\ttotal: 55.1s\tremaining: 42.8s\n",
      "563:\tlearn: 0.0980819\ttotal: 55.2s\tremaining: 42.7s\n",
      "564:\tlearn: 0.0980419\ttotal: 55.3s\tremaining: 42.6s\n",
      "565:\tlearn: 0.0980127\ttotal: 55.4s\tremaining: 42.5s\n",
      "566:\tlearn: 0.0979841\ttotal: 55.5s\tremaining: 42.4s\n",
      "567:\tlearn: 0.0979551\ttotal: 55.6s\tremaining: 42.3s\n",
      "568:\tlearn: 0.0978722\ttotal: 55.7s\tremaining: 42.2s\n",
      "569:\tlearn: 0.0978135\ttotal: 55.8s\tremaining: 42.1s\n",
      "570:\tlearn: 0.0977486\ttotal: 55.9s\tremaining: 42s\n",
      "571:\tlearn: 0.0976903\ttotal: 56s\tremaining: 41.9s\n",
      "572:\tlearn: 0.0976470\ttotal: 56.1s\tremaining: 41.8s\n",
      "573:\tlearn: 0.0976066\ttotal: 56.3s\tremaining: 41.8s\n",
      "574:\tlearn: 0.0975362\ttotal: 56.4s\tremaining: 41.7s\n",
      "575:\tlearn: 0.0975175\ttotal: 56.5s\tremaining: 41.6s\n",
      "576:\tlearn: 0.0974623\ttotal: 56.6s\tremaining: 41.5s\n",
      "577:\tlearn: 0.0974023\ttotal: 56.7s\tremaining: 41.4s\n",
      "578:\tlearn: 0.0973777\ttotal: 56.7s\tremaining: 41.3s\n",
      "579:\tlearn: 0.0973283\ttotal: 56.8s\tremaining: 41.2s\n",
      "580:\tlearn: 0.0972728\ttotal: 56.9s\tremaining: 41.1s\n",
      "581:\tlearn: 0.0972431\ttotal: 57s\tremaining: 41s\n",
      "582:\tlearn: 0.0971697\ttotal: 57.1s\tremaining: 40.9s\n",
      "583:\tlearn: 0.0971254\ttotal: 57.2s\tremaining: 40.8s\n",
      "584:\tlearn: 0.0970966\ttotal: 57.3s\tremaining: 40.7s\n",
      "585:\tlearn: 0.0970490\ttotal: 57.4s\tremaining: 40.6s\n",
      "586:\tlearn: 0.0970250\ttotal: 57.5s\tremaining: 40.5s\n",
      "587:\tlearn: 0.0969928\ttotal: 57.6s\tremaining: 40.4s\n",
      "588:\tlearn: 0.0969317\ttotal: 57.7s\tremaining: 40.3s\n",
      "589:\tlearn: 0.0968890\ttotal: 57.8s\tremaining: 40.2s\n",
      "590:\tlearn: 0.0968087\ttotal: 57.9s\tremaining: 40.1s\n",
      "591:\tlearn: 0.0967654\ttotal: 58s\tremaining: 40s\n",
      "592:\tlearn: 0.0966826\ttotal: 58.1s\tremaining: 39.9s\n",
      "593:\tlearn: 0.0966534\ttotal: 58.2s\tremaining: 39.8s\n",
      "594:\tlearn: 0.0966093\ttotal: 58.3s\tremaining: 39.7s\n",
      "595:\tlearn: 0.0965423\ttotal: 58.3s\tremaining: 39.6s\n",
      "596:\tlearn: 0.0964747\ttotal: 58.4s\tremaining: 39.5s\n",
      "597:\tlearn: 0.0964195\ttotal: 58.5s\tremaining: 39.4s\n",
      "598:\tlearn: 0.0964055\ttotal: 58.6s\tremaining: 39.3s\n",
      "599:\tlearn: 0.0963747\ttotal: 58.7s\tremaining: 39.2s\n",
      "600:\tlearn: 0.0963286\ttotal: 58.8s\tremaining: 39.1s\n",
      "601:\tlearn: 0.0962854\ttotal: 58.9s\tremaining: 39s\n",
      "602:\tlearn: 0.0962474\ttotal: 59s\tremaining: 38.9s\n",
      "603:\tlearn: 0.0962329\ttotal: 59.1s\tremaining: 38.8s\n",
      "604:\tlearn: 0.0962036\ttotal: 59.2s\tremaining: 38.7s\n",
      "605:\tlearn: 0.0961761\ttotal: 59.3s\tremaining: 38.6s\n",
      "606:\tlearn: 0.0961306\ttotal: 59.4s\tremaining: 38.5s\n",
      "607:\tlearn: 0.0961111\ttotal: 59.5s\tremaining: 38.4s\n",
      "608:\tlearn: 0.0960691\ttotal: 59.6s\tremaining: 38.3s\n",
      "609:\tlearn: 0.0960163\ttotal: 59.7s\tremaining: 38.2s\n",
      "610:\tlearn: 0.0959849\ttotal: 59.8s\tremaining: 38.1s\n",
      "611:\tlearn: 0.0959745\ttotal: 59.9s\tremaining: 38s\n",
      "612:\tlearn: 0.0959069\ttotal: 60s\tremaining: 37.9s\n",
      "613:\tlearn: 0.0958691\ttotal: 1m\tremaining: 37.8s\n",
      "614:\tlearn: 0.0958413\ttotal: 1m\tremaining: 37.7s\n",
      "615:\tlearn: 0.0958053\ttotal: 1m\tremaining: 37.6s\n",
      "616:\tlearn: 0.0957898\ttotal: 1m\tremaining: 37.5s\n",
      "617:\tlearn: 0.0957515\ttotal: 1m\tremaining: 37.4s\n",
      "618:\tlearn: 0.0957222\ttotal: 1m\tremaining: 37.3s\n",
      "619:\tlearn: 0.0956704\ttotal: 1m\tremaining: 37.2s\n",
      "620:\tlearn: 0.0956502\ttotal: 1m\tremaining: 37s\n",
      "621:\tlearn: 0.0956242\ttotal: 1m\tremaining: 36.9s\n",
      "622:\tlearn: 0.0955402\ttotal: 1m\tremaining: 36.8s\n",
      "623:\tlearn: 0.0954731\ttotal: 1m\tremaining: 36.7s\n",
      "624:\tlearn: 0.0953690\ttotal: 1m 1s\tremaining: 36.6s\n",
      "625:\tlearn: 0.0953135\ttotal: 1m 1s\tremaining: 36.5s\n",
      "626:\tlearn: 0.0953015\ttotal: 1m 1s\tremaining: 36.4s\n",
      "627:\tlearn: 0.0952752\ttotal: 1m 1s\tremaining: 36.3s\n",
      "628:\tlearn: 0.0952290\ttotal: 1m 1s\tremaining: 36.2s\n",
      "629:\tlearn: 0.0951999\ttotal: 1m 1s\tremaining: 36.1s\n",
      "630:\tlearn: 0.0951619\ttotal: 1m 1s\tremaining: 36s\n",
      "631:\tlearn: 0.0951113\ttotal: 1m 1s\tremaining: 35.9s\n",
      "632:\tlearn: 0.0950917\ttotal: 1m 1s\tremaining: 35.8s\n",
      "633:\tlearn: 0.0950475\ttotal: 1m 1s\tremaining: 35.7s\n",
      "634:\tlearn: 0.0950097\ttotal: 1m 2s\tremaining: 35.6s\n",
      "635:\tlearn: 0.0949405\ttotal: 1m 2s\tremaining: 35.5s\n",
      "636:\tlearn: 0.0948975\ttotal: 1m 2s\tremaining: 35.4s\n",
      "637:\tlearn: 0.0948659\ttotal: 1m 2s\tremaining: 35.3s\n",
      "638:\tlearn: 0.0948211\ttotal: 1m 2s\tremaining: 35.2s\n",
      "639:\tlearn: 0.0947552\ttotal: 1m 2s\tremaining: 35.1s\n",
      "640:\tlearn: 0.0946921\ttotal: 1m 2s\tremaining: 35s\n",
      "641:\tlearn: 0.0946100\ttotal: 1m 2s\tremaining: 34.9s\n",
      "642:\tlearn: 0.0945603\ttotal: 1m 2s\tremaining: 34.8s\n",
      "643:\tlearn: 0.0944899\ttotal: 1m 2s\tremaining: 34.7s\n",
      "644:\tlearn: 0.0944635\ttotal: 1m 2s\tremaining: 34.6s\n",
      "645:\tlearn: 0.0943775\ttotal: 1m 3s\tremaining: 34.5s\n",
      "646:\tlearn: 0.0943328\ttotal: 1m 3s\tremaining: 34.4s\n",
      "647:\tlearn: 0.0942536\ttotal: 1m 3s\tremaining: 34.3s\n",
      "648:\tlearn: 0.0942012\ttotal: 1m 3s\tremaining: 34.2s\n",
      "649:\tlearn: 0.0941685\ttotal: 1m 3s\tremaining: 34.1s\n",
      "650:\tlearn: 0.0941108\ttotal: 1m 3s\tremaining: 34s\n",
      "651:\tlearn: 0.0941067\ttotal: 1m 3s\tremaining: 33.9s\n",
      "652:\tlearn: 0.0940960\ttotal: 1m 3s\tremaining: 33.8s\n",
      "653:\tlearn: 0.0940707\ttotal: 1m 3s\tremaining: 33.7s\n",
      "654:\tlearn: 0.0940020\ttotal: 1m 3s\tremaining: 33.6s\n",
      "655:\tlearn: 0.0939653\ttotal: 1m 3s\tremaining: 33.5s\n",
      "656:\tlearn: 0.0939446\ttotal: 1m 4s\tremaining: 33.4s\n",
      "657:\tlearn: 0.0939106\ttotal: 1m 4s\tremaining: 33.3s\n",
      "658:\tlearn: 0.0938682\ttotal: 1m 4s\tremaining: 33.2s\n",
      "659:\tlearn: 0.0938046\ttotal: 1m 4s\tremaining: 33.1s\n",
      "660:\tlearn: 0.0937536\ttotal: 1m 4s\tremaining: 33s\n",
      "661:\tlearn: 0.0936947\ttotal: 1m 4s\tremaining: 32.9s\n",
      "662:\tlearn: 0.0936237\ttotal: 1m 4s\tremaining: 32.8s\n",
      "663:\tlearn: 0.0935985\ttotal: 1m 4s\tremaining: 32.7s\n",
      "664:\tlearn: 0.0935660\ttotal: 1m 4s\tremaining: 32.6s\n",
      "665:\tlearn: 0.0935289\ttotal: 1m 4s\tremaining: 32.5s\n",
      "666:\tlearn: 0.0935121\ttotal: 1m 4s\tremaining: 32.4s\n",
      "667:\tlearn: 0.0934773\ttotal: 1m 5s\tremaining: 32.3s\n",
      "668:\tlearn: 0.0934054\ttotal: 1m 5s\tremaining: 32.2s\n",
      "669:\tlearn: 0.0933405\ttotal: 1m 5s\tremaining: 32.1s\n",
      "670:\tlearn: 0.0933001\ttotal: 1m 5s\tremaining: 32s\n",
      "671:\tlearn: 0.0932363\ttotal: 1m 5s\tremaining: 31.9s\n",
      "672:\tlearn: 0.0932198\ttotal: 1m 5s\tremaining: 31.8s\n",
      "673:\tlearn: 0.0931718\ttotal: 1m 5s\tremaining: 31.7s\n",
      "674:\tlearn: 0.0931233\ttotal: 1m 5s\tremaining: 31.6s\n",
      "675:\tlearn: 0.0930840\ttotal: 1m 5s\tremaining: 31.6s\n",
      "676:\tlearn: 0.0930522\ttotal: 1m 5s\tremaining: 31.4s\n",
      "677:\tlearn: 0.0930260\ttotal: 1m 6s\tremaining: 31.4s\n",
      "678:\tlearn: 0.0929311\ttotal: 1m 6s\tremaining: 31.3s\n",
      "679:\tlearn: 0.0928908\ttotal: 1m 6s\tremaining: 31.2s\n",
      "680:\tlearn: 0.0928066\ttotal: 1m 6s\tremaining: 31.1s\n",
      "681:\tlearn: 0.0927777\ttotal: 1m 6s\tremaining: 31s\n",
      "682:\tlearn: 0.0927311\ttotal: 1m 6s\tremaining: 30.9s\n",
      "683:\tlearn: 0.0926918\ttotal: 1m 6s\tremaining: 30.8s\n",
      "684:\tlearn: 0.0926583\ttotal: 1m 6s\tremaining: 30.7s\n",
      "685:\tlearn: 0.0926200\ttotal: 1m 6s\tremaining: 30.6s\n",
      "686:\tlearn: 0.0925984\ttotal: 1m 6s\tremaining: 30.5s\n",
      "687:\tlearn: 0.0925639\ttotal: 1m 7s\tremaining: 30.4s\n",
      "688:\tlearn: 0.0925054\ttotal: 1m 7s\tremaining: 30.3s\n",
      "689:\tlearn: 0.0924581\ttotal: 1m 7s\tremaining: 30.2s\n",
      "690:\tlearn: 0.0924097\ttotal: 1m 7s\tremaining: 30.1s\n",
      "691:\tlearn: 0.0923712\ttotal: 1m 7s\tremaining: 30s\n",
      "692:\tlearn: 0.0923367\ttotal: 1m 7s\tremaining: 29.9s\n",
      "693:\tlearn: 0.0922838\ttotal: 1m 7s\tremaining: 29.8s\n",
      "694:\tlearn: 0.0922674\ttotal: 1m 7s\tremaining: 29.7s\n",
      "695:\tlearn: 0.0922156\ttotal: 1m 7s\tremaining: 29.6s\n",
      "696:\tlearn: 0.0921529\ttotal: 1m 7s\tremaining: 29.5s\n",
      "697:\tlearn: 0.0921155\ttotal: 1m 7s\tremaining: 29.4s\n",
      "698:\tlearn: 0.0920815\ttotal: 1m 8s\tremaining: 29.3s\n",
      "699:\tlearn: 0.0920465\ttotal: 1m 8s\tremaining: 29.2s\n",
      "700:\tlearn: 0.0920050\ttotal: 1m 8s\tremaining: 29.1s\n",
      "701:\tlearn: 0.0919578\ttotal: 1m 8s\tremaining: 29s\n",
      "702:\tlearn: 0.0918957\ttotal: 1m 8s\tremaining: 28.9s\n",
      "703:\tlearn: 0.0918069\ttotal: 1m 8s\tremaining: 28.8s\n",
      "704:\tlearn: 0.0917625\ttotal: 1m 8s\tremaining: 28.7s\n",
      "705:\tlearn: 0.0917528\ttotal: 1m 8s\tremaining: 28.6s\n",
      "706:\tlearn: 0.0917013\ttotal: 1m 8s\tremaining: 28.5s\n",
      "707:\tlearn: 0.0916161\ttotal: 1m 8s\tremaining: 28.4s\n",
      "708:\tlearn: 0.0915640\ttotal: 1m 9s\tremaining: 28.3s\n",
      "709:\tlearn: 0.0915297\ttotal: 1m 9s\tremaining: 28.2s\n",
      "710:\tlearn: 0.0914605\ttotal: 1m 9s\tremaining: 28.1s\n",
      "711:\tlearn: 0.0914177\ttotal: 1m 9s\tremaining: 28s\n",
      "712:\tlearn: 0.0913685\ttotal: 1m 9s\tremaining: 27.9s\n",
      "713:\tlearn: 0.0913253\ttotal: 1m 9s\tremaining: 27.8s\n",
      "714:\tlearn: 0.0912628\ttotal: 1m 9s\tremaining: 27.7s\n",
      "715:\tlearn: 0.0912335\ttotal: 1m 9s\tremaining: 27.6s\n",
      "716:\tlearn: 0.0912103\ttotal: 1m 9s\tremaining: 27.5s\n",
      "717:\tlearn: 0.0911808\ttotal: 1m 9s\tremaining: 27.4s\n",
      "718:\tlearn: 0.0911409\ttotal: 1m 9s\tremaining: 27.3s\n",
      "719:\tlearn: 0.0910928\ttotal: 1m 10s\tremaining: 27.2s\n",
      "720:\tlearn: 0.0910531\ttotal: 1m 10s\tremaining: 27.1s\n",
      "721:\tlearn: 0.0910265\ttotal: 1m 10s\tremaining: 27s\n",
      "722:\tlearn: 0.0910051\ttotal: 1m 10s\tremaining: 26.9s\n",
      "723:\tlearn: 0.0909816\ttotal: 1m 10s\tremaining: 26.8s\n",
      "724:\tlearn: 0.0909676\ttotal: 1m 10s\tremaining: 26.8s\n",
      "725:\tlearn: 0.0909414\ttotal: 1m 10s\tremaining: 26.7s\n",
      "726:\tlearn: 0.0908701\ttotal: 1m 10s\tremaining: 26.6s\n",
      "727:\tlearn: 0.0908116\ttotal: 1m 10s\tremaining: 26.5s\n",
      "728:\tlearn: 0.0907559\ttotal: 1m 10s\tremaining: 26.4s\n",
      "729:\tlearn: 0.0907224\ttotal: 1m 10s\tremaining: 26.3s\n",
      "730:\tlearn: 0.0907091\ttotal: 1m 11s\tremaining: 26.2s\n",
      "731:\tlearn: 0.0906616\ttotal: 1m 11s\tremaining: 26.1s\n",
      "732:\tlearn: 0.0906132\ttotal: 1m 11s\tremaining: 26s\n",
      "733:\tlearn: 0.0905816\ttotal: 1m 11s\tremaining: 25.9s\n",
      "734:\tlearn: 0.0905536\ttotal: 1m 11s\tremaining: 25.8s\n",
      "735:\tlearn: 0.0904917\ttotal: 1m 11s\tremaining: 25.7s\n",
      "736:\tlearn: 0.0904597\ttotal: 1m 11s\tremaining: 25.6s\n",
      "737:\tlearn: 0.0904320\ttotal: 1m 11s\tremaining: 25.5s\n",
      "738:\tlearn: 0.0903927\ttotal: 1m 11s\tremaining: 25.4s\n",
      "739:\tlearn: 0.0903545\ttotal: 1m 11s\tremaining: 25.3s\n",
      "740:\tlearn: 0.0902901\ttotal: 1m 12s\tremaining: 25.2s\n",
      "741:\tlearn: 0.0902699\ttotal: 1m 12s\tremaining: 25.1s\n",
      "742:\tlearn: 0.0902449\ttotal: 1m 12s\tremaining: 25s\n",
      "743:\tlearn: 0.0901986\ttotal: 1m 12s\tremaining: 24.9s\n",
      "744:\tlearn: 0.0901728\ttotal: 1m 12s\tremaining: 24.8s\n",
      "745:\tlearn: 0.0901181\ttotal: 1m 12s\tremaining: 24.7s\n",
      "746:\tlearn: 0.0900709\ttotal: 1m 12s\tremaining: 24.6s\n",
      "747:\tlearn: 0.0900304\ttotal: 1m 12s\tremaining: 24.5s\n",
      "748:\tlearn: 0.0899971\ttotal: 1m 12s\tremaining: 24.4s\n",
      "749:\tlearn: 0.0899646\ttotal: 1m 12s\tremaining: 24.3s\n",
      "750:\tlearn: 0.0899158\ttotal: 1m 12s\tremaining: 24.2s\n",
      "751:\tlearn: 0.0898911\ttotal: 1m 13s\tremaining: 24.1s\n",
      "752:\tlearn: 0.0897966\ttotal: 1m 13s\tremaining: 24s\n",
      "753:\tlearn: 0.0897492\ttotal: 1m 13s\tremaining: 23.9s\n",
      "754:\tlearn: 0.0897102\ttotal: 1m 13s\tremaining: 23.8s\n",
      "755:\tlearn: 0.0896679\ttotal: 1m 13s\tremaining: 23.7s\n",
      "756:\tlearn: 0.0896491\ttotal: 1m 13s\tremaining: 23.6s\n",
      "757:\tlearn: 0.0896233\ttotal: 1m 13s\tremaining: 23.5s\n",
      "758:\tlearn: 0.0895850\ttotal: 1m 13s\tremaining: 23.4s\n",
      "759:\tlearn: 0.0895459\ttotal: 1m 13s\tremaining: 23.3s\n",
      "760:\tlearn: 0.0895293\ttotal: 1m 13s\tremaining: 23.2s\n",
      "761:\tlearn: 0.0894688\ttotal: 1m 13s\tremaining: 23.1s\n",
      "762:\tlearn: 0.0894685\ttotal: 1m 14s\tremaining: 23s\n",
      "763:\tlearn: 0.0894477\ttotal: 1m 14s\tremaining: 22.9s\n",
      "764:\tlearn: 0.0893886\ttotal: 1m 14s\tremaining: 22.8s\n",
      "765:\tlearn: 0.0893262\ttotal: 1m 14s\tremaining: 22.7s\n",
      "766:\tlearn: 0.0892941\ttotal: 1m 14s\tremaining: 22.6s\n",
      "767:\tlearn: 0.0892204\ttotal: 1m 14s\tremaining: 22.5s\n",
      "768:\tlearn: 0.0891957\ttotal: 1m 14s\tremaining: 22.4s\n",
      "769:\tlearn: 0.0891777\ttotal: 1m 14s\tremaining: 22.3s\n",
      "770:\tlearn: 0.0891554\ttotal: 1m 14s\tremaining: 22.2s\n",
      "771:\tlearn: 0.0890855\ttotal: 1m 14s\tremaining: 22.1s\n",
      "772:\tlearn: 0.0890434\ttotal: 1m 14s\tremaining: 22s\n",
      "773:\tlearn: 0.0889934\ttotal: 1m 15s\tremaining: 21.9s\n",
      "774:\tlearn: 0.0889568\ttotal: 1m 15s\tremaining: 21.8s\n",
      "775:\tlearn: 0.0889091\ttotal: 1m 15s\tremaining: 21.7s\n",
      "776:\tlearn: 0.0888714\ttotal: 1m 15s\tremaining: 21.6s\n",
      "777:\tlearn: 0.0888332\ttotal: 1m 15s\tremaining: 21.5s\n",
      "778:\tlearn: 0.0887842\ttotal: 1m 15s\tremaining: 21.4s\n",
      "779:\tlearn: 0.0887516\ttotal: 1m 15s\tremaining: 21.3s\n",
      "780:\tlearn: 0.0887378\ttotal: 1m 15s\tremaining: 21.2s\n",
      "781:\tlearn: 0.0886768\ttotal: 1m 15s\tremaining: 21.1s\n",
      "782:\tlearn: 0.0886270\ttotal: 1m 15s\tremaining: 21s\n",
      "783:\tlearn: 0.0885925\ttotal: 1m 16s\tremaining: 20.9s\n",
      "784:\tlearn: 0.0885284\ttotal: 1m 16s\tremaining: 20.9s\n",
      "785:\tlearn: 0.0885122\ttotal: 1m 16s\tremaining: 20.8s\n",
      "786:\tlearn: 0.0884830\ttotal: 1m 16s\tremaining: 20.7s\n",
      "787:\tlearn: 0.0884510\ttotal: 1m 16s\tremaining: 20.6s\n",
      "788:\tlearn: 0.0884095\ttotal: 1m 16s\tremaining: 20.5s\n",
      "789:\tlearn: 0.0883806\ttotal: 1m 16s\tremaining: 20.4s\n",
      "790:\tlearn: 0.0883319\ttotal: 1m 16s\tremaining: 20.3s\n",
      "791:\tlearn: 0.0883054\ttotal: 1m 16s\tremaining: 20.2s\n",
      "792:\tlearn: 0.0882745\ttotal: 1m 16s\tremaining: 20.1s\n",
      "793:\tlearn: 0.0881953\ttotal: 1m 16s\tremaining: 20s\n",
      "794:\tlearn: 0.0881480\ttotal: 1m 17s\tremaining: 19.9s\n",
      "795:\tlearn: 0.0881202\ttotal: 1m 17s\tremaining: 19.8s\n",
      "796:\tlearn: 0.0880851\ttotal: 1m 17s\tremaining: 19.7s\n",
      "797:\tlearn: 0.0880665\ttotal: 1m 17s\tremaining: 19.6s\n",
      "798:\tlearn: 0.0880256\ttotal: 1m 17s\tremaining: 19.5s\n",
      "799:\tlearn: 0.0879920\ttotal: 1m 17s\tremaining: 19.4s\n",
      "800:\tlearn: 0.0879332\ttotal: 1m 17s\tremaining: 19.3s\n",
      "801:\tlearn: 0.0878767\ttotal: 1m 17s\tremaining: 19.2s\n",
      "802:\tlearn: 0.0878582\ttotal: 1m 17s\tremaining: 19.1s\n",
      "803:\tlearn: 0.0878028\ttotal: 1m 17s\tremaining: 19s\n",
      "804:\tlearn: 0.0877544\ttotal: 1m 17s\tremaining: 18.9s\n",
      "805:\tlearn: 0.0877127\ttotal: 1m 18s\tremaining: 18.8s\n",
      "806:\tlearn: 0.0876856\ttotal: 1m 18s\tremaining: 18.7s\n",
      "807:\tlearn: 0.0876560\ttotal: 1m 18s\tremaining: 18.6s\n",
      "808:\tlearn: 0.0876057\ttotal: 1m 18s\tremaining: 18.5s\n",
      "809:\tlearn: 0.0875698\ttotal: 1m 18s\tremaining: 18.4s\n",
      "810:\tlearn: 0.0875404\ttotal: 1m 18s\tremaining: 18.3s\n",
      "811:\tlearn: 0.0875089\ttotal: 1m 18s\tremaining: 18.2s\n",
      "812:\tlearn: 0.0874707\ttotal: 1m 18s\tremaining: 18.1s\n",
      "813:\tlearn: 0.0874318\ttotal: 1m 18s\tremaining: 18s\n",
      "814:\tlearn: 0.0873839\ttotal: 1m 18s\tremaining: 17.9s\n",
      "815:\tlearn: 0.0873508\ttotal: 1m 19s\tremaining: 17.8s\n",
      "816:\tlearn: 0.0873012\ttotal: 1m 19s\tremaining: 17.7s\n",
      "817:\tlearn: 0.0872633\ttotal: 1m 19s\tremaining: 17.6s\n",
      "818:\tlearn: 0.0872307\ttotal: 1m 19s\tremaining: 17.5s\n",
      "819:\tlearn: 0.0872026\ttotal: 1m 19s\tremaining: 17.4s\n",
      "820:\tlearn: 0.0871680\ttotal: 1m 19s\tremaining: 17.3s\n",
      "821:\tlearn: 0.0871111\ttotal: 1m 19s\tremaining: 17.2s\n",
      "822:\tlearn: 0.0870565\ttotal: 1m 19s\tremaining: 17.1s\n",
      "823:\tlearn: 0.0870229\ttotal: 1m 19s\tremaining: 17s\n",
      "824:\tlearn: 0.0869691\ttotal: 1m 19s\tremaining: 16.9s\n",
      "825:\tlearn: 0.0869314\ttotal: 1m 19s\tremaining: 16.8s\n",
      "826:\tlearn: 0.0869090\ttotal: 1m 20s\tremaining: 16.7s\n",
      "827:\tlearn: 0.0868684\ttotal: 1m 20s\tremaining: 16.6s\n",
      "828:\tlearn: 0.0868413\ttotal: 1m 20s\tremaining: 16.6s\n",
      "829:\tlearn: 0.0867739\ttotal: 1m 20s\tremaining: 16.5s\n",
      "830:\tlearn: 0.0867357\ttotal: 1m 20s\tremaining: 16.4s\n",
      "831:\tlearn: 0.0867078\ttotal: 1m 20s\tremaining: 16.3s\n",
      "832:\tlearn: 0.0866788\ttotal: 1m 20s\tremaining: 16.2s\n",
      "833:\tlearn: 0.0866262\ttotal: 1m 20s\tremaining: 16.1s\n",
      "834:\tlearn: 0.0865770\ttotal: 1m 20s\tremaining: 16s\n",
      "835:\tlearn: 0.0865532\ttotal: 1m 20s\tremaining: 15.9s\n",
      "836:\tlearn: 0.0865147\ttotal: 1m 20s\tremaining: 15.8s\n",
      "837:\tlearn: 0.0864291\ttotal: 1m 21s\tremaining: 15.7s\n",
      "838:\tlearn: 0.0863785\ttotal: 1m 21s\tremaining: 15.6s\n",
      "839:\tlearn: 0.0863620\ttotal: 1m 21s\tremaining: 15.5s\n",
      "840:\tlearn: 0.0863212\ttotal: 1m 21s\tremaining: 15.4s\n",
      "841:\tlearn: 0.0862502\ttotal: 1m 21s\tremaining: 15.3s\n",
      "842:\tlearn: 0.0862121\ttotal: 1m 21s\tremaining: 15.2s\n",
      "843:\tlearn: 0.0861792\ttotal: 1m 21s\tremaining: 15.1s\n",
      "844:\tlearn: 0.0861377\ttotal: 1m 21s\tremaining: 15s\n",
      "845:\tlearn: 0.0860605\ttotal: 1m 21s\tremaining: 14.9s\n",
      "846:\tlearn: 0.0859945\ttotal: 1m 21s\tremaining: 14.8s\n",
      "847:\tlearn: 0.0859622\ttotal: 1m 22s\tremaining: 14.7s\n",
      "848:\tlearn: 0.0859259\ttotal: 1m 22s\tremaining: 14.6s\n",
      "849:\tlearn: 0.0858951\ttotal: 1m 22s\tremaining: 14.5s\n",
      "850:\tlearn: 0.0858548\ttotal: 1m 22s\tremaining: 14.4s\n",
      "851:\tlearn: 0.0858097\ttotal: 1m 22s\tremaining: 14.3s\n",
      "852:\tlearn: 0.0857899\ttotal: 1m 22s\tremaining: 14.2s\n",
      "853:\tlearn: 0.0857700\ttotal: 1m 22s\tremaining: 14.1s\n",
      "854:\tlearn: 0.0857642\ttotal: 1m 22s\tremaining: 14s\n",
      "855:\tlearn: 0.0857464\ttotal: 1m 22s\tremaining: 13.9s\n",
      "856:\tlearn: 0.0857145\ttotal: 1m 22s\tremaining: 13.8s\n",
      "857:\tlearn: 0.0856779\ttotal: 1m 22s\tremaining: 13.7s\n",
      "858:\tlearn: 0.0856487\ttotal: 1m 23s\tremaining: 13.6s\n",
      "859:\tlearn: 0.0856165\ttotal: 1m 23s\tremaining: 13.5s\n",
      "860:\tlearn: 0.0855713\ttotal: 1m 23s\tremaining: 13.4s\n",
      "861:\tlearn: 0.0855261\ttotal: 1m 23s\tremaining: 13.3s\n",
      "862:\tlearn: 0.0854616\ttotal: 1m 23s\tremaining: 13.2s\n",
      "863:\tlearn: 0.0854125\ttotal: 1m 23s\tremaining: 13.1s\n",
      "864:\tlearn: 0.0853790\ttotal: 1m 23s\tremaining: 13.1s\n",
      "865:\tlearn: 0.0853309\ttotal: 1m 23s\tremaining: 13s\n",
      "866:\tlearn: 0.0852617\ttotal: 1m 23s\tremaining: 12.9s\n",
      "867:\tlearn: 0.0852362\ttotal: 1m 23s\tremaining: 12.8s\n",
      "868:\tlearn: 0.0851707\ttotal: 1m 24s\tremaining: 12.7s\n",
      "869:\tlearn: 0.0851354\ttotal: 1m 24s\tremaining: 12.6s\n",
      "870:\tlearn: 0.0851178\ttotal: 1m 24s\tremaining: 12.5s\n",
      "871:\tlearn: 0.0850674\ttotal: 1m 24s\tremaining: 12.4s\n",
      "872:\tlearn: 0.0850531\ttotal: 1m 24s\tremaining: 12.3s\n",
      "873:\tlearn: 0.0850319\ttotal: 1m 24s\tremaining: 12.2s\n",
      "874:\tlearn: 0.0850172\ttotal: 1m 24s\tremaining: 12.1s\n",
      "875:\tlearn: 0.0849881\ttotal: 1m 24s\tremaining: 12s\n",
      "876:\tlearn: 0.0849788\ttotal: 1m 24s\tremaining: 11.9s\n",
      "877:\tlearn: 0.0849525\ttotal: 1m 24s\tremaining: 11.8s\n",
      "878:\tlearn: 0.0849083\ttotal: 1m 24s\tremaining: 11.7s\n",
      "879:\tlearn: 0.0848459\ttotal: 1m 25s\tremaining: 11.6s\n",
      "880:\tlearn: 0.0848094\ttotal: 1m 25s\tremaining: 11.5s\n",
      "881:\tlearn: 0.0847855\ttotal: 1m 25s\tremaining: 11.4s\n",
      "882:\tlearn: 0.0847424\ttotal: 1m 25s\tremaining: 11.3s\n",
      "883:\tlearn: 0.0847184\ttotal: 1m 25s\tremaining: 11.2s\n",
      "884:\tlearn: 0.0846537\ttotal: 1m 25s\tremaining: 11.1s\n",
      "885:\tlearn: 0.0846005\ttotal: 1m 25s\tremaining: 11s\n",
      "886:\tlearn: 0.0846004\ttotal: 1m 25s\tremaining: 10.9s\n",
      "887:\tlearn: 0.0845753\ttotal: 1m 25s\tremaining: 10.8s\n",
      "888:\tlearn: 0.0845222\ttotal: 1m 26s\tremaining: 10.7s\n",
      "889:\tlearn: 0.0844958\ttotal: 1m 26s\tremaining: 10.7s\n",
      "890:\tlearn: 0.0844726\ttotal: 1m 26s\tremaining: 10.6s\n",
      "891:\tlearn: 0.0844472\ttotal: 1m 26s\tremaining: 10.5s\n",
      "892:\tlearn: 0.0844280\ttotal: 1m 26s\tremaining: 10.4s\n",
      "893:\tlearn: 0.0844021\ttotal: 1m 26s\tremaining: 10.3s\n",
      "894:\tlearn: 0.0843710\ttotal: 1m 26s\tremaining: 10.2s\n",
      "895:\tlearn: 0.0843454\ttotal: 1m 26s\tremaining: 10.1s\n",
      "896:\tlearn: 0.0843021\ttotal: 1m 26s\tremaining: 9.97s\n",
      "897:\tlearn: 0.0842736\ttotal: 1m 26s\tremaining: 9.88s\n",
      "898:\tlearn: 0.0842391\ttotal: 1m 27s\tremaining: 9.78s\n",
      "899:\tlearn: 0.0842048\ttotal: 1m 27s\tremaining: 9.69s\n",
      "900:\tlearn: 0.0841616\ttotal: 1m 27s\tremaining: 9.59s\n",
      "901:\tlearn: 0.0840625\ttotal: 1m 27s\tremaining: 9.49s\n",
      "902:\tlearn: 0.0840008\ttotal: 1m 27s\tremaining: 9.4s\n",
      "903:\tlearn: 0.0839645\ttotal: 1m 27s\tremaining: 9.3s\n",
      "904:\tlearn: 0.0839448\ttotal: 1m 27s\tremaining: 9.2s\n",
      "905:\tlearn: 0.0838825\ttotal: 1m 27s\tremaining: 9.11s\n",
      "906:\tlearn: 0.0838384\ttotal: 1m 27s\tremaining: 9.01s\n",
      "907:\tlearn: 0.0838068\ttotal: 1m 27s\tremaining: 8.91s\n",
      "908:\tlearn: 0.0837793\ttotal: 1m 28s\tremaining: 8.82s\n",
      "909:\tlearn: 0.0837165\ttotal: 1m 28s\tremaining: 8.72s\n",
      "910:\tlearn: 0.0836649\ttotal: 1m 28s\tremaining: 8.62s\n",
      "911:\tlearn: 0.0836462\ttotal: 1m 28s\tremaining: 8.53s\n",
      "912:\tlearn: 0.0836235\ttotal: 1m 28s\tremaining: 8.43s\n",
      "913:\tlearn: 0.0835971\ttotal: 1m 28s\tremaining: 8.33s\n",
      "914:\tlearn: 0.0835806\ttotal: 1m 28s\tremaining: 8.23s\n",
      "915:\tlearn: 0.0835506\ttotal: 1m 28s\tremaining: 8.13s\n",
      "916:\tlearn: 0.0835352\ttotal: 1m 28s\tremaining: 8.04s\n",
      "917:\tlearn: 0.0835104\ttotal: 1m 28s\tremaining: 7.94s\n",
      "918:\tlearn: 0.0834614\ttotal: 1m 29s\tremaining: 7.85s\n",
      "919:\tlearn: 0.0834295\ttotal: 1m 29s\tremaining: 7.75s\n",
      "920:\tlearn: 0.0833724\ttotal: 1m 29s\tremaining: 7.65s\n",
      "921:\tlearn: 0.0833408\ttotal: 1m 29s\tremaining: 7.56s\n",
      "922:\tlearn: 0.0833221\ttotal: 1m 29s\tremaining: 7.46s\n",
      "923:\tlearn: 0.0832632\ttotal: 1m 29s\tremaining: 7.37s\n",
      "924:\tlearn: 0.0832081\ttotal: 1m 29s\tremaining: 7.27s\n",
      "925:\tlearn: 0.0831443\ttotal: 1m 29s\tremaining: 7.17s\n",
      "926:\tlearn: 0.0831193\ttotal: 1m 29s\tremaining: 7.08s\n",
      "927:\tlearn: 0.0831108\ttotal: 1m 29s\tremaining: 6.98s\n",
      "928:\tlearn: 0.0830449\ttotal: 1m 30s\tremaining: 6.88s\n",
      "929:\tlearn: 0.0830224\ttotal: 1m 30s\tremaining: 6.79s\n",
      "930:\tlearn: 0.0829839\ttotal: 1m 30s\tremaining: 6.69s\n",
      "931:\tlearn: 0.0829338\ttotal: 1m 30s\tremaining: 6.59s\n",
      "932:\tlearn: 0.0829068\ttotal: 1m 30s\tremaining: 6.5s\n",
      "933:\tlearn: 0.0828895\ttotal: 1m 30s\tremaining: 6.4s\n",
      "934:\tlearn: 0.0828461\ttotal: 1m 30s\tremaining: 6.3s\n",
      "935:\tlearn: 0.0828058\ttotal: 1m 30s\tremaining: 6.21s\n",
      "936:\tlearn: 0.0827554\ttotal: 1m 30s\tremaining: 6.11s\n",
      "937:\tlearn: 0.0827414\ttotal: 1m 30s\tremaining: 6.01s\n",
      "938:\tlearn: 0.0826730\ttotal: 1m 31s\tremaining: 5.92s\n",
      "939:\tlearn: 0.0826427\ttotal: 1m 31s\tremaining: 5.82s\n",
      "940:\tlearn: 0.0825755\ttotal: 1m 31s\tremaining: 5.73s\n",
      "941:\tlearn: 0.0825289\ttotal: 1m 31s\tremaining: 5.64s\n",
      "942:\tlearn: 0.0825020\ttotal: 1m 31s\tremaining: 5.54s\n",
      "943:\tlearn: 0.0824464\ttotal: 1m 31s\tremaining: 5.44s\n",
      "944:\tlearn: 0.0823814\ttotal: 1m 31s\tremaining: 5.35s\n",
      "945:\tlearn: 0.0823327\ttotal: 1m 31s\tremaining: 5.25s\n",
      "946:\tlearn: 0.0823176\ttotal: 1m 32s\tremaining: 5.15s\n",
      "947:\tlearn: 0.0822751\ttotal: 1m 32s\tremaining: 5.06s\n",
      "948:\tlearn: 0.0822603\ttotal: 1m 32s\tremaining: 4.96s\n",
      "949:\tlearn: 0.0822381\ttotal: 1m 32s\tremaining: 4.86s\n",
      "950:\tlearn: 0.0822162\ttotal: 1m 32s\tremaining: 4.76s\n",
      "951:\tlearn: 0.0821859\ttotal: 1m 32s\tremaining: 4.67s\n",
      "952:\tlearn: 0.0821718\ttotal: 1m 32s\tremaining: 4.57s\n",
      "953:\tlearn: 0.0820853\ttotal: 1m 32s\tremaining: 4.47s\n",
      "954:\tlearn: 0.0820641\ttotal: 1m 32s\tremaining: 4.38s\n",
      "955:\tlearn: 0.0820000\ttotal: 1m 32s\tremaining: 4.28s\n",
      "956:\tlearn: 0.0819605\ttotal: 1m 33s\tremaining: 4.18s\n",
      "957:\tlearn: 0.0819225\ttotal: 1m 33s\tremaining: 4.09s\n",
      "958:\tlearn: 0.0818876\ttotal: 1m 33s\tremaining: 3.99s\n",
      "959:\tlearn: 0.0818305\ttotal: 1m 33s\tremaining: 3.89s\n",
      "960:\tlearn: 0.0817802\ttotal: 1m 33s\tremaining: 3.79s\n",
      "961:\tlearn: 0.0817356\ttotal: 1m 33s\tremaining: 3.7s\n",
      "962:\tlearn: 0.0816880\ttotal: 1m 33s\tremaining: 3.6s\n",
      "963:\tlearn: 0.0816655\ttotal: 1m 33s\tremaining: 3.5s\n",
      "964:\tlearn: 0.0816360\ttotal: 1m 33s\tremaining: 3.41s\n",
      "965:\tlearn: 0.0816063\ttotal: 1m 34s\tremaining: 3.31s\n",
      "966:\tlearn: 0.0815848\ttotal: 1m 34s\tremaining: 3.21s\n",
      "967:\tlearn: 0.0815504\ttotal: 1m 34s\tremaining: 3.11s\n",
      "968:\tlearn: 0.0814867\ttotal: 1m 34s\tremaining: 3.02s\n",
      "969:\tlearn: 0.0814561\ttotal: 1m 34s\tremaining: 2.92s\n",
      "970:\tlearn: 0.0814161\ttotal: 1m 34s\tremaining: 2.82s\n",
      "971:\tlearn: 0.0813838\ttotal: 1m 34s\tremaining: 2.73s\n",
      "972:\tlearn: 0.0813570\ttotal: 1m 34s\tremaining: 2.63s\n",
      "973:\tlearn: 0.0813233\ttotal: 1m 34s\tremaining: 2.53s\n",
      "974:\tlearn: 0.0812977\ttotal: 1m 34s\tremaining: 2.43s\n",
      "975:\tlearn: 0.0812669\ttotal: 1m 34s\tremaining: 2.34s\n",
      "976:\tlearn: 0.0812301\ttotal: 1m 35s\tremaining: 2.24s\n",
      "977:\tlearn: 0.0812177\ttotal: 1m 35s\tremaining: 2.14s\n",
      "978:\tlearn: 0.0812135\ttotal: 1m 35s\tremaining: 2.04s\n",
      "979:\tlearn: 0.0812043\ttotal: 1m 35s\tremaining: 1.95s\n",
      "980:\tlearn: 0.0811982\ttotal: 1m 35s\tremaining: 1.85s\n",
      "981:\tlearn: 0.0811786\ttotal: 1m 35s\tremaining: 1.75s\n",
      "982:\tlearn: 0.0811494\ttotal: 1m 35s\tremaining: 1.65s\n",
      "983:\tlearn: 0.0810933\ttotal: 1m 35s\tremaining: 1.56s\n",
      "984:\tlearn: 0.0810540\ttotal: 1m 35s\tremaining: 1.46s\n",
      "985:\tlearn: 0.0809934\ttotal: 1m 35s\tremaining: 1.36s\n",
      "986:\tlearn: 0.0809544\ttotal: 1m 36s\tremaining: 1.26s\n",
      "987:\tlearn: 0.0809314\ttotal: 1m 36s\tremaining: 1.17s\n",
      "988:\tlearn: 0.0809095\ttotal: 1m 36s\tremaining: 1.07s\n",
      "989:\tlearn: 0.0808853\ttotal: 1m 36s\tremaining: 973ms\n",
      "990:\tlearn: 0.0808420\ttotal: 1m 36s\tremaining: 876ms\n",
      "991:\tlearn: 0.0808417\ttotal: 1m 36s\tremaining: 779ms\n",
      "992:\tlearn: 0.0808080\ttotal: 1m 36s\tremaining: 681ms\n",
      "993:\tlearn: 0.0807731\ttotal: 1m 36s\tremaining: 584ms\n",
      "994:\tlearn: 0.0807251\ttotal: 1m 36s\tremaining: 487ms\n",
      "995:\tlearn: 0.0806791\ttotal: 1m 36s\tremaining: 389ms\n",
      "996:\tlearn: 0.0806544\ttotal: 1m 37s\tremaining: 292ms\n",
      "997:\tlearn: 0.0806219\ttotal: 1m 37s\tremaining: 195ms\n",
      "998:\tlearn: 0.0806011\ttotal: 1m 37s\tremaining: 97.3ms\n",
      "999:\tlearn: 0.0805827\ttotal: 1m 37s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5293080\ttotal: 96.9ms\tremaining: 1m 36s\n",
      "1:\tlearn: 0.4226092\ttotal: 205ms\tremaining: 1m 42s\n",
      "2:\tlearn: 0.3612002\ttotal: 302ms\tremaining: 1m 40s\n",
      "3:\tlearn: 0.3216669\ttotal: 441ms\tremaining: 1m 49s\n",
      "4:\tlearn: 0.2950487\ttotal: 570ms\tremaining: 1m 53s\n",
      "5:\tlearn: 0.2777141\ttotal: 674ms\tremaining: 1m 51s\n",
      "6:\tlearn: 0.2619102\ttotal: 772ms\tremaining: 1m 49s\n",
      "7:\tlearn: 0.2515128\ttotal: 864ms\tremaining: 1m 47s\n",
      "8:\tlearn: 0.2357007\ttotal: 963ms\tremaining: 1m 46s\n",
      "9:\tlearn: 0.2275091\ttotal: 1.06s\tremaining: 1m 44s\n",
      "10:\tlearn: 0.2214348\ttotal: 1.15s\tremaining: 1m 43s\n",
      "11:\tlearn: 0.2165170\ttotal: 1.26s\tremaining: 1m 43s\n",
      "12:\tlearn: 0.2126396\ttotal: 1.37s\tremaining: 1m 43s\n",
      "13:\tlearn: 0.2098227\ttotal: 1.48s\tremaining: 1m 43s\n",
      "14:\tlearn: 0.2062856\ttotal: 1.58s\tremaining: 1m 43s\n",
      "15:\tlearn: 0.2000575\ttotal: 1.69s\tremaining: 1m 43s\n",
      "16:\tlearn: 0.1960049\ttotal: 1.79s\tremaining: 1m 43s\n",
      "17:\tlearn: 0.1934664\ttotal: 1.89s\tremaining: 1m 42s\n",
      "18:\tlearn: 0.1911542\ttotal: 2.02s\tremaining: 1m 44s\n",
      "19:\tlearn: 0.1882423\ttotal: 2.12s\tremaining: 1m 43s\n",
      "20:\tlearn: 0.1866385\ttotal: 2.21s\tremaining: 1m 43s\n",
      "21:\tlearn: 0.1845704\ttotal: 2.31s\tremaining: 1m 42s\n",
      "22:\tlearn: 0.1832431\ttotal: 2.4s\tremaining: 1m 41s\n",
      "23:\tlearn: 0.1821054\ttotal: 2.49s\tremaining: 1m 41s\n",
      "24:\tlearn: 0.1802291\ttotal: 2.59s\tremaining: 1m 40s\n",
      "25:\tlearn: 0.1788500\ttotal: 2.69s\tremaining: 1m 40s\n",
      "26:\tlearn: 0.1771895\ttotal: 2.79s\tremaining: 1m 40s\n",
      "27:\tlearn: 0.1761279\ttotal: 2.9s\tremaining: 1m 40s\n",
      "28:\tlearn: 0.1744508\ttotal: 2.99s\tremaining: 1m 40s\n",
      "29:\tlearn: 0.1732665\ttotal: 3.09s\tremaining: 1m 39s\n",
      "30:\tlearn: 0.1719429\ttotal: 3.19s\tremaining: 1m 39s\n",
      "31:\tlearn: 0.1710706\ttotal: 3.28s\tremaining: 1m 39s\n",
      "32:\tlearn: 0.1693279\ttotal: 3.37s\tremaining: 1m 38s\n",
      "33:\tlearn: 0.1680570\ttotal: 3.47s\tremaining: 1m 38s\n",
      "34:\tlearn: 0.1669637\ttotal: 3.56s\tremaining: 1m 38s\n",
      "35:\tlearn: 0.1660921\ttotal: 3.65s\tremaining: 1m 37s\n",
      "36:\tlearn: 0.1651614\ttotal: 3.75s\tremaining: 1m 37s\n",
      "37:\tlearn: 0.1642093\ttotal: 3.85s\tremaining: 1m 37s\n",
      "38:\tlearn: 0.1633595\ttotal: 3.95s\tremaining: 1m 37s\n",
      "39:\tlearn: 0.1624492\ttotal: 4.05s\tremaining: 1m 37s\n",
      "40:\tlearn: 0.1617956\ttotal: 4.14s\tremaining: 1m 36s\n",
      "41:\tlearn: 0.1609815\ttotal: 4.23s\tremaining: 1m 36s\n",
      "42:\tlearn: 0.1601256\ttotal: 4.33s\tremaining: 1m 36s\n",
      "43:\tlearn: 0.1592379\ttotal: 4.43s\tremaining: 1m 36s\n",
      "44:\tlearn: 0.1586588\ttotal: 4.52s\tremaining: 1m 36s\n",
      "45:\tlearn: 0.1581984\ttotal: 4.61s\tremaining: 1m 35s\n",
      "46:\tlearn: 0.1577032\ttotal: 4.71s\tremaining: 1m 35s\n",
      "47:\tlearn: 0.1572804\ttotal: 4.8s\tremaining: 1m 35s\n",
      "48:\tlearn: 0.1566725\ttotal: 4.94s\tremaining: 1m 35s\n",
      "49:\tlearn: 0.1560189\ttotal: 5.04s\tremaining: 1m 35s\n",
      "50:\tlearn: 0.1554191\ttotal: 5.13s\tremaining: 1m 35s\n",
      "51:\tlearn: 0.1550057\ttotal: 5.23s\tremaining: 1m 35s\n",
      "52:\tlearn: 0.1544170\ttotal: 5.32s\tremaining: 1m 35s\n",
      "53:\tlearn: 0.1536813\ttotal: 5.42s\tremaining: 1m 34s\n",
      "54:\tlearn: 0.1531630\ttotal: 5.51s\tremaining: 1m 34s\n",
      "55:\tlearn: 0.1526945\ttotal: 5.6s\tremaining: 1m 34s\n",
      "56:\tlearn: 0.1522736\ttotal: 5.7s\tremaining: 1m 34s\n",
      "57:\tlearn: 0.1517863\ttotal: 5.79s\tremaining: 1m 34s\n",
      "58:\tlearn: 0.1514114\ttotal: 5.88s\tremaining: 1m 33s\n",
      "59:\tlearn: 0.1507929\ttotal: 5.98s\tremaining: 1m 33s\n",
      "60:\tlearn: 0.1503935\ttotal: 6.08s\tremaining: 1m 33s\n",
      "61:\tlearn: 0.1499971\ttotal: 6.18s\tremaining: 1m 33s\n",
      "62:\tlearn: 0.1494896\ttotal: 6.27s\tremaining: 1m 33s\n",
      "63:\tlearn: 0.1487996\ttotal: 6.37s\tremaining: 1m 33s\n",
      "64:\tlearn: 0.1483984\ttotal: 6.47s\tremaining: 1m 33s\n",
      "65:\tlearn: 0.1481213\ttotal: 6.56s\tremaining: 1m 32s\n",
      "66:\tlearn: 0.1478019\ttotal: 6.66s\tremaining: 1m 32s\n",
      "67:\tlearn: 0.1474684\ttotal: 6.75s\tremaining: 1m 32s\n",
      "68:\tlearn: 0.1471235\ttotal: 6.84s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1467792\ttotal: 6.94s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1463975\ttotal: 7.03s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1461927\ttotal: 7.12s\tremaining: 1m 31s\n",
      "72:\tlearn: 0.1457639\ttotal: 7.22s\tremaining: 1m 31s\n",
      "73:\tlearn: 0.1454365\ttotal: 7.32s\tremaining: 1m 31s\n",
      "74:\tlearn: 0.1450070\ttotal: 7.42s\tremaining: 1m 31s\n",
      "75:\tlearn: 0.1447017\ttotal: 7.51s\tremaining: 1m 31s\n",
      "76:\tlearn: 0.1443045\ttotal: 7.61s\tremaining: 1m 31s\n",
      "77:\tlearn: 0.1440663\ttotal: 7.7s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1438511\ttotal: 7.8s\tremaining: 1m 30s\n",
      "79:\tlearn: 0.1436094\ttotal: 8.01s\tremaining: 1m 32s\n",
      "80:\tlearn: 0.1433231\ttotal: 8.15s\tremaining: 1m 32s\n",
      "81:\tlearn: 0.1430424\ttotal: 8.25s\tremaining: 1m 32s\n",
      "82:\tlearn: 0.1427550\ttotal: 8.34s\tremaining: 1m 32s\n",
      "83:\tlearn: 0.1425547\ttotal: 8.43s\tremaining: 1m 31s\n",
      "84:\tlearn: 0.1423532\ttotal: 8.53s\tremaining: 1m 31s\n",
      "85:\tlearn: 0.1420926\ttotal: 8.62s\tremaining: 1m 31s\n",
      "86:\tlearn: 0.1418380\ttotal: 8.72s\tremaining: 1m 31s\n",
      "87:\tlearn: 0.1414727\ttotal: 8.82s\tremaining: 1m 31s\n",
      "88:\tlearn: 0.1412386\ttotal: 8.91s\tremaining: 1m 31s\n",
      "89:\tlearn: 0.1410639\ttotal: 9s\tremaining: 1m 31s\n",
      "90:\tlearn: 0.1408371\ttotal: 9.1s\tremaining: 1m 30s\n",
      "91:\tlearn: 0.1406135\ttotal: 9.2s\tremaining: 1m 30s\n",
      "92:\tlearn: 0.1402551\ttotal: 9.29s\tremaining: 1m 30s\n",
      "93:\tlearn: 0.1398866\ttotal: 9.39s\tremaining: 1m 30s\n",
      "94:\tlearn: 0.1397079\ttotal: 9.48s\tremaining: 1m 30s\n",
      "95:\tlearn: 0.1394619\ttotal: 9.57s\tremaining: 1m 30s\n",
      "96:\tlearn: 0.1391811\ttotal: 9.67s\tremaining: 1m 30s\n",
      "97:\tlearn: 0.1387832\ttotal: 9.76s\tremaining: 1m 29s\n",
      "98:\tlearn: 0.1385119\ttotal: 9.85s\tremaining: 1m 29s\n",
      "99:\tlearn: 0.1381249\ttotal: 9.95s\tremaining: 1m 29s\n",
      "100:\tlearn: 0.1378867\ttotal: 10s\tremaining: 1m 29s\n",
      "101:\tlearn: 0.1376255\ttotal: 10.1s\tremaining: 1m 29s\n",
      "102:\tlearn: 0.1373538\ttotal: 10.2s\tremaining: 1m 29s\n",
      "103:\tlearn: 0.1371233\ttotal: 10.3s\tremaining: 1m 28s\n",
      "104:\tlearn: 0.1369557\ttotal: 10.4s\tremaining: 1m 28s\n",
      "105:\tlearn: 0.1366956\ttotal: 10.5s\tremaining: 1m 28s\n",
      "106:\tlearn: 0.1364532\ttotal: 10.6s\tremaining: 1m 28s\n",
      "107:\tlearn: 0.1361191\ttotal: 10.7s\tremaining: 1m 28s\n",
      "108:\tlearn: 0.1359297\ttotal: 10.8s\tremaining: 1m 28s\n",
      "109:\tlearn: 0.1357129\ttotal: 10.9s\tremaining: 1m 28s\n",
      "110:\tlearn: 0.1355331\ttotal: 11s\tremaining: 1m 28s\n",
      "111:\tlearn: 0.1353467\ttotal: 11.1s\tremaining: 1m 28s\n",
      "112:\tlearn: 0.1351601\ttotal: 11.2s\tremaining: 1m 28s\n",
      "113:\tlearn: 0.1349837\ttotal: 11.3s\tremaining: 1m 27s\n",
      "114:\tlearn: 0.1348412\ttotal: 11.4s\tremaining: 1m 27s\n",
      "115:\tlearn: 0.1346663\ttotal: 11.5s\tremaining: 1m 27s\n",
      "116:\tlearn: 0.1343817\ttotal: 11.6s\tremaining: 1m 27s\n",
      "117:\tlearn: 0.1341648\ttotal: 11.7s\tremaining: 1m 27s\n",
      "118:\tlearn: 0.1339557\ttotal: 11.8s\tremaining: 1m 27s\n",
      "119:\tlearn: 0.1337692\ttotal: 11.9s\tremaining: 1m 27s\n",
      "120:\tlearn: 0.1335241\ttotal: 12s\tremaining: 1m 27s\n",
      "121:\tlearn: 0.1330884\ttotal: 12.1s\tremaining: 1m 26s\n",
      "122:\tlearn: 0.1328285\ttotal: 12.2s\tremaining: 1m 26s\n",
      "123:\tlearn: 0.1326496\ttotal: 12.3s\tremaining: 1m 26s\n",
      "124:\tlearn: 0.1324967\ttotal: 12.3s\tremaining: 1m 26s\n",
      "125:\tlearn: 0.1323372\ttotal: 12.4s\tremaining: 1m 26s\n",
      "126:\tlearn: 0.1321751\ttotal: 12.5s\tremaining: 1m 26s\n",
      "127:\tlearn: 0.1319166\ttotal: 12.6s\tremaining: 1m 26s\n",
      "128:\tlearn: 0.1317441\ttotal: 12.7s\tremaining: 1m 25s\n",
      "129:\tlearn: 0.1315653\ttotal: 12.8s\tremaining: 1m 25s\n",
      "130:\tlearn: 0.1313062\ttotal: 12.9s\tremaining: 1m 25s\n",
      "131:\tlearn: 0.1310611\ttotal: 13s\tremaining: 1m 25s\n",
      "132:\tlearn: 0.1308715\ttotal: 13.1s\tremaining: 1m 25s\n",
      "133:\tlearn: 0.1307184\ttotal: 13.2s\tremaining: 1m 25s\n",
      "134:\tlearn: 0.1306044\ttotal: 13.3s\tremaining: 1m 25s\n",
      "135:\tlearn: 0.1304553\ttotal: 13.4s\tremaining: 1m 25s\n",
      "136:\tlearn: 0.1303426\ttotal: 13.5s\tremaining: 1m 25s\n",
      "137:\tlearn: 0.1301554\ttotal: 13.6s\tremaining: 1m 24s\n",
      "138:\tlearn: 0.1300372\ttotal: 13.7s\tremaining: 1m 24s\n",
      "139:\tlearn: 0.1299172\ttotal: 13.8s\tremaining: 1m 24s\n",
      "140:\tlearn: 0.1297603\ttotal: 13.9s\tremaining: 1m 24s\n",
      "141:\tlearn: 0.1296765\ttotal: 14s\tremaining: 1m 24s\n",
      "142:\tlearn: 0.1295244\ttotal: 14.1s\tremaining: 1m 24s\n",
      "143:\tlearn: 0.1293954\ttotal: 14.2s\tremaining: 1m 24s\n",
      "144:\tlearn: 0.1292827\ttotal: 14.3s\tremaining: 1m 24s\n",
      "145:\tlearn: 0.1290705\ttotal: 14.4s\tremaining: 1m 24s\n",
      "146:\tlearn: 0.1288874\ttotal: 14.5s\tremaining: 1m 24s\n",
      "147:\tlearn: 0.1286961\ttotal: 14.6s\tremaining: 1m 23s\n",
      "148:\tlearn: 0.1284669\ttotal: 14.7s\tremaining: 1m 23s\n",
      "149:\tlearn: 0.1283067\ttotal: 14.8s\tremaining: 1m 23s\n",
      "150:\tlearn: 0.1281617\ttotal: 14.9s\tremaining: 1m 23s\n",
      "151:\tlearn: 0.1279691\ttotal: 15s\tremaining: 1m 23s\n",
      "152:\tlearn: 0.1278535\ttotal: 15.1s\tremaining: 1m 23s\n",
      "153:\tlearn: 0.1277367\ttotal: 15.2s\tremaining: 1m 23s\n",
      "154:\tlearn: 0.1275999\ttotal: 15.2s\tremaining: 1m 23s\n",
      "155:\tlearn: 0.1274381\ttotal: 15.3s\tremaining: 1m 23s\n",
      "156:\tlearn: 0.1273420\ttotal: 15.4s\tremaining: 1m 22s\n",
      "157:\tlearn: 0.1272095\ttotal: 15.5s\tremaining: 1m 22s\n",
      "158:\tlearn: 0.1270823\ttotal: 15.6s\tremaining: 1m 22s\n",
      "159:\tlearn: 0.1269454\ttotal: 15.7s\tremaining: 1m 22s\n",
      "160:\tlearn: 0.1267693\ttotal: 15.8s\tremaining: 1m 22s\n",
      "161:\tlearn: 0.1266322\ttotal: 15.9s\tremaining: 1m 22s\n",
      "162:\tlearn: 0.1264930\ttotal: 16s\tremaining: 1m 22s\n",
      "163:\tlearn: 0.1263851\ttotal: 16.1s\tremaining: 1m 22s\n",
      "164:\tlearn: 0.1263081\ttotal: 16.2s\tremaining: 1m 22s\n",
      "165:\tlearn: 0.1261634\ttotal: 16.3s\tremaining: 1m 21s\n",
      "166:\tlearn: 0.1260117\ttotal: 16.4s\tremaining: 1m 21s\n",
      "167:\tlearn: 0.1259086\ttotal: 16.5s\tremaining: 1m 21s\n",
      "168:\tlearn: 0.1257956\ttotal: 16.6s\tremaining: 1m 21s\n",
      "169:\tlearn: 0.1256874\ttotal: 16.7s\tremaining: 1m 21s\n",
      "170:\tlearn: 0.1255701\ttotal: 16.8s\tremaining: 1m 21s\n",
      "171:\tlearn: 0.1254256\ttotal: 16.9s\tremaining: 1m 21s\n",
      "172:\tlearn: 0.1253388\ttotal: 17s\tremaining: 1m 21s\n",
      "173:\tlearn: 0.1252366\ttotal: 17.1s\tremaining: 1m 21s\n",
      "174:\tlearn: 0.1251333\ttotal: 17.2s\tremaining: 1m 21s\n",
      "175:\tlearn: 0.1250125\ttotal: 17.3s\tremaining: 1m 20s\n",
      "176:\tlearn: 0.1248992\ttotal: 17.4s\tremaining: 1m 20s\n",
      "177:\tlearn: 0.1247949\ttotal: 17.6s\tremaining: 1m 21s\n",
      "178:\tlearn: 0.1247006\ttotal: 17.7s\tremaining: 1m 21s\n",
      "179:\tlearn: 0.1245899\ttotal: 17.8s\tremaining: 1m 21s\n",
      "180:\tlearn: 0.1244511\ttotal: 17.9s\tremaining: 1m 20s\n",
      "181:\tlearn: 0.1243299\ttotal: 18s\tremaining: 1m 20s\n",
      "182:\tlearn: 0.1242695\ttotal: 18.1s\tremaining: 1m 20s\n",
      "183:\tlearn: 0.1241739\ttotal: 18.2s\tremaining: 1m 20s\n",
      "184:\tlearn: 0.1239830\ttotal: 18.3s\tremaining: 1m 20s\n",
      "185:\tlearn: 0.1239137\ttotal: 18.4s\tremaining: 1m 20s\n",
      "186:\tlearn: 0.1238218\ttotal: 18.5s\tremaining: 1m 20s\n",
      "187:\tlearn: 0.1236566\ttotal: 18.6s\tremaining: 1m 20s\n",
      "188:\tlearn: 0.1235604\ttotal: 18.7s\tremaining: 1m 20s\n",
      "189:\tlearn: 0.1234640\ttotal: 18.8s\tremaining: 1m 19s\n",
      "190:\tlearn: 0.1233674\ttotal: 18.9s\tremaining: 1m 19s\n",
      "191:\tlearn: 0.1232854\ttotal: 19s\tremaining: 1m 19s\n",
      "192:\tlearn: 0.1231753\ttotal: 19s\tremaining: 1m 19s\n",
      "193:\tlearn: 0.1230735\ttotal: 19.1s\tremaining: 1m 19s\n",
      "194:\tlearn: 0.1229752\ttotal: 19.2s\tremaining: 1m 19s\n",
      "195:\tlearn: 0.1228621\ttotal: 19.3s\tremaining: 1m 19s\n",
      "196:\tlearn: 0.1227566\ttotal: 19.4s\tremaining: 1m 19s\n",
      "197:\tlearn: 0.1226199\ttotal: 19.5s\tremaining: 1m 19s\n",
      "198:\tlearn: 0.1225385\ttotal: 19.6s\tremaining: 1m 18s\n",
      "199:\tlearn: 0.1223389\ttotal: 19.7s\tremaining: 1m 18s\n",
      "200:\tlearn: 0.1222637\ttotal: 19.8s\tremaining: 1m 18s\n",
      "201:\tlearn: 0.1221273\ttotal: 19.9s\tremaining: 1m 18s\n",
      "202:\tlearn: 0.1220487\ttotal: 20s\tremaining: 1m 18s\n",
      "203:\tlearn: 0.1219758\ttotal: 20.1s\tremaining: 1m 18s\n",
      "204:\tlearn: 0.1218384\ttotal: 20.2s\tremaining: 1m 18s\n",
      "205:\tlearn: 0.1217301\ttotal: 20.3s\tremaining: 1m 18s\n",
      "206:\tlearn: 0.1216044\ttotal: 20.4s\tremaining: 1m 18s\n",
      "207:\tlearn: 0.1214759\ttotal: 20.5s\tremaining: 1m 18s\n",
      "208:\tlearn: 0.1214465\ttotal: 20.6s\tremaining: 1m 17s\n",
      "209:\tlearn: 0.1213383\ttotal: 20.7s\tremaining: 1m 17s\n",
      "210:\tlearn: 0.1212023\ttotal: 20.8s\tremaining: 1m 17s\n",
      "211:\tlearn: 0.1211154\ttotal: 20.9s\tremaining: 1m 17s\n",
      "212:\tlearn: 0.1210364\ttotal: 21s\tremaining: 1m 17s\n",
      "213:\tlearn: 0.1209798\ttotal: 21.1s\tremaining: 1m 17s\n",
      "214:\tlearn: 0.1208472\ttotal: 21.2s\tremaining: 1m 17s\n",
      "215:\tlearn: 0.1207502\ttotal: 21.3s\tremaining: 1m 17s\n",
      "216:\tlearn: 0.1206316\ttotal: 21.4s\tremaining: 1m 17s\n",
      "217:\tlearn: 0.1205820\ttotal: 21.5s\tremaining: 1m 16s\n",
      "218:\tlearn: 0.1205096\ttotal: 21.6s\tremaining: 1m 16s\n",
      "219:\tlearn: 0.1204042\ttotal: 21.6s\tremaining: 1m 16s\n",
      "220:\tlearn: 0.1202602\ttotal: 21.7s\tremaining: 1m 16s\n",
      "221:\tlearn: 0.1201899\ttotal: 21.8s\tremaining: 1m 16s\n",
      "222:\tlearn: 0.1201110\ttotal: 21.9s\tremaining: 1m 16s\n",
      "223:\tlearn: 0.1200338\ttotal: 22s\tremaining: 1m 16s\n",
      "224:\tlearn: 0.1199118\ttotal: 22.1s\tremaining: 1m 16s\n",
      "225:\tlearn: 0.1198088\ttotal: 22.2s\tremaining: 1m 16s\n",
      "226:\tlearn: 0.1197030\ttotal: 22.3s\tremaining: 1m 16s\n",
      "227:\tlearn: 0.1196269\ttotal: 22.4s\tremaining: 1m 15s\n",
      "228:\tlearn: 0.1195353\ttotal: 22.5s\tremaining: 1m 15s\n",
      "229:\tlearn: 0.1194716\ttotal: 22.6s\tremaining: 1m 15s\n",
      "230:\tlearn: 0.1192973\ttotal: 22.7s\tremaining: 1m 15s\n",
      "231:\tlearn: 0.1192011\ttotal: 22.8s\tremaining: 1m 15s\n",
      "232:\tlearn: 0.1190930\ttotal: 22.9s\tremaining: 1m 15s\n",
      "233:\tlearn: 0.1190215\ttotal: 23s\tremaining: 1m 15s\n",
      "234:\tlearn: 0.1189324\ttotal: 23.1s\tremaining: 1m 15s\n",
      "235:\tlearn: 0.1188551\ttotal: 23.2s\tremaining: 1m 15s\n",
      "236:\tlearn: 0.1187111\ttotal: 23.3s\tremaining: 1m 14s\n",
      "237:\tlearn: 0.1186098\ttotal: 23.4s\tremaining: 1m 14s\n",
      "238:\tlearn: 0.1184877\ttotal: 23.5s\tremaining: 1m 14s\n",
      "239:\tlearn: 0.1184049\ttotal: 23.6s\tremaining: 1m 14s\n",
      "240:\tlearn: 0.1183113\ttotal: 23.7s\tremaining: 1m 14s\n",
      "241:\tlearn: 0.1182756\ttotal: 23.8s\tremaining: 1m 14s\n",
      "242:\tlearn: 0.1181876\ttotal: 23.9s\tremaining: 1m 14s\n",
      "243:\tlearn: 0.1181325\ttotal: 24s\tremaining: 1m 14s\n",
      "244:\tlearn: 0.1180662\ttotal: 24.1s\tremaining: 1m 14s\n",
      "245:\tlearn: 0.1179074\ttotal: 24.2s\tremaining: 1m 14s\n",
      "246:\tlearn: 0.1177943\ttotal: 24.3s\tremaining: 1m 13s\n",
      "247:\tlearn: 0.1177028\ttotal: 24.4s\tremaining: 1m 13s\n",
      "248:\tlearn: 0.1176175\ttotal: 24.5s\tremaining: 1m 13s\n",
      "249:\tlearn: 0.1175514\ttotal: 24.6s\tremaining: 1m 13s\n",
      "250:\tlearn: 0.1174594\ttotal: 24.7s\tremaining: 1m 13s\n",
      "251:\tlearn: 0.1173926\ttotal: 24.8s\tremaining: 1m 13s\n",
      "252:\tlearn: 0.1173197\ttotal: 24.9s\tremaining: 1m 13s\n",
      "253:\tlearn: 0.1172508\ttotal: 25s\tremaining: 1m 13s\n",
      "254:\tlearn: 0.1171741\ttotal: 25.1s\tremaining: 1m 13s\n",
      "255:\tlearn: 0.1171083\ttotal: 25.2s\tremaining: 1m 13s\n",
      "256:\tlearn: 0.1170511\ttotal: 25.3s\tremaining: 1m 13s\n",
      "257:\tlearn: 0.1169564\ttotal: 25.4s\tremaining: 1m 12s\n",
      "258:\tlearn: 0.1168809\ttotal: 25.5s\tremaining: 1m 12s\n",
      "259:\tlearn: 0.1167987\ttotal: 25.5s\tremaining: 1m 12s\n",
      "260:\tlearn: 0.1166938\ttotal: 25.6s\tremaining: 1m 12s\n",
      "261:\tlearn: 0.1165997\ttotal: 25.7s\tremaining: 1m 12s\n",
      "262:\tlearn: 0.1165177\ttotal: 25.8s\tremaining: 1m 12s\n",
      "263:\tlearn: 0.1164810\ttotal: 25.9s\tremaining: 1m 12s\n",
      "264:\tlearn: 0.1163665\ttotal: 26s\tremaining: 1m 12s\n",
      "265:\tlearn: 0.1162853\ttotal: 26.1s\tremaining: 1m 12s\n",
      "266:\tlearn: 0.1162034\ttotal: 26.2s\tremaining: 1m 11s\n",
      "267:\tlearn: 0.1161514\ttotal: 26.3s\tremaining: 1m 11s\n",
      "268:\tlearn: 0.1160851\ttotal: 26.4s\tremaining: 1m 11s\n",
      "269:\tlearn: 0.1159893\ttotal: 26.5s\tremaining: 1m 11s\n",
      "270:\tlearn: 0.1159016\ttotal: 26.6s\tremaining: 1m 11s\n",
      "271:\tlearn: 0.1158227\ttotal: 26.7s\tremaining: 1m 11s\n",
      "272:\tlearn: 0.1157314\ttotal: 26.8s\tremaining: 1m 11s\n",
      "273:\tlearn: 0.1156703\ttotal: 26.9s\tremaining: 1m 11s\n",
      "274:\tlearn: 0.1156180\ttotal: 27s\tremaining: 1m 11s\n",
      "275:\tlearn: 0.1155099\ttotal: 27.1s\tremaining: 1m 11s\n",
      "276:\tlearn: 0.1154311\ttotal: 27.3s\tremaining: 1m 11s\n",
      "277:\tlearn: 0.1153263\ttotal: 27.4s\tremaining: 1m 11s\n",
      "278:\tlearn: 0.1152737\ttotal: 27.5s\tremaining: 1m 11s\n",
      "279:\tlearn: 0.1152080\ttotal: 27.6s\tremaining: 1m 10s\n",
      "280:\tlearn: 0.1151511\ttotal: 27.7s\tremaining: 1m 10s\n",
      "281:\tlearn: 0.1151016\ttotal: 27.8s\tremaining: 1m 10s\n",
      "282:\tlearn: 0.1150278\ttotal: 27.9s\tremaining: 1m 10s\n",
      "283:\tlearn: 0.1149628\ttotal: 28s\tremaining: 1m 10s\n",
      "284:\tlearn: 0.1149150\ttotal: 28.1s\tremaining: 1m 10s\n",
      "285:\tlearn: 0.1148546\ttotal: 28.1s\tremaining: 1m 10s\n",
      "286:\tlearn: 0.1147745\ttotal: 28.2s\tremaining: 1m 10s\n",
      "287:\tlearn: 0.1147125\ttotal: 28.3s\tremaining: 1m 10s\n",
      "288:\tlearn: 0.1146376\ttotal: 28.4s\tremaining: 1m 9s\n",
      "289:\tlearn: 0.1145708\ttotal: 28.5s\tremaining: 1m 9s\n",
      "290:\tlearn: 0.1145111\ttotal: 28.6s\tremaining: 1m 9s\n",
      "291:\tlearn: 0.1144546\ttotal: 28.7s\tremaining: 1m 9s\n",
      "292:\tlearn: 0.1144333\ttotal: 28.8s\tremaining: 1m 9s\n",
      "293:\tlearn: 0.1143392\ttotal: 28.9s\tremaining: 1m 9s\n",
      "294:\tlearn: 0.1142644\ttotal: 29s\tremaining: 1m 9s\n",
      "295:\tlearn: 0.1141265\ttotal: 29.1s\tremaining: 1m 9s\n",
      "296:\tlearn: 0.1140634\ttotal: 29.2s\tremaining: 1m 9s\n",
      "297:\tlearn: 0.1139845\ttotal: 29.3s\tremaining: 1m 9s\n",
      "298:\tlearn: 0.1139178\ttotal: 29.4s\tremaining: 1m 8s\n",
      "299:\tlearn: 0.1137370\ttotal: 29.5s\tremaining: 1m 8s\n",
      "300:\tlearn: 0.1136113\ttotal: 29.6s\tremaining: 1m 8s\n",
      "301:\tlearn: 0.1134880\ttotal: 29.7s\tremaining: 1m 8s\n",
      "302:\tlearn: 0.1133748\ttotal: 29.8s\tremaining: 1m 8s\n",
      "303:\tlearn: 0.1133330\ttotal: 29.9s\tremaining: 1m 8s\n",
      "304:\tlearn: 0.1132257\ttotal: 30s\tremaining: 1m 8s\n",
      "305:\tlearn: 0.1131578\ttotal: 30.1s\tremaining: 1m 8s\n",
      "306:\tlearn: 0.1130941\ttotal: 30.2s\tremaining: 1m 8s\n",
      "307:\tlearn: 0.1130344\ttotal: 30.3s\tremaining: 1m 8s\n",
      "308:\tlearn: 0.1129543\ttotal: 30.4s\tremaining: 1m 7s\n",
      "309:\tlearn: 0.1128784\ttotal: 30.5s\tremaining: 1m 7s\n",
      "310:\tlearn: 0.1128084\ttotal: 30.6s\tremaining: 1m 7s\n",
      "311:\tlearn: 0.1126932\ttotal: 30.7s\tremaining: 1m 7s\n",
      "312:\tlearn: 0.1126262\ttotal: 30.8s\tremaining: 1m 7s\n",
      "313:\tlearn: 0.1125723\ttotal: 30.9s\tremaining: 1m 7s\n",
      "314:\tlearn: 0.1125279\ttotal: 31s\tremaining: 1m 7s\n",
      "315:\tlearn: 0.1124599\ttotal: 31.1s\tremaining: 1m 7s\n",
      "316:\tlearn: 0.1124078\ttotal: 31.2s\tremaining: 1m 7s\n",
      "317:\tlearn: 0.1123591\ttotal: 31.2s\tremaining: 1m 7s\n",
      "318:\tlearn: 0.1122762\ttotal: 31.3s\tremaining: 1m 6s\n",
      "319:\tlearn: 0.1122053\ttotal: 31.4s\tremaining: 1m 6s\n",
      "320:\tlearn: 0.1121076\ttotal: 31.5s\tremaining: 1m 6s\n",
      "321:\tlearn: 0.1120445\ttotal: 31.6s\tremaining: 1m 6s\n",
      "322:\tlearn: 0.1120054\ttotal: 31.7s\tremaining: 1m 6s\n",
      "323:\tlearn: 0.1118892\ttotal: 31.8s\tremaining: 1m 6s\n",
      "324:\tlearn: 0.1118260\ttotal: 31.9s\tremaining: 1m 6s\n",
      "325:\tlearn: 0.1117205\ttotal: 32s\tremaining: 1m 6s\n",
      "326:\tlearn: 0.1116737\ttotal: 32.1s\tremaining: 1m 6s\n",
      "327:\tlearn: 0.1116085\ttotal: 32.2s\tremaining: 1m 5s\n",
      "328:\tlearn: 0.1115578\ttotal: 32.3s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1115093\ttotal: 32.4s\tremaining: 1m 5s\n",
      "330:\tlearn: 0.1114633\ttotal: 32.5s\tremaining: 1m 5s\n",
      "331:\tlearn: 0.1113739\ttotal: 32.6s\tremaining: 1m 5s\n",
      "332:\tlearn: 0.1113105\ttotal: 32.7s\tremaining: 1m 5s\n",
      "333:\tlearn: 0.1112233\ttotal: 32.8s\tremaining: 1m 5s\n",
      "334:\tlearn: 0.1111515\ttotal: 32.9s\tremaining: 1m 5s\n",
      "335:\tlearn: 0.1110813\ttotal: 33s\tremaining: 1m 5s\n",
      "336:\tlearn: 0.1110320\ttotal: 33.1s\tremaining: 1m 5s\n",
      "337:\tlearn: 0.1109610\ttotal: 33.2s\tremaining: 1m 4s\n",
      "338:\tlearn: 0.1108938\ttotal: 33.3s\tremaining: 1m 4s\n",
      "339:\tlearn: 0.1108286\ttotal: 33.4s\tremaining: 1m 4s\n",
      "340:\tlearn: 0.1107233\ttotal: 33.5s\tremaining: 1m 4s\n",
      "341:\tlearn: 0.1106457\ttotal: 33.6s\tremaining: 1m 4s\n",
      "342:\tlearn: 0.1105966\ttotal: 33.7s\tremaining: 1m 4s\n",
      "343:\tlearn: 0.1105263\ttotal: 33.8s\tremaining: 1m 4s\n",
      "344:\tlearn: 0.1104527\ttotal: 33.9s\tremaining: 1m 4s\n",
      "345:\tlearn: 0.1103891\ttotal: 34s\tremaining: 1m 4s\n",
      "346:\tlearn: 0.1103163\ttotal: 34.1s\tremaining: 1m 4s\n",
      "347:\tlearn: 0.1102808\ttotal: 34.1s\tremaining: 1m 3s\n",
      "348:\tlearn: 0.1102423\ttotal: 34.2s\tremaining: 1m 3s\n",
      "349:\tlearn: 0.1101703\ttotal: 34.3s\tremaining: 1m 3s\n",
      "350:\tlearn: 0.1101254\ttotal: 34.4s\tremaining: 1m 3s\n",
      "351:\tlearn: 0.1100886\ttotal: 34.5s\tremaining: 1m 3s\n",
      "352:\tlearn: 0.1100420\ttotal: 34.6s\tremaining: 1m 3s\n",
      "353:\tlearn: 0.1099837\ttotal: 34.7s\tremaining: 1m 3s\n",
      "354:\tlearn: 0.1099467\ttotal: 34.8s\tremaining: 1m 3s\n",
      "355:\tlearn: 0.1098952\ttotal: 34.9s\tremaining: 1m 3s\n",
      "356:\tlearn: 0.1098048\ttotal: 35s\tremaining: 1m 3s\n",
      "357:\tlearn: 0.1097712\ttotal: 35.1s\tremaining: 1m 3s\n",
      "358:\tlearn: 0.1097309\ttotal: 35.2s\tremaining: 1m 2s\n",
      "359:\tlearn: 0.1096630\ttotal: 35.3s\tremaining: 1m 2s\n",
      "360:\tlearn: 0.1096172\ttotal: 35.4s\tremaining: 1m 2s\n",
      "361:\tlearn: 0.1095028\ttotal: 35.5s\tremaining: 1m 2s\n",
      "362:\tlearn: 0.1094185\ttotal: 35.7s\tremaining: 1m 2s\n",
      "363:\tlearn: 0.1093354\ttotal: 35.9s\tremaining: 1m 2s\n",
      "364:\tlearn: 0.1092819\ttotal: 36s\tremaining: 1m 2s\n",
      "365:\tlearn: 0.1092520\ttotal: 36.1s\tremaining: 1m 2s\n",
      "366:\tlearn: 0.1091941\ttotal: 36.3s\tremaining: 1m 2s\n",
      "367:\tlearn: 0.1090997\ttotal: 36.4s\tremaining: 1m 2s\n",
      "368:\tlearn: 0.1090030\ttotal: 36.5s\tremaining: 1m 2s\n",
      "369:\tlearn: 0.1089431\ttotal: 36.6s\tremaining: 1m 2s\n",
      "370:\tlearn: 0.1088689\ttotal: 36.7s\tremaining: 1m 2s\n",
      "371:\tlearn: 0.1088227\ttotal: 36.8s\tremaining: 1m 2s\n",
      "372:\tlearn: 0.1087925\ttotal: 36.9s\tremaining: 1m 2s\n",
      "373:\tlearn: 0.1087366\ttotal: 37s\tremaining: 1m 1s\n",
      "374:\tlearn: 0.1086408\ttotal: 37.1s\tremaining: 1m 1s\n",
      "375:\tlearn: 0.1085843\ttotal: 37.3s\tremaining: 1m 1s\n",
      "376:\tlearn: 0.1085542\ttotal: 37.4s\tremaining: 1m 1s\n",
      "377:\tlearn: 0.1085018\ttotal: 37.5s\tremaining: 1m 1s\n",
      "378:\tlearn: 0.1084417\ttotal: 37.6s\tremaining: 1m 1s\n",
      "379:\tlearn: 0.1084013\ttotal: 37.7s\tremaining: 1m 1s\n",
      "380:\tlearn: 0.1083488\ttotal: 37.8s\tremaining: 1m 1s\n",
      "381:\tlearn: 0.1082773\ttotal: 37.9s\tremaining: 1m 1s\n",
      "382:\tlearn: 0.1082036\ttotal: 38s\tremaining: 1m 1s\n",
      "383:\tlearn: 0.1081227\ttotal: 38.1s\tremaining: 1m 1s\n",
      "384:\tlearn: 0.1080644\ttotal: 38.2s\tremaining: 1m 1s\n",
      "385:\tlearn: 0.1079755\ttotal: 38.4s\tremaining: 1m 1s\n",
      "386:\tlearn: 0.1079134\ttotal: 38.5s\tremaining: 1m\n",
      "387:\tlearn: 0.1078013\ttotal: 38.6s\tremaining: 1m\n",
      "388:\tlearn: 0.1077567\ttotal: 38.7s\tremaining: 1m\n",
      "389:\tlearn: 0.1077248\ttotal: 38.8s\tremaining: 1m\n",
      "390:\tlearn: 0.1076103\ttotal: 38.9s\tremaining: 1m\n",
      "391:\tlearn: 0.1075638\ttotal: 39s\tremaining: 1m\n",
      "392:\tlearn: 0.1075302\ttotal: 39.1s\tremaining: 1m\n",
      "393:\tlearn: 0.1074846\ttotal: 39.2s\tremaining: 1m\n",
      "394:\tlearn: 0.1074461\ttotal: 39.3s\tremaining: 1m\n",
      "395:\tlearn: 0.1073669\ttotal: 39.4s\tremaining: 1m\n",
      "396:\tlearn: 0.1073201\ttotal: 39.5s\tremaining: 60s\n",
      "397:\tlearn: 0.1072879\ttotal: 39.6s\tremaining: 59.9s\n",
      "398:\tlearn: 0.1071965\ttotal: 39.7s\tremaining: 59.8s\n",
      "399:\tlearn: 0.1071592\ttotal: 39.8s\tremaining: 59.7s\n",
      "400:\tlearn: 0.1071179\ttotal: 39.9s\tremaining: 59.6s\n",
      "401:\tlearn: 0.1070539\ttotal: 40s\tremaining: 59.5s\n",
      "402:\tlearn: 0.1069601\ttotal: 40.1s\tremaining: 59.4s\n",
      "403:\tlearn: 0.1068853\ttotal: 40.2s\tremaining: 59.3s\n",
      "404:\tlearn: 0.1068303\ttotal: 40.3s\tremaining: 59.2s\n",
      "405:\tlearn: 0.1067474\ttotal: 40.4s\tremaining: 59.1s\n",
      "406:\tlearn: 0.1066801\ttotal: 40.5s\tremaining: 59s\n",
      "407:\tlearn: 0.1066423\ttotal: 40.6s\tremaining: 58.9s\n",
      "408:\tlearn: 0.1066235\ttotal: 40.7s\tremaining: 58.8s\n",
      "409:\tlearn: 0.1065066\ttotal: 40.8s\tremaining: 58.7s\n",
      "410:\tlearn: 0.1064913\ttotal: 40.9s\tremaining: 58.6s\n",
      "411:\tlearn: 0.1064562\ttotal: 41s\tremaining: 58.5s\n",
      "412:\tlearn: 0.1064314\ttotal: 41.1s\tremaining: 58.4s\n",
      "413:\tlearn: 0.1063857\ttotal: 41.2s\tremaining: 58.3s\n",
      "414:\tlearn: 0.1063456\ttotal: 41.3s\tremaining: 58.2s\n",
      "415:\tlearn: 0.1062877\ttotal: 41.4s\tremaining: 58.1s\n",
      "416:\tlearn: 0.1062467\ttotal: 41.5s\tremaining: 58s\n",
      "417:\tlearn: 0.1061965\ttotal: 41.6s\tremaining: 57.9s\n",
      "418:\tlearn: 0.1061784\ttotal: 41.7s\tremaining: 57.8s\n",
      "419:\tlearn: 0.1060951\ttotal: 41.8s\tremaining: 57.7s\n",
      "420:\tlearn: 0.1060324\ttotal: 41.9s\tremaining: 57.6s\n",
      "421:\tlearn: 0.1059398\ttotal: 42s\tremaining: 57.5s\n",
      "422:\tlearn: 0.1058560\ttotal: 42.1s\tremaining: 57.4s\n",
      "423:\tlearn: 0.1058071\ttotal: 42.2s\tremaining: 57.3s\n",
      "424:\tlearn: 0.1057371\ttotal: 42.3s\tremaining: 57.2s\n",
      "425:\tlearn: 0.1056639\ttotal: 42.4s\tremaining: 57.1s\n",
      "426:\tlearn: 0.1055931\ttotal: 42.5s\tremaining: 57s\n",
      "427:\tlearn: 0.1055291\ttotal: 42.6s\tremaining: 56.9s\n",
      "428:\tlearn: 0.1054750\ttotal: 42.7s\tremaining: 56.8s\n",
      "429:\tlearn: 0.1053635\ttotal: 42.8s\tremaining: 56.7s\n",
      "430:\tlearn: 0.1053158\ttotal: 42.9s\tremaining: 56.6s\n",
      "431:\tlearn: 0.1052378\ttotal: 43s\tremaining: 56.5s\n",
      "432:\tlearn: 0.1051864\ttotal: 43.1s\tremaining: 56.4s\n",
      "433:\tlearn: 0.1051508\ttotal: 43.2s\tremaining: 56.3s\n",
      "434:\tlearn: 0.1051010\ttotal: 43.3s\tremaining: 56.2s\n",
      "435:\tlearn: 0.1050489\ttotal: 43.4s\tremaining: 56.1s\n",
      "436:\tlearn: 0.1049741\ttotal: 43.5s\tremaining: 56s\n",
      "437:\tlearn: 0.1049191\ttotal: 43.6s\tremaining: 55.9s\n",
      "438:\tlearn: 0.1048927\ttotal: 43.7s\tremaining: 55.9s\n",
      "439:\tlearn: 0.1048550\ttotal: 43.8s\tremaining: 55.8s\n",
      "440:\tlearn: 0.1047888\ttotal: 43.9s\tremaining: 55.7s\n",
      "441:\tlearn: 0.1047504\ttotal: 44s\tremaining: 55.6s\n",
      "442:\tlearn: 0.1046754\ttotal: 44.1s\tremaining: 55.5s\n",
      "443:\tlearn: 0.1045911\ttotal: 44.2s\tremaining: 55.4s\n",
      "444:\tlearn: 0.1045551\ttotal: 44.3s\tremaining: 55.3s\n",
      "445:\tlearn: 0.1044934\ttotal: 44.4s\tremaining: 55.2s\n",
      "446:\tlearn: 0.1044475\ttotal: 44.5s\tremaining: 55.1s\n",
      "447:\tlearn: 0.1043463\ttotal: 44.6s\tremaining: 55s\n",
      "448:\tlearn: 0.1042640\ttotal: 44.7s\tremaining: 54.9s\n",
      "449:\tlearn: 0.1042148\ttotal: 44.8s\tremaining: 54.8s\n",
      "450:\tlearn: 0.1041485\ttotal: 45s\tremaining: 54.7s\n",
      "451:\tlearn: 0.1040685\ttotal: 45.1s\tremaining: 54.7s\n",
      "452:\tlearn: 0.1040208\ttotal: 45.2s\tremaining: 54.6s\n",
      "453:\tlearn: 0.1039811\ttotal: 45.3s\tremaining: 54.5s\n",
      "454:\tlearn: 0.1039018\ttotal: 45.4s\tremaining: 54.4s\n",
      "455:\tlearn: 0.1038572\ttotal: 45.6s\tremaining: 54.4s\n",
      "456:\tlearn: 0.1038081\ttotal: 45.8s\tremaining: 54.4s\n",
      "457:\tlearn: 0.1037531\ttotal: 45.9s\tremaining: 54.3s\n",
      "458:\tlearn: 0.1037260\ttotal: 46s\tremaining: 54.2s\n",
      "459:\tlearn: 0.1036971\ttotal: 46.1s\tremaining: 54.1s\n",
      "460:\tlearn: 0.1036528\ttotal: 46.2s\tremaining: 54.1s\n",
      "461:\tlearn: 0.1036320\ttotal: 46.4s\tremaining: 54s\n",
      "462:\tlearn: 0.1035635\ttotal: 46.5s\tremaining: 53.9s\n",
      "463:\tlearn: 0.1035107\ttotal: 46.6s\tremaining: 53.8s\n",
      "464:\tlearn: 0.1034669\ttotal: 46.7s\tremaining: 53.7s\n",
      "465:\tlearn: 0.1034280\ttotal: 46.8s\tremaining: 53.6s\n",
      "466:\tlearn: 0.1033663\ttotal: 46.9s\tremaining: 53.5s\n",
      "467:\tlearn: 0.1033017\ttotal: 47s\tremaining: 53.4s\n",
      "468:\tlearn: 0.1032614\ttotal: 47.1s\tremaining: 53.3s\n",
      "469:\tlearn: 0.1032239\ttotal: 47.2s\tremaining: 53.3s\n",
      "470:\tlearn: 0.1031724\ttotal: 47.3s\tremaining: 53.2s\n",
      "471:\tlearn: 0.1030746\ttotal: 47.4s\tremaining: 53.1s\n",
      "472:\tlearn: 0.1030428\ttotal: 47.6s\tremaining: 53s\n",
      "473:\tlearn: 0.1029877\ttotal: 47.7s\tremaining: 52.9s\n",
      "474:\tlearn: 0.1029318\ttotal: 47.8s\tremaining: 52.8s\n",
      "475:\tlearn: 0.1028847\ttotal: 47.9s\tremaining: 52.7s\n",
      "476:\tlearn: 0.1028298\ttotal: 48s\tremaining: 52.6s\n",
      "477:\tlearn: 0.1027916\ttotal: 48.1s\tremaining: 52.5s\n",
      "478:\tlearn: 0.1027150\ttotal: 48.2s\tremaining: 52.5s\n",
      "479:\tlearn: 0.1026756\ttotal: 48.3s\tremaining: 52.4s\n",
      "480:\tlearn: 0.1025992\ttotal: 48.5s\tremaining: 52.3s\n",
      "481:\tlearn: 0.1025550\ttotal: 48.6s\tremaining: 52.2s\n",
      "482:\tlearn: 0.1025212\ttotal: 48.7s\tremaining: 52.1s\n",
      "483:\tlearn: 0.1024716\ttotal: 48.8s\tremaining: 52.1s\n",
      "484:\tlearn: 0.1024474\ttotal: 48.9s\tremaining: 52s\n",
      "485:\tlearn: 0.1023890\ttotal: 49.1s\tremaining: 51.9s\n",
      "486:\tlearn: 0.1023603\ttotal: 49.2s\tremaining: 51.8s\n",
      "487:\tlearn: 0.1023297\ttotal: 49.3s\tremaining: 51.7s\n",
      "488:\tlearn: 0.1022846\ttotal: 49.4s\tremaining: 51.6s\n",
      "489:\tlearn: 0.1022208\ttotal: 49.5s\tremaining: 51.5s\n",
      "490:\tlearn: 0.1021503\ttotal: 49.6s\tremaining: 51.4s\n",
      "491:\tlearn: 0.1021112\ttotal: 49.7s\tremaining: 51.3s\n",
      "492:\tlearn: 0.1020492\ttotal: 49.8s\tremaining: 51.2s\n",
      "493:\tlearn: 0.1019931\ttotal: 49.9s\tremaining: 51.1s\n",
      "494:\tlearn: 0.1019424\ttotal: 50s\tremaining: 51.1s\n",
      "495:\tlearn: 0.1019062\ttotal: 50.1s\tremaining: 51s\n",
      "496:\tlearn: 0.1018349\ttotal: 50.3s\tremaining: 50.9s\n",
      "497:\tlearn: 0.1017839\ttotal: 50.4s\tremaining: 50.8s\n",
      "498:\tlearn: 0.1017580\ttotal: 50.5s\tremaining: 50.7s\n",
      "499:\tlearn: 0.1016973\ttotal: 50.6s\tremaining: 50.6s\n",
      "500:\tlearn: 0.1016730\ttotal: 50.7s\tremaining: 50.5s\n",
      "501:\tlearn: 0.1016296\ttotal: 50.8s\tremaining: 50.4s\n",
      "502:\tlearn: 0.1015374\ttotal: 50.9s\tremaining: 50.3s\n",
      "503:\tlearn: 0.1014544\ttotal: 51s\tremaining: 50.2s\n",
      "504:\tlearn: 0.1013913\ttotal: 51.1s\tremaining: 50.1s\n",
      "505:\tlearn: 0.1012892\ttotal: 51.2s\tremaining: 50s\n",
      "506:\tlearn: 0.1012481\ttotal: 51.4s\tremaining: 49.9s\n",
      "507:\tlearn: 0.1012192\ttotal: 51.5s\tremaining: 49.8s\n",
      "508:\tlearn: 0.1011686\ttotal: 51.6s\tremaining: 49.7s\n",
      "509:\tlearn: 0.1011408\ttotal: 51.7s\tremaining: 49.6s\n",
      "510:\tlearn: 0.1010682\ttotal: 51.8s\tremaining: 49.5s\n",
      "511:\tlearn: 0.1010534\ttotal: 51.9s\tremaining: 49.5s\n",
      "512:\tlearn: 0.1010077\ttotal: 52s\tremaining: 49.4s\n",
      "513:\tlearn: 0.1009761\ttotal: 52.1s\tremaining: 49.3s\n",
      "514:\tlearn: 0.1009368\ttotal: 52.2s\tremaining: 49.2s\n",
      "515:\tlearn: 0.1009237\ttotal: 52.3s\tremaining: 49.1s\n",
      "516:\tlearn: 0.1008866\ttotal: 52.4s\tremaining: 49s\n",
      "517:\tlearn: 0.1008028\ttotal: 52.6s\tremaining: 48.9s\n",
      "518:\tlearn: 0.1007303\ttotal: 52.7s\tremaining: 48.8s\n",
      "519:\tlearn: 0.1007040\ttotal: 52.8s\tremaining: 48.7s\n",
      "520:\tlearn: 0.1006621\ttotal: 52.9s\tremaining: 48.7s\n",
      "521:\tlearn: 0.1006128\ttotal: 53s\tremaining: 48.6s\n",
      "522:\tlearn: 0.1005540\ttotal: 53.2s\tremaining: 48.5s\n",
      "523:\tlearn: 0.1005105\ttotal: 53.3s\tremaining: 48.4s\n",
      "524:\tlearn: 0.1004374\ttotal: 53.4s\tremaining: 48.3s\n",
      "525:\tlearn: 0.1004318\ttotal: 53.5s\tremaining: 48.2s\n",
      "526:\tlearn: 0.1003750\ttotal: 53.6s\tremaining: 48.1s\n",
      "527:\tlearn: 0.1003359\ttotal: 53.7s\tremaining: 48s\n",
      "528:\tlearn: 0.1002816\ttotal: 53.8s\tremaining: 47.9s\n",
      "529:\tlearn: 0.1002137\ttotal: 53.9s\tremaining: 47.8s\n",
      "530:\tlearn: 0.1001839\ttotal: 54s\tremaining: 47.7s\n",
      "531:\tlearn: 0.1001427\ttotal: 54.1s\tremaining: 47.6s\n",
      "532:\tlearn: 0.1000927\ttotal: 54.2s\tremaining: 47.5s\n",
      "533:\tlearn: 0.0999905\ttotal: 54.3s\tremaining: 47.4s\n",
      "534:\tlearn: 0.0999561\ttotal: 54.4s\tremaining: 47.3s\n",
      "535:\tlearn: 0.0999040\ttotal: 54.5s\tremaining: 47.2s\n",
      "536:\tlearn: 0.0998768\ttotal: 54.6s\tremaining: 47.1s\n",
      "537:\tlearn: 0.0998195\ttotal: 54.7s\tremaining: 47s\n",
      "538:\tlearn: 0.0997722\ttotal: 54.8s\tremaining: 46.9s\n",
      "539:\tlearn: 0.0997335\ttotal: 55s\tremaining: 46.8s\n",
      "540:\tlearn: 0.0997046\ttotal: 55.1s\tremaining: 46.8s\n",
      "541:\tlearn: 0.0996783\ttotal: 55.2s\tremaining: 46.7s\n",
      "542:\tlearn: 0.0996293\ttotal: 55.4s\tremaining: 46.6s\n",
      "543:\tlearn: 0.0995853\ttotal: 55.5s\tremaining: 46.5s\n",
      "544:\tlearn: 0.0995386\ttotal: 55.6s\tremaining: 46.4s\n",
      "545:\tlearn: 0.0995058\ttotal: 55.7s\tremaining: 46.3s\n",
      "546:\tlearn: 0.0994871\ttotal: 55.8s\tremaining: 46.2s\n",
      "547:\tlearn: 0.0994099\ttotal: 55.9s\tremaining: 46.1s\n",
      "548:\tlearn: 0.0993441\ttotal: 56s\tremaining: 46s\n",
      "549:\tlearn: 0.0993177\ttotal: 56.1s\tremaining: 45.9s\n",
      "550:\tlearn: 0.0993028\ttotal: 56.3s\tremaining: 45.8s\n",
      "551:\tlearn: 0.0992690\ttotal: 56.4s\tremaining: 45.7s\n",
      "552:\tlearn: 0.0992325\ttotal: 56.5s\tremaining: 45.6s\n",
      "553:\tlearn: 0.0991663\ttotal: 56.6s\tremaining: 45.5s\n",
      "554:\tlearn: 0.0991475\ttotal: 56.7s\tremaining: 45.4s\n",
      "555:\tlearn: 0.0990822\ttotal: 56.8s\tremaining: 45.3s\n",
      "556:\tlearn: 0.0990461\ttotal: 56.9s\tremaining: 45.2s\n",
      "557:\tlearn: 0.0989893\ttotal: 57s\tremaining: 45.2s\n",
      "558:\tlearn: 0.0989361\ttotal: 57.1s\tremaining: 45.1s\n",
      "559:\tlearn: 0.0989028\ttotal: 57.2s\tremaining: 45s\n",
      "560:\tlearn: 0.0988305\ttotal: 57.3s\tremaining: 44.9s\n",
      "561:\tlearn: 0.0988035\ttotal: 57.5s\tremaining: 44.8s\n",
      "562:\tlearn: 0.0987792\ttotal: 57.6s\tremaining: 44.7s\n",
      "563:\tlearn: 0.0987621\ttotal: 57.7s\tremaining: 44.6s\n",
      "564:\tlearn: 0.0987517\ttotal: 57.8s\tremaining: 44.5s\n",
      "565:\tlearn: 0.0987091\ttotal: 57.9s\tremaining: 44.4s\n",
      "566:\tlearn: 0.0986804\ttotal: 58s\tremaining: 44.3s\n",
      "567:\tlearn: 0.0986537\ttotal: 58.1s\tremaining: 44.2s\n",
      "568:\tlearn: 0.0986251\ttotal: 58.2s\tremaining: 44.1s\n",
      "569:\tlearn: 0.0985748\ttotal: 58.3s\tremaining: 44s\n",
      "570:\tlearn: 0.0985562\ttotal: 58.4s\tremaining: 43.9s\n",
      "571:\tlearn: 0.0984506\ttotal: 58.5s\tremaining: 43.8s\n",
      "572:\tlearn: 0.0984172\ttotal: 58.6s\tremaining: 43.7s\n",
      "573:\tlearn: 0.0983520\ttotal: 58.8s\tremaining: 43.6s\n",
      "574:\tlearn: 0.0983206\ttotal: 58.9s\tremaining: 43.5s\n",
      "575:\tlearn: 0.0982872\ttotal: 59s\tremaining: 43.4s\n",
      "576:\tlearn: 0.0982513\ttotal: 59s\tremaining: 43.3s\n",
      "577:\tlearn: 0.0981712\ttotal: 59.1s\tremaining: 43.2s\n",
      "578:\tlearn: 0.0981257\ttotal: 59.2s\tremaining: 43.1s\n",
      "579:\tlearn: 0.0981121\ttotal: 59.3s\tremaining: 43s\n",
      "580:\tlearn: 0.0980585\ttotal: 59.4s\tremaining: 42.9s\n",
      "581:\tlearn: 0.0980263\ttotal: 59.5s\tremaining: 42.7s\n",
      "582:\tlearn: 0.0979503\ttotal: 59.6s\tremaining: 42.6s\n",
      "583:\tlearn: 0.0979414\ttotal: 59.7s\tremaining: 42.5s\n",
      "584:\tlearn: 0.0978379\ttotal: 59.8s\tremaining: 42.5s\n",
      "585:\tlearn: 0.0977656\ttotal: 60s\tremaining: 42.4s\n",
      "586:\tlearn: 0.0977241\ttotal: 1m\tremaining: 42.2s\n",
      "587:\tlearn: 0.0976563\ttotal: 1m\tremaining: 42.1s\n",
      "588:\tlearn: 0.0975434\ttotal: 1m\tremaining: 42s\n",
      "589:\tlearn: 0.0974875\ttotal: 1m\tremaining: 41.9s\n",
      "590:\tlearn: 0.0974629\ttotal: 1m\tremaining: 41.8s\n",
      "591:\tlearn: 0.0974390\ttotal: 1m\tremaining: 41.7s\n",
      "592:\tlearn: 0.0973693\ttotal: 1m\tremaining: 41.6s\n",
      "593:\tlearn: 0.0973420\ttotal: 1m\tremaining: 41.5s\n",
      "594:\tlearn: 0.0972974\ttotal: 1m\tremaining: 41.4s\n",
      "595:\tlearn: 0.0972551\ttotal: 1m\tremaining: 41.3s\n",
      "596:\tlearn: 0.0972309\ttotal: 1m 1s\tremaining: 41.2s\n",
      "597:\tlearn: 0.0971103\ttotal: 1m 1s\tremaining: 41.1s\n",
      "598:\tlearn: 0.0970680\ttotal: 1m 1s\tremaining: 41s\n",
      "599:\tlearn: 0.0970216\ttotal: 1m 1s\tremaining: 40.9s\n",
      "600:\tlearn: 0.0969742\ttotal: 1m 1s\tremaining: 40.8s\n",
      "601:\tlearn: 0.0969316\ttotal: 1m 1s\tremaining: 40.7s\n",
      "602:\tlearn: 0.0968708\ttotal: 1m 1s\tremaining: 40.6s\n",
      "603:\tlearn: 0.0968278\ttotal: 1m 1s\tremaining: 40.5s\n",
      "604:\tlearn: 0.0967694\ttotal: 1m 1s\tremaining: 40.4s\n",
      "605:\tlearn: 0.0966912\ttotal: 1m 1s\tremaining: 40.3s\n",
      "606:\tlearn: 0.0966511\ttotal: 1m 2s\tremaining: 40.2s\n",
      "607:\tlearn: 0.0965963\ttotal: 1m 2s\tremaining: 40.1s\n",
      "608:\tlearn: 0.0965827\ttotal: 1m 2s\tremaining: 39.9s\n",
      "609:\tlearn: 0.0965159\ttotal: 1m 2s\tremaining: 39.8s\n",
      "610:\tlearn: 0.0965085\ttotal: 1m 2s\tremaining: 39.7s\n",
      "611:\tlearn: 0.0964577\ttotal: 1m 2s\tremaining: 39.6s\n",
      "612:\tlearn: 0.0963905\ttotal: 1m 2s\tremaining: 39.5s\n",
      "613:\tlearn: 0.0963673\ttotal: 1m 2s\tremaining: 39.4s\n",
      "614:\tlearn: 0.0962877\ttotal: 1m 2s\tremaining: 39.3s\n",
      "615:\tlearn: 0.0961573\ttotal: 1m 2s\tremaining: 39.2s\n",
      "616:\tlearn: 0.0961160\ttotal: 1m 3s\tremaining: 39.1s\n",
      "617:\tlearn: 0.0960643\ttotal: 1m 3s\tremaining: 39s\n",
      "618:\tlearn: 0.0960372\ttotal: 1m 3s\tremaining: 38.9s\n",
      "619:\tlearn: 0.0960212\ttotal: 1m 3s\tremaining: 38.8s\n",
      "620:\tlearn: 0.0959451\ttotal: 1m 3s\tremaining: 38.7s\n",
      "621:\tlearn: 0.0959269\ttotal: 1m 3s\tremaining: 38.6s\n",
      "622:\tlearn: 0.0958899\ttotal: 1m 3s\tremaining: 38.5s\n",
      "623:\tlearn: 0.0958619\ttotal: 1m 3s\tremaining: 38.4s\n",
      "624:\tlearn: 0.0958235\ttotal: 1m 3s\tremaining: 38.3s\n",
      "625:\tlearn: 0.0957403\ttotal: 1m 3s\tremaining: 38.2s\n",
      "626:\tlearn: 0.0957176\ttotal: 1m 4s\tremaining: 38.1s\n",
      "627:\tlearn: 0.0956489\ttotal: 1m 4s\tremaining: 38s\n",
      "628:\tlearn: 0.0956266\ttotal: 1m 4s\tremaining: 37.9s\n",
      "629:\tlearn: 0.0956038\ttotal: 1m 4s\tremaining: 37.8s\n",
      "630:\tlearn: 0.0955628\ttotal: 1m 4s\tremaining: 37.7s\n",
      "631:\tlearn: 0.0955313\ttotal: 1m 4s\tremaining: 37.5s\n",
      "632:\tlearn: 0.0955062\ttotal: 1m 4s\tremaining: 37.4s\n",
      "633:\tlearn: 0.0954144\ttotal: 1m 4s\tremaining: 37.3s\n",
      "634:\tlearn: 0.0953805\ttotal: 1m 4s\tremaining: 37.2s\n",
      "635:\tlearn: 0.0953088\ttotal: 1m 4s\tremaining: 37.1s\n",
      "636:\tlearn: 0.0952221\ttotal: 1m 4s\tremaining: 37s\n",
      "637:\tlearn: 0.0951921\ttotal: 1m 5s\tremaining: 36.9s\n",
      "638:\tlearn: 0.0951351\ttotal: 1m 5s\tremaining: 36.9s\n",
      "639:\tlearn: 0.0951131\ttotal: 1m 5s\tremaining: 36.8s\n",
      "640:\tlearn: 0.0950508\ttotal: 1m 5s\tremaining: 36.7s\n",
      "641:\tlearn: 0.0950064\ttotal: 1m 5s\tremaining: 36.6s\n",
      "642:\tlearn: 0.0949571\ttotal: 1m 5s\tremaining: 36.5s\n",
      "643:\tlearn: 0.0949087\ttotal: 1m 5s\tremaining: 36.4s\n",
      "644:\tlearn: 0.0948705\ttotal: 1m 5s\tremaining: 36.3s\n",
      "645:\tlearn: 0.0948489\ttotal: 1m 6s\tremaining: 36.2s\n",
      "646:\tlearn: 0.0948146\ttotal: 1m 6s\tremaining: 36.1s\n",
      "647:\tlearn: 0.0947810\ttotal: 1m 6s\tremaining: 36s\n",
      "648:\tlearn: 0.0947070\ttotal: 1m 6s\tremaining: 35.9s\n",
      "649:\tlearn: 0.0946656\ttotal: 1m 6s\tremaining: 35.8s\n",
      "650:\tlearn: 0.0946089\ttotal: 1m 6s\tremaining: 35.7s\n",
      "651:\tlearn: 0.0945647\ttotal: 1m 6s\tremaining: 35.5s\n",
      "652:\tlearn: 0.0945413\ttotal: 1m 6s\tremaining: 35.4s\n",
      "653:\tlearn: 0.0944801\ttotal: 1m 6s\tremaining: 35.3s\n",
      "654:\tlearn: 0.0944455\ttotal: 1m 6s\tremaining: 35.2s\n",
      "655:\tlearn: 0.0944225\ttotal: 1m 6s\tremaining: 35.1s\n",
      "656:\tlearn: 0.0943631\ttotal: 1m 7s\tremaining: 35s\n",
      "657:\tlearn: 0.0942919\ttotal: 1m 7s\tremaining: 34.9s\n",
      "658:\tlearn: 0.0942482\ttotal: 1m 7s\tremaining: 34.8s\n",
      "659:\tlearn: 0.0942140\ttotal: 1m 7s\tremaining: 34.7s\n",
      "660:\tlearn: 0.0941845\ttotal: 1m 7s\tremaining: 34.6s\n",
      "661:\tlearn: 0.0941224\ttotal: 1m 7s\tremaining: 34.5s\n",
      "662:\tlearn: 0.0940840\ttotal: 1m 7s\tremaining: 34.4s\n",
      "663:\tlearn: 0.0940324\ttotal: 1m 7s\tremaining: 34.3s\n",
      "664:\tlearn: 0.0939911\ttotal: 1m 7s\tremaining: 34.2s\n",
      "665:\tlearn: 0.0939438\ttotal: 1m 7s\tremaining: 34.1s\n",
      "666:\tlearn: 0.0938898\ttotal: 1m 8s\tremaining: 34s\n",
      "667:\tlearn: 0.0938441\ttotal: 1m 8s\tremaining: 33.9s\n",
      "668:\tlearn: 0.0937765\ttotal: 1m 8s\tremaining: 33.8s\n",
      "669:\tlearn: 0.0937306\ttotal: 1m 8s\tremaining: 33.7s\n",
      "670:\tlearn: 0.0936673\ttotal: 1m 8s\tremaining: 33.6s\n",
      "671:\tlearn: 0.0936161\ttotal: 1m 8s\tremaining: 33.5s\n",
      "672:\tlearn: 0.0935879\ttotal: 1m 8s\tremaining: 33.4s\n",
      "673:\tlearn: 0.0935363\ttotal: 1m 8s\tremaining: 33.3s\n",
      "674:\tlearn: 0.0934636\ttotal: 1m 8s\tremaining: 33.2s\n",
      "675:\tlearn: 0.0933994\ttotal: 1m 9s\tremaining: 33.1s\n",
      "676:\tlearn: 0.0933838\ttotal: 1m 9s\tremaining: 33s\n",
      "677:\tlearn: 0.0933621\ttotal: 1m 9s\tremaining: 32.9s\n",
      "678:\tlearn: 0.0933423\ttotal: 1m 9s\tremaining: 32.8s\n",
      "679:\tlearn: 0.0933231\ttotal: 1m 9s\tremaining: 32.7s\n",
      "680:\tlearn: 0.0932979\ttotal: 1m 9s\tremaining: 32.6s\n",
      "681:\tlearn: 0.0932553\ttotal: 1m 9s\tremaining: 32.5s\n",
      "682:\tlearn: 0.0932169\ttotal: 1m 9s\tremaining: 32.4s\n",
      "683:\tlearn: 0.0931764\ttotal: 1m 9s\tremaining: 32.3s\n",
      "684:\tlearn: 0.0931129\ttotal: 1m 9s\tremaining: 32.1s\n",
      "685:\tlearn: 0.0930674\ttotal: 1m 10s\tremaining: 32s\n",
      "686:\tlearn: 0.0930299\ttotal: 1m 10s\tremaining: 31.9s\n",
      "687:\tlearn: 0.0929974\ttotal: 1m 10s\tremaining: 31.8s\n",
      "688:\tlearn: 0.0929297\ttotal: 1m 10s\tremaining: 31.7s\n",
      "689:\tlearn: 0.0928640\ttotal: 1m 10s\tremaining: 31.6s\n",
      "690:\tlearn: 0.0927864\ttotal: 1m 10s\tremaining: 31.5s\n",
      "691:\tlearn: 0.0927324\ttotal: 1m 10s\tremaining: 31.4s\n",
      "692:\tlearn: 0.0926281\ttotal: 1m 10s\tremaining: 31.3s\n",
      "693:\tlearn: 0.0925821\ttotal: 1m 10s\tremaining: 31.2s\n",
      "694:\tlearn: 0.0925086\ttotal: 1m 10s\tremaining: 31.1s\n",
      "695:\tlearn: 0.0924994\ttotal: 1m 11s\tremaining: 31s\n",
      "696:\tlearn: 0.0924720\ttotal: 1m 11s\tremaining: 30.9s\n",
      "697:\tlearn: 0.0924472\ttotal: 1m 11s\tremaining: 30.8s\n",
      "698:\tlearn: 0.0923819\ttotal: 1m 11s\tremaining: 30.7s\n",
      "699:\tlearn: 0.0923160\ttotal: 1m 11s\tremaining: 30.6s\n",
      "700:\tlearn: 0.0922752\ttotal: 1m 11s\tremaining: 30.5s\n",
      "701:\tlearn: 0.0922348\ttotal: 1m 11s\tremaining: 30.4s\n",
      "702:\tlearn: 0.0922010\ttotal: 1m 11s\tremaining: 30.3s\n",
      "703:\tlearn: 0.0921669\ttotal: 1m 11s\tremaining: 30.2s\n",
      "704:\tlearn: 0.0921345\ttotal: 1m 11s\tremaining: 30.1s\n",
      "705:\tlearn: 0.0921017\ttotal: 1m 12s\tremaining: 30s\n",
      "706:\tlearn: 0.0920149\ttotal: 1m 12s\tremaining: 29.9s\n",
      "707:\tlearn: 0.0919751\ttotal: 1m 12s\tremaining: 29.8s\n",
      "708:\tlearn: 0.0919389\ttotal: 1m 12s\tremaining: 29.7s\n",
      "709:\tlearn: 0.0919046\ttotal: 1m 12s\tremaining: 29.6s\n",
      "710:\tlearn: 0.0918628\ttotal: 1m 12s\tremaining: 29.5s\n",
      "711:\tlearn: 0.0918104\ttotal: 1m 12s\tremaining: 29.4s\n",
      "712:\tlearn: 0.0917984\ttotal: 1m 12s\tremaining: 29.3s\n",
      "713:\tlearn: 0.0917321\ttotal: 1m 12s\tremaining: 29.2s\n",
      "714:\tlearn: 0.0916685\ttotal: 1m 12s\tremaining: 29.1s\n",
      "715:\tlearn: 0.0916258\ttotal: 1m 12s\tremaining: 28.9s\n",
      "716:\tlearn: 0.0915967\ttotal: 1m 13s\tremaining: 28.8s\n",
      "717:\tlearn: 0.0915568\ttotal: 1m 13s\tremaining: 28.7s\n",
      "718:\tlearn: 0.0915062\ttotal: 1m 13s\tremaining: 28.6s\n",
      "719:\tlearn: 0.0914726\ttotal: 1m 13s\tremaining: 28.5s\n",
      "720:\tlearn: 0.0914258\ttotal: 1m 13s\tremaining: 28.4s\n",
      "721:\tlearn: 0.0913843\ttotal: 1m 13s\tremaining: 28.3s\n",
      "722:\tlearn: 0.0913567\ttotal: 1m 13s\tremaining: 28.2s\n",
      "723:\tlearn: 0.0913292\ttotal: 1m 13s\tremaining: 28.1s\n",
      "724:\tlearn: 0.0912796\ttotal: 1m 13s\tremaining: 28s\n",
      "725:\tlearn: 0.0912382\ttotal: 1m 14s\tremaining: 27.9s\n",
      "726:\tlearn: 0.0911798\ttotal: 1m 14s\tremaining: 27.8s\n",
      "727:\tlearn: 0.0911469\ttotal: 1m 14s\tremaining: 27.7s\n",
      "728:\tlearn: 0.0911090\ttotal: 1m 14s\tremaining: 27.6s\n",
      "729:\tlearn: 0.0910748\ttotal: 1m 14s\tremaining: 27.5s\n",
      "730:\tlearn: 0.0910558\ttotal: 1m 14s\tremaining: 27.4s\n",
      "731:\tlearn: 0.0909910\ttotal: 1m 14s\tremaining: 27.3s\n",
      "732:\tlearn: 0.0909494\ttotal: 1m 14s\tremaining: 27.2s\n",
      "733:\tlearn: 0.0909179\ttotal: 1m 14s\tremaining: 27.1s\n",
      "734:\tlearn: 0.0908905\ttotal: 1m 14s\tremaining: 27s\n",
      "735:\tlearn: 0.0908560\ttotal: 1m 14s\tremaining: 26.9s\n",
      "736:\tlearn: 0.0908359\ttotal: 1m 15s\tremaining: 26.8s\n",
      "737:\tlearn: 0.0907446\ttotal: 1m 15s\tremaining: 26.7s\n",
      "738:\tlearn: 0.0906729\ttotal: 1m 15s\tremaining: 26.6s\n",
      "739:\tlearn: 0.0906044\ttotal: 1m 15s\tremaining: 26.5s\n",
      "740:\tlearn: 0.0905760\ttotal: 1m 15s\tremaining: 26.4s\n",
      "741:\tlearn: 0.0905101\ttotal: 1m 15s\tremaining: 26.3s\n",
      "742:\tlearn: 0.0904943\ttotal: 1m 15s\tremaining: 26.2s\n",
      "743:\tlearn: 0.0904286\ttotal: 1m 15s\tremaining: 26.1s\n",
      "744:\tlearn: 0.0903989\ttotal: 1m 15s\tremaining: 26s\n",
      "745:\tlearn: 0.0903438\ttotal: 1m 16s\tremaining: 25.9s\n",
      "746:\tlearn: 0.0903049\ttotal: 1m 16s\tremaining: 25.8s\n",
      "747:\tlearn: 0.0902721\ttotal: 1m 16s\tremaining: 25.7s\n",
      "748:\tlearn: 0.0902352\ttotal: 1m 16s\tremaining: 25.6s\n",
      "749:\tlearn: 0.0902066\ttotal: 1m 16s\tremaining: 25.5s\n",
      "750:\tlearn: 0.0901766\ttotal: 1m 16s\tremaining: 25.4s\n",
      "751:\tlearn: 0.0901602\ttotal: 1m 16s\tremaining: 25.3s\n",
      "752:\tlearn: 0.0901231\ttotal: 1m 16s\tremaining: 25.2s\n",
      "753:\tlearn: 0.0900813\ttotal: 1m 16s\tremaining: 25.1s\n",
      "754:\tlearn: 0.0900389\ttotal: 1m 16s\tremaining: 25s\n",
      "755:\tlearn: 0.0900041\ttotal: 1m 17s\tremaining: 24.9s\n",
      "756:\tlearn: 0.0899332\ttotal: 1m 17s\tremaining: 24.8s\n",
      "757:\tlearn: 0.0898727\ttotal: 1m 17s\tremaining: 24.7s\n",
      "758:\tlearn: 0.0898283\ttotal: 1m 17s\tremaining: 24.6s\n",
      "759:\tlearn: 0.0897384\ttotal: 1m 17s\tremaining: 24.5s\n",
      "760:\tlearn: 0.0897060\ttotal: 1m 17s\tremaining: 24.4s\n",
      "761:\tlearn: 0.0896537\ttotal: 1m 17s\tremaining: 24.3s\n",
      "762:\tlearn: 0.0896179\ttotal: 1m 17s\tremaining: 24.2s\n",
      "763:\tlearn: 0.0895797\ttotal: 1m 17s\tremaining: 24.1s\n",
      "764:\tlearn: 0.0895372\ttotal: 1m 17s\tremaining: 23.9s\n",
      "765:\tlearn: 0.0894911\ttotal: 1m 18s\tremaining: 23.8s\n",
      "766:\tlearn: 0.0894464\ttotal: 1m 18s\tremaining: 23.7s\n",
      "767:\tlearn: 0.0893723\ttotal: 1m 18s\tremaining: 23.6s\n",
      "768:\tlearn: 0.0893388\ttotal: 1m 18s\tremaining: 23.5s\n",
      "769:\tlearn: 0.0893195\ttotal: 1m 18s\tremaining: 23.4s\n",
      "770:\tlearn: 0.0892972\ttotal: 1m 18s\tremaining: 23.3s\n",
      "771:\tlearn: 0.0892447\ttotal: 1m 18s\tremaining: 23.2s\n",
      "772:\tlearn: 0.0891960\ttotal: 1m 18s\tremaining: 23.1s\n",
      "773:\tlearn: 0.0891510\ttotal: 1m 18s\tremaining: 23s\n",
      "774:\tlearn: 0.0891178\ttotal: 1m 19s\tremaining: 22.9s\n",
      "775:\tlearn: 0.0890949\ttotal: 1m 19s\tremaining: 22.8s\n",
      "776:\tlearn: 0.0890297\ttotal: 1m 19s\tremaining: 22.7s\n",
      "777:\tlearn: 0.0890042\ttotal: 1m 19s\tremaining: 22.6s\n",
      "778:\tlearn: 0.0889720\ttotal: 1m 19s\tremaining: 22.5s\n",
      "779:\tlearn: 0.0889279\ttotal: 1m 19s\tremaining: 22.4s\n",
      "780:\tlearn: 0.0889090\ttotal: 1m 19s\tremaining: 22.3s\n",
      "781:\tlearn: 0.0888743\ttotal: 1m 19s\tremaining: 22.2s\n",
      "782:\tlearn: 0.0888166\ttotal: 1m 19s\tremaining: 22.1s\n",
      "783:\tlearn: 0.0887855\ttotal: 1m 19s\tremaining: 22s\n",
      "784:\tlearn: 0.0887002\ttotal: 1m 19s\tremaining: 21.9s\n",
      "785:\tlearn: 0.0886396\ttotal: 1m 20s\tremaining: 21.8s\n",
      "786:\tlearn: 0.0886031\ttotal: 1m 20s\tremaining: 21.7s\n",
      "787:\tlearn: 0.0885483\ttotal: 1m 20s\tremaining: 21.6s\n",
      "788:\tlearn: 0.0885104\ttotal: 1m 20s\tremaining: 21.5s\n",
      "789:\tlearn: 0.0884591\ttotal: 1m 20s\tremaining: 21.4s\n",
      "790:\tlearn: 0.0883924\ttotal: 1m 20s\tremaining: 21.3s\n",
      "791:\tlearn: 0.0883578\ttotal: 1m 20s\tremaining: 21.2s\n",
      "792:\tlearn: 0.0883255\ttotal: 1m 20s\tremaining: 21.1s\n",
      "793:\tlearn: 0.0882980\ttotal: 1m 20s\tremaining: 21s\n",
      "794:\tlearn: 0.0882737\ttotal: 1m 21s\tremaining: 20.9s\n",
      "795:\tlearn: 0.0882487\ttotal: 1m 21s\tremaining: 20.8s\n",
      "796:\tlearn: 0.0882237\ttotal: 1m 21s\tremaining: 20.7s\n",
      "797:\tlearn: 0.0881952\ttotal: 1m 21s\tremaining: 20.6s\n",
      "798:\tlearn: 0.0881393\ttotal: 1m 21s\tremaining: 20.5s\n",
      "799:\tlearn: 0.0881120\ttotal: 1m 21s\tremaining: 20.4s\n",
      "800:\tlearn: 0.0880834\ttotal: 1m 21s\tremaining: 20.3s\n",
      "801:\tlearn: 0.0880024\ttotal: 1m 21s\tremaining: 20.2s\n",
      "802:\tlearn: 0.0879396\ttotal: 1m 21s\tremaining: 20.1s\n",
      "803:\tlearn: 0.0878938\ttotal: 1m 21s\tremaining: 20s\n",
      "804:\tlearn: 0.0878195\ttotal: 1m 21s\tremaining: 19.9s\n",
      "805:\tlearn: 0.0877955\ttotal: 1m 22s\tremaining: 19.8s\n",
      "806:\tlearn: 0.0877662\ttotal: 1m 22s\tremaining: 19.7s\n",
      "807:\tlearn: 0.0877360\ttotal: 1m 22s\tremaining: 19.6s\n",
      "808:\tlearn: 0.0876972\ttotal: 1m 22s\tremaining: 19.5s\n",
      "809:\tlearn: 0.0876571\ttotal: 1m 22s\tremaining: 19.4s\n",
      "810:\tlearn: 0.0876159\ttotal: 1m 22s\tremaining: 19.2s\n",
      "811:\tlearn: 0.0875820\ttotal: 1m 22s\tremaining: 19.1s\n",
      "812:\tlearn: 0.0875401\ttotal: 1m 22s\tremaining: 19s\n",
      "813:\tlearn: 0.0874877\ttotal: 1m 22s\tremaining: 18.9s\n",
      "814:\tlearn: 0.0874424\ttotal: 1m 22s\tremaining: 18.8s\n",
      "815:\tlearn: 0.0873857\ttotal: 1m 23s\tremaining: 18.7s\n",
      "816:\tlearn: 0.0873551\ttotal: 1m 23s\tremaining: 18.6s\n",
      "817:\tlearn: 0.0872936\ttotal: 1m 23s\tremaining: 18.5s\n",
      "818:\tlearn: 0.0872572\ttotal: 1m 23s\tremaining: 18.4s\n",
      "819:\tlearn: 0.0872200\ttotal: 1m 23s\tremaining: 18.3s\n",
      "820:\tlearn: 0.0871789\ttotal: 1m 23s\tremaining: 18.2s\n",
      "821:\tlearn: 0.0871428\ttotal: 1m 23s\tremaining: 18.1s\n",
      "822:\tlearn: 0.0871019\ttotal: 1m 23s\tremaining: 18s\n",
      "823:\tlearn: 0.0870568\ttotal: 1m 23s\tremaining: 17.9s\n",
      "824:\tlearn: 0.0870013\ttotal: 1m 23s\tremaining: 17.8s\n",
      "825:\tlearn: 0.0869530\ttotal: 1m 24s\tremaining: 17.7s\n",
      "826:\tlearn: 0.0869444\ttotal: 1m 24s\tremaining: 17.6s\n",
      "827:\tlearn: 0.0869086\ttotal: 1m 24s\tremaining: 17.5s\n",
      "828:\tlearn: 0.0868362\ttotal: 1m 24s\tremaining: 17.4s\n",
      "829:\tlearn: 0.0868088\ttotal: 1m 24s\tremaining: 17.3s\n",
      "830:\tlearn: 0.0867672\ttotal: 1m 24s\tremaining: 17.2s\n",
      "831:\tlearn: 0.0867454\ttotal: 1m 24s\tremaining: 17.1s\n",
      "832:\tlearn: 0.0867097\ttotal: 1m 24s\tremaining: 17s\n",
      "833:\tlearn: 0.0866451\ttotal: 1m 24s\tremaining: 16.9s\n",
      "834:\tlearn: 0.0866212\ttotal: 1m 24s\tremaining: 16.8s\n",
      "835:\tlearn: 0.0865805\ttotal: 1m 25s\tremaining: 16.7s\n",
      "836:\tlearn: 0.0865529\ttotal: 1m 25s\tremaining: 16.6s\n",
      "837:\tlearn: 0.0865431\ttotal: 1m 25s\tremaining: 16.5s\n",
      "838:\tlearn: 0.0865103\ttotal: 1m 25s\tremaining: 16.4s\n",
      "839:\tlearn: 0.0864270\ttotal: 1m 25s\tremaining: 16.3s\n",
      "840:\tlearn: 0.0864025\ttotal: 1m 25s\tremaining: 16.2s\n",
      "841:\tlearn: 0.0863457\ttotal: 1m 25s\tremaining: 16.1s\n",
      "842:\tlearn: 0.0863275\ttotal: 1m 25s\tremaining: 16s\n",
      "843:\tlearn: 0.0862603\ttotal: 1m 26s\tremaining: 15.9s\n",
      "844:\tlearn: 0.0861880\ttotal: 1m 26s\tremaining: 15.8s\n",
      "845:\tlearn: 0.0861077\ttotal: 1m 26s\tremaining: 15.7s\n",
      "846:\tlearn: 0.0860761\ttotal: 1m 26s\tremaining: 15.6s\n",
      "847:\tlearn: 0.0860405\ttotal: 1m 26s\tremaining: 15.5s\n",
      "848:\tlearn: 0.0859882\ttotal: 1m 26s\tremaining: 15.4s\n",
      "849:\tlearn: 0.0859589\ttotal: 1m 26s\tremaining: 15.3s\n",
      "850:\tlearn: 0.0859103\ttotal: 1m 26s\tremaining: 15.2s\n",
      "851:\tlearn: 0.0858831\ttotal: 1m 26s\tremaining: 15.1s\n",
      "852:\tlearn: 0.0858224\ttotal: 1m 26s\tremaining: 15s\n",
      "853:\tlearn: 0.0857839\ttotal: 1m 27s\tremaining: 14.9s\n",
      "854:\tlearn: 0.0857589\ttotal: 1m 27s\tremaining: 14.8s\n",
      "855:\tlearn: 0.0857267\ttotal: 1m 27s\tremaining: 14.7s\n",
      "856:\tlearn: 0.0857018\ttotal: 1m 27s\tremaining: 14.6s\n",
      "857:\tlearn: 0.0856595\ttotal: 1m 27s\tremaining: 14.5s\n",
      "858:\tlearn: 0.0856181\ttotal: 1m 27s\tremaining: 14.4s\n",
      "859:\tlearn: 0.0856014\ttotal: 1m 27s\tremaining: 14.3s\n",
      "860:\tlearn: 0.0855674\ttotal: 1m 27s\tremaining: 14.2s\n",
      "861:\tlearn: 0.0855170\ttotal: 1m 27s\tremaining: 14.1s\n",
      "862:\tlearn: 0.0854602\ttotal: 1m 27s\tremaining: 14s\n",
      "863:\tlearn: 0.0854427\ttotal: 1m 28s\tremaining: 13.9s\n",
      "864:\tlearn: 0.0854134\ttotal: 1m 28s\tremaining: 13.8s\n",
      "865:\tlearn: 0.0853725\ttotal: 1m 28s\tremaining: 13.7s\n",
      "866:\tlearn: 0.0853364\ttotal: 1m 28s\tremaining: 13.6s\n",
      "867:\tlearn: 0.0852941\ttotal: 1m 28s\tremaining: 13.5s\n",
      "868:\tlearn: 0.0852605\ttotal: 1m 28s\tremaining: 13.4s\n",
      "869:\tlearn: 0.0852235\ttotal: 1m 28s\tremaining: 13.3s\n",
      "870:\tlearn: 0.0851936\ttotal: 1m 28s\tremaining: 13.1s\n",
      "871:\tlearn: 0.0851690\ttotal: 1m 28s\tremaining: 13s\n",
      "872:\tlearn: 0.0851465\ttotal: 1m 28s\tremaining: 12.9s\n",
      "873:\tlearn: 0.0850782\ttotal: 1m 29s\tremaining: 12.8s\n",
      "874:\tlearn: 0.0850567\ttotal: 1m 29s\tremaining: 12.7s\n",
      "875:\tlearn: 0.0849950\ttotal: 1m 29s\tremaining: 12.6s\n",
      "876:\tlearn: 0.0849626\ttotal: 1m 29s\tremaining: 12.5s\n",
      "877:\tlearn: 0.0849561\ttotal: 1m 29s\tremaining: 12.4s\n",
      "878:\tlearn: 0.0849285\ttotal: 1m 29s\tremaining: 12.3s\n",
      "879:\tlearn: 0.0848829\ttotal: 1m 29s\tremaining: 12.2s\n",
      "880:\tlearn: 0.0848566\ttotal: 1m 29s\tremaining: 12.1s\n",
      "881:\tlearn: 0.0848007\ttotal: 1m 29s\tremaining: 12s\n",
      "882:\tlearn: 0.0847570\ttotal: 1m 30s\tremaining: 11.9s\n",
      "883:\tlearn: 0.0846999\ttotal: 1m 30s\tremaining: 11.8s\n",
      "884:\tlearn: 0.0846185\ttotal: 1m 30s\tremaining: 11.7s\n",
      "885:\tlearn: 0.0845726\ttotal: 1m 30s\tremaining: 11.6s\n",
      "886:\tlearn: 0.0844910\ttotal: 1m 30s\tremaining: 11.5s\n",
      "887:\tlearn: 0.0844294\ttotal: 1m 30s\tremaining: 11.4s\n",
      "888:\tlearn: 0.0843752\ttotal: 1m 30s\tremaining: 11.3s\n",
      "889:\tlearn: 0.0843264\ttotal: 1m 30s\tremaining: 11.2s\n",
      "890:\tlearn: 0.0842859\ttotal: 1m 30s\tremaining: 11.1s\n",
      "891:\tlearn: 0.0842577\ttotal: 1m 31s\tremaining: 11s\n",
      "892:\tlearn: 0.0842115\ttotal: 1m 31s\tremaining: 10.9s\n",
      "893:\tlearn: 0.0841797\ttotal: 1m 31s\tremaining: 10.8s\n",
      "894:\tlearn: 0.0841453\ttotal: 1m 31s\tremaining: 10.7s\n",
      "895:\tlearn: 0.0840404\ttotal: 1m 31s\tremaining: 10.6s\n",
      "896:\tlearn: 0.0839890\ttotal: 1m 31s\tremaining: 10.5s\n",
      "897:\tlearn: 0.0839578\ttotal: 1m 31s\tremaining: 10.4s\n",
      "898:\tlearn: 0.0838954\ttotal: 1m 31s\tremaining: 10.3s\n",
      "899:\tlearn: 0.0838740\ttotal: 1m 31s\tremaining: 10.2s\n",
      "900:\tlearn: 0.0838322\ttotal: 1m 32s\tremaining: 10.1s\n",
      "901:\tlearn: 0.0838022\ttotal: 1m 32s\tremaining: 10s\n",
      "902:\tlearn: 0.0837675\ttotal: 1m 32s\tremaining: 9.91s\n",
      "903:\tlearn: 0.0837451\ttotal: 1m 32s\tremaining: 9.81s\n",
      "904:\tlearn: 0.0836728\ttotal: 1m 32s\tremaining: 9.71s\n",
      "905:\tlearn: 0.0836009\ttotal: 1m 32s\tremaining: 9.61s\n",
      "906:\tlearn: 0.0835525\ttotal: 1m 32s\tremaining: 9.51s\n",
      "907:\tlearn: 0.0835280\ttotal: 1m 32s\tremaining: 9.4s\n",
      "908:\tlearn: 0.0835077\ttotal: 1m 32s\tremaining: 9.3s\n",
      "909:\tlearn: 0.0834770\ttotal: 1m 33s\tremaining: 9.2s\n",
      "910:\tlearn: 0.0834229\ttotal: 1m 33s\tremaining: 9.1s\n",
      "911:\tlearn: 0.0833853\ttotal: 1m 33s\tremaining: 9s\n",
      "912:\tlearn: 0.0833685\ttotal: 1m 33s\tremaining: 8.9s\n",
      "913:\tlearn: 0.0833314\ttotal: 1m 33s\tremaining: 8.79s\n",
      "914:\tlearn: 0.0833075\ttotal: 1m 33s\tremaining: 8.69s\n",
      "915:\tlearn: 0.0832663\ttotal: 1m 33s\tremaining: 8.59s\n",
      "916:\tlearn: 0.0832285\ttotal: 1m 33s\tremaining: 8.49s\n",
      "917:\tlearn: 0.0832214\ttotal: 1m 33s\tremaining: 8.39s\n",
      "918:\tlearn: 0.0831569\ttotal: 1m 34s\tremaining: 8.29s\n",
      "919:\tlearn: 0.0831292\ttotal: 1m 34s\tremaining: 8.18s\n",
      "920:\tlearn: 0.0830751\ttotal: 1m 34s\tremaining: 8.08s\n",
      "921:\tlearn: 0.0830366\ttotal: 1m 34s\tremaining: 7.98s\n",
      "922:\tlearn: 0.0829828\ttotal: 1m 34s\tremaining: 7.88s\n",
      "923:\tlearn: 0.0829475\ttotal: 1m 34s\tremaining: 7.78s\n",
      "924:\tlearn: 0.0829125\ttotal: 1m 34s\tremaining: 7.69s\n",
      "925:\tlearn: 0.0828846\ttotal: 1m 34s\tremaining: 7.59s\n",
      "926:\tlearn: 0.0828432\ttotal: 1m 35s\tremaining: 7.49s\n",
      "927:\tlearn: 0.0828129\ttotal: 1m 35s\tremaining: 7.38s\n",
      "928:\tlearn: 0.0827569\ttotal: 1m 35s\tremaining: 7.28s\n",
      "929:\tlearn: 0.0827399\ttotal: 1m 35s\tremaining: 7.18s\n",
      "930:\tlearn: 0.0827009\ttotal: 1m 35s\tremaining: 7.08s\n",
      "931:\tlearn: 0.0826543\ttotal: 1m 35s\tremaining: 6.97s\n",
      "932:\tlearn: 0.0825962\ttotal: 1m 35s\tremaining: 6.87s\n",
      "933:\tlearn: 0.0825492\ttotal: 1m 35s\tremaining: 6.77s\n",
      "934:\tlearn: 0.0825237\ttotal: 1m 35s\tremaining: 6.67s\n",
      "935:\tlearn: 0.0824958\ttotal: 1m 36s\tremaining: 6.57s\n",
      "936:\tlearn: 0.0824930\ttotal: 1m 36s\tremaining: 6.46s\n",
      "937:\tlearn: 0.0824412\ttotal: 1m 36s\tremaining: 6.36s\n",
      "938:\tlearn: 0.0824207\ttotal: 1m 36s\tremaining: 6.26s\n",
      "939:\tlearn: 0.0823809\ttotal: 1m 36s\tremaining: 6.15s\n",
      "940:\tlearn: 0.0823627\ttotal: 1m 36s\tremaining: 6.05s\n",
      "941:\tlearn: 0.0823346\ttotal: 1m 36s\tremaining: 5.95s\n",
      "942:\tlearn: 0.0823283\ttotal: 1m 36s\tremaining: 5.85s\n",
      "943:\tlearn: 0.0823137\ttotal: 1m 36s\tremaining: 5.74s\n",
      "944:\tlearn: 0.0822494\ttotal: 1m 36s\tremaining: 5.64s\n",
      "945:\tlearn: 0.0822158\ttotal: 1m 37s\tremaining: 5.54s\n",
      "946:\tlearn: 0.0821810\ttotal: 1m 37s\tremaining: 5.43s\n",
      "947:\tlearn: 0.0820871\ttotal: 1m 37s\tremaining: 5.33s\n",
      "948:\tlearn: 0.0820617\ttotal: 1m 37s\tremaining: 5.23s\n",
      "949:\tlearn: 0.0820359\ttotal: 1m 37s\tremaining: 5.13s\n",
      "950:\tlearn: 0.0820041\ttotal: 1m 37s\tremaining: 5.03s\n",
      "951:\tlearn: 0.0819751\ttotal: 1m 37s\tremaining: 4.93s\n",
      "952:\tlearn: 0.0819550\ttotal: 1m 37s\tremaining: 4.82s\n",
      "953:\tlearn: 0.0819304\ttotal: 1m 37s\tremaining: 4.72s\n",
      "954:\tlearn: 0.0819123\ttotal: 1m 38s\tremaining: 4.62s\n",
      "955:\tlearn: 0.0818755\ttotal: 1m 38s\tremaining: 4.52s\n",
      "956:\tlearn: 0.0818373\ttotal: 1m 38s\tremaining: 4.41s\n",
      "957:\tlearn: 0.0818215\ttotal: 1m 38s\tremaining: 4.31s\n",
      "958:\tlearn: 0.0817790\ttotal: 1m 38s\tremaining: 4.21s\n",
      "959:\tlearn: 0.0817632\ttotal: 1m 38s\tremaining: 4.11s\n",
      "960:\tlearn: 0.0817088\ttotal: 1m 38s\tremaining: 4s\n",
      "961:\tlearn: 0.0816807\ttotal: 1m 38s\tremaining: 3.9s\n",
      "962:\tlearn: 0.0816051\ttotal: 1m 38s\tremaining: 3.8s\n",
      "963:\tlearn: 0.0815750\ttotal: 1m 39s\tremaining: 3.7s\n",
      "964:\tlearn: 0.0815597\ttotal: 1m 39s\tremaining: 3.6s\n",
      "965:\tlearn: 0.0815406\ttotal: 1m 39s\tremaining: 3.49s\n",
      "966:\tlearn: 0.0815017\ttotal: 1m 39s\tremaining: 3.39s\n",
      "967:\tlearn: 0.0814761\ttotal: 1m 39s\tremaining: 3.29s\n",
      "968:\tlearn: 0.0814455\ttotal: 1m 39s\tremaining: 3.19s\n",
      "969:\tlearn: 0.0814048\ttotal: 1m 39s\tremaining: 3.08s\n",
      "970:\tlearn: 0.0813825\ttotal: 1m 39s\tremaining: 2.98s\n",
      "971:\tlearn: 0.0813423\ttotal: 1m 39s\tremaining: 2.88s\n",
      "972:\tlearn: 0.0812952\ttotal: 1m 40s\tremaining: 2.78s\n",
      "973:\tlearn: 0.0812902\ttotal: 1m 40s\tremaining: 2.67s\n",
      "974:\tlearn: 0.0812830\ttotal: 1m 40s\tremaining: 2.57s\n",
      "975:\tlearn: 0.0812421\ttotal: 1m 40s\tremaining: 2.47s\n",
      "976:\tlearn: 0.0812048\ttotal: 1m 40s\tremaining: 2.37s\n",
      "977:\tlearn: 0.0811698\ttotal: 1m 40s\tremaining: 2.26s\n",
      "978:\tlearn: 0.0811248\ttotal: 1m 40s\tremaining: 2.16s\n",
      "979:\tlearn: 0.0810942\ttotal: 1m 40s\tremaining: 2.06s\n",
      "980:\tlearn: 0.0810742\ttotal: 1m 41s\tremaining: 1.96s\n",
      "981:\tlearn: 0.0810276\ttotal: 1m 41s\tremaining: 1.85s\n",
      "982:\tlearn: 0.0810020\ttotal: 1m 41s\tremaining: 1.75s\n",
      "983:\tlearn: 0.0809661\ttotal: 1m 41s\tremaining: 1.65s\n",
      "984:\tlearn: 0.0809486\ttotal: 1m 41s\tremaining: 1.54s\n",
      "985:\tlearn: 0.0809220\ttotal: 1m 41s\tremaining: 1.44s\n",
      "986:\tlearn: 0.0808962\ttotal: 1m 41s\tremaining: 1.34s\n",
      "987:\tlearn: 0.0808706\ttotal: 1m 41s\tremaining: 1.24s\n",
      "988:\tlearn: 0.0808463\ttotal: 1m 41s\tremaining: 1.13s\n",
      "989:\tlearn: 0.0808337\ttotal: 1m 42s\tremaining: 1.03s\n",
      "990:\tlearn: 0.0807922\ttotal: 1m 42s\tremaining: 928ms\n",
      "991:\tlearn: 0.0807349\ttotal: 1m 42s\tremaining: 825ms\n",
      "992:\tlearn: 0.0806940\ttotal: 1m 42s\tremaining: 722ms\n",
      "993:\tlearn: 0.0806379\ttotal: 1m 42s\tremaining: 619ms\n",
      "994:\tlearn: 0.0806054\ttotal: 1m 42s\tremaining: 516ms\n",
      "995:\tlearn: 0.0805683\ttotal: 1m 42s\tremaining: 413ms\n",
      "996:\tlearn: 0.0805168\ttotal: 1m 42s\tremaining: 309ms\n",
      "997:\tlearn: 0.0804996\ttotal: 1m 42s\tremaining: 206ms\n",
      "998:\tlearn: 0.0804480\ttotal: 1m 43s\tremaining: 103ms\n",
      "999:\tlearn: 0.0804174\ttotal: 1m 43s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:241: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5268607\ttotal: 100ms\tremaining: 1m 40s\n",
      "1:\tlearn: 0.4269820\ttotal: 207ms\tremaining: 1m 43s\n",
      "2:\tlearn: 0.3681616\ttotal: 300ms\tremaining: 1m 39s\n",
      "3:\tlearn: 0.3268981\ttotal: 400ms\tremaining: 1m 39s\n",
      "4:\tlearn: 0.3002497\ttotal: 500ms\tremaining: 1m 39s\n",
      "5:\tlearn: 0.2817856\ttotal: 596ms\tremaining: 1m 38s\n",
      "6:\tlearn: 0.2573488\ttotal: 690ms\tremaining: 1m 37s\n",
      "7:\tlearn: 0.2462725\ttotal: 779ms\tremaining: 1m 36s\n",
      "8:\tlearn: 0.2380101\ttotal: 872ms\tremaining: 1m 36s\n",
      "9:\tlearn: 0.2280518\ttotal: 977ms\tremaining: 1m 36s\n",
      "10:\tlearn: 0.2231006\ttotal: 1.08s\tremaining: 1m 37s\n",
      "11:\tlearn: 0.2143295\ttotal: 1.19s\tremaining: 1m 38s\n",
      "12:\tlearn: 0.2105775\ttotal: 1.29s\tremaining: 1m 38s\n",
      "13:\tlearn: 0.2044587\ttotal: 1.39s\tremaining: 1m 38s\n",
      "14:\tlearn: 0.2017606\ttotal: 1.5s\tremaining: 1m 38s\n",
      "15:\tlearn: 0.1984549\ttotal: 1.6s\tremaining: 1m 38s\n",
      "16:\tlearn: 0.1960099\ttotal: 1.7s\tremaining: 1m 38s\n",
      "17:\tlearn: 0.1939983\ttotal: 1.8s\tremaining: 1m 38s\n",
      "18:\tlearn: 0.1920979\ttotal: 1.89s\tremaining: 1m 37s\n",
      "19:\tlearn: 0.1889975\ttotal: 2s\tremaining: 1m 37s\n",
      "20:\tlearn: 0.1862246\ttotal: 2.09s\tremaining: 1m 37s\n",
      "21:\tlearn: 0.1850184\ttotal: 2.19s\tremaining: 1m 37s\n",
      "22:\tlearn: 0.1833773\ttotal: 2.29s\tremaining: 1m 37s\n",
      "23:\tlearn: 0.1822404\ttotal: 2.39s\tremaining: 1m 37s\n",
      "24:\tlearn: 0.1809510\ttotal: 2.49s\tremaining: 1m 37s\n",
      "25:\tlearn: 0.1791770\ttotal: 2.59s\tremaining: 1m 37s\n",
      "26:\tlearn: 0.1780129\ttotal: 2.69s\tremaining: 1m 37s\n",
      "27:\tlearn: 0.1759416\ttotal: 2.79s\tremaining: 1m 36s\n",
      "28:\tlearn: 0.1747437\ttotal: 2.88s\tremaining: 1m 36s\n",
      "29:\tlearn: 0.1736159\ttotal: 2.98s\tremaining: 1m 36s\n",
      "30:\tlearn: 0.1726243\ttotal: 3.08s\tremaining: 1m 36s\n",
      "31:\tlearn: 0.1717540\ttotal: 3.18s\tremaining: 1m 36s\n",
      "32:\tlearn: 0.1707693\ttotal: 3.28s\tremaining: 1m 36s\n",
      "33:\tlearn: 0.1697840\ttotal: 3.37s\tremaining: 1m 35s\n",
      "34:\tlearn: 0.1690638\ttotal: 3.47s\tremaining: 1m 35s\n",
      "35:\tlearn: 0.1680833\ttotal: 3.56s\tremaining: 1m 35s\n",
      "36:\tlearn: 0.1669650\ttotal: 3.66s\tremaining: 1m 35s\n",
      "37:\tlearn: 0.1661950\ttotal: 3.76s\tremaining: 1m 35s\n",
      "38:\tlearn: 0.1648763\ttotal: 3.85s\tremaining: 1m 34s\n",
      "39:\tlearn: 0.1636315\ttotal: 3.94s\tremaining: 1m 34s\n",
      "40:\tlearn: 0.1630486\ttotal: 4.03s\tremaining: 1m 34s\n",
      "41:\tlearn: 0.1624553\ttotal: 4.13s\tremaining: 1m 34s\n",
      "42:\tlearn: 0.1615738\ttotal: 4.23s\tremaining: 1m 34s\n",
      "43:\tlearn: 0.1611372\ttotal: 4.33s\tremaining: 1m 34s\n",
      "44:\tlearn: 0.1604625\ttotal: 4.42s\tremaining: 1m 33s\n",
      "45:\tlearn: 0.1599535\ttotal: 4.52s\tremaining: 1m 33s\n",
      "46:\tlearn: 0.1593361\ttotal: 4.62s\tremaining: 1m 33s\n",
      "47:\tlearn: 0.1588303\ttotal: 4.71s\tremaining: 1m 33s\n",
      "48:\tlearn: 0.1581380\ttotal: 4.81s\tremaining: 1m 33s\n",
      "49:\tlearn: 0.1571699\ttotal: 4.91s\tremaining: 1m 33s\n",
      "50:\tlearn: 0.1567341\ttotal: 5s\tremaining: 1m 33s\n",
      "51:\tlearn: 0.1562317\ttotal: 5.1s\tremaining: 1m 32s\n",
      "52:\tlearn: 0.1556617\ttotal: 5.2s\tremaining: 1m 32s\n",
      "53:\tlearn: 0.1550293\ttotal: 5.3s\tremaining: 1m 32s\n",
      "54:\tlearn: 0.1545285\ttotal: 5.4s\tremaining: 1m 32s\n",
      "55:\tlearn: 0.1538878\ttotal: 5.5s\tremaining: 1m 32s\n",
      "56:\tlearn: 0.1534715\ttotal: 5.61s\tremaining: 1m 32s\n",
      "57:\tlearn: 0.1530334\ttotal: 5.72s\tremaining: 1m 32s\n",
      "58:\tlearn: 0.1525355\ttotal: 5.84s\tremaining: 1m 33s\n",
      "59:\tlearn: 0.1520597\ttotal: 5.94s\tremaining: 1m 33s\n",
      "60:\tlearn: 0.1516275\ttotal: 6.04s\tremaining: 1m 32s\n",
      "61:\tlearn: 0.1512854\ttotal: 6.13s\tremaining: 1m 32s\n",
      "62:\tlearn: 0.1506867\ttotal: 6.23s\tremaining: 1m 32s\n",
      "63:\tlearn: 0.1503658\ttotal: 6.33s\tremaining: 1m 32s\n",
      "64:\tlearn: 0.1500679\ttotal: 6.42s\tremaining: 1m 32s\n",
      "65:\tlearn: 0.1498187\ttotal: 6.52s\tremaining: 1m 32s\n",
      "66:\tlearn: 0.1494450\ttotal: 6.62s\tremaining: 1m 32s\n",
      "67:\tlearn: 0.1491698\ttotal: 6.71s\tremaining: 1m 31s\n",
      "68:\tlearn: 0.1489159\ttotal: 6.81s\tremaining: 1m 31s\n",
      "69:\tlearn: 0.1485396\ttotal: 6.91s\tremaining: 1m 31s\n",
      "70:\tlearn: 0.1481494\ttotal: 7.01s\tremaining: 1m 31s\n",
      "71:\tlearn: 0.1478616\ttotal: 7.11s\tremaining: 1m 31s\n",
      "72:\tlearn: 0.1474092\ttotal: 7.21s\tremaining: 1m 31s\n",
      "73:\tlearn: 0.1470613\ttotal: 7.3s\tremaining: 1m 31s\n",
      "74:\tlearn: 0.1466039\ttotal: 7.4s\tremaining: 1m 31s\n",
      "75:\tlearn: 0.1462449\ttotal: 7.51s\tremaining: 1m 31s\n",
      "76:\tlearn: 0.1459978\ttotal: 7.61s\tremaining: 1m 31s\n",
      "77:\tlearn: 0.1455905\ttotal: 7.71s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1452998\ttotal: 7.83s\tremaining: 1m 31s\n",
      "79:\tlearn: 0.1448523\ttotal: 7.92s\tremaining: 1m 31s\n",
      "80:\tlearn: 0.1446336\ttotal: 8.02s\tremaining: 1m 31s\n",
      "81:\tlearn: 0.1443779\ttotal: 8.12s\tremaining: 1m 30s\n",
      "82:\tlearn: 0.1441293\ttotal: 8.22s\tremaining: 1m 30s\n",
      "83:\tlearn: 0.1436414\ttotal: 8.32s\tremaining: 1m 30s\n",
      "84:\tlearn: 0.1431684\ttotal: 8.42s\tremaining: 1m 30s\n",
      "85:\tlearn: 0.1428985\ttotal: 8.52s\tremaining: 1m 30s\n",
      "86:\tlearn: 0.1426856\ttotal: 8.63s\tremaining: 1m 30s\n",
      "87:\tlearn: 0.1424852\ttotal: 8.73s\tremaining: 1m 30s\n",
      "88:\tlearn: 0.1422724\ttotal: 8.84s\tremaining: 1m 30s\n",
      "89:\tlearn: 0.1420304\ttotal: 8.93s\tremaining: 1m 30s\n",
      "90:\tlearn: 0.1416389\ttotal: 9.04s\tremaining: 1m 30s\n",
      "91:\tlearn: 0.1413915\ttotal: 9.13s\tremaining: 1m 30s\n",
      "92:\tlearn: 0.1411444\ttotal: 9.23s\tremaining: 1m 29s\n",
      "93:\tlearn: 0.1409195\ttotal: 9.32s\tremaining: 1m 29s\n",
      "94:\tlearn: 0.1405878\ttotal: 9.42s\tremaining: 1m 29s\n",
      "95:\tlearn: 0.1403854\ttotal: 9.52s\tremaining: 1m 29s\n",
      "96:\tlearn: 0.1400603\ttotal: 9.63s\tremaining: 1m 29s\n",
      "97:\tlearn: 0.1397621\ttotal: 9.73s\tremaining: 1m 29s\n",
      "98:\tlearn: 0.1394985\ttotal: 9.84s\tremaining: 1m 29s\n",
      "99:\tlearn: 0.1391571\ttotal: 9.95s\tremaining: 1m 29s\n",
      "100:\tlearn: 0.1388610\ttotal: 10s\tremaining: 1m 29s\n",
      "101:\tlearn: 0.1386445\ttotal: 10.2s\tremaining: 1m 29s\n",
      "102:\tlearn: 0.1384397\ttotal: 10.4s\tremaining: 1m 30s\n",
      "103:\tlearn: 0.1381609\ttotal: 10.5s\tremaining: 1m 30s\n",
      "104:\tlearn: 0.1379193\ttotal: 10.5s\tremaining: 1m 29s\n",
      "105:\tlearn: 0.1376623\ttotal: 10.6s\tremaining: 1m 29s\n",
      "106:\tlearn: 0.1374115\ttotal: 10.7s\tremaining: 1m 29s\n",
      "107:\tlearn: 0.1372035\ttotal: 10.8s\tremaining: 1m 29s\n",
      "108:\tlearn: 0.1369530\ttotal: 10.9s\tremaining: 1m 29s\n",
      "109:\tlearn: 0.1367694\ttotal: 11s\tremaining: 1m 29s\n",
      "110:\tlearn: 0.1365778\ttotal: 11.1s\tremaining: 1m 29s\n",
      "111:\tlearn: 0.1363358\ttotal: 11.2s\tremaining: 1m 28s\n",
      "112:\tlearn: 0.1361752\ttotal: 11.3s\tremaining: 1m 28s\n",
      "113:\tlearn: 0.1359588\ttotal: 11.4s\tremaining: 1m 28s\n",
      "114:\tlearn: 0.1357564\ttotal: 11.5s\tremaining: 1m 28s\n",
      "115:\tlearn: 0.1355401\ttotal: 11.6s\tremaining: 1m 28s\n",
      "116:\tlearn: 0.1353664\ttotal: 11.7s\tremaining: 1m 28s\n",
      "117:\tlearn: 0.1352324\ttotal: 11.8s\tremaining: 1m 28s\n",
      "118:\tlearn: 0.1350380\ttotal: 11.9s\tremaining: 1m 27s\n",
      "119:\tlearn: 0.1348003\ttotal: 12s\tremaining: 1m 27s\n",
      "120:\tlearn: 0.1346391\ttotal: 12.1s\tremaining: 1m 27s\n",
      "121:\tlearn: 0.1344749\ttotal: 12.2s\tremaining: 1m 27s\n",
      "122:\tlearn: 0.1342808\ttotal: 12.3s\tremaining: 1m 27s\n",
      "123:\tlearn: 0.1340956\ttotal: 12.4s\tremaining: 1m 27s\n",
      "124:\tlearn: 0.1337636\ttotal: 12.5s\tremaining: 1m 27s\n",
      "125:\tlearn: 0.1335054\ttotal: 12.5s\tremaining: 1m 26s\n",
      "126:\tlearn: 0.1332535\ttotal: 12.6s\tremaining: 1m 26s\n",
      "127:\tlearn: 0.1331183\ttotal: 12.7s\tremaining: 1m 26s\n",
      "128:\tlearn: 0.1328604\ttotal: 12.8s\tremaining: 1m 26s\n",
      "129:\tlearn: 0.1327031\ttotal: 12.9s\tremaining: 1m 26s\n",
      "130:\tlearn: 0.1325598\ttotal: 13s\tremaining: 1m 26s\n",
      "131:\tlearn: 0.1324171\ttotal: 13.1s\tremaining: 1m 26s\n",
      "132:\tlearn: 0.1322828\ttotal: 13.2s\tremaining: 1m 26s\n",
      "133:\tlearn: 0.1321579\ttotal: 13.3s\tremaining: 1m 25s\n",
      "134:\tlearn: 0.1319771\ttotal: 13.4s\tremaining: 1m 25s\n",
      "135:\tlearn: 0.1318266\ttotal: 13.5s\tremaining: 1m 25s\n",
      "136:\tlearn: 0.1316904\ttotal: 13.6s\tremaining: 1m 25s\n",
      "137:\tlearn: 0.1315639\ttotal: 13.7s\tremaining: 1m 25s\n",
      "138:\tlearn: 0.1314143\ttotal: 13.8s\tremaining: 1m 25s\n",
      "139:\tlearn: 0.1311796\ttotal: 13.9s\tremaining: 1m 25s\n",
      "140:\tlearn: 0.1310619\ttotal: 14s\tremaining: 1m 25s\n",
      "141:\tlearn: 0.1308967\ttotal: 14.1s\tremaining: 1m 24s\n",
      "142:\tlearn: 0.1306634\ttotal: 14.2s\tremaining: 1m 24s\n",
      "143:\tlearn: 0.1305444\ttotal: 14.3s\tremaining: 1m 24s\n",
      "144:\tlearn: 0.1303831\ttotal: 14.4s\tremaining: 1m 24s\n",
      "145:\tlearn: 0.1301791\ttotal: 14.5s\tremaining: 1m 24s\n",
      "146:\tlearn: 0.1300869\ttotal: 14.6s\tremaining: 1m 24s\n",
      "147:\tlearn: 0.1298838\ttotal: 14.7s\tremaining: 1m 24s\n",
      "148:\tlearn: 0.1297737\ttotal: 14.8s\tremaining: 1m 24s\n",
      "149:\tlearn: 0.1295595\ttotal: 14.9s\tremaining: 1m 24s\n",
      "150:\tlearn: 0.1294348\ttotal: 14.9s\tremaining: 1m 24s\n",
      "151:\tlearn: 0.1292791\ttotal: 15s\tremaining: 1m 23s\n",
      "152:\tlearn: 0.1291057\ttotal: 15.2s\tremaining: 1m 23s\n",
      "153:\tlearn: 0.1289654\ttotal: 15.3s\tremaining: 1m 23s\n",
      "154:\tlearn: 0.1288361\ttotal: 15.3s\tremaining: 1m 23s\n",
      "155:\tlearn: 0.1286630\ttotal: 15.4s\tremaining: 1m 23s\n",
      "156:\tlearn: 0.1285243\ttotal: 15.5s\tremaining: 1m 23s\n",
      "157:\tlearn: 0.1283835\ttotal: 15.6s\tremaining: 1m 23s\n",
      "158:\tlearn: 0.1283015\ttotal: 15.7s\tremaining: 1m 23s\n",
      "159:\tlearn: 0.1281388\ttotal: 15.8s\tremaining: 1m 23s\n",
      "160:\tlearn: 0.1280158\ttotal: 15.9s\tremaining: 1m 23s\n",
      "161:\tlearn: 0.1278931\ttotal: 16s\tremaining: 1m 22s\n",
      "162:\tlearn: 0.1277901\ttotal: 16.1s\tremaining: 1m 22s\n",
      "163:\tlearn: 0.1276182\ttotal: 16.2s\tremaining: 1m 22s\n",
      "164:\tlearn: 0.1274463\ttotal: 16.3s\tremaining: 1m 22s\n",
      "165:\tlearn: 0.1273051\ttotal: 16.4s\tremaining: 1m 22s\n",
      "166:\tlearn: 0.1271763\ttotal: 16.5s\tremaining: 1m 22s\n",
      "167:\tlearn: 0.1270587\ttotal: 16.6s\tremaining: 1m 22s\n",
      "168:\tlearn: 0.1269730\ttotal: 16.7s\tremaining: 1m 22s\n",
      "169:\tlearn: 0.1268438\ttotal: 16.8s\tremaining: 1m 22s\n",
      "170:\tlearn: 0.1267019\ttotal: 16.9s\tremaining: 1m 22s\n",
      "171:\tlearn: 0.1265801\ttotal: 17s\tremaining: 1m 21s\n",
      "172:\tlearn: 0.1264997\ttotal: 17.1s\tremaining: 1m 21s\n",
      "173:\tlearn: 0.1263765\ttotal: 17.2s\tremaining: 1m 21s\n",
      "174:\tlearn: 0.1261718\ttotal: 17.3s\tremaining: 1m 21s\n",
      "175:\tlearn: 0.1260292\ttotal: 17.4s\tremaining: 1m 21s\n",
      "176:\tlearn: 0.1258658\ttotal: 17.5s\tremaining: 1m 21s\n",
      "177:\tlearn: 0.1257798\ttotal: 17.6s\tremaining: 1m 21s\n",
      "178:\tlearn: 0.1256956\ttotal: 17.7s\tremaining: 1m 21s\n",
      "179:\tlearn: 0.1255935\ttotal: 17.8s\tremaining: 1m 21s\n",
      "180:\tlearn: 0.1254653\ttotal: 17.9s\tremaining: 1m 21s\n",
      "181:\tlearn: 0.1252999\ttotal: 18.1s\tremaining: 1m 21s\n",
      "182:\tlearn: 0.1251772\ttotal: 18.2s\tremaining: 1m 21s\n",
      "183:\tlearn: 0.1250018\ttotal: 18.4s\tremaining: 1m 21s\n",
      "184:\tlearn: 0.1248675\ttotal: 18.5s\tremaining: 1m 21s\n",
      "185:\tlearn: 0.1247740\ttotal: 18.6s\tremaining: 1m 21s\n",
      "186:\tlearn: 0.1246235\ttotal: 18.7s\tremaining: 1m 21s\n",
      "187:\tlearn: 0.1245189\ttotal: 18.8s\tremaining: 1m 21s\n",
      "188:\tlearn: 0.1244232\ttotal: 18.9s\tremaining: 1m 20s\n",
      "189:\tlearn: 0.1243023\ttotal: 19s\tremaining: 1m 20s\n",
      "190:\tlearn: 0.1241678\ttotal: 19s\tremaining: 1m 20s\n",
      "191:\tlearn: 0.1240883\ttotal: 19.1s\tremaining: 1m 20s\n",
      "192:\tlearn: 0.1239963\ttotal: 19.2s\tremaining: 1m 20s\n",
      "193:\tlearn: 0.1238766\ttotal: 19.3s\tremaining: 1m 20s\n",
      "194:\tlearn: 0.1237634\ttotal: 19.4s\tremaining: 1m 20s\n",
      "195:\tlearn: 0.1236590\ttotal: 19.5s\tremaining: 1m 20s\n",
      "196:\tlearn: 0.1235154\ttotal: 19.7s\tremaining: 1m 20s\n",
      "197:\tlearn: 0.1233674\ttotal: 19.8s\tremaining: 1m 20s\n",
      "198:\tlearn: 0.1232850\ttotal: 19.9s\tremaining: 1m 19s\n",
      "199:\tlearn: 0.1232030\ttotal: 20s\tremaining: 1m 19s\n",
      "200:\tlearn: 0.1231343\ttotal: 20.1s\tremaining: 1m 19s\n",
      "201:\tlearn: 0.1230357\ttotal: 20.2s\tremaining: 1m 19s\n",
      "202:\tlearn: 0.1229563\ttotal: 20.2s\tremaining: 1m 19s\n",
      "203:\tlearn: 0.1228525\ttotal: 20.3s\tremaining: 1m 19s\n",
      "204:\tlearn: 0.1227472\ttotal: 20.4s\tremaining: 1m 19s\n",
      "205:\tlearn: 0.1226346\ttotal: 20.5s\tremaining: 1m 19s\n",
      "206:\tlearn: 0.1225470\ttotal: 20.6s\tremaining: 1m 19s\n",
      "207:\tlearn: 0.1224360\ttotal: 20.7s\tremaining: 1m 18s\n",
      "208:\tlearn: 0.1223440\ttotal: 20.8s\tremaining: 1m 18s\n",
      "209:\tlearn: 0.1222354\ttotal: 20.9s\tremaining: 1m 18s\n",
      "210:\tlearn: 0.1221762\ttotal: 21s\tremaining: 1m 18s\n",
      "211:\tlearn: 0.1220674\ttotal: 21.1s\tremaining: 1m 18s\n",
      "212:\tlearn: 0.1219843\ttotal: 21.2s\tremaining: 1m 18s\n",
      "213:\tlearn: 0.1218660\ttotal: 21.3s\tremaining: 1m 18s\n",
      "214:\tlearn: 0.1217430\ttotal: 21.4s\tremaining: 1m 18s\n",
      "215:\tlearn: 0.1216662\ttotal: 21.5s\tremaining: 1m 18s\n",
      "216:\tlearn: 0.1215650\ttotal: 21.6s\tremaining: 1m 18s\n",
      "217:\tlearn: 0.1214617\ttotal: 21.7s\tremaining: 1m 17s\n",
      "218:\tlearn: 0.1213566\ttotal: 21.8s\tremaining: 1m 17s\n",
      "219:\tlearn: 0.1212073\ttotal: 21.9s\tremaining: 1m 17s\n",
      "220:\tlearn: 0.1211008\ttotal: 22s\tremaining: 1m 17s\n",
      "221:\tlearn: 0.1209761\ttotal: 22.1s\tremaining: 1m 17s\n",
      "222:\tlearn: 0.1208318\ttotal: 22.2s\tremaining: 1m 17s\n",
      "223:\tlearn: 0.1207367\ttotal: 22.3s\tremaining: 1m 17s\n",
      "224:\tlearn: 0.1206508\ttotal: 22.4s\tremaining: 1m 17s\n",
      "225:\tlearn: 0.1204819\ttotal: 22.5s\tremaining: 1m 17s\n",
      "226:\tlearn: 0.1204202\ttotal: 22.6s\tremaining: 1m 16s\n",
      "227:\tlearn: 0.1203006\ttotal: 22.7s\tremaining: 1m 16s\n",
      "228:\tlearn: 0.1202015\ttotal: 22.8s\tremaining: 1m 16s\n",
      "229:\tlearn: 0.1200898\ttotal: 22.9s\tremaining: 1m 16s\n",
      "230:\tlearn: 0.1199982\ttotal: 23s\tremaining: 1m 16s\n",
      "231:\tlearn: 0.1199072\ttotal: 23.1s\tremaining: 1m 16s\n",
      "232:\tlearn: 0.1196949\ttotal: 23.2s\tremaining: 1m 16s\n",
      "233:\tlearn: 0.1196116\ttotal: 23.3s\tremaining: 1m 16s\n",
      "234:\tlearn: 0.1195102\ttotal: 23.4s\tremaining: 1m 16s\n",
      "235:\tlearn: 0.1194351\ttotal: 23.5s\tremaining: 1m 16s\n",
      "236:\tlearn: 0.1193870\ttotal: 23.6s\tremaining: 1m 16s\n",
      "237:\tlearn: 0.1192903\ttotal: 23.7s\tremaining: 1m 15s\n",
      "238:\tlearn: 0.1191917\ttotal: 23.8s\tremaining: 1m 15s\n",
      "239:\tlearn: 0.1191190\ttotal: 23.9s\tremaining: 1m 15s\n",
      "240:\tlearn: 0.1190360\ttotal: 24s\tremaining: 1m 15s\n",
      "241:\tlearn: 0.1189667\ttotal: 24.1s\tremaining: 1m 15s\n",
      "242:\tlearn: 0.1188091\ttotal: 24.2s\tremaining: 1m 15s\n",
      "243:\tlearn: 0.1186939\ttotal: 24.3s\tremaining: 1m 15s\n",
      "244:\tlearn: 0.1186246\ttotal: 24.4s\tremaining: 1m 15s\n",
      "245:\tlearn: 0.1185713\ttotal: 24.5s\tremaining: 1m 15s\n",
      "246:\tlearn: 0.1184642\ttotal: 24.6s\tremaining: 1m 14s\n",
      "247:\tlearn: 0.1183165\ttotal: 24.7s\tremaining: 1m 14s\n",
      "248:\tlearn: 0.1181996\ttotal: 24.8s\tremaining: 1m 14s\n",
      "249:\tlearn: 0.1181496\ttotal: 24.9s\tremaining: 1m 14s\n",
      "250:\tlearn: 0.1180268\ttotal: 25s\tremaining: 1m 14s\n",
      "251:\tlearn: 0.1179813\ttotal: 25.1s\tremaining: 1m 14s\n",
      "252:\tlearn: 0.1179278\ttotal: 25.2s\tremaining: 1m 14s\n",
      "253:\tlearn: 0.1178308\ttotal: 25.3s\tremaining: 1m 14s\n",
      "254:\tlearn: 0.1177507\ttotal: 25.4s\tremaining: 1m 14s\n",
      "255:\tlearn: 0.1176785\ttotal: 25.5s\tremaining: 1m 14s\n",
      "256:\tlearn: 0.1176023\ttotal: 25.6s\tremaining: 1m 13s\n",
      "257:\tlearn: 0.1175053\ttotal: 25.7s\tremaining: 1m 13s\n",
      "258:\tlearn: 0.1173955\ttotal: 25.8s\tremaining: 1m 13s\n",
      "259:\tlearn: 0.1172984\ttotal: 25.9s\tremaining: 1m 13s\n",
      "260:\tlearn: 0.1172130\ttotal: 26s\tremaining: 1m 13s\n",
      "261:\tlearn: 0.1171593\ttotal: 26.1s\tremaining: 1m 13s\n",
      "262:\tlearn: 0.1171059\ttotal: 26.3s\tremaining: 1m 13s\n",
      "263:\tlearn: 0.1169691\ttotal: 26.4s\tremaining: 1m 13s\n",
      "264:\tlearn: 0.1168836\ttotal: 26.5s\tremaining: 1m 13s\n",
      "265:\tlearn: 0.1168102\ttotal: 26.6s\tremaining: 1m 13s\n",
      "266:\tlearn: 0.1167102\ttotal: 26.7s\tremaining: 1m 13s\n",
      "267:\tlearn: 0.1166016\ttotal: 26.8s\tremaining: 1m 13s\n",
      "268:\tlearn: 0.1165584\ttotal: 26.9s\tremaining: 1m 13s\n",
      "269:\tlearn: 0.1164041\ttotal: 27s\tremaining: 1m 13s\n",
      "270:\tlearn: 0.1163614\ttotal: 27.1s\tremaining: 1m 12s\n",
      "271:\tlearn: 0.1163034\ttotal: 27.2s\tremaining: 1m 12s\n",
      "272:\tlearn: 0.1162619\ttotal: 27.3s\tremaining: 1m 12s\n",
      "273:\tlearn: 0.1162052\ttotal: 27.4s\tremaining: 1m 12s\n",
      "274:\tlearn: 0.1161460\ttotal: 27.5s\tremaining: 1m 12s\n",
      "275:\tlearn: 0.1160980\ttotal: 27.6s\tremaining: 1m 12s\n",
      "276:\tlearn: 0.1159902\ttotal: 27.7s\tremaining: 1m 12s\n",
      "277:\tlearn: 0.1158668\ttotal: 27.8s\tremaining: 1m 12s\n",
      "278:\tlearn: 0.1158112\ttotal: 27.9s\tremaining: 1m 12s\n",
      "279:\tlearn: 0.1156830\ttotal: 28s\tremaining: 1m 11s\n",
      "280:\tlearn: 0.1156241\ttotal: 28.1s\tremaining: 1m 11s\n",
      "281:\tlearn: 0.1155715\ttotal: 28.2s\tremaining: 1m 11s\n",
      "282:\tlearn: 0.1154883\ttotal: 28.3s\tremaining: 1m 11s\n",
      "283:\tlearn: 0.1154318\ttotal: 28.4s\tremaining: 1m 11s\n",
      "284:\tlearn: 0.1153323\ttotal: 28.5s\tremaining: 1m 11s\n",
      "285:\tlearn: 0.1152554\ttotal: 28.6s\tremaining: 1m 11s\n",
      "286:\tlearn: 0.1151541\ttotal: 28.7s\tremaining: 1m 11s\n",
      "287:\tlearn: 0.1150950\ttotal: 28.9s\tremaining: 1m 11s\n",
      "288:\tlearn: 0.1149792\ttotal: 29s\tremaining: 1m 11s\n",
      "289:\tlearn: 0.1149214\ttotal: 29.1s\tremaining: 1m 11s\n",
      "290:\tlearn: 0.1148017\ttotal: 29.2s\tremaining: 1m 11s\n",
      "291:\tlearn: 0.1147290\ttotal: 29.4s\tremaining: 1m 11s\n",
      "292:\tlearn: 0.1146816\ttotal: 29.5s\tremaining: 1m 11s\n",
      "293:\tlearn: 0.1146140\ttotal: 29.6s\tremaining: 1m 11s\n",
      "294:\tlearn: 0.1145324\ttotal: 29.7s\tremaining: 1m 10s\n",
      "295:\tlearn: 0.1144643\ttotal: 29.8s\tremaining: 1m 10s\n",
      "296:\tlearn: 0.1144124\ttotal: 29.9s\tremaining: 1m 10s\n",
      "297:\tlearn: 0.1143456\ttotal: 30s\tremaining: 1m 10s\n",
      "298:\tlearn: 0.1142568\ttotal: 30.1s\tremaining: 1m 10s\n",
      "299:\tlearn: 0.1142061\ttotal: 30.2s\tremaining: 1m 10s\n",
      "300:\tlearn: 0.1141308\ttotal: 30.3s\tremaining: 1m 10s\n",
      "301:\tlearn: 0.1140720\ttotal: 30.4s\tremaining: 1m 10s\n",
      "302:\tlearn: 0.1139677\ttotal: 30.5s\tremaining: 1m 10s\n",
      "303:\tlearn: 0.1138966\ttotal: 30.6s\tremaining: 1m 9s\n",
      "304:\tlearn: 0.1138539\ttotal: 30.7s\tremaining: 1m 9s\n",
      "305:\tlearn: 0.1137663\ttotal: 30.8s\tremaining: 1m 9s\n",
      "306:\tlearn: 0.1136827\ttotal: 30.9s\tremaining: 1m 9s\n",
      "307:\tlearn: 0.1136191\ttotal: 31s\tremaining: 1m 9s\n",
      "308:\tlearn: 0.1135369\ttotal: 31.1s\tremaining: 1m 9s\n",
      "309:\tlearn: 0.1134610\ttotal: 31.2s\tremaining: 1m 9s\n",
      "310:\tlearn: 0.1133372\ttotal: 31.3s\tremaining: 1m 9s\n",
      "311:\tlearn: 0.1132885\ttotal: 31.4s\tremaining: 1m 9s\n",
      "312:\tlearn: 0.1132097\ttotal: 31.5s\tremaining: 1m 9s\n",
      "313:\tlearn: 0.1131379\ttotal: 31.7s\tremaining: 1m 9s\n",
      "314:\tlearn: 0.1130884\ttotal: 31.8s\tremaining: 1m 9s\n",
      "315:\tlearn: 0.1130160\ttotal: 31.9s\tremaining: 1m 9s\n",
      "316:\tlearn: 0.1129398\ttotal: 32s\tremaining: 1m 9s\n",
      "317:\tlearn: 0.1128060\ttotal: 32.1s\tremaining: 1m 8s\n",
      "318:\tlearn: 0.1127520\ttotal: 32.2s\tremaining: 1m 8s\n",
      "319:\tlearn: 0.1126874\ttotal: 32.3s\tremaining: 1m 8s\n",
      "320:\tlearn: 0.1125919\ttotal: 32.4s\tremaining: 1m 8s\n",
      "321:\tlearn: 0.1125258\ttotal: 32.5s\tremaining: 1m 8s\n",
      "322:\tlearn: 0.1123926\ttotal: 32.6s\tremaining: 1m 8s\n",
      "323:\tlearn: 0.1123490\ttotal: 32.7s\tremaining: 1m 8s\n",
      "324:\tlearn: 0.1122619\ttotal: 32.8s\tremaining: 1m 8s\n",
      "325:\tlearn: 0.1121595\ttotal: 32.9s\tremaining: 1m 8s\n",
      "326:\tlearn: 0.1121187\ttotal: 33s\tremaining: 1m 8s\n",
      "327:\tlearn: 0.1120610\ttotal: 33.2s\tremaining: 1m 7s\n",
      "328:\tlearn: 0.1120146\ttotal: 33.3s\tremaining: 1m 7s\n",
      "329:\tlearn: 0.1119559\ttotal: 33.4s\tremaining: 1m 7s\n",
      "330:\tlearn: 0.1119161\ttotal: 33.5s\tremaining: 1m 7s\n",
      "331:\tlearn: 0.1118606\ttotal: 33.6s\tremaining: 1m 7s\n",
      "332:\tlearn: 0.1118120\ttotal: 33.7s\tremaining: 1m 7s\n",
      "333:\tlearn: 0.1116933\ttotal: 33.8s\tremaining: 1m 7s\n",
      "334:\tlearn: 0.1115994\ttotal: 33.9s\tremaining: 1m 7s\n",
      "335:\tlearn: 0.1115390\ttotal: 34s\tremaining: 1m 7s\n",
      "336:\tlearn: 0.1114746\ttotal: 34.1s\tremaining: 1m 7s\n",
      "337:\tlearn: 0.1114000\ttotal: 34.2s\tremaining: 1m 6s\n",
      "338:\tlearn: 0.1113520\ttotal: 34.3s\tremaining: 1m 6s\n",
      "339:\tlearn: 0.1113132\ttotal: 34.4s\tremaining: 1m 6s\n",
      "340:\tlearn: 0.1112665\ttotal: 34.6s\tremaining: 1m 6s\n",
      "341:\tlearn: 0.1111996\ttotal: 34.7s\tremaining: 1m 6s\n",
      "342:\tlearn: 0.1111083\ttotal: 34.8s\tremaining: 1m 6s\n",
      "343:\tlearn: 0.1109961\ttotal: 34.9s\tremaining: 1m 6s\n",
      "344:\tlearn: 0.1109102\ttotal: 35s\tremaining: 1m 6s\n",
      "345:\tlearn: 0.1108340\ttotal: 35.1s\tremaining: 1m 6s\n",
      "346:\tlearn: 0.1107933\ttotal: 35.2s\tremaining: 1m 6s\n",
      "347:\tlearn: 0.1107562\ttotal: 35.3s\tremaining: 1m 6s\n",
      "348:\tlearn: 0.1107027\ttotal: 35.4s\tremaining: 1m 6s\n",
      "349:\tlearn: 0.1106319\ttotal: 35.5s\tremaining: 1m 5s\n",
      "350:\tlearn: 0.1106017\ttotal: 35.6s\tremaining: 1m 5s\n",
      "351:\tlearn: 0.1105413\ttotal: 35.7s\tremaining: 1m 5s\n",
      "352:\tlearn: 0.1104976\ttotal: 35.8s\tremaining: 1m 5s\n",
      "353:\tlearn: 0.1104406\ttotal: 35.9s\tremaining: 1m 5s\n",
      "354:\tlearn: 0.1103206\ttotal: 36s\tremaining: 1m 5s\n",
      "355:\tlearn: 0.1102795\ttotal: 36.1s\tremaining: 1m 5s\n",
      "356:\tlearn: 0.1102216\ttotal: 36.2s\tremaining: 1m 5s\n",
      "357:\tlearn: 0.1101729\ttotal: 36.3s\tremaining: 1m 5s\n",
      "358:\tlearn: 0.1101206\ttotal: 36.4s\tremaining: 1m 4s\n",
      "359:\tlearn: 0.1100653\ttotal: 36.5s\tremaining: 1m 4s\n",
      "360:\tlearn: 0.1099345\ttotal: 36.6s\tremaining: 1m 4s\n",
      "361:\tlearn: 0.1098405\ttotal: 36.7s\tremaining: 1m 4s\n",
      "362:\tlearn: 0.1097425\ttotal: 36.8s\tremaining: 1m 4s\n",
      "363:\tlearn: 0.1096913\ttotal: 36.9s\tremaining: 1m 4s\n",
      "364:\tlearn: 0.1096190\ttotal: 37.1s\tremaining: 1m 4s\n",
      "365:\tlearn: 0.1095766\ttotal: 37.2s\tremaining: 1m 4s\n",
      "366:\tlearn: 0.1095159\ttotal: 37.2s\tremaining: 1m 4s\n",
      "367:\tlearn: 0.1094287\ttotal: 37.3s\tremaining: 1m 4s\n",
      "368:\tlearn: 0.1093722\ttotal: 37.4s\tremaining: 1m 4s\n",
      "369:\tlearn: 0.1092738\ttotal: 37.5s\tremaining: 1m 3s\n",
      "370:\tlearn: 0.1092311\ttotal: 37.6s\tremaining: 1m 3s\n",
      "371:\tlearn: 0.1091775\ttotal: 37.7s\tremaining: 1m 3s\n",
      "372:\tlearn: 0.1091402\ttotal: 37.8s\tremaining: 1m 3s\n",
      "373:\tlearn: 0.1090840\ttotal: 37.9s\tremaining: 1m 3s\n",
      "374:\tlearn: 0.1090522\ttotal: 38s\tremaining: 1m 3s\n",
      "375:\tlearn: 0.1089735\ttotal: 38.1s\tremaining: 1m 3s\n",
      "376:\tlearn: 0.1088690\ttotal: 38.2s\tremaining: 1m 3s\n",
      "377:\tlearn: 0.1088153\ttotal: 38.3s\tremaining: 1m 3s\n",
      "378:\tlearn: 0.1087550\ttotal: 38.4s\tremaining: 1m 2s\n",
      "379:\tlearn: 0.1086844\ttotal: 38.5s\tremaining: 1m 2s\n",
      "380:\tlearn: 0.1086224\ttotal: 38.6s\tremaining: 1m 2s\n",
      "381:\tlearn: 0.1085290\ttotal: 38.7s\tremaining: 1m 2s\n",
      "382:\tlearn: 0.1084175\ttotal: 38.8s\tremaining: 1m 2s\n",
      "383:\tlearn: 0.1083730\ttotal: 38.9s\tremaining: 1m 2s\n",
      "384:\tlearn: 0.1083370\ttotal: 39s\tremaining: 1m 2s\n",
      "385:\tlearn: 0.1082561\ttotal: 39.1s\tremaining: 1m 2s\n",
      "386:\tlearn: 0.1082050\ttotal: 39.2s\tremaining: 1m 2s\n",
      "387:\tlearn: 0.1081321\ttotal: 39.3s\tremaining: 1m 2s\n",
      "388:\tlearn: 0.1080594\ttotal: 39.4s\tremaining: 1m 1s\n",
      "389:\tlearn: 0.1080021\ttotal: 39.5s\tremaining: 1m 1s\n",
      "390:\tlearn: 0.1079459\ttotal: 39.6s\tremaining: 1m 1s\n",
      "391:\tlearn: 0.1078878\ttotal: 39.7s\tremaining: 1m 1s\n",
      "392:\tlearn: 0.1078359\ttotal: 39.8s\tremaining: 1m 1s\n",
      "393:\tlearn: 0.1077565\ttotal: 39.9s\tremaining: 1m 1s\n",
      "394:\tlearn: 0.1076956\ttotal: 40s\tremaining: 1m 1s\n",
      "395:\tlearn: 0.1076372\ttotal: 40.1s\tremaining: 1m 1s\n",
      "396:\tlearn: 0.1075912\ttotal: 40.2s\tremaining: 1m 1s\n",
      "397:\tlearn: 0.1075188\ttotal: 40.3s\tremaining: 1m\n",
      "398:\tlearn: 0.1074547\ttotal: 40.4s\tremaining: 1m\n",
      "399:\tlearn: 0.1074055\ttotal: 40.5s\tremaining: 1m\n",
      "400:\tlearn: 0.1073740\ttotal: 40.6s\tremaining: 1m\n",
      "401:\tlearn: 0.1073353\ttotal: 40.7s\tremaining: 1m\n",
      "402:\tlearn: 0.1072981\ttotal: 40.8s\tremaining: 1m\n",
      "403:\tlearn: 0.1072322\ttotal: 40.9s\tremaining: 1m\n",
      "404:\tlearn: 0.1072032\ttotal: 41s\tremaining: 1m\n",
      "405:\tlearn: 0.1070845\ttotal: 41.1s\tremaining: 1m\n",
      "406:\tlearn: 0.1069629\ttotal: 41.2s\tremaining: 1m\n",
      "407:\tlearn: 0.1069156\ttotal: 41.3s\tremaining: 59.9s\n",
      "408:\tlearn: 0.1068596\ttotal: 41.4s\tremaining: 59.8s\n",
      "409:\tlearn: 0.1067911\ttotal: 41.5s\tremaining: 59.7s\n",
      "410:\tlearn: 0.1067102\ttotal: 41.6s\tremaining: 59.6s\n",
      "411:\tlearn: 0.1066133\ttotal: 41.7s\tremaining: 59.5s\n",
      "412:\tlearn: 0.1065311\ttotal: 41.8s\tremaining: 59.4s\n",
      "413:\tlearn: 0.1064668\ttotal: 41.9s\tremaining: 59.3s\n",
      "414:\tlearn: 0.1064219\ttotal: 42s\tremaining: 59.2s\n",
      "415:\tlearn: 0.1063630\ttotal: 42.1s\tremaining: 59.1s\n",
      "416:\tlearn: 0.1063236\ttotal: 42.2s\tremaining: 59s\n",
      "417:\tlearn: 0.1062441\ttotal: 42.3s\tremaining: 58.9s\n",
      "418:\tlearn: 0.1060964\ttotal: 42.4s\tremaining: 58.8s\n",
      "419:\tlearn: 0.1060085\ttotal: 42.5s\tremaining: 58.7s\n",
      "420:\tlearn: 0.1059371\ttotal: 42.6s\tremaining: 58.6s\n",
      "421:\tlearn: 0.1058656\ttotal: 42.7s\tremaining: 58.4s\n",
      "422:\tlearn: 0.1058231\ttotal: 42.8s\tremaining: 58.3s\n",
      "423:\tlearn: 0.1057320\ttotal: 42.9s\tremaining: 58.2s\n",
      "424:\tlearn: 0.1056566\ttotal: 43s\tremaining: 58.1s\n",
      "425:\tlearn: 0.1055982\ttotal: 43.1s\tremaining: 58s\n",
      "426:\tlearn: 0.1055493\ttotal: 43.2s\tremaining: 57.9s\n",
      "427:\tlearn: 0.1055111\ttotal: 43.2s\tremaining: 57.8s\n",
      "428:\tlearn: 0.1054509\ttotal: 43.3s\tremaining: 57.7s\n",
      "429:\tlearn: 0.1053899\ttotal: 43.4s\tremaining: 57.6s\n",
      "430:\tlearn: 0.1053627\ttotal: 43.5s\tremaining: 57.5s\n",
      "431:\tlearn: 0.1052984\ttotal: 43.6s\tremaining: 57.4s\n",
      "432:\tlearn: 0.1052237\ttotal: 43.7s\tremaining: 57.3s\n",
      "433:\tlearn: 0.1051998\ttotal: 43.8s\tremaining: 57.1s\n",
      "434:\tlearn: 0.1051745\ttotal: 43.9s\tremaining: 57s\n",
      "435:\tlearn: 0.1050984\ttotal: 44s\tremaining: 56.9s\n",
      "436:\tlearn: 0.1050525\ttotal: 44.1s\tremaining: 56.8s\n",
      "437:\tlearn: 0.1050192\ttotal: 44.2s\tremaining: 56.7s\n",
      "438:\tlearn: 0.1049298\ttotal: 44.3s\tremaining: 56.6s\n",
      "439:\tlearn: 0.1048939\ttotal: 44.4s\tremaining: 56.5s\n",
      "440:\tlearn: 0.1048248\ttotal: 44.5s\tremaining: 56.4s\n",
      "441:\tlearn: 0.1047913\ttotal: 44.6s\tremaining: 56.3s\n",
      "442:\tlearn: 0.1047529\ttotal: 44.7s\tremaining: 56.2s\n",
      "443:\tlearn: 0.1046771\ttotal: 44.8s\tremaining: 56.1s\n",
      "444:\tlearn: 0.1046207\ttotal: 44.9s\tremaining: 56s\n",
      "445:\tlearn: 0.1044995\ttotal: 45s\tremaining: 55.9s\n",
      "446:\tlearn: 0.1044582\ttotal: 45.1s\tremaining: 55.8s\n",
      "447:\tlearn: 0.1044027\ttotal: 45.2s\tremaining: 55.7s\n",
      "448:\tlearn: 0.1043562\ttotal: 45.3s\tremaining: 55.5s\n",
      "449:\tlearn: 0.1042889\ttotal: 45.4s\tremaining: 55.5s\n",
      "450:\tlearn: 0.1042313\ttotal: 45.5s\tremaining: 55.4s\n",
      "451:\tlearn: 0.1041694\ttotal: 45.6s\tremaining: 55.3s\n",
      "452:\tlearn: 0.1040984\ttotal: 45.7s\tremaining: 55.2s\n",
      "453:\tlearn: 0.1040385\ttotal: 45.8s\tremaining: 55s\n",
      "454:\tlearn: 0.1039949\ttotal: 45.9s\tremaining: 54.9s\n",
      "455:\tlearn: 0.1038895\ttotal: 46s\tremaining: 54.8s\n",
      "456:\tlearn: 0.1038325\ttotal: 46.1s\tremaining: 54.7s\n",
      "457:\tlearn: 0.1037561\ttotal: 46.2s\tremaining: 54.6s\n",
      "458:\tlearn: 0.1037000\ttotal: 46.3s\tremaining: 54.5s\n",
      "459:\tlearn: 0.1036289\ttotal: 46.4s\tremaining: 54.5s\n",
      "460:\tlearn: 0.1035799\ttotal: 46.5s\tremaining: 54.4s\n",
      "461:\tlearn: 0.1035177\ttotal: 46.6s\tremaining: 54.3s\n",
      "462:\tlearn: 0.1034864\ttotal: 46.7s\tremaining: 54.2s\n",
      "463:\tlearn: 0.1034590\ttotal: 46.8s\tremaining: 54.1s\n",
      "464:\tlearn: 0.1033898\ttotal: 46.9s\tremaining: 54s\n",
      "465:\tlearn: 0.1033042\ttotal: 47s\tremaining: 53.9s\n",
      "466:\tlearn: 0.1032103\ttotal: 47.3s\tremaining: 53.9s\n",
      "467:\tlearn: 0.1031351\ttotal: 47.4s\tremaining: 53.9s\n",
      "468:\tlearn: 0.1030860\ttotal: 47.5s\tremaining: 53.8s\n",
      "469:\tlearn: 0.1030631\ttotal: 47.6s\tremaining: 53.7s\n",
      "470:\tlearn: 0.1030259\ttotal: 47.7s\tremaining: 53.6s\n",
      "471:\tlearn: 0.1029938\ttotal: 47.8s\tremaining: 53.5s\n",
      "472:\tlearn: 0.1028833\ttotal: 47.9s\tremaining: 53.4s\n",
      "473:\tlearn: 0.1028097\ttotal: 48s\tremaining: 53.2s\n",
      "474:\tlearn: 0.1027862\ttotal: 48.1s\tremaining: 53.1s\n",
      "475:\tlearn: 0.1027162\ttotal: 48.2s\tremaining: 53s\n",
      "476:\tlearn: 0.1026755\ttotal: 48.3s\tremaining: 52.9s\n",
      "477:\tlearn: 0.1026549\ttotal: 48.4s\tremaining: 52.8s\n",
      "478:\tlearn: 0.1026251\ttotal: 48.5s\tremaining: 52.7s\n",
      "479:\tlearn: 0.1025878\ttotal: 48.5s\tremaining: 52.6s\n",
      "480:\tlearn: 0.1025361\ttotal: 48.6s\tremaining: 52.5s\n",
      "481:\tlearn: 0.1024861\ttotal: 48.7s\tremaining: 52.4s\n",
      "482:\tlearn: 0.1024437\ttotal: 48.8s\tremaining: 52.3s\n",
      "483:\tlearn: 0.1024066\ttotal: 48.9s\tremaining: 52.2s\n",
      "484:\tlearn: 0.1023713\ttotal: 49s\tremaining: 52.1s\n",
      "485:\tlearn: 0.1023109\ttotal: 49.1s\tremaining: 51.9s\n",
      "486:\tlearn: 0.1022574\ttotal: 49.2s\tremaining: 51.8s\n",
      "487:\tlearn: 0.1022163\ttotal: 49.3s\tremaining: 51.7s\n",
      "488:\tlearn: 0.1021451\ttotal: 49.4s\tremaining: 51.6s\n",
      "489:\tlearn: 0.1020817\ttotal: 49.5s\tremaining: 51.5s\n",
      "490:\tlearn: 0.1020182\ttotal: 49.6s\tremaining: 51.4s\n",
      "491:\tlearn: 0.1019708\ttotal: 49.7s\tremaining: 51.3s\n",
      "492:\tlearn: 0.1019123\ttotal: 49.8s\tremaining: 51.2s\n",
      "493:\tlearn: 0.1018660\ttotal: 49.9s\tremaining: 51.1s\n",
      "494:\tlearn: 0.1017651\ttotal: 50s\tremaining: 51s\n",
      "495:\tlearn: 0.1017090\ttotal: 50.1s\tremaining: 50.9s\n",
      "496:\tlearn: 0.1016331\ttotal: 50.2s\tremaining: 50.8s\n",
      "497:\tlearn: 0.1015746\ttotal: 50.3s\tremaining: 50.7s\n",
      "498:\tlearn: 0.1014801\ttotal: 50.4s\tremaining: 50.6s\n",
      "499:\tlearn: 0.1014184\ttotal: 50.5s\tremaining: 50.5s\n",
      "500:\tlearn: 0.1013925\ttotal: 50.6s\tremaining: 50.4s\n",
      "501:\tlearn: 0.1013701\ttotal: 50.7s\tremaining: 50.3s\n",
      "502:\tlearn: 0.1012575\ttotal: 50.8s\tremaining: 50.2s\n",
      "503:\tlearn: 0.1011925\ttotal: 50.9s\tremaining: 50.1s\n",
      "504:\tlearn: 0.1011324\ttotal: 51s\tremaining: 50s\n",
      "505:\tlearn: 0.1010925\ttotal: 51.1s\tremaining: 49.9s\n",
      "506:\tlearn: 0.1010333\ttotal: 51.2s\tremaining: 49.7s\n",
      "507:\tlearn: 0.1009471\ttotal: 51.3s\tremaining: 49.6s\n",
      "508:\tlearn: 0.1008977\ttotal: 51.4s\tremaining: 49.5s\n",
      "509:\tlearn: 0.1008496\ttotal: 51.4s\tremaining: 49.4s\n",
      "510:\tlearn: 0.1007901\ttotal: 51.5s\tremaining: 49.3s\n",
      "511:\tlearn: 0.1007600\ttotal: 51.6s\tremaining: 49.2s\n",
      "512:\tlearn: 0.1007084\ttotal: 51.7s\tremaining: 49.1s\n",
      "513:\tlearn: 0.1006685\ttotal: 51.8s\tremaining: 49s\n",
      "514:\tlearn: 0.1006464\ttotal: 51.9s\tremaining: 48.9s\n",
      "515:\tlearn: 0.1005762\ttotal: 52s\tremaining: 48.8s\n",
      "516:\tlearn: 0.1005382\ttotal: 52.1s\tremaining: 48.7s\n",
      "517:\tlearn: 0.1004418\ttotal: 52.2s\tremaining: 48.6s\n",
      "518:\tlearn: 0.1004000\ttotal: 52.3s\tremaining: 48.5s\n",
      "519:\tlearn: 0.1003716\ttotal: 52.4s\tremaining: 48.4s\n",
      "520:\tlearn: 0.1003344\ttotal: 52.5s\tremaining: 48.3s\n",
      "521:\tlearn: 0.1002352\ttotal: 52.6s\tremaining: 48.2s\n",
      "522:\tlearn: 0.1001807\ttotal: 52.7s\tremaining: 48s\n",
      "523:\tlearn: 0.1001111\ttotal: 52.8s\tremaining: 47.9s\n",
      "524:\tlearn: 0.1000516\ttotal: 52.9s\tremaining: 47.8s\n",
      "525:\tlearn: 0.1000196\ttotal: 53s\tremaining: 47.7s\n",
      "526:\tlearn: 0.0999298\ttotal: 53.1s\tremaining: 47.6s\n",
      "527:\tlearn: 0.0998698\ttotal: 53.2s\tremaining: 47.5s\n",
      "528:\tlearn: 0.0998429\ttotal: 53.3s\tremaining: 47.4s\n",
      "529:\tlearn: 0.0998284\ttotal: 53.3s\tremaining: 47.3s\n",
      "530:\tlearn: 0.0997813\ttotal: 53.4s\tremaining: 47.2s\n",
      "531:\tlearn: 0.0997484\ttotal: 53.5s\tremaining: 47.1s\n",
      "532:\tlearn: 0.0997146\ttotal: 53.6s\tremaining: 47s\n",
      "533:\tlearn: 0.0996414\ttotal: 53.7s\tremaining: 46.9s\n",
      "534:\tlearn: 0.0995815\ttotal: 53.8s\tremaining: 46.8s\n",
      "535:\tlearn: 0.0995382\ttotal: 53.9s\tremaining: 46.7s\n",
      "536:\tlearn: 0.0994989\ttotal: 54s\tremaining: 46.6s\n",
      "537:\tlearn: 0.0994539\ttotal: 54.1s\tremaining: 46.5s\n",
      "538:\tlearn: 0.0994183\ttotal: 54.2s\tremaining: 46.3s\n",
      "539:\tlearn: 0.0993571\ttotal: 54.3s\tremaining: 46.2s\n",
      "540:\tlearn: 0.0993016\ttotal: 54.4s\tremaining: 46.1s\n",
      "541:\tlearn: 0.0992686\ttotal: 54.5s\tremaining: 46s\n",
      "542:\tlearn: 0.0992401\ttotal: 54.6s\tremaining: 45.9s\n",
      "543:\tlearn: 0.0992287\ttotal: 54.7s\tremaining: 45.8s\n",
      "544:\tlearn: 0.0991902\ttotal: 54.7s\tremaining: 45.7s\n",
      "545:\tlearn: 0.0991833\ttotal: 54.8s\tremaining: 45.6s\n",
      "546:\tlearn: 0.0991040\ttotal: 54.9s\tremaining: 45.5s\n",
      "547:\tlearn: 0.0990838\ttotal: 55s\tremaining: 45.4s\n",
      "548:\tlearn: 0.0990497\ttotal: 55.1s\tremaining: 45.3s\n",
      "549:\tlearn: 0.0989911\ttotal: 55.2s\tremaining: 45.2s\n",
      "550:\tlearn: 0.0989180\ttotal: 55.3s\tremaining: 45.1s\n",
      "551:\tlearn: 0.0988747\ttotal: 55.4s\tremaining: 45s\n",
      "552:\tlearn: 0.0988080\ttotal: 55.5s\tremaining: 44.9s\n",
      "553:\tlearn: 0.0987348\ttotal: 55.6s\tremaining: 44.8s\n",
      "554:\tlearn: 0.0986941\ttotal: 55.7s\tremaining: 44.7s\n",
      "555:\tlearn: 0.0986555\ttotal: 55.8s\tremaining: 44.5s\n",
      "556:\tlearn: 0.0985957\ttotal: 55.9s\tremaining: 44.4s\n",
      "557:\tlearn: 0.0985564\ttotal: 56s\tremaining: 44.3s\n",
      "558:\tlearn: 0.0984865\ttotal: 56.1s\tremaining: 44.2s\n",
      "559:\tlearn: 0.0984425\ttotal: 56.2s\tremaining: 44.1s\n",
      "560:\tlearn: 0.0983452\ttotal: 56.3s\tremaining: 44s\n",
      "561:\tlearn: 0.0983170\ttotal: 56.4s\tremaining: 43.9s\n",
      "562:\tlearn: 0.0982325\ttotal: 56.5s\tremaining: 43.8s\n",
      "563:\tlearn: 0.0981875\ttotal: 56.6s\tremaining: 43.7s\n",
      "564:\tlearn: 0.0981615\ttotal: 56.7s\tremaining: 43.6s\n",
      "565:\tlearn: 0.0981176\ttotal: 56.8s\tremaining: 43.5s\n",
      "566:\tlearn: 0.0980736\ttotal: 56.9s\tremaining: 43.4s\n",
      "567:\tlearn: 0.0980360\ttotal: 57s\tremaining: 43.3s\n",
      "568:\tlearn: 0.0979939\ttotal: 57.1s\tremaining: 43.2s\n",
      "569:\tlearn: 0.0979679\ttotal: 57.2s\tremaining: 43.1s\n",
      "570:\tlearn: 0.0978865\ttotal: 57.3s\tremaining: 43s\n",
      "571:\tlearn: 0.0978285\ttotal: 57.4s\tremaining: 42.9s\n",
      "572:\tlearn: 0.0977676\ttotal: 57.5s\tremaining: 42.8s\n",
      "573:\tlearn: 0.0977309\ttotal: 57.6s\tremaining: 42.7s\n",
      "574:\tlearn: 0.0977017\ttotal: 57.8s\tremaining: 42.7s\n",
      "575:\tlearn: 0.0976539\ttotal: 57.9s\tremaining: 42.6s\n",
      "576:\tlearn: 0.0976037\ttotal: 58s\tremaining: 42.5s\n",
      "577:\tlearn: 0.0975717\ttotal: 58.1s\tremaining: 42.4s\n",
      "578:\tlearn: 0.0975332\ttotal: 58.1s\tremaining: 42.3s\n",
      "579:\tlearn: 0.0974218\ttotal: 58.2s\tremaining: 42.2s\n",
      "580:\tlearn: 0.0973910\ttotal: 58.3s\tremaining: 42.1s\n",
      "581:\tlearn: 0.0973398\ttotal: 58.4s\tremaining: 42s\n",
      "582:\tlearn: 0.0972992\ttotal: 58.5s\tremaining: 41.9s\n",
      "583:\tlearn: 0.0972528\ttotal: 58.6s\tremaining: 41.8s\n",
      "584:\tlearn: 0.0971898\ttotal: 58.7s\tremaining: 41.6s\n",
      "585:\tlearn: 0.0971315\ttotal: 58.8s\tremaining: 41.5s\n",
      "586:\tlearn: 0.0970943\ttotal: 58.9s\tremaining: 41.4s\n",
      "587:\tlearn: 0.0970660\ttotal: 59s\tremaining: 41.3s\n",
      "588:\tlearn: 0.0969891\ttotal: 59.1s\tremaining: 41.2s\n",
      "589:\tlearn: 0.0969551\ttotal: 59.2s\tremaining: 41.1s\n",
      "590:\tlearn: 0.0968932\ttotal: 59.3s\tremaining: 41s\n",
      "591:\tlearn: 0.0968445\ttotal: 59.4s\tremaining: 40.9s\n",
      "592:\tlearn: 0.0967952\ttotal: 59.5s\tremaining: 40.8s\n",
      "593:\tlearn: 0.0967010\ttotal: 59.6s\tremaining: 40.7s\n",
      "594:\tlearn: 0.0966325\ttotal: 59.7s\tremaining: 40.6s\n",
      "595:\tlearn: 0.0965786\ttotal: 59.8s\tremaining: 40.5s\n",
      "596:\tlearn: 0.0965195\ttotal: 59.8s\tremaining: 40.4s\n",
      "597:\tlearn: 0.0964636\ttotal: 59.9s\tremaining: 40.3s\n",
      "598:\tlearn: 0.0964237\ttotal: 1m\tremaining: 40.2s\n",
      "599:\tlearn: 0.0963890\ttotal: 1m\tremaining: 40.1s\n",
      "600:\tlearn: 0.0963449\ttotal: 1m\tremaining: 40s\n",
      "601:\tlearn: 0.0962687\ttotal: 1m\tremaining: 39.9s\n",
      "602:\tlearn: 0.0962201\ttotal: 1m\tremaining: 39.8s\n",
      "603:\tlearn: 0.0961486\ttotal: 1m\tremaining: 39.7s\n",
      "604:\tlearn: 0.0960690\ttotal: 1m\tremaining: 39.6s\n",
      "605:\tlearn: 0.0960303\ttotal: 1m\tremaining: 39.5s\n",
      "606:\tlearn: 0.0959689\ttotal: 1m\tremaining: 39.4s\n",
      "607:\tlearn: 0.0959449\ttotal: 1m\tremaining: 39.3s\n",
      "608:\tlearn: 0.0958971\ttotal: 1m\tremaining: 39.2s\n",
      "609:\tlearn: 0.0958782\ttotal: 1m 1s\tremaining: 39s\n",
      "610:\tlearn: 0.0958490\ttotal: 1m 1s\tremaining: 38.9s\n",
      "611:\tlearn: 0.0958011\ttotal: 1m 1s\tremaining: 38.8s\n",
      "612:\tlearn: 0.0957782\ttotal: 1m 1s\tremaining: 38.7s\n",
      "613:\tlearn: 0.0957257\ttotal: 1m 1s\tremaining: 38.6s\n",
      "614:\tlearn: 0.0956810\ttotal: 1m 1s\tremaining: 38.5s\n",
      "615:\tlearn: 0.0956620\ttotal: 1m 1s\tremaining: 38.4s\n",
      "616:\tlearn: 0.0955953\ttotal: 1m 1s\tremaining: 38.3s\n",
      "617:\tlearn: 0.0955726\ttotal: 1m 1s\tremaining: 38.2s\n",
      "618:\tlearn: 0.0955168\ttotal: 1m 1s\tremaining: 38.1s\n",
      "619:\tlearn: 0.0954812\ttotal: 1m 2s\tremaining: 38s\n",
      "620:\tlearn: 0.0954486\ttotal: 1m 2s\tremaining: 37.9s\n",
      "621:\tlearn: 0.0954225\ttotal: 1m 2s\tremaining: 37.8s\n",
      "622:\tlearn: 0.0953873\ttotal: 1m 2s\tremaining: 37.7s\n",
      "623:\tlearn: 0.0953148\ttotal: 1m 2s\tremaining: 37.6s\n",
      "624:\tlearn: 0.0952666\ttotal: 1m 2s\tremaining: 37.5s\n",
      "625:\tlearn: 0.0952421\ttotal: 1m 2s\tremaining: 37.4s\n",
      "626:\tlearn: 0.0952331\ttotal: 1m 2s\tremaining: 37.3s\n",
      "627:\tlearn: 0.0951834\ttotal: 1m 2s\tremaining: 37.2s\n",
      "628:\tlearn: 0.0951540\ttotal: 1m 2s\tremaining: 37.1s\n",
      "629:\tlearn: 0.0951020\ttotal: 1m 3s\tremaining: 37s\n",
      "630:\tlearn: 0.0950829\ttotal: 1m 3s\tremaining: 36.9s\n",
      "631:\tlearn: 0.0950474\ttotal: 1m 3s\tremaining: 36.8s\n",
      "632:\tlearn: 0.0949808\ttotal: 1m 3s\tremaining: 36.7s\n",
      "633:\tlearn: 0.0949523\ttotal: 1m 3s\tremaining: 36.6s\n",
      "634:\tlearn: 0.0948882\ttotal: 1m 3s\tremaining: 36.5s\n",
      "635:\tlearn: 0.0948299\ttotal: 1m 3s\tremaining: 36.4s\n",
      "636:\tlearn: 0.0947895\ttotal: 1m 3s\tremaining: 36.3s\n",
      "637:\tlearn: 0.0947588\ttotal: 1m 3s\tremaining: 36.2s\n",
      "638:\tlearn: 0.0947142\ttotal: 1m 3s\tremaining: 36.1s\n",
      "639:\tlearn: 0.0946905\ttotal: 1m 3s\tremaining: 36s\n",
      "640:\tlearn: 0.0946365\ttotal: 1m 4s\tremaining: 35.9s\n",
      "641:\tlearn: 0.0945863\ttotal: 1m 4s\tremaining: 35.8s\n",
      "642:\tlearn: 0.0945598\ttotal: 1m 4s\tremaining: 35.7s\n",
      "643:\tlearn: 0.0945232\ttotal: 1m 4s\tremaining: 35.6s\n",
      "644:\tlearn: 0.0944666\ttotal: 1m 4s\tremaining: 35.5s\n",
      "645:\tlearn: 0.0944137\ttotal: 1m 4s\tremaining: 35.4s\n",
      "646:\tlearn: 0.0943834\ttotal: 1m 4s\tremaining: 35.3s\n",
      "647:\tlearn: 0.0943440\ttotal: 1m 4s\tremaining: 35.2s\n",
      "648:\tlearn: 0.0943027\ttotal: 1m 4s\tremaining: 35s\n",
      "649:\tlearn: 0.0942741\ttotal: 1m 4s\tremaining: 34.9s\n",
      "650:\tlearn: 0.0942298\ttotal: 1m 4s\tremaining: 34.8s\n",
      "651:\tlearn: 0.0941972\ttotal: 1m 5s\tremaining: 34.7s\n",
      "652:\tlearn: 0.0941660\ttotal: 1m 5s\tremaining: 34.6s\n",
      "653:\tlearn: 0.0940963\ttotal: 1m 5s\tremaining: 34.5s\n",
      "654:\tlearn: 0.0940655\ttotal: 1m 5s\tremaining: 34.4s\n",
      "655:\tlearn: 0.0940313\ttotal: 1m 5s\tremaining: 34.3s\n",
      "656:\tlearn: 0.0939815\ttotal: 1m 5s\tremaining: 34.2s\n",
      "657:\tlearn: 0.0939295\ttotal: 1m 5s\tremaining: 34.1s\n",
      "658:\tlearn: 0.0938751\ttotal: 1m 5s\tremaining: 34s\n",
      "659:\tlearn: 0.0938327\ttotal: 1m 5s\tremaining: 33.9s\n",
      "660:\tlearn: 0.0937807\ttotal: 1m 5s\tremaining: 33.8s\n",
      "661:\tlearn: 0.0937565\ttotal: 1m 6s\tremaining: 33.7s\n",
      "662:\tlearn: 0.0936789\ttotal: 1m 6s\tremaining: 33.6s\n",
      "663:\tlearn: 0.0936515\ttotal: 1m 6s\tremaining: 33.5s\n",
      "664:\tlearn: 0.0936297\ttotal: 1m 6s\tremaining: 33.4s\n",
      "665:\tlearn: 0.0935856\ttotal: 1m 6s\tremaining: 33.3s\n",
      "666:\tlearn: 0.0935616\ttotal: 1m 6s\tremaining: 33.2s\n",
      "667:\tlearn: 0.0934966\ttotal: 1m 6s\tremaining: 33.1s\n",
      "668:\tlearn: 0.0934587\ttotal: 1m 6s\tremaining: 33s\n",
      "669:\tlearn: 0.0934319\ttotal: 1m 6s\tremaining: 32.9s\n",
      "670:\tlearn: 0.0933700\ttotal: 1m 6s\tremaining: 32.8s\n",
      "671:\tlearn: 0.0932815\ttotal: 1m 7s\tremaining: 32.7s\n",
      "672:\tlearn: 0.0932319\ttotal: 1m 7s\tremaining: 32.6s\n",
      "673:\tlearn: 0.0931783\ttotal: 1m 7s\tremaining: 32.5s\n",
      "674:\tlearn: 0.0931365\ttotal: 1m 7s\tremaining: 32.4s\n",
      "675:\tlearn: 0.0930814\ttotal: 1m 7s\tremaining: 32.3s\n",
      "676:\tlearn: 0.0930375\ttotal: 1m 7s\tremaining: 32.2s\n",
      "677:\tlearn: 0.0929837\ttotal: 1m 7s\tremaining: 32.1s\n",
      "678:\tlearn: 0.0929215\ttotal: 1m 7s\tremaining: 32s\n",
      "679:\tlearn: 0.0928851\ttotal: 1m 7s\tremaining: 31.9s\n",
      "680:\tlearn: 0.0928553\ttotal: 1m 7s\tremaining: 31.8s\n",
      "681:\tlearn: 0.0927671\ttotal: 1m 7s\tremaining: 31.7s\n",
      "682:\tlearn: 0.0927436\ttotal: 1m 8s\tremaining: 31.6s\n",
      "683:\tlearn: 0.0927022\ttotal: 1m 8s\tremaining: 31.5s\n",
      "684:\tlearn: 0.0926832\ttotal: 1m 8s\tremaining: 31.4s\n",
      "685:\tlearn: 0.0926534\ttotal: 1m 8s\tremaining: 31.3s\n",
      "686:\tlearn: 0.0926307\ttotal: 1m 8s\tremaining: 31.2s\n",
      "687:\tlearn: 0.0926029\ttotal: 1m 8s\tremaining: 31.1s\n",
      "688:\tlearn: 0.0925776\ttotal: 1m 8s\tremaining: 31s\n",
      "689:\tlearn: 0.0925347\ttotal: 1m 8s\tremaining: 30.9s\n",
      "690:\tlearn: 0.0924938\ttotal: 1m 8s\tremaining: 30.8s\n",
      "691:\tlearn: 0.0924626\ttotal: 1m 9s\tremaining: 30.7s\n",
      "692:\tlearn: 0.0924184\ttotal: 1m 9s\tremaining: 30.6s\n",
      "693:\tlearn: 0.0923711\ttotal: 1m 9s\tremaining: 30.5s\n",
      "694:\tlearn: 0.0923355\ttotal: 1m 9s\tremaining: 30.4s\n",
      "695:\tlearn: 0.0923022\ttotal: 1m 9s\tremaining: 30.3s\n",
      "696:\tlearn: 0.0922252\ttotal: 1m 9s\tremaining: 30.2s\n",
      "697:\tlearn: 0.0921896\ttotal: 1m 9s\tremaining: 30.1s\n",
      "698:\tlearn: 0.0921362\ttotal: 1m 9s\tremaining: 30s\n",
      "699:\tlearn: 0.0920932\ttotal: 1m 9s\tremaining: 29.9s\n",
      "700:\tlearn: 0.0920000\ttotal: 1m 9s\tremaining: 29.8s\n",
      "701:\tlearn: 0.0919572\ttotal: 1m 9s\tremaining: 29.7s\n",
      "702:\tlearn: 0.0918968\ttotal: 1m 10s\tremaining: 29.6s\n",
      "703:\tlearn: 0.0918060\ttotal: 1m 10s\tremaining: 29.5s\n",
      "704:\tlearn: 0.0917554\ttotal: 1m 10s\tremaining: 29.4s\n",
      "705:\tlearn: 0.0917313\ttotal: 1m 10s\tremaining: 29.3s\n",
      "706:\tlearn: 0.0917065\ttotal: 1m 10s\tremaining: 29.2s\n",
      "707:\tlearn: 0.0916832\ttotal: 1m 10s\tremaining: 29.1s\n",
      "708:\tlearn: 0.0916284\ttotal: 1m 10s\tremaining: 29s\n",
      "709:\tlearn: 0.0915722\ttotal: 1m 10s\tremaining: 28.9s\n",
      "710:\tlearn: 0.0915156\ttotal: 1m 10s\tremaining: 28.8s\n",
      "711:\tlearn: 0.0914619\ttotal: 1m 10s\tremaining: 28.7s\n",
      "712:\tlearn: 0.0914208\ttotal: 1m 11s\tremaining: 28.6s\n",
      "713:\tlearn: 0.0913642\ttotal: 1m 11s\tremaining: 28.5s\n",
      "714:\tlearn: 0.0913174\ttotal: 1m 11s\tremaining: 28.4s\n",
      "715:\tlearn: 0.0912747\ttotal: 1m 11s\tremaining: 28.3s\n",
      "716:\tlearn: 0.0912602\ttotal: 1m 11s\tremaining: 28.2s\n",
      "717:\tlearn: 0.0911684\ttotal: 1m 11s\tremaining: 28.1s\n",
      "718:\tlearn: 0.0911295\ttotal: 1m 11s\tremaining: 28s\n",
      "719:\tlearn: 0.0910830\ttotal: 1m 11s\tremaining: 27.9s\n",
      "720:\tlearn: 0.0910508\ttotal: 1m 11s\tremaining: 27.8s\n",
      "721:\tlearn: 0.0909968\ttotal: 1m 11s\tremaining: 27.7s\n",
      "722:\tlearn: 0.0909582\ttotal: 1m 11s\tremaining: 27.6s\n",
      "723:\tlearn: 0.0908864\ttotal: 1m 12s\tremaining: 27.5s\n",
      "724:\tlearn: 0.0908421\ttotal: 1m 12s\tremaining: 27.4s\n",
      "725:\tlearn: 0.0908037\ttotal: 1m 12s\tremaining: 27.3s\n",
      "726:\tlearn: 0.0907607\ttotal: 1m 12s\tremaining: 27.2s\n",
      "727:\tlearn: 0.0907371\ttotal: 1m 12s\tremaining: 27.1s\n",
      "728:\tlearn: 0.0907252\ttotal: 1m 12s\tremaining: 27s\n",
      "729:\tlearn: 0.0906981\ttotal: 1m 12s\tremaining: 26.9s\n",
      "730:\tlearn: 0.0906652\ttotal: 1m 12s\tremaining: 26.8s\n",
      "731:\tlearn: 0.0906268\ttotal: 1m 12s\tremaining: 26.7s\n",
      "732:\tlearn: 0.0906216\ttotal: 1m 12s\tremaining: 26.6s\n",
      "733:\tlearn: 0.0905835\ttotal: 1m 13s\tremaining: 26.5s\n",
      "734:\tlearn: 0.0905330\ttotal: 1m 13s\tremaining: 26.4s\n",
      "735:\tlearn: 0.0904948\ttotal: 1m 13s\tremaining: 26.3s\n",
      "736:\tlearn: 0.0904500\ttotal: 1m 13s\tremaining: 26.2s\n",
      "737:\tlearn: 0.0904026\ttotal: 1m 13s\tremaining: 26.1s\n",
      "738:\tlearn: 0.0903596\ttotal: 1m 13s\tremaining: 26s\n",
      "739:\tlearn: 0.0902523\ttotal: 1m 13s\tremaining: 25.9s\n",
      "740:\tlearn: 0.0902030\ttotal: 1m 13s\tremaining: 25.8s\n",
      "741:\tlearn: 0.0901312\ttotal: 1m 13s\tremaining: 25.7s\n",
      "742:\tlearn: 0.0900960\ttotal: 1m 13s\tremaining: 25.6s\n",
      "743:\tlearn: 0.0900860\ttotal: 1m 14s\tremaining: 25.5s\n",
      "744:\tlearn: 0.0900743\ttotal: 1m 14s\tremaining: 25.4s\n",
      "745:\tlearn: 0.0900147\ttotal: 1m 14s\tremaining: 25.3s\n",
      "746:\tlearn: 0.0899773\ttotal: 1m 14s\tremaining: 25.2s\n",
      "747:\tlearn: 0.0899682\ttotal: 1m 14s\tremaining: 25.1s\n",
      "748:\tlearn: 0.0899191\ttotal: 1m 14s\tremaining: 25s\n",
      "749:\tlearn: 0.0898746\ttotal: 1m 14s\tremaining: 24.9s\n",
      "750:\tlearn: 0.0898332\ttotal: 1m 14s\tremaining: 24.8s\n",
      "751:\tlearn: 0.0897862\ttotal: 1m 14s\tremaining: 24.7s\n",
      "752:\tlearn: 0.0897064\ttotal: 1m 14s\tremaining: 24.6s\n",
      "753:\tlearn: 0.0896686\ttotal: 1m 14s\tremaining: 24.5s\n",
      "754:\tlearn: 0.0896608\ttotal: 1m 15s\tremaining: 24.4s\n",
      "755:\tlearn: 0.0896340\ttotal: 1m 15s\tremaining: 24.3s\n",
      "756:\tlearn: 0.0895661\ttotal: 1m 15s\tremaining: 24.2s\n",
      "757:\tlearn: 0.0895314\ttotal: 1m 15s\tremaining: 24.1s\n",
      "758:\tlearn: 0.0895031\ttotal: 1m 15s\tremaining: 24s\n",
      "759:\tlearn: 0.0894616\ttotal: 1m 15s\tremaining: 23.9s\n",
      "760:\tlearn: 0.0894335\ttotal: 1m 15s\tremaining: 23.7s\n",
      "761:\tlearn: 0.0893986\ttotal: 1m 15s\tremaining: 23.6s\n",
      "762:\tlearn: 0.0893753\ttotal: 1m 15s\tremaining: 23.5s\n",
      "763:\tlearn: 0.0893266\ttotal: 1m 15s\tremaining: 23.4s\n",
      "764:\tlearn: 0.0892692\ttotal: 1m 16s\tremaining: 23.3s\n",
      "765:\tlearn: 0.0892262\ttotal: 1m 16s\tremaining: 23.2s\n",
      "766:\tlearn: 0.0891771\ttotal: 1m 16s\tremaining: 23.1s\n",
      "767:\tlearn: 0.0891509\ttotal: 1m 16s\tremaining: 23s\n",
      "768:\tlearn: 0.0891070\ttotal: 1m 16s\tremaining: 22.9s\n",
      "769:\tlearn: 0.0890614\ttotal: 1m 16s\tremaining: 22.8s\n",
      "770:\tlearn: 0.0890159\ttotal: 1m 16s\tremaining: 22.7s\n",
      "771:\tlearn: 0.0889898\ttotal: 1m 16s\tremaining: 22.6s\n",
      "772:\tlearn: 0.0889361\ttotal: 1m 16s\tremaining: 22.5s\n",
      "773:\tlearn: 0.0889054\ttotal: 1m 16s\tremaining: 22.4s\n",
      "774:\tlearn: 0.0888588\ttotal: 1m 16s\tremaining: 22.3s\n",
      "775:\tlearn: 0.0887957\ttotal: 1m 17s\tremaining: 22.2s\n",
      "776:\tlearn: 0.0887673\ttotal: 1m 17s\tremaining: 22.1s\n",
      "777:\tlearn: 0.0886998\ttotal: 1m 17s\tremaining: 22s\n",
      "778:\tlearn: 0.0886917\ttotal: 1m 17s\tremaining: 21.9s\n",
      "779:\tlearn: 0.0886541\ttotal: 1m 17s\tremaining: 21.8s\n",
      "780:\tlearn: 0.0886037\ttotal: 1m 17s\tremaining: 21.7s\n",
      "781:\tlearn: 0.0885834\ttotal: 1m 17s\tremaining: 21.6s\n",
      "782:\tlearn: 0.0884994\ttotal: 1m 17s\tremaining: 21.5s\n",
      "783:\tlearn: 0.0884850\ttotal: 1m 17s\tremaining: 21.4s\n",
      "784:\tlearn: 0.0884275\ttotal: 1m 17s\tremaining: 21.3s\n",
      "785:\tlearn: 0.0883301\ttotal: 1m 18s\tremaining: 21.2s\n",
      "786:\tlearn: 0.0882605\ttotal: 1m 18s\tremaining: 21.1s\n",
      "787:\tlearn: 0.0882126\ttotal: 1m 18s\tremaining: 21s\n",
      "788:\tlearn: 0.0881540\ttotal: 1m 18s\tremaining: 20.9s\n",
      "789:\tlearn: 0.0881212\ttotal: 1m 18s\tremaining: 20.8s\n",
      "790:\tlearn: 0.0880761\ttotal: 1m 18s\tremaining: 20.7s\n",
      "791:\tlearn: 0.0880448\ttotal: 1m 18s\tremaining: 20.6s\n",
      "792:\tlearn: 0.0880144\ttotal: 1m 18s\tremaining: 20.5s\n",
      "793:\tlearn: 0.0879583\ttotal: 1m 18s\tremaining: 20.4s\n",
      "794:\tlearn: 0.0879391\ttotal: 1m 18s\tremaining: 20.4s\n",
      "795:\tlearn: 0.0879141\ttotal: 1m 19s\tremaining: 20.3s\n",
      "796:\tlearn: 0.0879110\ttotal: 1m 19s\tremaining: 20.2s\n",
      "797:\tlearn: 0.0878602\ttotal: 1m 19s\tremaining: 20.1s\n",
      "798:\tlearn: 0.0878374\ttotal: 1m 19s\tremaining: 20s\n",
      "799:\tlearn: 0.0878128\ttotal: 1m 19s\tremaining: 19.9s\n",
      "800:\tlearn: 0.0877718\ttotal: 1m 19s\tremaining: 19.8s\n",
      "801:\tlearn: 0.0877463\ttotal: 1m 19s\tremaining: 19.7s\n",
      "802:\tlearn: 0.0877013\ttotal: 1m 19s\tremaining: 19.6s\n",
      "803:\tlearn: 0.0876619\ttotal: 1m 19s\tremaining: 19.5s\n",
      "804:\tlearn: 0.0876314\ttotal: 1m 19s\tremaining: 19.4s\n",
      "805:\tlearn: 0.0875443\ttotal: 1m 20s\tremaining: 19.3s\n",
      "806:\tlearn: 0.0875146\ttotal: 1m 20s\tremaining: 19.2s\n",
      "807:\tlearn: 0.0874864\ttotal: 1m 20s\tremaining: 19.1s\n",
      "808:\tlearn: 0.0874413\ttotal: 1m 20s\tremaining: 19s\n",
      "809:\tlearn: 0.0873945\ttotal: 1m 20s\tremaining: 18.9s\n",
      "810:\tlearn: 0.0873923\ttotal: 1m 20s\tremaining: 18.8s\n",
      "811:\tlearn: 0.0873713\ttotal: 1m 20s\tremaining: 18.7s\n",
      "812:\tlearn: 0.0873467\ttotal: 1m 20s\tremaining: 18.6s\n",
      "813:\tlearn: 0.0872789\ttotal: 1m 20s\tremaining: 18.5s\n",
      "814:\tlearn: 0.0872787\ttotal: 1m 20s\tremaining: 18.4s\n",
      "815:\tlearn: 0.0872384\ttotal: 1m 20s\tremaining: 18.3s\n",
      "816:\tlearn: 0.0871977\ttotal: 1m 21s\tremaining: 18.2s\n",
      "817:\tlearn: 0.0871808\ttotal: 1m 21s\tremaining: 18.1s\n",
      "818:\tlearn: 0.0871504\ttotal: 1m 21s\tremaining: 18s\n",
      "819:\tlearn: 0.0871009\ttotal: 1m 21s\tremaining: 17.9s\n",
      "820:\tlearn: 0.0870712\ttotal: 1m 21s\tremaining: 17.8s\n",
      "821:\tlearn: 0.0870203\ttotal: 1m 21s\tremaining: 17.7s\n",
      "822:\tlearn: 0.0869722\ttotal: 1m 21s\tremaining: 17.6s\n",
      "823:\tlearn: 0.0869267\ttotal: 1m 21s\tremaining: 17.5s\n",
      "824:\tlearn: 0.0868910\ttotal: 1m 21s\tremaining: 17.4s\n",
      "825:\tlearn: 0.0868703\ttotal: 1m 21s\tremaining: 17.3s\n",
      "826:\tlearn: 0.0868395\ttotal: 1m 22s\tremaining: 17.2s\n",
      "827:\tlearn: 0.0867417\ttotal: 1m 22s\tremaining: 17.1s\n",
      "828:\tlearn: 0.0866837\ttotal: 1m 22s\tremaining: 17s\n",
      "829:\tlearn: 0.0866497\ttotal: 1m 22s\tremaining: 16.9s\n",
      "830:\tlearn: 0.0865959\ttotal: 1m 22s\tremaining: 16.8s\n",
      "831:\tlearn: 0.0865462\ttotal: 1m 22s\tremaining: 16.7s\n",
      "832:\tlearn: 0.0865130\ttotal: 1m 22s\tremaining: 16.6s\n",
      "833:\tlearn: 0.0865052\ttotal: 1m 22s\tremaining: 16.5s\n",
      "834:\tlearn: 0.0864572\ttotal: 1m 22s\tremaining: 16.4s\n",
      "835:\tlearn: 0.0864219\ttotal: 1m 22s\tremaining: 16.3s\n",
      "836:\tlearn: 0.0863532\ttotal: 1m 22s\tremaining: 16.2s\n",
      "837:\tlearn: 0.0863174\ttotal: 1m 23s\tremaining: 16.1s\n",
      "838:\tlearn: 0.0862952\ttotal: 1m 23s\tremaining: 16s\n",
      "839:\tlearn: 0.0862500\ttotal: 1m 23s\tremaining: 15.9s\n",
      "840:\tlearn: 0.0862233\ttotal: 1m 23s\tremaining: 15.8s\n",
      "841:\tlearn: 0.0861940\ttotal: 1m 23s\tremaining: 15.7s\n",
      "842:\tlearn: 0.0861544\ttotal: 1m 23s\tremaining: 15.6s\n",
      "843:\tlearn: 0.0860868\ttotal: 1m 23s\tremaining: 15.5s\n",
      "844:\tlearn: 0.0860611\ttotal: 1m 23s\tremaining: 15.4s\n",
      "845:\tlearn: 0.0860379\ttotal: 1m 23s\tremaining: 15.3s\n",
      "846:\tlearn: 0.0859970\ttotal: 1m 23s\tremaining: 15.2s\n",
      "847:\tlearn: 0.0859537\ttotal: 1m 24s\tremaining: 15.1s\n",
      "848:\tlearn: 0.0859151\ttotal: 1m 24s\tremaining: 15s\n",
      "849:\tlearn: 0.0858897\ttotal: 1m 24s\tremaining: 14.9s\n",
      "850:\tlearn: 0.0858701\ttotal: 1m 24s\tremaining: 14.8s\n",
      "851:\tlearn: 0.0858469\ttotal: 1m 24s\tremaining: 14.7s\n",
      "852:\tlearn: 0.0858056\ttotal: 1m 24s\tremaining: 14.6s\n",
      "853:\tlearn: 0.0857782\ttotal: 1m 24s\tremaining: 14.5s\n",
      "854:\tlearn: 0.0857331\ttotal: 1m 24s\tremaining: 14.4s\n",
      "855:\tlearn: 0.0857009\ttotal: 1m 24s\tremaining: 14.3s\n",
      "856:\tlearn: 0.0856595\ttotal: 1m 24s\tremaining: 14.2s\n",
      "857:\tlearn: 0.0856253\ttotal: 1m 25s\tremaining: 14.1s\n",
      "858:\tlearn: 0.0855916\ttotal: 1m 25s\tremaining: 14s\n",
      "859:\tlearn: 0.0855730\ttotal: 1m 25s\tremaining: 13.9s\n",
      "860:\tlearn: 0.0855232\ttotal: 1m 25s\tremaining: 13.8s\n",
      "861:\tlearn: 0.0854990\ttotal: 1m 25s\tremaining: 13.7s\n",
      "862:\tlearn: 0.0854636\ttotal: 1m 25s\tremaining: 13.6s\n",
      "863:\tlearn: 0.0854052\ttotal: 1m 25s\tremaining: 13.5s\n",
      "864:\tlearn: 0.0853828\ttotal: 1m 25s\tremaining: 13.4s\n",
      "865:\tlearn: 0.0853762\ttotal: 1m 25s\tremaining: 13.3s\n",
      "866:\tlearn: 0.0853537\ttotal: 1m 25s\tremaining: 13.2s\n",
      "867:\tlearn: 0.0853245\ttotal: 1m 25s\tremaining: 13.1s\n",
      "868:\tlearn: 0.0852981\ttotal: 1m 26s\tremaining: 13s\n",
      "869:\tlearn: 0.0852670\ttotal: 1m 26s\tremaining: 12.9s\n",
      "870:\tlearn: 0.0852255\ttotal: 1m 26s\tremaining: 12.8s\n",
      "871:\tlearn: 0.0852240\ttotal: 1m 26s\tremaining: 12.7s\n",
      "872:\tlearn: 0.0851882\ttotal: 1m 26s\tremaining: 12.6s\n",
      "873:\tlearn: 0.0851758\ttotal: 1m 26s\tremaining: 12.5s\n",
      "874:\tlearn: 0.0851356\ttotal: 1m 26s\tremaining: 12.4s\n",
      "875:\tlearn: 0.0851100\ttotal: 1m 26s\tremaining: 12.3s\n",
      "876:\tlearn: 0.0850813\ttotal: 1m 26s\tremaining: 12.2s\n",
      "877:\tlearn: 0.0850431\ttotal: 1m 26s\tremaining: 12.1s\n",
      "878:\tlearn: 0.0850004\ttotal: 1m 26s\tremaining: 12s\n",
      "879:\tlearn: 0.0849878\ttotal: 1m 27s\tremaining: 11.9s\n",
      "880:\tlearn: 0.0849661\ttotal: 1m 27s\tremaining: 11.8s\n",
      "881:\tlearn: 0.0849547\ttotal: 1m 27s\tremaining: 11.7s\n",
      "882:\tlearn: 0.0848921\ttotal: 1m 27s\tremaining: 11.6s\n",
      "883:\tlearn: 0.0848796\ttotal: 1m 27s\tremaining: 11.5s\n",
      "884:\tlearn: 0.0848440\ttotal: 1m 27s\tremaining: 11.4s\n",
      "885:\tlearn: 0.0848326\ttotal: 1m 27s\tremaining: 11.3s\n",
      "886:\tlearn: 0.0847918\ttotal: 1m 27s\tremaining: 11.2s\n",
      "887:\tlearn: 0.0847584\ttotal: 1m 27s\tremaining: 11.1s\n",
      "888:\tlearn: 0.0847281\ttotal: 1m 28s\tremaining: 11s\n",
      "889:\tlearn: 0.0847005\ttotal: 1m 28s\tremaining: 10.9s\n",
      "890:\tlearn: 0.0846560\ttotal: 1m 28s\tremaining: 10.8s\n",
      "891:\tlearn: 0.0846327\ttotal: 1m 28s\tremaining: 10.7s\n",
      "892:\tlearn: 0.0845780\ttotal: 1m 28s\tremaining: 10.6s\n",
      "893:\tlearn: 0.0845601\ttotal: 1m 28s\tremaining: 10.5s\n",
      "894:\tlearn: 0.0845403\ttotal: 1m 28s\tremaining: 10.4s\n",
      "895:\tlearn: 0.0845190\ttotal: 1m 28s\tremaining: 10.3s\n",
      "896:\tlearn: 0.0844936\ttotal: 1m 28s\tremaining: 10.2s\n",
      "897:\tlearn: 0.0844691\ttotal: 1m 28s\tremaining: 10.1s\n",
      "898:\tlearn: 0.0844366\ttotal: 1m 28s\tremaining: 10s\n",
      "899:\tlearn: 0.0844279\ttotal: 1m 29s\tremaining: 9.9s\n",
      "900:\tlearn: 0.0844003\ttotal: 1m 29s\tremaining: 9.8s\n",
      "901:\tlearn: 0.0843755\ttotal: 1m 29s\tremaining: 9.7s\n",
      "902:\tlearn: 0.0843432\ttotal: 1m 29s\tremaining: 9.6s\n",
      "903:\tlearn: 0.0843358\ttotal: 1m 29s\tremaining: 9.5s\n",
      "904:\tlearn: 0.0842879\ttotal: 1m 29s\tremaining: 9.4s\n",
      "905:\tlearn: 0.0842538\ttotal: 1m 29s\tremaining: 9.3s\n",
      "906:\tlearn: 0.0842134\ttotal: 1m 29s\tremaining: 9.21s\n",
      "907:\tlearn: 0.0841971\ttotal: 1m 29s\tremaining: 9.11s\n",
      "908:\tlearn: 0.0841587\ttotal: 1m 29s\tremaining: 9.01s\n",
      "909:\tlearn: 0.0841127\ttotal: 1m 30s\tremaining: 8.91s\n",
      "910:\tlearn: 0.0840864\ttotal: 1m 30s\tremaining: 8.81s\n",
      "911:\tlearn: 0.0840372\ttotal: 1m 30s\tremaining: 8.71s\n",
      "912:\tlearn: 0.0840005\ttotal: 1m 30s\tremaining: 8.61s\n",
      "913:\tlearn: 0.0839884\ttotal: 1m 30s\tremaining: 8.51s\n",
      "914:\tlearn: 0.0839448\ttotal: 1m 30s\tremaining: 8.41s\n",
      "915:\tlearn: 0.0838899\ttotal: 1m 30s\tremaining: 8.31s\n",
      "916:\tlearn: 0.0838297\ttotal: 1m 30s\tremaining: 8.21s\n",
      "917:\tlearn: 0.0838107\ttotal: 1m 30s\tremaining: 8.11s\n",
      "918:\tlearn: 0.0837399\ttotal: 1m 30s\tremaining: 8.02s\n",
      "919:\tlearn: 0.0837332\ttotal: 1m 31s\tremaining: 7.92s\n",
      "920:\tlearn: 0.0836994\ttotal: 1m 31s\tremaining: 7.82s\n",
      "921:\tlearn: 0.0836678\ttotal: 1m 31s\tremaining: 7.72s\n",
      "922:\tlearn: 0.0836367\ttotal: 1m 31s\tremaining: 7.62s\n",
      "923:\tlearn: 0.0835794\ttotal: 1m 31s\tremaining: 7.52s\n",
      "924:\tlearn: 0.0835337\ttotal: 1m 31s\tremaining: 7.42s\n",
      "925:\tlearn: 0.0834712\ttotal: 1m 31s\tremaining: 7.32s\n",
      "926:\tlearn: 0.0834235\ttotal: 1m 31s\tremaining: 7.22s\n",
      "927:\tlearn: 0.0833784\ttotal: 1m 31s\tremaining: 7.12s\n",
      "928:\tlearn: 0.0833642\ttotal: 1m 31s\tremaining: 7.02s\n",
      "929:\tlearn: 0.0833110\ttotal: 1m 32s\tremaining: 6.92s\n",
      "930:\tlearn: 0.0832857\ttotal: 1m 32s\tremaining: 6.83s\n",
      "931:\tlearn: 0.0832443\ttotal: 1m 32s\tremaining: 6.73s\n",
      "932:\tlearn: 0.0832393\ttotal: 1m 32s\tremaining: 6.63s\n",
      "933:\tlearn: 0.0831671\ttotal: 1m 32s\tremaining: 6.53s\n",
      "934:\tlearn: 0.0831579\ttotal: 1m 32s\tremaining: 6.43s\n",
      "935:\tlearn: 0.0831258\ttotal: 1m 32s\tremaining: 6.33s\n",
      "936:\tlearn: 0.0831033\ttotal: 1m 32s\tremaining: 6.23s\n",
      "937:\tlearn: 0.0830741\ttotal: 1m 32s\tremaining: 6.13s\n",
      "938:\tlearn: 0.0830382\ttotal: 1m 32s\tremaining: 6.03s\n",
      "939:\tlearn: 0.0829956\ttotal: 1m 32s\tremaining: 5.93s\n",
      "940:\tlearn: 0.0829601\ttotal: 1m 33s\tremaining: 5.83s\n",
      "941:\tlearn: 0.0829405\ttotal: 1m 33s\tremaining: 5.73s\n",
      "942:\tlearn: 0.0828888\ttotal: 1m 33s\tremaining: 5.63s\n",
      "943:\tlearn: 0.0828719\ttotal: 1m 33s\tremaining: 5.54s\n",
      "944:\tlearn: 0.0828429\ttotal: 1m 33s\tremaining: 5.44s\n",
      "945:\tlearn: 0.0828020\ttotal: 1m 33s\tremaining: 5.34s\n",
      "946:\tlearn: 0.0827779\ttotal: 1m 33s\tremaining: 5.24s\n",
      "947:\tlearn: 0.0827367\ttotal: 1m 33s\tremaining: 5.14s\n",
      "948:\tlearn: 0.0827218\ttotal: 1m 33s\tremaining: 5.04s\n",
      "949:\tlearn: 0.0826648\ttotal: 1m 33s\tremaining: 4.94s\n",
      "950:\tlearn: 0.0825964\ttotal: 1m 33s\tremaining: 4.84s\n",
      "951:\tlearn: 0.0825402\ttotal: 1m 34s\tremaining: 4.74s\n",
      "952:\tlearn: 0.0825062\ttotal: 1m 34s\tremaining: 4.64s\n",
      "953:\tlearn: 0.0824808\ttotal: 1m 34s\tremaining: 4.55s\n",
      "954:\tlearn: 0.0824547\ttotal: 1m 34s\tremaining: 4.45s\n",
      "955:\tlearn: 0.0824123\ttotal: 1m 34s\tremaining: 4.35s\n",
      "956:\tlearn: 0.0823745\ttotal: 1m 34s\tremaining: 4.25s\n",
      "957:\tlearn: 0.0823275\ttotal: 1m 34s\tremaining: 4.15s\n",
      "958:\tlearn: 0.0823097\ttotal: 1m 34s\tremaining: 4.05s\n",
      "959:\tlearn: 0.0822568\ttotal: 1m 34s\tremaining: 3.95s\n",
      "960:\tlearn: 0.0822325\ttotal: 1m 34s\tremaining: 3.85s\n",
      "961:\tlearn: 0.0821962\ttotal: 1m 35s\tremaining: 3.75s\n",
      "962:\tlearn: 0.0821598\ttotal: 1m 35s\tremaining: 3.65s\n",
      "963:\tlearn: 0.0821322\ttotal: 1m 35s\tremaining: 3.56s\n",
      "964:\tlearn: 0.0820874\ttotal: 1m 35s\tremaining: 3.46s\n",
      "965:\tlearn: 0.0820173\ttotal: 1m 35s\tremaining: 3.36s\n",
      "966:\tlearn: 0.0819875\ttotal: 1m 35s\tremaining: 3.26s\n",
      "967:\tlearn: 0.0819329\ttotal: 1m 35s\tremaining: 3.16s\n",
      "968:\tlearn: 0.0818950\ttotal: 1m 35s\tremaining: 3.06s\n",
      "969:\tlearn: 0.0818568\ttotal: 1m 35s\tremaining: 2.96s\n",
      "970:\tlearn: 0.0818498\ttotal: 1m 35s\tremaining: 2.86s\n",
      "971:\tlearn: 0.0817927\ttotal: 1m 36s\tremaining: 2.77s\n",
      "972:\tlearn: 0.0817556\ttotal: 1m 36s\tremaining: 2.67s\n",
      "973:\tlearn: 0.0817349\ttotal: 1m 36s\tremaining: 2.57s\n",
      "974:\tlearn: 0.0817011\ttotal: 1m 36s\tremaining: 2.47s\n",
      "975:\tlearn: 0.0816690\ttotal: 1m 36s\tremaining: 2.37s\n",
      "976:\tlearn: 0.0816346\ttotal: 1m 36s\tremaining: 2.27s\n",
      "977:\tlearn: 0.0816118\ttotal: 1m 36s\tremaining: 2.17s\n",
      "978:\tlearn: 0.0815759\ttotal: 1m 36s\tremaining: 2.07s\n",
      "979:\tlearn: 0.0815432\ttotal: 1m 36s\tremaining: 1.98s\n",
      "980:\tlearn: 0.0815139\ttotal: 1m 36s\tremaining: 1.88s\n",
      "981:\tlearn: 0.0814732\ttotal: 1m 36s\tremaining: 1.78s\n",
      "982:\tlearn: 0.0814522\ttotal: 1m 37s\tremaining: 1.68s\n",
      "983:\tlearn: 0.0814096\ttotal: 1m 37s\tremaining: 1.58s\n",
      "984:\tlearn: 0.0813404\ttotal: 1m 37s\tremaining: 1.48s\n",
      "985:\tlearn: 0.0812773\ttotal: 1m 37s\tremaining: 1.38s\n",
      "986:\tlearn: 0.0812708\ttotal: 1m 37s\tremaining: 1.28s\n",
      "987:\tlearn: 0.0812477\ttotal: 1m 37s\tremaining: 1.18s\n",
      "988:\tlearn: 0.0811997\ttotal: 1m 37s\tremaining: 1.09s\n",
      "989:\tlearn: 0.0811564\ttotal: 1m 37s\tremaining: 987ms\n",
      "990:\tlearn: 0.0811173\ttotal: 1m 37s\tremaining: 889ms\n",
      "991:\tlearn: 0.0810857\ttotal: 1m 37s\tremaining: 790ms\n",
      "992:\tlearn: 0.0810359\ttotal: 1m 38s\tremaining: 691ms\n",
      "993:\tlearn: 0.0810180\ttotal: 1m 38s\tremaining: 592ms\n",
      "994:\tlearn: 0.0809777\ttotal: 1m 38s\tremaining: 494ms\n",
      "995:\tlearn: 0.0809706\ttotal: 1m 38s\tremaining: 395ms\n",
      "996:\tlearn: 0.0809552\ttotal: 1m 38s\tremaining: 296ms\n",
      "997:\tlearn: 0.0809192\ttotal: 1m 38s\tremaining: 197ms\n",
      "998:\tlearn: 0.0808906\ttotal: 1m 38s\tremaining: 98.7ms\n",
      "999:\tlearn: 0.0808143\ttotal: 1m 38s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.17583970e-01, 3.82416030e-01],\n",
       "       [2.99600958e-05, 9.99970040e-01],\n",
       "       [9.31340134e-01, 6.86598656e-02],\n",
       "       ...,\n",
       "       [2.08286368e-02, 9.79171363e-01],\n",
       "       [9.68336627e-01, 3.16633729e-02],\n",
       "       [1.41689354e-03, 9.98583106e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "y_true takes value in {'CHGOFF', 'P I F'} and pos_label is not specified: either make y_true take value in {0, 1} or {-1, 1} or pass pos_label explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m y_scores_cat \u001b[38;5;241m=\u001b[39m y_probas_cat[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# A partir des probabilités on calcule les combinaisons de scores pour recall et precision en fonction du seuil\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m precisions_cat, recalls_cat, thresholds_cat \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scores_cat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:952\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    853\u001b[0m     {\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    863\u001b[0m     y_true, probas_pred, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    864\u001b[0m ):\n\u001b[1;32m    865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;124;03m    array([0.1 , 0.35, 0.4 , 0.8 ])\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobas_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;66;03m# Drop thresholds corresponding to points where true positives (tps)\u001b[39;00m\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;66;03m# do not change from the previous or subsequent point. This will keep\u001b[39;00m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;66;03m# only the first and last point for each tps value. All points\u001b[39;00m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;66;03m# with the same tps value have the same recall and thus x coordinate.\u001b[39;00m\n\u001b[1;32m    961\u001b[0m         \u001b[38;5;66;03m# They appear as a vertical line on the plot.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m         optimal_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    963\u001b[0m             np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m    964\u001b[0m                 [[\u001b[38;5;28;01mTrue\u001b[39;00m], np\u001b[38;5;241m.\u001b[39mlogical_or(np\u001b[38;5;241m.\u001b[39mdiff(tps[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), np\u001b[38;5;241m.\u001b[39mdiff(tps[\u001b[38;5;241m1\u001b[39m:])), [\u001b[38;5;28;01mTrue\u001b[39;00m]]\n\u001b[1;32m    965\u001b[0m             )\n\u001b[1;32m    966\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:821\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    818\u001b[0m     y_score \u001b[38;5;241m=\u001b[39m y_score[nonzero_weight_mask]\n\u001b[1;32m    819\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[nonzero_weight_mask]\n\u001b[0;32m--> 821\u001b[0m pos_label \u001b[38;5;241m=\u001b[39m \u001b[43m_check_pos_label_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# make y_true a boolean vector\u001b[39;00m\n\u001b[1;32m    824\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m==\u001b[39m pos_label\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2367\u001b[0m, in \u001b[0;36m_check_pos_label_consistency\u001b[0;34m(pos_label, y_true)\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   2357\u001b[0m     classes\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2358\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2364\u001b[0m     )\n\u001b[1;32m   2365\u001b[0m ):\n\u001b[1;32m   2366\u001b[0m     classes_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes\u001b[38;5;241m.\u001b[39mtolist()])\n\u001b[0;32m-> 2367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true takes value in \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclasses_repr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m and pos_label is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified: either make y_true take value in \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0, 1} or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m-1, 1} or pass pos_label explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2371\u001b[0m     )\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2373\u001b[0m     pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: y_true takes value in {'CHGOFF', 'P I F'} and pos_label is not specified: either make y_true take value in {0, 1} or {-1, 1} or pass pos_label explicitly."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Au lieu d'obtenir les prédictions, on récupere les scores de probabilités pour chaque observations\n",
    "y_probas_cat = cross_val_predict(catboost_pipeline, X_train, y_train, method=\"predict_proba\")\n",
    "display(y_probas_cat)\n",
    "\n",
    "# On récupere uniquement les probabilités pour la classe positive\n",
    "y_scores_cat = y_probas_cat[:, 1]\n",
    "\n",
    "# A partir des probabilités on calcule les combinaisons de scores pour recall et precision en fonction du seuil\n",
    "precisions_cat, recalls_cat, thresholds_cat = precision_recall_curve(y_train, y_scores_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thresholds_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mthresholds_cat\u001b[49m, precisions_cat[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb--\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(thresholds_cat, recalls_cat[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg--\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvolution du score precision/recall en fonction du seuil\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'thresholds_cat' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(thresholds_cat, precisions_cat[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds_cat, recalls_cat[:-1], \"g--\", label=\"Recall\")\n",
    "plt.title(\"Evolution du score precision/recall en fonction du seuil\")\n",
    "plt.xlabel(\"Seuil\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "plt.plot(recalls_cat[:-1], precisions_cat[:-1])\n",
    "\n",
    "plt.title(\"Evolution de la precision en fonction du recall\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "auc_pr = average_precision_score(y_train, y_scores_cat)\n",
    "display(auc_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# La fonction roc_curve retourne le FPR, TPR et les seuils \n",
    "fpr_cat, tpr_cat, tresholds_cat = roc_curve(y_train, y_scores_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour représenter la courbe ROC\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.title(\"Courbe ROC\")\n",
    "    plt.xlabel(\"Taux de Faux Positif\")\n",
    "    plt.ylabel(\"Taux de Vrai Positif\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr_cat, tpr_cat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "display(roc_auc_score(y_train, y_scores_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.25      0.88      0.38      6950\n",
      "       P I F       0.91      0.31      0.47     27361\n",
      "\n",
      "    accuracy                           0.43     34311\n",
      "   macro avg       0.58      0.60      0.43     34311\n",
      "weighted avg       0.78      0.43      0.45     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.37      0.62      0.46      6950\n",
      "       P I F       0.88      0.73      0.80     27361\n",
      "\n",
      "    accuracy                           0.71     34311\n",
      "   macro avg       0.63      0.67      0.63     34311\n",
      "weighted avg       0.78      0.71      0.73     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', BernoulliNB())\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.73      0.42      0.53      6950\n",
      "       P I F       0.87      0.96      0.91     27361\n",
      "\n",
      "    accuracy                           0.85     34311\n",
      "   macro avg       0.80      0.69      0.72     34311\n",
      "weighted avg       0.84      0.85      0.83     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
