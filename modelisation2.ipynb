{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 437765, number of negative: 111204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 548969, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.797431 -> initscore=1.370316\n",
      "[LightGBM] [Info] Start training from score 1.370316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.89      0.84      0.87     27801\n",
      "       P I F       0.96      0.97      0.97    109442\n",
      "\n",
      "    accuracy                           0.95    137243\n",
      "   macro avg       0.93      0.91      0.92    137243\n",
      "weighted avg       0.95      0.95      0.95    137243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', lgb.LGBMClassifier())\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2035\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2026\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2018\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2032\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 394236, number of negative: 100812\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2022\n",
      "[LightGBM] [Info] Number of data points in the train set: 495048, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.796359 -> initscore=1.363692\n",
      "[LightGBM] [Info] Start training from score 1.363692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Effectuer la recherche sur la grille\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Meilleurs hyperparamètres trouvés\u001b[39;00m\n\u001b[1;32m     86\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:890\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1307\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:914\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[0;32m--> 914\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:823\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    811\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    812\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    813\u001b[0m             delayed(func)(\n\u001b[1;32m    814\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m         )\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1307\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:543\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    541\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    548\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    549\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/base.py:1064\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:1023\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1022\u001b[0m }\n\u001b[0;32m-> 1023\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:239\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[1;32m    236\u001b[0m             Xi[\u001b[38;5;241m~\u001b[39mvalid_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# We use check_unknown=False, since _check_unknown was\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# already called above.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     X_int[:, i] \u001b[38;5;241m=\u001b[39m _encode(Xi, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], check_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns_with_unknown:\n\u001b[1;32m    241\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    242\u001b[0m         (\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories in columns \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    248\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist()\n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', lgb.LGBMClassifier())])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__num_leaves': [20, 30],\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__min_child_samples': [20, 30],\n",
    "    'classifier__subsample': [0.6, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 1.0],\n",
    "}\n",
    "\n",
    "# rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Prédire les étiquettes sur l'ensemble de test\n",
    "# y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Créer une instance de GridSearchCV\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Effectuer la recherche sur la grille\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres trouvés\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Meilleur score obtenu sur l'ensemble d'entraînement\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Utiliser le meilleur modèle pour prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres:\", best_params)\n",
    "print(\"Meilleur score sur l'ensemble d'entraînement:\", best_score)\n",
    "print(\"Rapport de classification:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gbdt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'np.random.Generator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'split'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      LightGBM classifier.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Construct a gradient boosting model.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "boosting_type : str, optional (default='gbdt')\n",
      "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "    'rf', Random Forest.\n",
      "num_leaves : int, optional (default=31)\n",
      "    Maximum tree leaves for base learners.\n",
      "max_depth : int, optional (default=-1)\n",
      "    Maximum tree depth for base learners, <=0 means no limit.\n",
      "learning_rate : float, optional (default=0.1)\n",
      "    Boosting learning rate.\n",
      "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "    in training using ``reset_parameter`` callback.\n",
      "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "n_estimators : int, optional (default=100)\n",
      "    Number of boosted trees to fit.\n",
      "subsample_for_bin : int, optional (default=200000)\n",
      "    Number of samples for constructing bins.\n",
      "objective : str, callable or None, optional (default=None)\n",
      "    Specify the learning task and the corresponding learning objective or\n",
      "    a custom objective function to be used (see note below).\n",
      "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "    Weights associated with classes in the form ``{class_label: weight}``.\n",
      "    Use this parameter only for multi-class classification task;\n",
      "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "    You may want to consider performing probability calibration\n",
      "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "    If None, all classes are supposed to have weight one.\n",
      "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "    if ``sample_weight`` is specified.\n",
      "min_split_gain : float, optional (default=0.)\n",
      "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "min_child_weight : float, optional (default=1e-3)\n",
      "    Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
      "min_child_samples : int, optional (default=20)\n",
      "    Minimum number of data needed in a child (leaf).\n",
      "subsample : float, optional (default=1.)\n",
      "    Subsample ratio of the training instance.\n",
      "subsample_freq : int, optional (default=0)\n",
      "    Frequency of subsample, <=0 means no enable.\n",
      "colsample_bytree : float, optional (default=1.)\n",
      "    Subsample ratio of columns when constructing each tree.\n",
      "reg_alpha : float, optional (default=0.)\n",
      "    L1 regularization term on weights.\n",
      "reg_lambda : float, optional (default=0.)\n",
      "    L2 regularization term on weights.\n",
      "random_state : int, RandomState object or None, optional (default=None)\n",
      "    Random number seed.\n",
      "    If int, this number is used to seed the C++ code.\n",
      "    If RandomState or Generator object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "    If None, default seeds in C++ code are used.\n",
      "n_jobs : int or None, optional (default=None)\n",
      "    Number of parallel threads to use for training (can be changed at prediction time by\n",
      "    passing it as an extra keyword argument).\n",
      "\n",
      "    For better performance, it is recommended to set this to the number of physical cores\n",
      "    in the CPU.\n",
      "\n",
      "    Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
      "    scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
      "    threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
      "    to using the number of physical cores in the system (its correct detection requires\n",
      "    either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
      "\n",
      "    .. versionchanged:: 4.0.0\n",
      "\n",
      "importance_type : str, optional (default='split')\n",
      "    The type of feature importance to be filled into ``feature_importances_``.\n",
      "    If 'split', result contains numbers of times the feature is used in a model.\n",
      "    If 'gain', result contains total gains of splits which use the feature.\n",
      "**kwargs\n",
      "    Other parameters for the model.\n",
      "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "\n",
      "Note\n",
      "----\n",
      "A custom objective function can be provided for the ``objective`` parameter.\n",
      "In this case, it should have the signature\n",
      "``objective(y_true, y_pred) -> grad, hess``,\n",
      "``objective(y_true, y_pred, weight) -> grad, hess``\n",
      "or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
      "\n",
      "    y_true : numpy 1-D array of shape = [n_samples]\n",
      "        The target values.\n",
      "    y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "        The predicted values.\n",
      "        Predicted values are returned before any transformation,\n",
      "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
      "    weight : numpy 1-D array of shape = [n_samples]\n",
      "        The weight of samples. Weights should be non-negative.\n",
      "    group : numpy 1-D array\n",
      "        Group/query data.\n",
      "        Only used in the learning-to-rank task.\n",
      "        sum(group) = n_samples.\n",
      "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "    grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "        The value of the first order derivative (gradient) of the loss\n",
      "        with respect to the elements of y_pred for each sample point.\n",
      "    hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "        The value of the second order derivative (Hessian) of the loss\n",
      "        with respect to the elements of y_pred for each sample point.\n",
      "\n",
      "For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
      "and grad and hess should be returned in the same format.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/lightgbm/sklearn.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     DaskLGBMClassifier"
     ]
    }
   ],
   "source": [
    "lgb.LGBMClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.163904\n",
      "0:\tlearn: 0.5133029\ttotal: 378ms\tremaining: 6m 17s\n",
      "1:\tlearn: 0.4105906\ttotal: 749ms\tremaining: 6m 13s\n",
      "2:\tlearn: 0.3491371\ttotal: 1.08s\tremaining: 5m 57s\n",
      "3:\tlearn: 0.3135733\ttotal: 1.32s\tremaining: 5m 28s\n",
      "4:\tlearn: 0.2853640\ttotal: 1.6s\tremaining: 5m 18s\n",
      "5:\tlearn: 0.2609983\ttotal: 1.88s\tremaining: 5m 11s\n",
      "6:\tlearn: 0.2431547\ttotal: 2.16s\tremaining: 5m 6s\n",
      "7:\tlearn: 0.2328906\ttotal: 2.39s\tremaining: 4m 56s\n",
      "8:\tlearn: 0.2258380\ttotal: 2.64s\tremaining: 4m 50s\n",
      "9:\tlearn: 0.2197624\ttotal: 2.89s\tremaining: 4m 46s\n",
      "10:\tlearn: 0.2139795\ttotal: 3.16s\tremaining: 4m 43s\n",
      "11:\tlearn: 0.2076103\ttotal: 3.44s\tremaining: 4m 43s\n",
      "12:\tlearn: 0.2041205\ttotal: 3.72s\tremaining: 4m 42s\n",
      "13:\tlearn: 0.2011688\ttotal: 3.98s\tremaining: 4m 40s\n",
      "14:\tlearn: 0.1985291\ttotal: 4.27s\tremaining: 4m 40s\n",
      "15:\tlearn: 0.1944664\ttotal: 4.55s\tremaining: 4m 39s\n",
      "16:\tlearn: 0.1911155\ttotal: 4.83s\tremaining: 4m 39s\n",
      "17:\tlearn: 0.1889843\ttotal: 5.05s\tremaining: 4m 35s\n",
      "18:\tlearn: 0.1865982\ttotal: 5.29s\tremaining: 4m 32s\n",
      "19:\tlearn: 0.1848886\ttotal: 5.58s\tremaining: 4m 33s\n",
      "20:\tlearn: 0.1830139\ttotal: 5.87s\tremaining: 4m 33s\n",
      "21:\tlearn: 0.1813308\ttotal: 6.18s\tremaining: 4m 34s\n",
      "22:\tlearn: 0.1800142\ttotal: 6.51s\tremaining: 4m 36s\n",
      "23:\tlearn: 0.1773159\ttotal: 6.8s\tremaining: 4m 36s\n",
      "24:\tlearn: 0.1758629\ttotal: 7.12s\tremaining: 4m 37s\n",
      "25:\tlearn: 0.1747780\ttotal: 7.4s\tremaining: 4m 37s\n",
      "26:\tlearn: 0.1733055\ttotal: 7.7s\tremaining: 4m 37s\n",
      "27:\tlearn: 0.1724486\ttotal: 8.06s\tremaining: 4m 39s\n",
      "28:\tlearn: 0.1706811\ttotal: 8.32s\tremaining: 4m 38s\n",
      "29:\tlearn: 0.1696458\ttotal: 8.6s\tremaining: 4m 38s\n",
      "30:\tlearn: 0.1688176\ttotal: 8.97s\tremaining: 4m 40s\n",
      "31:\tlearn: 0.1679481\ttotal: 9.22s\tremaining: 4m 38s\n",
      "32:\tlearn: 0.1668786\ttotal: 9.52s\tremaining: 4m 38s\n",
      "33:\tlearn: 0.1661693\ttotal: 9.78s\tremaining: 4m 37s\n",
      "34:\tlearn: 0.1652167\ttotal: 10.1s\tremaining: 4m 37s\n",
      "35:\tlearn: 0.1645101\ttotal: 10.4s\tremaining: 4m 37s\n",
      "36:\tlearn: 0.1631914\ttotal: 10.6s\tremaining: 4m 35s\n",
      "37:\tlearn: 0.1626315\ttotal: 10.8s\tremaining: 4m 34s\n",
      "38:\tlearn: 0.1616258\ttotal: 11.2s\tremaining: 4m 34s\n",
      "39:\tlearn: 0.1609099\ttotal: 11.5s\tremaining: 4m 34s\n",
      "40:\tlearn: 0.1602982\ttotal: 11.7s\tremaining: 4m 33s\n",
      "41:\tlearn: 0.1595055\ttotal: 12s\tremaining: 4m 33s\n",
      "42:\tlearn: 0.1581370\ttotal: 12.3s\tremaining: 4m 33s\n",
      "43:\tlearn: 0.1576116\ttotal: 12.5s\tremaining: 4m 32s\n",
      "44:\tlearn: 0.1571362\ttotal: 12.8s\tremaining: 4m 31s\n",
      "45:\tlearn: 0.1566349\ttotal: 13s\tremaining: 4m 30s\n",
      "46:\tlearn: 0.1560391\ttotal: 13.3s\tremaining: 4m 30s\n",
      "47:\tlearn: 0.1554296\ttotal: 13.5s\tremaining: 4m 28s\n",
      "48:\tlearn: 0.1549825\ttotal: 13.8s\tremaining: 4m 28s\n",
      "49:\tlearn: 0.1546806\ttotal: 14.1s\tremaining: 4m 27s\n",
      "50:\tlearn: 0.1539479\ttotal: 14.4s\tremaining: 4m 27s\n",
      "51:\tlearn: 0.1533331\ttotal: 14.7s\tremaining: 4m 27s\n",
      "52:\tlearn: 0.1528744\ttotal: 15s\tremaining: 4m 27s\n",
      "53:\tlearn: 0.1524663\ttotal: 15.3s\tremaining: 4m 27s\n",
      "54:\tlearn: 0.1521308\ttotal: 15.5s\tremaining: 4m 26s\n",
      "55:\tlearn: 0.1516760\ttotal: 15.9s\tremaining: 4m 27s\n",
      "56:\tlearn: 0.1511920\ttotal: 16.1s\tremaining: 4m 26s\n",
      "57:\tlearn: 0.1506963\ttotal: 16.3s\tremaining: 4m 25s\n",
      "58:\tlearn: 0.1503141\ttotal: 16.6s\tremaining: 4m 24s\n",
      "59:\tlearn: 0.1500804\ttotal: 16.8s\tremaining: 4m 23s\n",
      "60:\tlearn: 0.1497068\ttotal: 17.1s\tremaining: 4m 23s\n",
      "61:\tlearn: 0.1493490\ttotal: 17.4s\tremaining: 4m 22s\n",
      "62:\tlearn: 0.1489953\ttotal: 17.6s\tremaining: 4m 22s\n",
      "63:\tlearn: 0.1483546\ttotal: 17.8s\tremaining: 4m 20s\n",
      "64:\tlearn: 0.1480857\ttotal: 18.1s\tremaining: 4m 21s\n",
      "65:\tlearn: 0.1477322\ttotal: 18.5s\tremaining: 4m 21s\n",
      "66:\tlearn: 0.1471626\ttotal: 18.7s\tremaining: 4m 21s\n",
      "67:\tlearn: 0.1468113\ttotal: 19s\tremaining: 4m 20s\n",
      "68:\tlearn: 0.1465684\ttotal: 19.3s\tremaining: 4m 20s\n",
      "69:\tlearn: 0.1461284\ttotal: 19.5s\tremaining: 4m 19s\n",
      "70:\tlearn: 0.1457951\ttotal: 19.9s\tremaining: 4m 19s\n",
      "71:\tlearn: 0.1454287\ttotal: 20.1s\tremaining: 4m 18s\n",
      "72:\tlearn: 0.1450956\ttotal: 20.3s\tremaining: 4m 18s\n",
      "73:\tlearn: 0.1447428\ttotal: 20.6s\tremaining: 4m 17s\n",
      "74:\tlearn: 0.1445329\ttotal: 20.9s\tremaining: 4m 17s\n",
      "75:\tlearn: 0.1442521\ttotal: 21.1s\tremaining: 4m 16s\n",
      "76:\tlearn: 0.1439434\ttotal: 21.4s\tremaining: 4m 16s\n",
      "77:\tlearn: 0.1436973\ttotal: 21.7s\tremaining: 4m 16s\n",
      "78:\tlearn: 0.1433928\ttotal: 22.1s\tremaining: 4m 17s\n",
      "79:\tlearn: 0.1430298\ttotal: 22.4s\tremaining: 4m 17s\n",
      "80:\tlearn: 0.1427762\ttotal: 22.7s\tremaining: 4m 17s\n",
      "81:\tlearn: 0.1425715\ttotal: 23s\tremaining: 4m 17s\n",
      "82:\tlearn: 0.1422116\ttotal: 23.3s\tremaining: 4m 17s\n",
      "83:\tlearn: 0.1419422\ttotal: 23.6s\tremaining: 4m 17s\n",
      "84:\tlearn: 0.1416328\ttotal: 23.9s\tremaining: 4m 17s\n",
      "85:\tlearn: 0.1413018\ttotal: 24.2s\tremaining: 4m 17s\n",
      "86:\tlearn: 0.1409401\ttotal: 24.5s\tremaining: 4m 16s\n",
      "87:\tlearn: 0.1406539\ttotal: 24.8s\tremaining: 4m 16s\n",
      "88:\tlearn: 0.1404359\ttotal: 25s\tremaining: 4m 16s\n",
      "89:\tlearn: 0.1401606\ttotal: 25.3s\tremaining: 4m 15s\n",
      "90:\tlearn: 0.1399761\ttotal: 25.6s\tremaining: 4m 15s\n",
      "91:\tlearn: 0.1397028\ttotal: 25.9s\tremaining: 4m 15s\n",
      "92:\tlearn: 0.1394922\ttotal: 26.2s\tremaining: 4m 15s\n",
      "93:\tlearn: 0.1393146\ttotal: 26.4s\tremaining: 4m 14s\n",
      "94:\tlearn: 0.1390141\ttotal: 26.7s\tremaining: 4m 14s\n",
      "95:\tlearn: 0.1386461\ttotal: 26.9s\tremaining: 4m 13s\n",
      "96:\tlearn: 0.1385194\ttotal: 27.2s\tremaining: 4m 13s\n",
      "97:\tlearn: 0.1381554\ttotal: 27.5s\tremaining: 4m 13s\n",
      "98:\tlearn: 0.1379346\ttotal: 27.7s\tremaining: 4m 12s\n",
      "99:\tlearn: 0.1377024\ttotal: 28s\tremaining: 4m 12s\n",
      "100:\tlearn: 0.1375426\ttotal: 28.2s\tremaining: 4m 11s\n",
      "101:\tlearn: 0.1373099\ttotal: 28.5s\tremaining: 4m 10s\n",
      "102:\tlearn: 0.1370323\ttotal: 28.7s\tremaining: 4m 10s\n",
      "103:\tlearn: 0.1367818\ttotal: 29s\tremaining: 4m 9s\n",
      "104:\tlearn: 0.1363140\ttotal: 29.3s\tremaining: 4m 9s\n",
      "105:\tlearn: 0.1359757\ttotal: 29.5s\tremaining: 4m 8s\n",
      "106:\tlearn: 0.1358083\ttotal: 29.8s\tremaining: 4m 8s\n",
      "107:\tlearn: 0.1356046\ttotal: 30.1s\tremaining: 4m 8s\n",
      "108:\tlearn: 0.1354733\ttotal: 30.4s\tremaining: 4m 8s\n",
      "109:\tlearn: 0.1352382\ttotal: 30.6s\tremaining: 4m 7s\n",
      "110:\tlearn: 0.1350350\ttotal: 30.9s\tremaining: 4m 7s\n",
      "111:\tlearn: 0.1348894\ttotal: 31.1s\tremaining: 4m 6s\n",
      "112:\tlearn: 0.1347158\ttotal: 31.4s\tremaining: 4m 6s\n",
      "113:\tlearn: 0.1345919\ttotal: 31.7s\tremaining: 4m 6s\n",
      "114:\tlearn: 0.1343611\ttotal: 31.9s\tremaining: 4m 5s\n",
      "115:\tlearn: 0.1341561\ttotal: 32.2s\tremaining: 4m 5s\n",
      "116:\tlearn: 0.1340318\ttotal: 32.4s\tremaining: 4m 4s\n",
      "117:\tlearn: 0.1338646\ttotal: 32.7s\tremaining: 4m 4s\n",
      "118:\tlearn: 0.1337146\ttotal: 33s\tremaining: 4m 4s\n",
      "119:\tlearn: 0.1334720\ttotal: 33.3s\tremaining: 4m 4s\n",
      "120:\tlearn: 0.1333480\ttotal: 33.6s\tremaining: 4m 3s\n",
      "121:\tlearn: 0.1330938\ttotal: 33.8s\tremaining: 4m 3s\n",
      "122:\tlearn: 0.1328448\ttotal: 34.1s\tremaining: 4m 2s\n",
      "123:\tlearn: 0.1326976\ttotal: 34.4s\tremaining: 4m 2s\n",
      "124:\tlearn: 0.1325118\ttotal: 34.6s\tremaining: 4m 2s\n",
      "125:\tlearn: 0.1323107\ttotal: 35s\tremaining: 4m 2s\n",
      "126:\tlearn: 0.1320056\ttotal: 35.3s\tremaining: 4m 2s\n",
      "127:\tlearn: 0.1318350\ttotal: 35.5s\tremaining: 4m 1s\n",
      "128:\tlearn: 0.1316351\ttotal: 35.8s\tremaining: 4m 1s\n",
      "129:\tlearn: 0.1314643\ttotal: 36s\tremaining: 4m\n",
      "130:\tlearn: 0.1312548\ttotal: 36.2s\tremaining: 4m\n",
      "131:\tlearn: 0.1311209\ttotal: 36.5s\tremaining: 4m\n",
      "132:\tlearn: 0.1308947\ttotal: 36.8s\tremaining: 3m 59s\n",
      "133:\tlearn: 0.1307360\ttotal: 37s\tremaining: 3m 59s\n",
      "134:\tlearn: 0.1305838\ttotal: 37.3s\tremaining: 3m 58s\n",
      "135:\tlearn: 0.1304544\ttotal: 37.6s\tremaining: 3m 58s\n",
      "136:\tlearn: 0.1303163\ttotal: 37.9s\tremaining: 3m 58s\n",
      "137:\tlearn: 0.1300878\ttotal: 38.2s\tremaining: 3m 58s\n",
      "138:\tlearn: 0.1299515\ttotal: 38.5s\tremaining: 3m 58s\n",
      "139:\tlearn: 0.1297764\ttotal: 38.8s\tremaining: 3m 58s\n",
      "140:\tlearn: 0.1296765\ttotal: 39.1s\tremaining: 3m 58s\n",
      "141:\tlearn: 0.1294690\ttotal: 39.3s\tremaining: 3m 57s\n",
      "142:\tlearn: 0.1292569\ttotal: 39.6s\tremaining: 3m 57s\n",
      "143:\tlearn: 0.1291402\ttotal: 39.9s\tremaining: 3m 57s\n",
      "144:\tlearn: 0.1290739\ttotal: 40.1s\tremaining: 3m 56s\n",
      "145:\tlearn: 0.1289420\ttotal: 40.4s\tremaining: 3m 56s\n",
      "146:\tlearn: 0.1288455\ttotal: 40.7s\tremaining: 3m 56s\n",
      "147:\tlearn: 0.1287412\ttotal: 41s\tremaining: 3m 56s\n",
      "148:\tlearn: 0.1285933\ttotal: 41.3s\tremaining: 3m 55s\n",
      "149:\tlearn: 0.1284472\ttotal: 41.6s\tremaining: 3m 55s\n",
      "150:\tlearn: 0.1283222\ttotal: 41.8s\tremaining: 3m 55s\n",
      "151:\tlearn: 0.1282064\ttotal: 42.1s\tremaining: 3m 54s\n",
      "152:\tlearn: 0.1281148\ttotal: 42.4s\tremaining: 3m 54s\n",
      "153:\tlearn: 0.1279605\ttotal: 42.6s\tremaining: 3m 54s\n",
      "154:\tlearn: 0.1278295\ttotal: 42.9s\tremaining: 3m 54s\n",
      "155:\tlearn: 0.1276970\ttotal: 43.2s\tremaining: 3m 53s\n",
      "156:\tlearn: 0.1275998\ttotal: 43.4s\tremaining: 3m 53s\n",
      "157:\tlearn: 0.1274455\ttotal: 43.7s\tremaining: 3m 52s\n",
      "158:\tlearn: 0.1272721\ttotal: 43.9s\tremaining: 3m 52s\n",
      "159:\tlearn: 0.1270878\ttotal: 44.2s\tremaining: 3m 51s\n",
      "160:\tlearn: 0.1269814\ttotal: 44.5s\tremaining: 3m 51s\n",
      "161:\tlearn: 0.1268110\ttotal: 44.7s\tremaining: 3m 51s\n",
      "162:\tlearn: 0.1266993\ttotal: 45s\tremaining: 3m 51s\n",
      "163:\tlearn: 0.1266166\ttotal: 45.3s\tremaining: 3m 50s\n",
      "164:\tlearn: 0.1265068\ttotal: 45.6s\tremaining: 3m 50s\n",
      "165:\tlearn: 0.1264091\ttotal: 45.9s\tremaining: 3m 50s\n",
      "166:\tlearn: 0.1262711\ttotal: 46.1s\tremaining: 3m 49s\n",
      "167:\tlearn: 0.1260977\ttotal: 46.4s\tremaining: 3m 49s\n",
      "168:\tlearn: 0.1259316\ttotal: 46.6s\tremaining: 3m 48s\n",
      "169:\tlearn: 0.1258530\ttotal: 46.8s\tremaining: 3m 48s\n",
      "170:\tlearn: 0.1257102\ttotal: 47s\tremaining: 3m 47s\n",
      "171:\tlearn: 0.1256175\ttotal: 47.2s\tremaining: 3m 47s\n",
      "172:\tlearn: 0.1255319\ttotal: 47.5s\tremaining: 3m 47s\n",
      "173:\tlearn: 0.1254452\ttotal: 47.8s\tremaining: 3m 46s\n",
      "174:\tlearn: 0.1253362\ttotal: 48.1s\tremaining: 3m 46s\n",
      "175:\tlearn: 0.1252385\ttotal: 48.3s\tremaining: 3m 46s\n",
      "176:\tlearn: 0.1251094\ttotal: 48.6s\tremaining: 3m 45s\n",
      "177:\tlearn: 0.1250099\ttotal: 48.8s\tremaining: 3m 45s\n",
      "178:\tlearn: 0.1249197\ttotal: 49.1s\tremaining: 3m 45s\n",
      "179:\tlearn: 0.1247425\ttotal: 49.4s\tremaining: 3m 45s\n",
      "180:\tlearn: 0.1246196\ttotal: 49.7s\tremaining: 3m 44s\n",
      "181:\tlearn: 0.1245551\ttotal: 50s\tremaining: 3m 44s\n",
      "182:\tlearn: 0.1244370\ttotal: 50.2s\tremaining: 3m 44s\n",
      "183:\tlearn: 0.1243239\ttotal: 50.5s\tremaining: 3m 43s\n",
      "184:\tlearn: 0.1241534\ttotal: 50.8s\tremaining: 3m 43s\n",
      "185:\tlearn: 0.1240990\ttotal: 51s\tremaining: 3m 43s\n",
      "186:\tlearn: 0.1239653\ttotal: 51.3s\tremaining: 3m 42s\n",
      "187:\tlearn: 0.1238783\ttotal: 51.5s\tremaining: 3m 42s\n",
      "188:\tlearn: 0.1237749\ttotal: 51.7s\tremaining: 3m 42s\n",
      "189:\tlearn: 0.1236190\ttotal: 52s\tremaining: 3m 41s\n",
      "190:\tlearn: 0.1235118\ttotal: 52.3s\tremaining: 3m 41s\n",
      "191:\tlearn: 0.1234218\ttotal: 52.5s\tremaining: 3m 40s\n",
      "192:\tlearn: 0.1233513\ttotal: 52.8s\tremaining: 3m 40s\n",
      "193:\tlearn: 0.1232262\ttotal: 53s\tremaining: 3m 40s\n",
      "194:\tlearn: 0.1231613\ttotal: 53.2s\tremaining: 3m 39s\n",
      "195:\tlearn: 0.1230559\ttotal: 53.5s\tremaining: 3m 39s\n",
      "196:\tlearn: 0.1229235\ttotal: 53.8s\tremaining: 3m 39s\n",
      "197:\tlearn: 0.1228171\ttotal: 54s\tremaining: 3m 38s\n",
      "198:\tlearn: 0.1227154\ttotal: 54.3s\tremaining: 3m 38s\n",
      "199:\tlearn: 0.1226421\ttotal: 54.6s\tremaining: 3m 38s\n",
      "200:\tlearn: 0.1225695\ttotal: 54.9s\tremaining: 3m 38s\n",
      "201:\tlearn: 0.1224557\ttotal: 55.2s\tremaining: 3m 38s\n",
      "202:\tlearn: 0.1223679\ttotal: 55.4s\tremaining: 3m 37s\n",
      "203:\tlearn: 0.1223159\ttotal: 55.7s\tremaining: 3m 37s\n",
      "204:\tlearn: 0.1221357\ttotal: 56s\tremaining: 3m 37s\n",
      "205:\tlearn: 0.1220534\ttotal: 56.3s\tremaining: 3m 37s\n",
      "206:\tlearn: 0.1219259\ttotal: 56.6s\tremaining: 3m 36s\n",
      "207:\tlearn: 0.1217541\ttotal: 56.8s\tremaining: 3m 36s\n",
      "208:\tlearn: 0.1216545\ttotal: 57.1s\tremaining: 3m 36s\n",
      "209:\tlearn: 0.1215440\ttotal: 57.4s\tremaining: 3m 35s\n",
      "210:\tlearn: 0.1214602\ttotal: 57.7s\tremaining: 3m 35s\n",
      "211:\tlearn: 0.1213839\ttotal: 57.9s\tremaining: 3m 35s\n",
      "212:\tlearn: 0.1212800\ttotal: 58.2s\tremaining: 3m 34s\n",
      "213:\tlearn: 0.1212083\ttotal: 58.5s\tremaining: 3m 34s\n",
      "214:\tlearn: 0.1210755\ttotal: 58.7s\tremaining: 3m 34s\n",
      "215:\tlearn: 0.1210073\ttotal: 59.1s\tremaining: 3m 34s\n",
      "216:\tlearn: 0.1209146\ttotal: 59.3s\tremaining: 3m 34s\n",
      "217:\tlearn: 0.1208010\ttotal: 59.6s\tremaining: 3m 33s\n",
      "218:\tlearn: 0.1207039\ttotal: 59.8s\tremaining: 3m 33s\n",
      "219:\tlearn: 0.1206148\ttotal: 1m\tremaining: 3m 33s\n",
      "220:\tlearn: 0.1205536\ttotal: 1m\tremaining: 3m 32s\n",
      "221:\tlearn: 0.1204686\ttotal: 1m\tremaining: 3m 32s\n",
      "222:\tlearn: 0.1203499\ttotal: 1m\tremaining: 3m 32s\n",
      "223:\tlearn: 0.1202943\ttotal: 1m 1s\tremaining: 3m 31s\n",
      "224:\tlearn: 0.1202101\ttotal: 1m 1s\tremaining: 3m 31s\n",
      "225:\tlearn: 0.1201639\ttotal: 1m 1s\tremaining: 3m 31s\n",
      "226:\tlearn: 0.1200611\ttotal: 1m 1s\tremaining: 3m 30s\n",
      "227:\tlearn: 0.1199494\ttotal: 1m 2s\tremaining: 3m 30s\n",
      "228:\tlearn: 0.1198877\ttotal: 1m 2s\tremaining: 3m 30s\n",
      "229:\tlearn: 0.1197766\ttotal: 1m 2s\tremaining: 3m 30s\n",
      "230:\tlearn: 0.1197114\ttotal: 1m 3s\tremaining: 3m 30s\n",
      "231:\tlearn: 0.1196403\ttotal: 1m 3s\tremaining: 3m 29s\n",
      "232:\tlearn: 0.1195563\ttotal: 1m 3s\tremaining: 3m 29s\n",
      "233:\tlearn: 0.1194597\ttotal: 1m 3s\tremaining: 3m 29s\n",
      "234:\tlearn: 0.1194064\ttotal: 1m 4s\tremaining: 3m 28s\n",
      "235:\tlearn: 0.1193449\ttotal: 1m 4s\tremaining: 3m 28s\n",
      "236:\tlearn: 0.1192690\ttotal: 1m 4s\tremaining: 3m 28s\n",
      "237:\tlearn: 0.1191743\ttotal: 1m 4s\tremaining: 3m 27s\n",
      "238:\tlearn: 0.1190520\ttotal: 1m 5s\tremaining: 3m 27s\n",
      "239:\tlearn: 0.1189790\ttotal: 1m 5s\tremaining: 3m 27s\n",
      "240:\tlearn: 0.1188962\ttotal: 1m 5s\tremaining: 3m 26s\n",
      "241:\tlearn: 0.1188579\ttotal: 1m 5s\tremaining: 3m 26s\n",
      "242:\tlearn: 0.1187638\ttotal: 1m 6s\tremaining: 3m 26s\n",
      "243:\tlearn: 0.1186739\ttotal: 1m 6s\tremaining: 3m 26s\n",
      "244:\tlearn: 0.1185810\ttotal: 1m 6s\tremaining: 3m 25s\n",
      "245:\tlearn: 0.1184853\ttotal: 1m 7s\tremaining: 3m 25s\n",
      "246:\tlearn: 0.1184184\ttotal: 1m 7s\tremaining: 3m 25s\n",
      "247:\tlearn: 0.1183510\ttotal: 1m 7s\tremaining: 3m 24s\n",
      "248:\tlearn: 0.1182774\ttotal: 1m 7s\tremaining: 3m 24s\n",
      "249:\tlearn: 0.1182215\ttotal: 1m 8s\tremaining: 3m 24s\n",
      "250:\tlearn: 0.1181347\ttotal: 1m 8s\tremaining: 3m 23s\n",
      "251:\tlearn: 0.1180736\ttotal: 1m 8s\tremaining: 3m 23s\n",
      "252:\tlearn: 0.1180044\ttotal: 1m 8s\tremaining: 3m 23s\n",
      "253:\tlearn: 0.1178964\ttotal: 1m 9s\tremaining: 3m 23s\n",
      "254:\tlearn: 0.1178105\ttotal: 1m 9s\tremaining: 3m 22s\n",
      "255:\tlearn: 0.1176968\ttotal: 1m 9s\tremaining: 3m 22s\n",
      "256:\tlearn: 0.1176252\ttotal: 1m 9s\tremaining: 3m 22s\n",
      "257:\tlearn: 0.1175248\ttotal: 1m 10s\tremaining: 3m 21s\n",
      "258:\tlearn: 0.1174290\ttotal: 1m 10s\tremaining: 3m 21s\n",
      "259:\tlearn: 0.1173594\ttotal: 1m 10s\tremaining: 3m 21s\n",
      "260:\tlearn: 0.1172950\ttotal: 1m 11s\tremaining: 3m 21s\n",
      "261:\tlearn: 0.1172529\ttotal: 1m 11s\tremaining: 3m 20s\n",
      "262:\tlearn: 0.1171709\ttotal: 1m 11s\tremaining: 3m 20s\n",
      "263:\tlearn: 0.1171170\ttotal: 1m 11s\tremaining: 3m 20s\n",
      "264:\tlearn: 0.1170452\ttotal: 1m 12s\tremaining: 3m 20s\n",
      "265:\tlearn: 0.1169328\ttotal: 1m 12s\tremaining: 3m 19s\n",
      "266:\tlearn: 0.1168879\ttotal: 1m 12s\tremaining: 3m 19s\n",
      "267:\tlearn: 0.1168445\ttotal: 1m 12s\tremaining: 3m 19s\n",
      "268:\tlearn: 0.1167068\ttotal: 1m 13s\tremaining: 3m 18s\n",
      "269:\tlearn: 0.1166233\ttotal: 1m 13s\tremaining: 3m 18s\n",
      "270:\tlearn: 0.1165550\ttotal: 1m 13s\tremaining: 3m 18s\n",
      "271:\tlearn: 0.1164610\ttotal: 1m 13s\tremaining: 3m 17s\n",
      "272:\tlearn: 0.1163781\ttotal: 1m 14s\tremaining: 3m 17s\n",
      "273:\tlearn: 0.1163130\ttotal: 1m 14s\tremaining: 3m 17s\n",
      "274:\tlearn: 0.1162537\ttotal: 1m 14s\tremaining: 3m 16s\n",
      "275:\tlearn: 0.1161534\ttotal: 1m 15s\tremaining: 3m 16s\n",
      "276:\tlearn: 0.1160772\ttotal: 1m 15s\tremaining: 3m 16s\n",
      "277:\tlearn: 0.1160302\ttotal: 1m 15s\tremaining: 3m 16s\n",
      "278:\tlearn: 0.1159848\ttotal: 1m 15s\tremaining: 3m 15s\n",
      "279:\tlearn: 0.1158755\ttotal: 1m 16s\tremaining: 3m 15s\n",
      "280:\tlearn: 0.1157707\ttotal: 1m 16s\tremaining: 3m 15s\n",
      "281:\tlearn: 0.1157199\ttotal: 1m 16s\tremaining: 3m 15s\n",
      "282:\tlearn: 0.1156156\ttotal: 1m 16s\tremaining: 3m 14s\n",
      "283:\tlearn: 0.1154584\ttotal: 1m 17s\tremaining: 3m 14s\n",
      "284:\tlearn: 0.1153590\ttotal: 1m 17s\tremaining: 3m 14s\n",
      "285:\tlearn: 0.1152553\ttotal: 1m 17s\tremaining: 3m 13s\n",
      "286:\tlearn: 0.1152061\ttotal: 1m 17s\tremaining: 3m 13s\n",
      "287:\tlearn: 0.1151397\ttotal: 1m 18s\tremaining: 3m 13s\n",
      "288:\tlearn: 0.1150787\ttotal: 1m 18s\tremaining: 3m 13s\n",
      "289:\tlearn: 0.1149984\ttotal: 1m 18s\tremaining: 3m 12s\n",
      "290:\tlearn: 0.1149067\ttotal: 1m 19s\tremaining: 3m 12s\n",
      "291:\tlearn: 0.1148573\ttotal: 1m 19s\tremaining: 3m 12s\n",
      "292:\tlearn: 0.1147746\ttotal: 1m 19s\tremaining: 3m 11s\n",
      "293:\tlearn: 0.1146476\ttotal: 1m 19s\tremaining: 3m 11s\n",
      "294:\tlearn: 0.1145974\ttotal: 1m 20s\tremaining: 3m 11s\n",
      "295:\tlearn: 0.1145079\ttotal: 1m 20s\tremaining: 3m 11s\n",
      "296:\tlearn: 0.1144126\ttotal: 1m 20s\tremaining: 3m 10s\n",
      "297:\tlearn: 0.1143760\ttotal: 1m 20s\tremaining: 3m 10s\n",
      "298:\tlearn: 0.1143348\ttotal: 1m 21s\tremaining: 3m 10s\n",
      "299:\tlearn: 0.1142234\ttotal: 1m 21s\tremaining: 3m 9s\n",
      "300:\tlearn: 0.1141234\ttotal: 1m 21s\tremaining: 3m 9s\n",
      "301:\tlearn: 0.1140962\ttotal: 1m 21s\tremaining: 3m 9s\n",
      "302:\tlearn: 0.1140498\ttotal: 1m 22s\tremaining: 3m 8s\n",
      "303:\tlearn: 0.1139720\ttotal: 1m 22s\tremaining: 3m 8s\n",
      "304:\tlearn: 0.1138223\ttotal: 1m 22s\tremaining: 3m 8s\n",
      "305:\tlearn: 0.1137633\ttotal: 1m 22s\tremaining: 3m 8s\n",
      "306:\tlearn: 0.1136992\ttotal: 1m 23s\tremaining: 3m 7s\n",
      "307:\tlearn: 0.1136263\ttotal: 1m 23s\tremaining: 3m 7s\n",
      "308:\tlearn: 0.1135713\ttotal: 1m 23s\tremaining: 3m 7s\n",
      "309:\tlearn: 0.1135082\ttotal: 1m 24s\tremaining: 3m 6s\n",
      "310:\tlearn: 0.1134527\ttotal: 1m 24s\tremaining: 3m 6s\n",
      "311:\tlearn: 0.1133913\ttotal: 1m 24s\tremaining: 3m 6s\n",
      "312:\tlearn: 0.1133601\ttotal: 1m 24s\tremaining: 3m 6s\n",
      "313:\tlearn: 0.1132375\ttotal: 1m 25s\tremaining: 3m 5s\n",
      "314:\tlearn: 0.1131701\ttotal: 1m 25s\tremaining: 3m 5s\n",
      "315:\tlearn: 0.1130873\ttotal: 1m 25s\tremaining: 3m 5s\n",
      "316:\tlearn: 0.1130118\ttotal: 1m 25s\tremaining: 3m 4s\n",
      "317:\tlearn: 0.1129618\ttotal: 1m 26s\tremaining: 3m 4s\n",
      "318:\tlearn: 0.1128953\ttotal: 1m 26s\tremaining: 3m 4s\n",
      "319:\tlearn: 0.1128367\ttotal: 1m 26s\tremaining: 3m 4s\n",
      "320:\tlearn: 0.1127926\ttotal: 1m 26s\tremaining: 3m 3s\n",
      "321:\tlearn: 0.1127511\ttotal: 1m 27s\tremaining: 3m 3s\n",
      "322:\tlearn: 0.1126926\ttotal: 1m 27s\tremaining: 3m 3s\n",
      "323:\tlearn: 0.1126674\ttotal: 1m 27s\tremaining: 3m 2s\n",
      "324:\tlearn: 0.1126116\ttotal: 1m 27s\tremaining: 3m 2s\n",
      "325:\tlearn: 0.1125423\ttotal: 1m 28s\tremaining: 3m 2s\n",
      "326:\tlearn: 0.1124807\ttotal: 1m 28s\tremaining: 3m 2s\n",
      "327:\tlearn: 0.1123988\ttotal: 1m 28s\tremaining: 3m 1s\n",
      "328:\tlearn: 0.1123342\ttotal: 1m 28s\tremaining: 3m 1s\n",
      "329:\tlearn: 0.1122800\ttotal: 1m 29s\tremaining: 3m 1s\n",
      "330:\tlearn: 0.1122507\ttotal: 1m 29s\tremaining: 3m\n",
      "331:\tlearn: 0.1121745\ttotal: 1m 29s\tremaining: 3m\n",
      "332:\tlearn: 0.1121092\ttotal: 1m 30s\tremaining: 3m\n",
      "333:\tlearn: 0.1120350\ttotal: 1m 30s\tremaining: 3m\n",
      "334:\tlearn: 0.1119711\ttotal: 1m 30s\tremaining: 2m 59s\n",
      "335:\tlearn: 0.1119227\ttotal: 1m 30s\tremaining: 2m 59s\n",
      "336:\tlearn: 0.1118588\ttotal: 1m 31s\tremaining: 2m 59s\n",
      "337:\tlearn: 0.1117893\ttotal: 1m 31s\tremaining: 2m 59s\n",
      "338:\tlearn: 0.1117415\ttotal: 1m 31s\tremaining: 2m 58s\n",
      "339:\tlearn: 0.1116500\ttotal: 1m 31s\tremaining: 2m 58s\n",
      "340:\tlearn: 0.1116020\ttotal: 1m 32s\tremaining: 2m 58s\n",
      "341:\tlearn: 0.1115137\ttotal: 1m 32s\tremaining: 2m 57s\n",
      "342:\tlearn: 0.1114728\ttotal: 1m 32s\tremaining: 2m 57s\n",
      "343:\tlearn: 0.1114262\ttotal: 1m 32s\tremaining: 2m 57s\n",
      "344:\tlearn: 0.1113517\ttotal: 1m 33s\tremaining: 2m 57s\n",
      "345:\tlearn: 0.1112887\ttotal: 1m 33s\tremaining: 2m 56s\n",
      "346:\tlearn: 0.1112279\ttotal: 1m 33s\tremaining: 2m 56s\n",
      "347:\tlearn: 0.1111685\ttotal: 1m 34s\tremaining: 2m 56s\n",
      "348:\tlearn: 0.1111373\ttotal: 1m 34s\tremaining: 2m 55s\n",
      "349:\tlearn: 0.1110855\ttotal: 1m 34s\tremaining: 2m 55s\n",
      "350:\tlearn: 0.1110119\ttotal: 1m 34s\tremaining: 2m 55s\n",
      "351:\tlearn: 0.1109611\ttotal: 1m 35s\tremaining: 2m 55s\n",
      "352:\tlearn: 0.1109119\ttotal: 1m 35s\tremaining: 2m 54s\n",
      "353:\tlearn: 0.1108175\ttotal: 1m 35s\tremaining: 2m 54s\n",
      "354:\tlearn: 0.1107644\ttotal: 1m 35s\tremaining: 2m 54s\n",
      "355:\tlearn: 0.1106587\ttotal: 1m 36s\tremaining: 2m 53s\n",
      "356:\tlearn: 0.1105958\ttotal: 1m 36s\tremaining: 2m 53s\n",
      "357:\tlearn: 0.1105351\ttotal: 1m 36s\tremaining: 2m 53s\n",
      "358:\tlearn: 0.1104695\ttotal: 1m 36s\tremaining: 2m 53s\n",
      "359:\tlearn: 0.1104154\ttotal: 1m 37s\tremaining: 2m 52s\n",
      "360:\tlearn: 0.1103098\ttotal: 1m 37s\tremaining: 2m 52s\n",
      "361:\tlearn: 0.1102232\ttotal: 1m 37s\tremaining: 2m 52s\n",
      "362:\tlearn: 0.1101488\ttotal: 1m 37s\tremaining: 2m 51s\n",
      "363:\tlearn: 0.1100980\ttotal: 1m 38s\tremaining: 2m 51s\n",
      "364:\tlearn: 0.1100403\ttotal: 1m 38s\tremaining: 2m 51s\n",
      "365:\tlearn: 0.1099910\ttotal: 1m 38s\tremaining: 2m 51s\n",
      "366:\tlearn: 0.1099507\ttotal: 1m 39s\tremaining: 2m 50s\n",
      "367:\tlearn: 0.1098484\ttotal: 1m 39s\tremaining: 2m 50s\n",
      "368:\tlearn: 0.1098258\ttotal: 1m 39s\tremaining: 2m 50s\n",
      "369:\tlearn: 0.1097673\ttotal: 1m 39s\tremaining: 2m 50s\n",
      "370:\tlearn: 0.1096623\ttotal: 1m 40s\tremaining: 2m 49s\n",
      "371:\tlearn: 0.1096220\ttotal: 1m 40s\tremaining: 2m 49s\n",
      "372:\tlearn: 0.1095508\ttotal: 1m 40s\tremaining: 2m 49s\n",
      "373:\tlearn: 0.1095006\ttotal: 1m 40s\tremaining: 2m 48s\n",
      "374:\tlearn: 0.1094265\ttotal: 1m 41s\tremaining: 2m 48s\n",
      "375:\tlearn: 0.1093778\ttotal: 1m 41s\tremaining: 2m 48s\n",
      "376:\tlearn: 0.1092827\ttotal: 1m 41s\tremaining: 2m 48s\n",
      "377:\tlearn: 0.1092285\ttotal: 1m 42s\tremaining: 2m 47s\n",
      "378:\tlearn: 0.1091567\ttotal: 1m 42s\tremaining: 2m 47s\n",
      "379:\tlearn: 0.1090996\ttotal: 1m 42s\tremaining: 2m 47s\n",
      "380:\tlearn: 0.1090388\ttotal: 1m 42s\tremaining: 2m 47s\n",
      "381:\tlearn: 0.1090097\ttotal: 1m 43s\tremaining: 2m 46s\n",
      "382:\tlearn: 0.1089436\ttotal: 1m 43s\tremaining: 2m 46s\n",
      "383:\tlearn: 0.1089032\ttotal: 1m 43s\tremaining: 2m 46s\n",
      "384:\tlearn: 0.1088575\ttotal: 1m 43s\tremaining: 2m 45s\n",
      "385:\tlearn: 0.1088051\ttotal: 1m 44s\tremaining: 2m 45s\n",
      "386:\tlearn: 0.1087626\ttotal: 1m 44s\tremaining: 2m 45s\n",
      "387:\tlearn: 0.1087243\ttotal: 1m 44s\tremaining: 2m 45s\n",
      "388:\tlearn: 0.1086838\ttotal: 1m 44s\tremaining: 2m 44s\n",
      "389:\tlearn: 0.1086306\ttotal: 1m 45s\tremaining: 2m 44s\n",
      "390:\tlearn: 0.1086010\ttotal: 1m 45s\tremaining: 2m 44s\n",
      "391:\tlearn: 0.1085340\ttotal: 1m 45s\tremaining: 2m 44s\n",
      "392:\tlearn: 0.1084906\ttotal: 1m 46s\tremaining: 2m 43s\n",
      "393:\tlearn: 0.1083785\ttotal: 1m 46s\tremaining: 2m 43s\n",
      "394:\tlearn: 0.1083128\ttotal: 1m 46s\tremaining: 2m 43s\n",
      "395:\tlearn: 0.1082715\ttotal: 1m 46s\tremaining: 2m 42s\n",
      "396:\tlearn: 0.1081749\ttotal: 1m 47s\tremaining: 2m 42s\n",
      "397:\tlearn: 0.1081105\ttotal: 1m 47s\tremaining: 2m 42s\n",
      "398:\tlearn: 0.1079905\ttotal: 1m 47s\tremaining: 2m 42s\n",
      "399:\tlearn: 0.1079626\ttotal: 1m 47s\tremaining: 2m 41s\n",
      "400:\tlearn: 0.1078786\ttotal: 1m 48s\tremaining: 2m 41s\n",
      "401:\tlearn: 0.1078327\ttotal: 1m 48s\tremaining: 2m 41s\n",
      "402:\tlearn: 0.1077560\ttotal: 1m 48s\tremaining: 2m 41s\n",
      "403:\tlearn: 0.1076656\ttotal: 1m 48s\tremaining: 2m 40s\n",
      "404:\tlearn: 0.1076105\ttotal: 1m 49s\tremaining: 2m 40s\n",
      "405:\tlearn: 0.1075529\ttotal: 1m 49s\tremaining: 2m 40s\n",
      "406:\tlearn: 0.1075043\ttotal: 1m 49s\tremaining: 2m 40s\n",
      "407:\tlearn: 0.1074764\ttotal: 1m 50s\tremaining: 2m 39s\n",
      "408:\tlearn: 0.1074049\ttotal: 1m 50s\tremaining: 2m 39s\n",
      "409:\tlearn: 0.1073740\ttotal: 1m 50s\tremaining: 2m 39s\n",
      "410:\tlearn: 0.1073016\ttotal: 1m 50s\tremaining: 2m 38s\n",
      "411:\tlearn: 0.1072594\ttotal: 1m 51s\tremaining: 2m 38s\n",
      "412:\tlearn: 0.1072336\ttotal: 1m 51s\tremaining: 2m 38s\n",
      "413:\tlearn: 0.1071820\ttotal: 1m 51s\tremaining: 2m 38s\n",
      "414:\tlearn: 0.1071432\ttotal: 1m 51s\tremaining: 2m 37s\n",
      "415:\tlearn: 0.1070825\ttotal: 1m 52s\tremaining: 2m 37s\n",
      "416:\tlearn: 0.1070356\ttotal: 1m 52s\tremaining: 2m 37s\n",
      "417:\tlearn: 0.1069991\ttotal: 1m 52s\tremaining: 2m 37s\n",
      "418:\tlearn: 0.1069739\ttotal: 1m 53s\tremaining: 2m 36s\n",
      "419:\tlearn: 0.1069037\ttotal: 1m 53s\tremaining: 2m 36s\n",
      "420:\tlearn: 0.1068402\ttotal: 1m 53s\tremaining: 2m 36s\n",
      "421:\tlearn: 0.1067863\ttotal: 1m 53s\tremaining: 2m 36s\n",
      "422:\tlearn: 0.1067467\ttotal: 1m 54s\tremaining: 2m 35s\n",
      "423:\tlearn: 0.1066591\ttotal: 1m 54s\tremaining: 2m 35s\n",
      "424:\tlearn: 0.1066163\ttotal: 1m 54s\tremaining: 2m 35s\n",
      "425:\tlearn: 0.1065456\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "426:\tlearn: 0.1065125\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "427:\tlearn: 0.1064624\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "428:\tlearn: 0.1064084\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "429:\tlearn: 0.1063489\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "430:\tlearn: 0.1062816\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "431:\tlearn: 0.1062437\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "432:\tlearn: 0.1061687\ttotal: 1m 56s\tremaining: 2m 32s\n",
      "433:\tlearn: 0.1061339\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "434:\tlearn: 0.1060805\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "435:\tlearn: 0.1059968\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "436:\tlearn: 0.1059499\ttotal: 1m 57s\tremaining: 2m 31s\n",
      "437:\tlearn: 0.1059149\ttotal: 1m 58s\tremaining: 2m 31s\n",
      "438:\tlearn: 0.1058827\ttotal: 1m 58s\tremaining: 2m 31s\n",
      "439:\tlearn: 0.1058522\ttotal: 1m 58s\tremaining: 2m 31s\n",
      "440:\tlearn: 0.1057946\ttotal: 1m 58s\tremaining: 2m 30s\n",
      "441:\tlearn: 0.1057453\ttotal: 1m 59s\tremaining: 2m 30s\n",
      "442:\tlearn: 0.1057156\ttotal: 1m 59s\tremaining: 2m 30s\n",
      "443:\tlearn: 0.1056819\ttotal: 1m 59s\tremaining: 2m 29s\n",
      "444:\tlearn: 0.1056104\ttotal: 1m 59s\tremaining: 2m 29s\n",
      "445:\tlearn: 0.1055491\ttotal: 2m\tremaining: 2m 29s\n",
      "446:\tlearn: 0.1055290\ttotal: 2m\tremaining: 2m 29s\n",
      "447:\tlearn: 0.1054141\ttotal: 2m\tremaining: 2m 28s\n",
      "448:\tlearn: 0.1053639\ttotal: 2m\tremaining: 2m 28s\n",
      "449:\tlearn: 0.1053237\ttotal: 2m 1s\tremaining: 2m 28s\n",
      "450:\tlearn: 0.1052601\ttotal: 2m 1s\tremaining: 2m 27s\n",
      "451:\tlearn: 0.1051996\ttotal: 2m 1s\tremaining: 2m 27s\n",
      "452:\tlearn: 0.1051283\ttotal: 2m 2s\tremaining: 2m 27s\n",
      "453:\tlearn: 0.1050965\ttotal: 2m 2s\tremaining: 2m 27s\n",
      "454:\tlearn: 0.1050355\ttotal: 2m 2s\tremaining: 2m 27s\n",
      "455:\tlearn: 0.1049534\ttotal: 2m 3s\tremaining: 2m 26s\n",
      "456:\tlearn: 0.1049092\ttotal: 2m 3s\tremaining: 2m 26s\n",
      "457:\tlearn: 0.1048188\ttotal: 2m 3s\tremaining: 2m 26s\n",
      "458:\tlearn: 0.1047324\ttotal: 2m 3s\tremaining: 2m 25s\n",
      "459:\tlearn: 0.1046986\ttotal: 2m 4s\tremaining: 2m 25s\n",
      "460:\tlearn: 0.1046521\ttotal: 2m 4s\tremaining: 2m 25s\n",
      "461:\tlearn: 0.1045944\ttotal: 2m 4s\tremaining: 2m 25s\n",
      "462:\tlearn: 0.1045263\ttotal: 2m 4s\tremaining: 2m 24s\n",
      "463:\tlearn: 0.1045044\ttotal: 2m 5s\tremaining: 2m 24s\n",
      "464:\tlearn: 0.1044669\ttotal: 2m 5s\tremaining: 2m 24s\n",
      "465:\tlearn: 0.1044204\ttotal: 2m 5s\tremaining: 2m 24s\n",
      "466:\tlearn: 0.1043994\ttotal: 2m 6s\tremaining: 2m 23s\n",
      "467:\tlearn: 0.1043551\ttotal: 2m 6s\tremaining: 2m 23s\n",
      "468:\tlearn: 0.1042839\ttotal: 2m 6s\tremaining: 2m 23s\n",
      "469:\tlearn: 0.1042216\ttotal: 2m 6s\tremaining: 2m 23s\n",
      "470:\tlearn: 0.1041566\ttotal: 2m 7s\tremaining: 2m 22s\n",
      "471:\tlearn: 0.1040742\ttotal: 2m 7s\tremaining: 2m 22s\n",
      "472:\tlearn: 0.1040384\ttotal: 2m 7s\tremaining: 2m 22s\n",
      "473:\tlearn: 0.1039850\ttotal: 2m 7s\tremaining: 2m 21s\n",
      "474:\tlearn: 0.1039421\ttotal: 2m 8s\tremaining: 2m 21s\n",
      "475:\tlearn: 0.1039004\ttotal: 2m 8s\tremaining: 2m 21s\n",
      "476:\tlearn: 0.1038608\ttotal: 2m 8s\tremaining: 2m 21s\n",
      "477:\tlearn: 0.1038312\ttotal: 2m 8s\tremaining: 2m 20s\n",
      "478:\tlearn: 0.1037623\ttotal: 2m 9s\tremaining: 2m 20s\n",
      "479:\tlearn: 0.1037375\ttotal: 2m 9s\tremaining: 2m 20s\n",
      "480:\tlearn: 0.1036981\ttotal: 2m 9s\tremaining: 2m 19s\n",
      "481:\tlearn: 0.1036534\ttotal: 2m 10s\tremaining: 2m 19s\n",
      "482:\tlearn: 0.1036017\ttotal: 2m 10s\tremaining: 2m 19s\n",
      "483:\tlearn: 0.1035757\ttotal: 2m 10s\tremaining: 2m 19s\n",
      "484:\tlearn: 0.1035174\ttotal: 2m 10s\tremaining: 2m 18s\n",
      "485:\tlearn: 0.1034490\ttotal: 2m 11s\tremaining: 2m 18s\n",
      "486:\tlearn: 0.1034134\ttotal: 2m 11s\tremaining: 2m 18s\n",
      "487:\tlearn: 0.1033306\ttotal: 2m 11s\tremaining: 2m 18s\n",
      "488:\tlearn: 0.1033013\ttotal: 2m 12s\tremaining: 2m 17s\n",
      "489:\tlearn: 0.1032785\ttotal: 2m 12s\tremaining: 2m 17s\n",
      "490:\tlearn: 0.1032495\ttotal: 2m 12s\tremaining: 2m 17s\n",
      "491:\tlearn: 0.1032365\ttotal: 2m 12s\tremaining: 2m 17s\n",
      "492:\tlearn: 0.1031607\ttotal: 2m 13s\tremaining: 2m 16s\n",
      "493:\tlearn: 0.1031061\ttotal: 2m 13s\tremaining: 2m 16s\n",
      "494:\tlearn: 0.1030269\ttotal: 2m 13s\tremaining: 2m 16s\n",
      "495:\tlearn: 0.1029925\ttotal: 2m 13s\tremaining: 2m 16s\n",
      "496:\tlearn: 0.1029375\ttotal: 2m 14s\tremaining: 2m 15s\n",
      "497:\tlearn: 0.1028295\ttotal: 2m 14s\tremaining: 2m 15s\n",
      "498:\tlearn: 0.1027957\ttotal: 2m 14s\tremaining: 2m 15s\n",
      "499:\tlearn: 0.1027471\ttotal: 2m 15s\tremaining: 2m 15s\n",
      "500:\tlearn: 0.1027159\ttotal: 2m 15s\tremaining: 2m 14s\n",
      "501:\tlearn: 0.1026688\ttotal: 2m 15s\tremaining: 2m 14s\n",
      "502:\tlearn: 0.1026246\ttotal: 2m 15s\tremaining: 2m 14s\n",
      "503:\tlearn: 0.1025764\ttotal: 2m 16s\tremaining: 2m 13s\n",
      "504:\tlearn: 0.1025371\ttotal: 2m 16s\tremaining: 2m 13s\n",
      "505:\tlearn: 0.1024692\ttotal: 2m 16s\tremaining: 2m 13s\n",
      "506:\tlearn: 0.1023991\ttotal: 2m 16s\tremaining: 2m 13s\n",
      "507:\tlearn: 0.1023759\ttotal: 2m 17s\tremaining: 2m 12s\n",
      "508:\tlearn: 0.1023499\ttotal: 2m 17s\tremaining: 2m 12s\n",
      "509:\tlearn: 0.1023127\ttotal: 2m 17s\tremaining: 2m 12s\n",
      "510:\tlearn: 0.1022449\ttotal: 2m 17s\tremaining: 2m 11s\n",
      "511:\tlearn: 0.1022089\ttotal: 2m 18s\tremaining: 2m 11s\n",
      "512:\tlearn: 0.1021461\ttotal: 2m 18s\tremaining: 2m 11s\n",
      "513:\tlearn: 0.1020947\ttotal: 2m 18s\tremaining: 2m 11s\n",
      "514:\tlearn: 0.1020444\ttotal: 2m 19s\tremaining: 2m 10s\n",
      "515:\tlearn: 0.1020005\ttotal: 2m 19s\tremaining: 2m 10s\n",
      "516:\tlearn: 0.1019549\ttotal: 2m 19s\tremaining: 2m 10s\n",
      "517:\tlearn: 0.1018453\ttotal: 2m 19s\tremaining: 2m 10s\n",
      "518:\tlearn: 0.1017801\ttotal: 2m 20s\tremaining: 2m 9s\n",
      "519:\tlearn: 0.1017288\ttotal: 2m 20s\tremaining: 2m 9s\n",
      "520:\tlearn: 0.1016653\ttotal: 2m 20s\tremaining: 2m 9s\n",
      "521:\tlearn: 0.1015951\ttotal: 2m 20s\tremaining: 2m 8s\n",
      "522:\tlearn: 0.1015811\ttotal: 2m 21s\tremaining: 2m 8s\n",
      "523:\tlearn: 0.1015439\ttotal: 2m 21s\tremaining: 2m 8s\n",
      "524:\tlearn: 0.1014893\ttotal: 2m 21s\tremaining: 2m 8s\n",
      "525:\tlearn: 0.1014564\ttotal: 2m 21s\tremaining: 2m 7s\n",
      "526:\tlearn: 0.1014033\ttotal: 2m 22s\tremaining: 2m 7s\n",
      "527:\tlearn: 0.1013427\ttotal: 2m 22s\tremaining: 2m 7s\n",
      "528:\tlearn: 0.1013086\ttotal: 2m 22s\tremaining: 2m 7s\n",
      "529:\tlearn: 0.1012704\ttotal: 2m 23s\tremaining: 2m 6s\n",
      "530:\tlearn: 0.1012440\ttotal: 2m 23s\tremaining: 2m 6s\n",
      "531:\tlearn: 0.1011788\ttotal: 2m 23s\tremaining: 2m 6s\n",
      "532:\tlearn: 0.1011391\ttotal: 2m 23s\tremaining: 2m 6s\n",
      "533:\tlearn: 0.1011056\ttotal: 2m 24s\tremaining: 2m 5s\n",
      "534:\tlearn: 0.1010676\ttotal: 2m 24s\tremaining: 2m 5s\n",
      "535:\tlearn: 0.1010368\ttotal: 2m 24s\tremaining: 2m 5s\n",
      "536:\tlearn: 0.1009905\ttotal: 2m 24s\tremaining: 2m 4s\n",
      "537:\tlearn: 0.1009381\ttotal: 2m 25s\tremaining: 2m 4s\n",
      "538:\tlearn: 0.1008772\ttotal: 2m 25s\tremaining: 2m 4s\n",
      "539:\tlearn: 0.1008328\ttotal: 2m 25s\tremaining: 2m 4s\n",
      "540:\tlearn: 0.1008034\ttotal: 2m 25s\tremaining: 2m 3s\n",
      "541:\tlearn: 0.1007708\ttotal: 2m 26s\tremaining: 2m 3s\n",
      "542:\tlearn: 0.1007109\ttotal: 2m 26s\tremaining: 2m 3s\n",
      "543:\tlearn: 0.1006581\ttotal: 2m 26s\tremaining: 2m 2s\n",
      "544:\tlearn: 0.1006345\ttotal: 2m 26s\tremaining: 2m 2s\n",
      "545:\tlearn: 0.1005342\ttotal: 2m 27s\tremaining: 2m 2s\n",
      "546:\tlearn: 0.1004907\ttotal: 2m 27s\tremaining: 2m 2s\n",
      "547:\tlearn: 0.1004335\ttotal: 2m 27s\tremaining: 2m 1s\n",
      "548:\tlearn: 0.1003598\ttotal: 2m 28s\tremaining: 2m 1s\n",
      "549:\tlearn: 0.1003272\ttotal: 2m 28s\tremaining: 2m 1s\n",
      "550:\tlearn: 0.1002599\ttotal: 2m 28s\tremaining: 2m 1s\n",
      "551:\tlearn: 0.1002162\ttotal: 2m 28s\tremaining: 2m\n",
      "552:\tlearn: 0.1001839\ttotal: 2m 29s\tremaining: 2m\n",
      "553:\tlearn: 0.1001329\ttotal: 2m 29s\tremaining: 2m\n",
      "554:\tlearn: 0.1000744\ttotal: 2m 29s\tremaining: 2m\n",
      "555:\tlearn: 0.1000180\ttotal: 2m 30s\tremaining: 1m 59s\n",
      "556:\tlearn: 0.0999797\ttotal: 2m 30s\tremaining: 1m 59s\n",
      "557:\tlearn: 0.0999427\ttotal: 2m 30s\tremaining: 1m 59s\n",
      "558:\tlearn: 0.0998790\ttotal: 2m 30s\tremaining: 1m 58s\n",
      "559:\tlearn: 0.0998106\ttotal: 2m 31s\tremaining: 1m 58s\n",
      "560:\tlearn: 0.0997799\ttotal: 2m 31s\tremaining: 1m 58s\n",
      "561:\tlearn: 0.0997455\ttotal: 2m 31s\tremaining: 1m 58s\n",
      "562:\tlearn: 0.0996887\ttotal: 2m 31s\tremaining: 1m 57s\n",
      "563:\tlearn: 0.0996182\ttotal: 2m 32s\tremaining: 1m 57s\n",
      "564:\tlearn: 0.0995675\ttotal: 2m 32s\tremaining: 1m 57s\n",
      "565:\tlearn: 0.0995356\ttotal: 2m 32s\tremaining: 1m 56s\n",
      "566:\tlearn: 0.0994799\ttotal: 2m 32s\tremaining: 1m 56s\n",
      "567:\tlearn: 0.0994425\ttotal: 2m 33s\tremaining: 1m 56s\n",
      "568:\tlearn: 0.0994340\ttotal: 2m 33s\tremaining: 1m 56s\n",
      "569:\tlearn: 0.0993969\ttotal: 2m 33s\tremaining: 1m 55s\n",
      "570:\tlearn: 0.0993904\ttotal: 2m 33s\tremaining: 1m 55s\n",
      "571:\tlearn: 0.0993173\ttotal: 2m 34s\tremaining: 1m 55s\n",
      "572:\tlearn: 0.0993129\ttotal: 2m 34s\tremaining: 1m 55s\n",
      "573:\tlearn: 0.0992800\ttotal: 2m 34s\tremaining: 1m 54s\n",
      "574:\tlearn: 0.0992477\ttotal: 2m 34s\tremaining: 1m 54s\n",
      "575:\tlearn: 0.0991567\ttotal: 2m 35s\tremaining: 1m 54s\n",
      "576:\tlearn: 0.0991129\ttotal: 2m 35s\tremaining: 1m 54s\n",
      "577:\tlearn: 0.0990775\ttotal: 2m 35s\tremaining: 1m 53s\n",
      "578:\tlearn: 0.0990064\ttotal: 2m 36s\tremaining: 1m 53s\n",
      "579:\tlearn: 0.0989578\ttotal: 2m 36s\tremaining: 1m 53s\n",
      "580:\tlearn: 0.0988581\ttotal: 2m 36s\tremaining: 1m 52s\n",
      "581:\tlearn: 0.0988225\ttotal: 2m 36s\tremaining: 1m 52s\n",
      "582:\tlearn: 0.0987645\ttotal: 2m 37s\tremaining: 1m 52s\n",
      "583:\tlearn: 0.0987380\ttotal: 2m 37s\tremaining: 1m 52s\n",
      "584:\tlearn: 0.0986893\ttotal: 2m 37s\tremaining: 1m 51s\n",
      "585:\tlearn: 0.0986504\ttotal: 2m 37s\tremaining: 1m 51s\n",
      "586:\tlearn: 0.0986276\ttotal: 2m 38s\tremaining: 1m 51s\n",
      "587:\tlearn: 0.0985672\ttotal: 2m 38s\tremaining: 1m 51s\n",
      "588:\tlearn: 0.0985364\ttotal: 2m 38s\tremaining: 1m 50s\n",
      "589:\tlearn: 0.0984643\ttotal: 2m 38s\tremaining: 1m 50s\n",
      "590:\tlearn: 0.0984330\ttotal: 2m 39s\tremaining: 1m 50s\n",
      "591:\tlearn: 0.0983840\ttotal: 2m 39s\tremaining: 1m 49s\n",
      "592:\tlearn: 0.0983383\ttotal: 2m 39s\tremaining: 1m 49s\n",
      "593:\tlearn: 0.0983156\ttotal: 2m 39s\tremaining: 1m 49s\n",
      "594:\tlearn: 0.0982568\ttotal: 2m 40s\tremaining: 1m 49s\n",
      "595:\tlearn: 0.0982201\ttotal: 2m 40s\tremaining: 1m 48s\n",
      "596:\tlearn: 0.0981006\ttotal: 2m 40s\tremaining: 1m 48s\n",
      "597:\tlearn: 0.0980024\ttotal: 2m 40s\tremaining: 1m 48s\n",
      "598:\tlearn: 0.0979606\ttotal: 2m 41s\tremaining: 1m 47s\n",
      "599:\tlearn: 0.0979308\ttotal: 2m 41s\tremaining: 1m 47s\n",
      "600:\tlearn: 0.0978551\ttotal: 2m 41s\tremaining: 1m 47s\n",
      "601:\tlearn: 0.0977983\ttotal: 2m 42s\tremaining: 1m 47s\n",
      "602:\tlearn: 0.0977585\ttotal: 2m 42s\tremaining: 1m 46s\n",
      "603:\tlearn: 0.0977313\ttotal: 2m 42s\tremaining: 1m 46s\n",
      "604:\tlearn: 0.0976764\ttotal: 2m 42s\tremaining: 1m 46s\n",
      "605:\tlearn: 0.0976447\ttotal: 2m 43s\tremaining: 1m 46s\n",
      "606:\tlearn: 0.0975670\ttotal: 2m 43s\tremaining: 1m 45s\n",
      "607:\tlearn: 0.0975174\ttotal: 2m 43s\tremaining: 1m 45s\n",
      "608:\tlearn: 0.0974784\ttotal: 2m 44s\tremaining: 1m 45s\n",
      "609:\tlearn: 0.0974367\ttotal: 2m 44s\tremaining: 1m 45s\n",
      "610:\tlearn: 0.0974011\ttotal: 2m 44s\tremaining: 1m 44s\n",
      "611:\tlearn: 0.0973413\ttotal: 2m 44s\tremaining: 1m 44s\n",
      "612:\tlearn: 0.0973054\ttotal: 2m 45s\tremaining: 1m 44s\n",
      "613:\tlearn: 0.0972431\ttotal: 2m 45s\tremaining: 1m 43s\n",
      "614:\tlearn: 0.0971858\ttotal: 2m 45s\tremaining: 1m 43s\n",
      "615:\tlearn: 0.0971559\ttotal: 2m 46s\tremaining: 1m 43s\n",
      "616:\tlearn: 0.0971142\ttotal: 2m 46s\tremaining: 1m 43s\n",
      "617:\tlearn: 0.0970813\ttotal: 2m 46s\tremaining: 1m 43s\n",
      "618:\tlearn: 0.0970311\ttotal: 2m 46s\tremaining: 1m 42s\n",
      "619:\tlearn: 0.0969947\ttotal: 2m 47s\tremaining: 1m 42s\n",
      "620:\tlearn: 0.0969368\ttotal: 2m 47s\tremaining: 1m 42s\n",
      "621:\tlearn: 0.0969087\ttotal: 2m 47s\tremaining: 1m 41s\n",
      "622:\tlearn: 0.0968675\ttotal: 2m 48s\tremaining: 1m 41s\n",
      "623:\tlearn: 0.0968293\ttotal: 2m 48s\tremaining: 1m 41s\n",
      "624:\tlearn: 0.0968152\ttotal: 2m 48s\tremaining: 1m 41s\n",
      "625:\tlearn: 0.0967530\ttotal: 2m 48s\tremaining: 1m 40s\n",
      "626:\tlearn: 0.0967320\ttotal: 2m 49s\tremaining: 1m 40s\n",
      "627:\tlearn: 0.0967106\ttotal: 2m 49s\tremaining: 1m 40s\n",
      "628:\tlearn: 0.0966751\ttotal: 2m 49s\tremaining: 1m 40s\n",
      "629:\tlearn: 0.0966219\ttotal: 2m 49s\tremaining: 1m 39s\n",
      "630:\tlearn: 0.0965715\ttotal: 2m 50s\tremaining: 1m 39s\n",
      "631:\tlearn: 0.0965073\ttotal: 2m 50s\tremaining: 1m 39s\n",
      "632:\tlearn: 0.0964736\ttotal: 2m 50s\tremaining: 1m 39s\n",
      "633:\tlearn: 0.0964435\ttotal: 2m 51s\tremaining: 1m 38s\n",
      "634:\tlearn: 0.0964125\ttotal: 2m 51s\tremaining: 1m 38s\n",
      "635:\tlearn: 0.0963376\ttotal: 2m 51s\tremaining: 1m 38s\n",
      "636:\tlearn: 0.0962763\ttotal: 2m 51s\tremaining: 1m 37s\n",
      "637:\tlearn: 0.0962404\ttotal: 2m 52s\tremaining: 1m 37s\n",
      "638:\tlearn: 0.0962014\ttotal: 2m 52s\tremaining: 1m 37s\n",
      "639:\tlearn: 0.0961649\ttotal: 2m 52s\tremaining: 1m 37s\n",
      "640:\tlearn: 0.0961238\ttotal: 2m 52s\tremaining: 1m 36s\n",
      "641:\tlearn: 0.0960871\ttotal: 2m 53s\tremaining: 1m 36s\n",
      "642:\tlearn: 0.0960583\ttotal: 2m 53s\tremaining: 1m 36s\n",
      "643:\tlearn: 0.0960261\ttotal: 2m 53s\tremaining: 1m 35s\n",
      "644:\tlearn: 0.0960073\ttotal: 2m 53s\tremaining: 1m 35s\n",
      "645:\tlearn: 0.0959679\ttotal: 2m 54s\tremaining: 1m 35s\n",
      "646:\tlearn: 0.0958947\ttotal: 2m 54s\tremaining: 1m 35s\n",
      "647:\tlearn: 0.0958533\ttotal: 2m 54s\tremaining: 1m 34s\n",
      "648:\tlearn: 0.0958201\ttotal: 2m 54s\tremaining: 1m 34s\n",
      "649:\tlearn: 0.0958032\ttotal: 2m 55s\tremaining: 1m 34s\n",
      "650:\tlearn: 0.0957492\ttotal: 2m 55s\tremaining: 1m 33s\n",
      "651:\tlearn: 0.0956818\ttotal: 2m 55s\tremaining: 1m 33s\n",
      "652:\tlearn: 0.0956437\ttotal: 2m 55s\tremaining: 1m 33s\n",
      "653:\tlearn: 0.0955942\ttotal: 2m 56s\tremaining: 1m 33s\n",
      "654:\tlearn: 0.0955281\ttotal: 2m 56s\tremaining: 1m 32s\n",
      "655:\tlearn: 0.0954924\ttotal: 2m 56s\tremaining: 1m 32s\n",
      "656:\tlearn: 0.0954477\ttotal: 2m 56s\tremaining: 1m 32s\n",
      "657:\tlearn: 0.0954059\ttotal: 2m 57s\tremaining: 1m 32s\n",
      "658:\tlearn: 0.0953785\ttotal: 2m 57s\tremaining: 1m 31s\n",
      "659:\tlearn: 0.0953513\ttotal: 2m 57s\tremaining: 1m 31s\n",
      "660:\tlearn: 0.0952824\ttotal: 2m 57s\tremaining: 1m 31s\n",
      "661:\tlearn: 0.0951998\ttotal: 2m 58s\tremaining: 1m 30s\n",
      "662:\tlearn: 0.0951800\ttotal: 2m 58s\tremaining: 1m 30s\n",
      "663:\tlearn: 0.0951591\ttotal: 2m 58s\tremaining: 1m 30s\n",
      "664:\tlearn: 0.0951090\ttotal: 2m 58s\tremaining: 1m 30s\n",
      "665:\tlearn: 0.0950640\ttotal: 2m 59s\tremaining: 1m 29s\n",
      "666:\tlearn: 0.0950272\ttotal: 2m 59s\tremaining: 1m 29s\n",
      "667:\tlearn: 0.0949712\ttotal: 2m 59s\tremaining: 1m 29s\n",
      "668:\tlearn: 0.0949329\ttotal: 3m\tremaining: 1m 29s\n",
      "669:\tlearn: 0.0948863\ttotal: 3m\tremaining: 1m 28s\n",
      "670:\tlearn: 0.0948606\ttotal: 3m\tremaining: 1m 28s\n",
      "671:\tlearn: 0.0948250\ttotal: 3m\tremaining: 1m 28s\n",
      "672:\tlearn: 0.0947831\ttotal: 3m 1s\tremaining: 1m 28s\n",
      "673:\tlearn: 0.0947230\ttotal: 3m 1s\tremaining: 1m 27s\n",
      "674:\tlearn: 0.0946493\ttotal: 3m 1s\tremaining: 1m 27s\n",
      "675:\tlearn: 0.0946056\ttotal: 3m 1s\tremaining: 1m 27s\n",
      "676:\tlearn: 0.0945638\ttotal: 3m 2s\tremaining: 1m 26s\n",
      "677:\tlearn: 0.0945057\ttotal: 3m 2s\tremaining: 1m 26s\n",
      "678:\tlearn: 0.0944709\ttotal: 3m 2s\tremaining: 1m 26s\n",
      "679:\tlearn: 0.0944599\ttotal: 3m 2s\tremaining: 1m 26s\n",
      "680:\tlearn: 0.0944424\ttotal: 3m 3s\tremaining: 1m 25s\n",
      "681:\tlearn: 0.0944201\ttotal: 3m 3s\tremaining: 1m 25s\n",
      "682:\tlearn: 0.0943819\ttotal: 3m 3s\tremaining: 1m 25s\n",
      "683:\tlearn: 0.0943513\ttotal: 3m 4s\tremaining: 1m 25s\n",
      "684:\tlearn: 0.0943174\ttotal: 3m 4s\tremaining: 1m 24s\n",
      "685:\tlearn: 0.0942904\ttotal: 3m 4s\tremaining: 1m 24s\n",
      "686:\tlearn: 0.0942176\ttotal: 3m 4s\tremaining: 1m 24s\n",
      "687:\tlearn: 0.0941677\ttotal: 3m 5s\tremaining: 1m 23s\n",
      "688:\tlearn: 0.0941117\ttotal: 3m 5s\tremaining: 1m 23s\n",
      "689:\tlearn: 0.0940397\ttotal: 3m 5s\tremaining: 1m 23s\n",
      "690:\tlearn: 0.0940148\ttotal: 3m 5s\tremaining: 1m 23s\n",
      "691:\tlearn: 0.0939691\ttotal: 3m 6s\tremaining: 1m 22s\n",
      "692:\tlearn: 0.0939112\ttotal: 3m 6s\tremaining: 1m 22s\n",
      "693:\tlearn: 0.0938627\ttotal: 3m 6s\tremaining: 1m 22s\n",
      "694:\tlearn: 0.0937958\ttotal: 3m 7s\tremaining: 1m 22s\n",
      "695:\tlearn: 0.0937636\ttotal: 3m 7s\tremaining: 1m 21s\n",
      "696:\tlearn: 0.0937218\ttotal: 3m 7s\tremaining: 1m 21s\n",
      "697:\tlearn: 0.0936804\ttotal: 3m 7s\tremaining: 1m 21s\n",
      "698:\tlearn: 0.0936484\ttotal: 3m 8s\tremaining: 1m 21s\n",
      "699:\tlearn: 0.0936227\ttotal: 3m 8s\tremaining: 1m 20s\n",
      "700:\tlearn: 0.0935692\ttotal: 3m 8s\tremaining: 1m 20s\n",
      "701:\tlearn: 0.0935391\ttotal: 3m 8s\tremaining: 1m 20s\n",
      "702:\tlearn: 0.0934861\ttotal: 3m 9s\tremaining: 1m 19s\n",
      "703:\tlearn: 0.0934128\ttotal: 3m 9s\tremaining: 1m 19s\n",
      "704:\tlearn: 0.0933859\ttotal: 3m 9s\tremaining: 1m 19s\n",
      "705:\tlearn: 0.0933396\ttotal: 3m 10s\tremaining: 1m 19s\n",
      "706:\tlearn: 0.0932848\ttotal: 3m 10s\tremaining: 1m 18s\n",
      "707:\tlearn: 0.0932376\ttotal: 3m 10s\tremaining: 1m 18s\n",
      "708:\tlearn: 0.0932030\ttotal: 3m 10s\tremaining: 1m 18s\n",
      "709:\tlearn: 0.0931718\ttotal: 3m 11s\tremaining: 1m 18s\n",
      "710:\tlearn: 0.0931495\ttotal: 3m 11s\tremaining: 1m 17s\n",
      "711:\tlearn: 0.0931269\ttotal: 3m 11s\tremaining: 1m 17s\n",
      "712:\tlearn: 0.0931025\ttotal: 3m 11s\tremaining: 1m 17s\n",
      "713:\tlearn: 0.0930660\ttotal: 3m 12s\tremaining: 1m 16s\n",
      "714:\tlearn: 0.0929932\ttotal: 3m 12s\tremaining: 1m 16s\n",
      "715:\tlearn: 0.0929756\ttotal: 3m 12s\tremaining: 1m 16s\n",
      "716:\tlearn: 0.0929110\ttotal: 3m 12s\tremaining: 1m 16s\n",
      "717:\tlearn: 0.0928837\ttotal: 3m 13s\tremaining: 1m 15s\n",
      "718:\tlearn: 0.0928520\ttotal: 3m 13s\tremaining: 1m 15s\n",
      "719:\tlearn: 0.0928158\ttotal: 3m 13s\tremaining: 1m 15s\n",
      "720:\tlearn: 0.0927967\ttotal: 3m 13s\tremaining: 1m 15s\n",
      "721:\tlearn: 0.0927799\ttotal: 3m 14s\tremaining: 1m 14s\n",
      "722:\tlearn: 0.0927345\ttotal: 3m 14s\tremaining: 1m 14s\n",
      "723:\tlearn: 0.0926273\ttotal: 3m 14s\tremaining: 1m 14s\n",
      "724:\tlearn: 0.0925892\ttotal: 3m 14s\tremaining: 1m 13s\n",
      "725:\tlearn: 0.0925242\ttotal: 3m 15s\tremaining: 1m 13s\n",
      "726:\tlearn: 0.0924353\ttotal: 3m 15s\tremaining: 1m 13s\n",
      "727:\tlearn: 0.0924290\ttotal: 3m 15s\tremaining: 1m 13s\n",
      "728:\tlearn: 0.0923741\ttotal: 3m 16s\tremaining: 1m 12s\n",
      "729:\tlearn: 0.0923448\ttotal: 3m 16s\tremaining: 1m 12s\n",
      "730:\tlearn: 0.0922978\ttotal: 3m 16s\tremaining: 1m 12s\n",
      "731:\tlearn: 0.0922760\ttotal: 3m 16s\tremaining: 1m 12s\n",
      "732:\tlearn: 0.0922429\ttotal: 3m 17s\tremaining: 1m 11s\n",
      "733:\tlearn: 0.0922401\ttotal: 3m 17s\tremaining: 1m 11s\n",
      "734:\tlearn: 0.0921987\ttotal: 3m 17s\tremaining: 1m 11s\n",
      "735:\tlearn: 0.0921696\ttotal: 3m 17s\tremaining: 1m 10s\n",
      "736:\tlearn: 0.0920878\ttotal: 3m 18s\tremaining: 1m 10s\n",
      "737:\tlearn: 0.0920571\ttotal: 3m 18s\tremaining: 1m 10s\n",
      "738:\tlearn: 0.0920269\ttotal: 3m 18s\tremaining: 1m 10s\n",
      "739:\tlearn: 0.0919865\ttotal: 3m 18s\tremaining: 1m 9s\n",
      "740:\tlearn: 0.0919611\ttotal: 3m 19s\tremaining: 1m 9s\n",
      "741:\tlearn: 0.0919072\ttotal: 3m 19s\tremaining: 1m 9s\n",
      "742:\tlearn: 0.0918520\ttotal: 3m 19s\tremaining: 1m 9s\n",
      "743:\tlearn: 0.0918175\ttotal: 3m 19s\tremaining: 1m 8s\n",
      "744:\tlearn: 0.0917964\ttotal: 3m 20s\tremaining: 1m 8s\n",
      "745:\tlearn: 0.0917680\ttotal: 3m 20s\tremaining: 1m 8s\n",
      "746:\tlearn: 0.0917012\ttotal: 3m 20s\tremaining: 1m 7s\n",
      "747:\tlearn: 0.0916977\ttotal: 3m 21s\tremaining: 1m 7s\n",
      "748:\tlearn: 0.0916691\ttotal: 3m 21s\tremaining: 1m 7s\n",
      "749:\tlearn: 0.0916438\ttotal: 3m 21s\tremaining: 1m 7s\n",
      "750:\tlearn: 0.0916377\ttotal: 3m 21s\tremaining: 1m 6s\n",
      "751:\tlearn: 0.0916162\ttotal: 3m 22s\tremaining: 1m 6s\n",
      "752:\tlearn: 0.0915852\ttotal: 3m 22s\tremaining: 1m 6s\n",
      "753:\tlearn: 0.0915599\ttotal: 3m 22s\tremaining: 1m 6s\n",
      "754:\tlearn: 0.0915227\ttotal: 3m 23s\tremaining: 1m 5s\n",
      "755:\tlearn: 0.0914872\ttotal: 3m 23s\tremaining: 1m 5s\n",
      "756:\tlearn: 0.0914532\ttotal: 3m 23s\tremaining: 1m 5s\n",
      "757:\tlearn: 0.0914128\ttotal: 3m 23s\tremaining: 1m 5s\n",
      "758:\tlearn: 0.0913872\ttotal: 3m 24s\tremaining: 1m 4s\n",
      "759:\tlearn: 0.0913582\ttotal: 3m 24s\tremaining: 1m 4s\n",
      "760:\tlearn: 0.0913433\ttotal: 3m 24s\tremaining: 1m 4s\n",
      "761:\tlearn: 0.0913061\ttotal: 3m 24s\tremaining: 1m 4s\n",
      "762:\tlearn: 0.0912418\ttotal: 3m 25s\tremaining: 1m 3s\n",
      "763:\tlearn: 0.0912120\ttotal: 3m 25s\tremaining: 1m 3s\n",
      "764:\tlearn: 0.0911883\ttotal: 3m 25s\tremaining: 1m 3s\n",
      "765:\tlearn: 0.0911420\ttotal: 3m 26s\tremaining: 1m 2s\n",
      "766:\tlearn: 0.0911193\ttotal: 3m 26s\tremaining: 1m 2s\n",
      "767:\tlearn: 0.0911043\ttotal: 3m 26s\tremaining: 1m 2s\n",
      "768:\tlearn: 0.0910456\ttotal: 3m 26s\tremaining: 1m 2s\n",
      "769:\tlearn: 0.0909986\ttotal: 3m 27s\tremaining: 1m 1s\n",
      "770:\tlearn: 0.0909682\ttotal: 3m 27s\tremaining: 1m 1s\n",
      "771:\tlearn: 0.0909334\ttotal: 3m 27s\tremaining: 1m 1s\n",
      "772:\tlearn: 0.0908916\ttotal: 3m 28s\tremaining: 1m 1s\n",
      "773:\tlearn: 0.0908536\ttotal: 3m 28s\tremaining: 1m\n",
      "774:\tlearn: 0.0908342\ttotal: 3m 28s\tremaining: 1m\n",
      "775:\tlearn: 0.0907767\ttotal: 3m 28s\tremaining: 1m\n",
      "776:\tlearn: 0.0907206\ttotal: 3m 29s\tremaining: 1m\n",
      "777:\tlearn: 0.0906985\ttotal: 3m 29s\tremaining: 59.8s\n",
      "778:\tlearn: 0.0906247\ttotal: 3m 29s\tremaining: 59.5s\n",
      "779:\tlearn: 0.0906098\ttotal: 3m 29s\tremaining: 59.2s\n",
      "780:\tlearn: 0.0905738\ttotal: 3m 30s\tremaining: 58.9s\n",
      "781:\tlearn: 0.0905290\ttotal: 3m 30s\tremaining: 58.7s\n",
      "782:\tlearn: 0.0905119\ttotal: 3m 30s\tremaining: 58.4s\n",
      "783:\tlearn: 0.0904733\ttotal: 3m 30s\tremaining: 58.1s\n",
      "784:\tlearn: 0.0904468\ttotal: 3m 31s\tremaining: 57.8s\n",
      "785:\tlearn: 0.0904204\ttotal: 3m 31s\tremaining: 57.5s\n",
      "786:\tlearn: 0.0903685\ttotal: 3m 31s\tremaining: 57.3s\n",
      "787:\tlearn: 0.0903409\ttotal: 3m 31s\tremaining: 57s\n",
      "788:\tlearn: 0.0903101\ttotal: 3m 32s\tremaining: 56.8s\n",
      "789:\tlearn: 0.0902723\ttotal: 3m 32s\tremaining: 56.5s\n",
      "790:\tlearn: 0.0902224\ttotal: 3m 32s\tremaining: 56.2s\n",
      "791:\tlearn: 0.0902093\ttotal: 3m 33s\tremaining: 56s\n",
      "792:\tlearn: 0.0901659\ttotal: 3m 33s\tremaining: 55.7s\n",
      "793:\tlearn: 0.0901398\ttotal: 3m 33s\tremaining: 55.4s\n",
      "794:\tlearn: 0.0901014\ttotal: 3m 33s\tremaining: 55.2s\n",
      "795:\tlearn: 0.0900749\ttotal: 3m 34s\tremaining: 54.9s\n",
      "796:\tlearn: 0.0900416\ttotal: 3m 34s\tremaining: 54.6s\n",
      "797:\tlearn: 0.0900027\ttotal: 3m 34s\tremaining: 54.3s\n",
      "798:\tlearn: 0.0899412\ttotal: 3m 34s\tremaining: 54.1s\n",
      "799:\tlearn: 0.0899085\ttotal: 3m 35s\tremaining: 53.8s\n",
      "800:\tlearn: 0.0898666\ttotal: 3m 35s\tremaining: 53.5s\n",
      "801:\tlearn: 0.0898467\ttotal: 3m 35s\tremaining: 53.2s\n",
      "802:\tlearn: 0.0898042\ttotal: 3m 35s\tremaining: 53s\n",
      "803:\tlearn: 0.0897477\ttotal: 3m 36s\tremaining: 52.7s\n",
      "804:\tlearn: 0.0897007\ttotal: 3m 36s\tremaining: 52.5s\n",
      "805:\tlearn: 0.0896674\ttotal: 3m 36s\tremaining: 52.2s\n",
      "806:\tlearn: 0.0896392\ttotal: 3m 37s\tremaining: 51.9s\n",
      "807:\tlearn: 0.0895674\ttotal: 3m 37s\tremaining: 51.7s\n",
      "808:\tlearn: 0.0895247\ttotal: 3m 37s\tremaining: 51.4s\n",
      "809:\tlearn: 0.0895077\ttotal: 3m 38s\tremaining: 51.2s\n",
      "810:\tlearn: 0.0894948\ttotal: 3m 38s\tremaining: 50.9s\n",
      "811:\tlearn: 0.0894802\ttotal: 3m 38s\tremaining: 50.6s\n",
      "812:\tlearn: 0.0894446\ttotal: 3m 39s\tremaining: 50.4s\n",
      "813:\tlearn: 0.0893782\ttotal: 3m 39s\tremaining: 50.1s\n",
      "814:\tlearn: 0.0893438\ttotal: 3m 39s\tremaining: 49.9s\n",
      "815:\tlearn: 0.0893300\ttotal: 3m 39s\tremaining: 49.6s\n",
      "816:\tlearn: 0.0892710\ttotal: 3m 40s\tremaining: 49.3s\n",
      "817:\tlearn: 0.0892491\ttotal: 3m 40s\tremaining: 49.1s\n",
      "818:\tlearn: 0.0892078\ttotal: 3m 40s\tremaining: 48.8s\n",
      "819:\tlearn: 0.0891679\ttotal: 3m 41s\tremaining: 48.5s\n",
      "820:\tlearn: 0.0891060\ttotal: 3m 41s\tremaining: 48.3s\n",
      "821:\tlearn: 0.0890688\ttotal: 3m 41s\tremaining: 48s\n",
      "822:\tlearn: 0.0890200\ttotal: 3m 42s\tremaining: 47.8s\n",
      "823:\tlearn: 0.0889724\ttotal: 3m 42s\tremaining: 47.5s\n",
      "824:\tlearn: 0.0889286\ttotal: 3m 42s\tremaining: 47.2s\n",
      "825:\tlearn: 0.0888798\ttotal: 3m 42s\tremaining: 47s\n",
      "826:\tlearn: 0.0888620\ttotal: 3m 43s\tremaining: 46.7s\n",
      "827:\tlearn: 0.0888487\ttotal: 3m 43s\tremaining: 46.4s\n",
      "828:\tlearn: 0.0888042\ttotal: 3m 43s\tremaining: 46.1s\n",
      "829:\tlearn: 0.0887557\ttotal: 3m 43s\tremaining: 45.9s\n",
      "830:\tlearn: 0.0887354\ttotal: 3m 44s\tremaining: 45.6s\n",
      "831:\tlearn: 0.0887068\ttotal: 3m 44s\tremaining: 45.3s\n",
      "832:\tlearn: 0.0886765\ttotal: 3m 44s\tremaining: 45.1s\n",
      "833:\tlearn: 0.0886429\ttotal: 3m 44s\tremaining: 44.8s\n",
      "834:\tlearn: 0.0886044\ttotal: 3m 45s\tremaining: 44.5s\n",
      "835:\tlearn: 0.0885849\ttotal: 3m 45s\tremaining: 44.2s\n",
      "836:\tlearn: 0.0885438\ttotal: 3m 45s\tremaining: 44s\n",
      "837:\tlearn: 0.0885057\ttotal: 3m 46s\tremaining: 43.7s\n",
      "838:\tlearn: 0.0884856\ttotal: 3m 46s\tremaining: 43.4s\n",
      "839:\tlearn: 0.0884539\ttotal: 3m 46s\tremaining: 43.2s\n",
      "840:\tlearn: 0.0884065\ttotal: 3m 46s\tremaining: 42.9s\n",
      "841:\tlearn: 0.0884019\ttotal: 3m 47s\tremaining: 42.6s\n",
      "842:\tlearn: 0.0883532\ttotal: 3m 47s\tremaining: 42.3s\n",
      "843:\tlearn: 0.0883188\ttotal: 3m 47s\tremaining: 42.1s\n",
      "844:\tlearn: 0.0882926\ttotal: 3m 47s\tremaining: 41.8s\n",
      "845:\tlearn: 0.0882807\ttotal: 3m 48s\tremaining: 41.5s\n",
      "846:\tlearn: 0.0882510\ttotal: 3m 48s\tremaining: 41.2s\n",
      "847:\tlearn: 0.0881924\ttotal: 3m 48s\tremaining: 41s\n",
      "848:\tlearn: 0.0881318\ttotal: 3m 48s\tremaining: 40.7s\n",
      "849:\tlearn: 0.0880821\ttotal: 3m 49s\tremaining: 40.4s\n",
      "850:\tlearn: 0.0880777\ttotal: 3m 49s\tremaining: 40.2s\n",
      "851:\tlearn: 0.0880346\ttotal: 3m 49s\tremaining: 39.9s\n",
      "852:\tlearn: 0.0879672\ttotal: 3m 50s\tremaining: 39.6s\n",
      "853:\tlearn: 0.0879460\ttotal: 3m 50s\tremaining: 39.4s\n",
      "854:\tlearn: 0.0879068\ttotal: 3m 50s\tremaining: 39.1s\n",
      "855:\tlearn: 0.0878873\ttotal: 3m 50s\tremaining: 38.8s\n",
      "856:\tlearn: 0.0878673\ttotal: 3m 51s\tremaining: 38.6s\n",
      "857:\tlearn: 0.0878380\ttotal: 3m 51s\tremaining: 38.3s\n",
      "858:\tlearn: 0.0877933\ttotal: 3m 51s\tremaining: 38s\n",
      "859:\tlearn: 0.0877585\ttotal: 3m 51s\tremaining: 37.7s\n",
      "860:\tlearn: 0.0876928\ttotal: 3m 52s\tremaining: 37.5s\n",
      "861:\tlearn: 0.0876518\ttotal: 3m 52s\tremaining: 37.2s\n",
      "862:\tlearn: 0.0876340\ttotal: 3m 52s\tremaining: 36.9s\n",
      "863:\tlearn: 0.0876022\ttotal: 3m 52s\tremaining: 36.7s\n",
      "864:\tlearn: 0.0875567\ttotal: 3m 53s\tremaining: 36.4s\n",
      "865:\tlearn: 0.0875298\ttotal: 3m 53s\tremaining: 36.1s\n",
      "866:\tlearn: 0.0874589\ttotal: 3m 53s\tremaining: 35.8s\n",
      "867:\tlearn: 0.0874488\ttotal: 3m 53s\tremaining: 35.6s\n",
      "868:\tlearn: 0.0874302\ttotal: 3m 54s\tremaining: 35.3s\n",
      "869:\tlearn: 0.0873739\ttotal: 3m 54s\tremaining: 35s\n",
      "870:\tlearn: 0.0873128\ttotal: 3m 54s\tremaining: 34.7s\n",
      "871:\tlearn: 0.0872628\ttotal: 3m 54s\tremaining: 34.5s\n",
      "872:\tlearn: 0.0872413\ttotal: 3m 55s\tremaining: 34.2s\n",
      "873:\tlearn: 0.0871881\ttotal: 3m 55s\tremaining: 33.9s\n",
      "874:\tlearn: 0.0871525\ttotal: 3m 55s\tremaining: 33.7s\n",
      "875:\tlearn: 0.0871001\ttotal: 3m 55s\tremaining: 33.4s\n",
      "876:\tlearn: 0.0870854\ttotal: 3m 56s\tremaining: 33.1s\n",
      "877:\tlearn: 0.0870529\ttotal: 3m 56s\tremaining: 32.8s\n",
      "878:\tlearn: 0.0870398\ttotal: 3m 56s\tremaining: 32.6s\n",
      "879:\tlearn: 0.0870201\ttotal: 3m 56s\tremaining: 32.3s\n",
      "880:\tlearn: 0.0870161\ttotal: 3m 57s\tremaining: 32s\n",
      "881:\tlearn: 0.0869962\ttotal: 3m 57s\tremaining: 31.8s\n",
      "882:\tlearn: 0.0869067\ttotal: 3m 57s\tremaining: 31.5s\n",
      "883:\tlearn: 0.0868778\ttotal: 3m 58s\tremaining: 31.2s\n",
      "884:\tlearn: 0.0868585\ttotal: 3m 58s\tremaining: 31s\n",
      "885:\tlearn: 0.0868279\ttotal: 3m 58s\tremaining: 30.7s\n",
      "886:\tlearn: 0.0867931\ttotal: 3m 58s\tremaining: 30.4s\n",
      "887:\tlearn: 0.0867602\ttotal: 3m 59s\tremaining: 30.2s\n",
      "888:\tlearn: 0.0867159\ttotal: 3m 59s\tremaining: 29.9s\n",
      "889:\tlearn: 0.0866841\ttotal: 3m 59s\tremaining: 29.6s\n",
      "890:\tlearn: 0.0865954\ttotal: 3m 59s\tremaining: 29.4s\n",
      "891:\tlearn: 0.0865504\ttotal: 4m\tremaining: 29.1s\n",
      "892:\tlearn: 0.0865382\ttotal: 4m\tremaining: 28.8s\n",
      "893:\tlearn: 0.0864788\ttotal: 4m\tremaining: 28.5s\n",
      "894:\tlearn: 0.0864657\ttotal: 4m 1s\tremaining: 28.3s\n",
      "895:\tlearn: 0.0864158\ttotal: 4m 1s\tremaining: 28s\n",
      "896:\tlearn: 0.0863993\ttotal: 4m 1s\tremaining: 27.7s\n",
      "897:\tlearn: 0.0863460\ttotal: 4m 1s\tremaining: 27.5s\n",
      "898:\tlearn: 0.0862986\ttotal: 4m 2s\tremaining: 27.2s\n",
      "899:\tlearn: 0.0862569\ttotal: 4m 2s\tremaining: 26.9s\n",
      "900:\tlearn: 0.0862227\ttotal: 4m 2s\tremaining: 26.7s\n",
      "901:\tlearn: 0.0861908\ttotal: 4m 2s\tremaining: 26.4s\n",
      "902:\tlearn: 0.0861353\ttotal: 4m 3s\tremaining: 26.1s\n",
      "903:\tlearn: 0.0860952\ttotal: 4m 3s\tremaining: 25.8s\n",
      "904:\tlearn: 0.0860786\ttotal: 4m 3s\tremaining: 25.6s\n",
      "905:\tlearn: 0.0860506\ttotal: 4m 3s\tremaining: 25.3s\n",
      "906:\tlearn: 0.0860119\ttotal: 4m 4s\tremaining: 25s\n",
      "907:\tlearn: 0.0859945\ttotal: 4m 4s\tremaining: 24.8s\n",
      "908:\tlearn: 0.0859548\ttotal: 4m 4s\tremaining: 24.5s\n",
      "909:\tlearn: 0.0858998\ttotal: 4m 4s\tremaining: 24.2s\n",
      "910:\tlearn: 0.0858660\ttotal: 4m 5s\tremaining: 23.9s\n",
      "911:\tlearn: 0.0858421\ttotal: 4m 5s\tremaining: 23.7s\n",
      "912:\tlearn: 0.0857971\ttotal: 4m 5s\tremaining: 23.4s\n",
      "913:\tlearn: 0.0857767\ttotal: 4m 5s\tremaining: 23.1s\n",
      "914:\tlearn: 0.0857606\ttotal: 4m 6s\tremaining: 22.9s\n",
      "915:\tlearn: 0.0857424\ttotal: 4m 6s\tremaining: 22.6s\n",
      "916:\tlearn: 0.0856960\ttotal: 4m 6s\tremaining: 22.3s\n",
      "917:\tlearn: 0.0856618\ttotal: 4m 6s\tremaining: 22.1s\n",
      "918:\tlearn: 0.0856484\ttotal: 4m 7s\tremaining: 21.8s\n",
      "919:\tlearn: 0.0856127\ttotal: 4m 7s\tremaining: 21.5s\n",
      "920:\tlearn: 0.0855799\ttotal: 4m 7s\tremaining: 21.3s\n",
      "921:\tlearn: 0.0855267\ttotal: 4m 8s\tremaining: 21s\n",
      "922:\tlearn: 0.0855166\ttotal: 4m 8s\tremaining: 20.7s\n",
      "923:\tlearn: 0.0854659\ttotal: 4m 8s\tremaining: 20.4s\n",
      "924:\tlearn: 0.0854470\ttotal: 4m 8s\tremaining: 20.2s\n",
      "925:\tlearn: 0.0854195\ttotal: 4m 8s\tremaining: 19.9s\n",
      "926:\tlearn: 0.0853816\ttotal: 4m 9s\tremaining: 19.6s\n",
      "927:\tlearn: 0.0853597\ttotal: 4m 9s\tremaining: 19.4s\n",
      "928:\tlearn: 0.0853323\ttotal: 4m 9s\tremaining: 19.1s\n",
      "929:\tlearn: 0.0853165\ttotal: 4m 10s\tremaining: 18.8s\n",
      "930:\tlearn: 0.0852837\ttotal: 4m 10s\tremaining: 18.6s\n",
      "931:\tlearn: 0.0852581\ttotal: 4m 10s\tremaining: 18.3s\n",
      "932:\tlearn: 0.0852109\ttotal: 4m 10s\tremaining: 18s\n",
      "933:\tlearn: 0.0851901\ttotal: 4m 11s\tremaining: 17.7s\n",
      "934:\tlearn: 0.0851408\ttotal: 4m 11s\tremaining: 17.5s\n",
      "935:\tlearn: 0.0851306\ttotal: 4m 11s\tremaining: 17.2s\n",
      "936:\tlearn: 0.0850856\ttotal: 4m 11s\tremaining: 16.9s\n",
      "937:\tlearn: 0.0850611\ttotal: 4m 12s\tremaining: 16.7s\n",
      "938:\tlearn: 0.0850090\ttotal: 4m 12s\tremaining: 16.4s\n",
      "939:\tlearn: 0.0849771\ttotal: 4m 12s\tremaining: 16.1s\n",
      "940:\tlearn: 0.0849527\ttotal: 4m 12s\tremaining: 15.9s\n",
      "941:\tlearn: 0.0849041\ttotal: 4m 13s\tremaining: 15.6s\n",
      "942:\tlearn: 0.0848755\ttotal: 4m 13s\tremaining: 15.3s\n",
      "943:\tlearn: 0.0848261\ttotal: 4m 13s\tremaining: 15s\n",
      "944:\tlearn: 0.0847882\ttotal: 4m 13s\tremaining: 14.8s\n",
      "945:\tlearn: 0.0847342\ttotal: 4m 14s\tremaining: 14.5s\n",
      "946:\tlearn: 0.0846879\ttotal: 4m 14s\tremaining: 14.2s\n",
      "947:\tlearn: 0.0846613\ttotal: 4m 14s\tremaining: 14s\n",
      "948:\tlearn: 0.0846358\ttotal: 4m 14s\tremaining: 13.7s\n",
      "949:\tlearn: 0.0846055\ttotal: 4m 15s\tremaining: 13.4s\n",
      "950:\tlearn: 0.0845929\ttotal: 4m 15s\tremaining: 13.2s\n",
      "951:\tlearn: 0.0845744\ttotal: 4m 15s\tremaining: 12.9s\n",
      "952:\tlearn: 0.0845554\ttotal: 4m 16s\tremaining: 12.6s\n",
      "953:\tlearn: 0.0844587\ttotal: 4m 16s\tremaining: 12.4s\n",
      "954:\tlearn: 0.0844142\ttotal: 4m 16s\tremaining: 12.1s\n",
      "955:\tlearn: 0.0843806\ttotal: 4m 16s\tremaining: 11.8s\n",
      "956:\tlearn: 0.0843626\ttotal: 4m 17s\tremaining: 11.6s\n",
      "957:\tlearn: 0.0843327\ttotal: 4m 17s\tremaining: 11.3s\n",
      "958:\tlearn: 0.0842726\ttotal: 4m 17s\tremaining: 11s\n",
      "959:\tlearn: 0.0842065\ttotal: 4m 18s\tremaining: 10.8s\n",
      "960:\tlearn: 0.0842007\ttotal: 4m 18s\tremaining: 10.5s\n",
      "961:\tlearn: 0.0841668\ttotal: 4m 18s\tremaining: 10.2s\n",
      "962:\tlearn: 0.0841367\ttotal: 4m 18s\tremaining: 9.94s\n",
      "963:\tlearn: 0.0840881\ttotal: 4m 19s\tremaining: 9.67s\n",
      "964:\tlearn: 0.0840321\ttotal: 4m 19s\tremaining: 9.4s\n",
      "965:\tlearn: 0.0839896\ttotal: 4m 19s\tremaining: 9.13s\n",
      "966:\tlearn: 0.0839403\ttotal: 4m 19s\tremaining: 8.87s\n",
      "967:\tlearn: 0.0838926\ttotal: 4m 20s\tremaining: 8.6s\n",
      "968:\tlearn: 0.0838428\ttotal: 4m 20s\tremaining: 8.33s\n",
      "969:\tlearn: 0.0838020\ttotal: 4m 20s\tremaining: 8.06s\n",
      "970:\tlearn: 0.0837579\ttotal: 4m 20s\tremaining: 7.79s\n",
      "971:\tlearn: 0.0837189\ttotal: 4m 21s\tremaining: 7.52s\n",
      "972:\tlearn: 0.0836816\ttotal: 4m 21s\tremaining: 7.25s\n",
      "973:\tlearn: 0.0836428\ttotal: 4m 21s\tremaining: 6.99s\n",
      "974:\tlearn: 0.0835929\ttotal: 4m 22s\tremaining: 6.72s\n",
      "975:\tlearn: 0.0835499\ttotal: 4m 22s\tremaining: 6.45s\n",
      "976:\tlearn: 0.0835189\ttotal: 4m 22s\tremaining: 6.18s\n",
      "977:\tlearn: 0.0835024\ttotal: 4m 22s\tremaining: 5.91s\n",
      "978:\tlearn: 0.0834730\ttotal: 4m 22s\tremaining: 5.64s\n",
      "979:\tlearn: 0.0834558\ttotal: 4m 23s\tremaining: 5.37s\n",
      "980:\tlearn: 0.0834387\ttotal: 4m 23s\tremaining: 5.1s\n",
      "981:\tlearn: 0.0833977\ttotal: 4m 23s\tremaining: 4.83s\n",
      "982:\tlearn: 0.0833745\ttotal: 4m 23s\tremaining: 4.56s\n",
      "983:\tlearn: 0.0833584\ttotal: 4m 24s\tremaining: 4.29s\n",
      "984:\tlearn: 0.0833260\ttotal: 4m 24s\tremaining: 4.03s\n",
      "985:\tlearn: 0.0832834\ttotal: 4m 24s\tremaining: 3.76s\n",
      "986:\tlearn: 0.0832639\ttotal: 4m 24s\tremaining: 3.49s\n",
      "987:\tlearn: 0.0832298\ttotal: 4m 25s\tremaining: 3.22s\n",
      "988:\tlearn: 0.0832044\ttotal: 4m 25s\tremaining: 2.95s\n",
      "989:\tlearn: 0.0831759\ttotal: 4m 25s\tremaining: 2.68s\n",
      "990:\tlearn: 0.0831116\ttotal: 4m 26s\tremaining: 2.42s\n",
      "991:\tlearn: 0.0830846\ttotal: 4m 26s\tremaining: 2.15s\n",
      "992:\tlearn: 0.0830455\ttotal: 4m 26s\tremaining: 1.88s\n",
      "993:\tlearn: 0.0830125\ttotal: 4m 26s\tremaining: 1.61s\n",
      "994:\tlearn: 0.0829701\ttotal: 4m 27s\tremaining: 1.34s\n",
      "995:\tlearn: 0.0829420\ttotal: 4m 27s\tremaining: 1.07s\n",
      "996:\tlearn: 0.0829069\ttotal: 4m 27s\tremaining: 805ms\n",
      "997:\tlearn: 0.0828814\ttotal: 4m 27s\tremaining: 537ms\n",
      "998:\tlearn: 0.0828184\ttotal: 4m 28s\tremaining: 268ms\n",
      "999:\tlearn: 0.0827691\ttotal: 4m 28s\tremaining: 0us\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     27361\n",
      "           1       0.91      0.88      0.90      6950\n",
      "\n",
      "    accuracy                           0.96     34311\n",
      "   macro avg       0.94      0.93      0.93     34311\n",
      "weighted avg       0.96      0.96      0.96     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "\n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "catboost_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', CatBoostClassifier(max_depth=10, one_hot_max_size=10))\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, test_size=0.05, random_state=42, stratify=y)\n",
    "\n",
    "catboost_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = catboost_pipeline.predict(X_test)\n",
    "\n",
    "# param_grid = {\n",
    "#     'num_leaves': [20, 30, 40],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'min_child_weight': [0.001, 0.05, 0.01]\n",
    "# }\n",
    "# # Créer une instance de GridSearchCV\n",
    "# grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Effectuer la recherche sur la grille\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Meilleurs hyperparamètres trouvés\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Meilleur score obtenu sur l'ensemble d'entraînement\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# # Meilleur modèle\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Utiliser le meilleur modèle pour prédire les étiquettes sur l'ensemble de test\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# print(\"Meilleurs hyperparamètres:\", best_params)\n",
    "# print(\"Meilleur score sur l'ensemble d'entraînement:\", best_score)\n",
    "# print(\"Rapport de classification:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5258156\ttotal: 99ms\tremaining: 1m 38s\n",
      "1:\tlearn: 0.4275702\ttotal: 197ms\tremaining: 1m 38s\n",
      "2:\tlearn: 0.3595619\ttotal: 299ms\tremaining: 1m 39s\n",
      "3:\tlearn: 0.3146526\ttotal: 396ms\tremaining: 1m 38s\n",
      "4:\tlearn: 0.2914315\ttotal: 485ms\tremaining: 1m 36s\n",
      "5:\tlearn: 0.2723684\ttotal: 585ms\tremaining: 1m 36s\n",
      "6:\tlearn: 0.2555552\ttotal: 687ms\tremaining: 1m 37s\n",
      "7:\tlearn: 0.2449120\ttotal: 785ms\tremaining: 1m 37s\n",
      "8:\tlearn: 0.2367658\ttotal: 877ms\tremaining: 1m 36s\n",
      "9:\tlearn: 0.2305051\ttotal: 973ms\tremaining: 1m 36s\n",
      "10:\tlearn: 0.2236396\ttotal: 1.06s\tremaining: 1m 35s\n",
      "11:\tlearn: 0.2149944\ttotal: 1.15s\tremaining: 1m 34s\n",
      "12:\tlearn: 0.2106634\ttotal: 1.24s\tremaining: 1m 34s\n",
      "13:\tlearn: 0.2070814\ttotal: 1.34s\tremaining: 1m 34s\n",
      "14:\tlearn: 0.2032672\ttotal: 1.44s\tremaining: 1m 34s\n",
      "15:\tlearn: 0.2011847\ttotal: 1.56s\tremaining: 1m 36s\n",
      "16:\tlearn: 0.1985610\ttotal: 1.66s\tremaining: 1m 35s\n",
      "17:\tlearn: 0.1964792\ttotal: 1.75s\tremaining: 1m 35s\n",
      "18:\tlearn: 0.1915291\ttotal: 1.85s\tremaining: 1m 35s\n",
      "19:\tlearn: 0.1889605\ttotal: 1.95s\tremaining: 1m 35s\n",
      "20:\tlearn: 0.1872897\ttotal: 2.05s\tremaining: 1m 35s\n",
      "21:\tlearn: 0.1856656\ttotal: 2.15s\tremaining: 1m 35s\n",
      "22:\tlearn: 0.1843362\ttotal: 2.24s\tremaining: 1m 35s\n",
      "23:\tlearn: 0.1832623\ttotal: 2.32s\tremaining: 1m 34s\n",
      "24:\tlearn: 0.1814054\ttotal: 2.42s\tremaining: 1m 34s\n",
      "25:\tlearn: 0.1800819\ttotal: 2.53s\tremaining: 1m 34s\n",
      "26:\tlearn: 0.1787554\ttotal: 2.63s\tremaining: 1m 34s\n",
      "27:\tlearn: 0.1776322\ttotal: 2.72s\tremaining: 1m 34s\n",
      "28:\tlearn: 0.1758824\ttotal: 2.82s\tremaining: 1m 34s\n",
      "29:\tlearn: 0.1746993\ttotal: 2.91s\tremaining: 1m 34s\n",
      "30:\tlearn: 0.1738582\ttotal: 3.02s\tremaining: 1m 34s\n",
      "31:\tlearn: 0.1726512\ttotal: 3.13s\tremaining: 1m 34s\n",
      "32:\tlearn: 0.1717344\ttotal: 3.25s\tremaining: 1m 35s\n",
      "33:\tlearn: 0.1711089\ttotal: 3.36s\tremaining: 1m 35s\n",
      "34:\tlearn: 0.1700743\ttotal: 3.46s\tremaining: 1m 35s\n",
      "35:\tlearn: 0.1693322\ttotal: 3.55s\tremaining: 1m 35s\n",
      "36:\tlearn: 0.1680947\ttotal: 3.65s\tremaining: 1m 34s\n",
      "37:\tlearn: 0.1673574\ttotal: 3.74s\tremaining: 1m 34s\n",
      "38:\tlearn: 0.1663959\ttotal: 3.84s\tremaining: 1m 34s\n",
      "39:\tlearn: 0.1654508\ttotal: 3.93s\tremaining: 1m 34s\n",
      "40:\tlearn: 0.1647577\ttotal: 4.03s\tremaining: 1m 34s\n",
      "41:\tlearn: 0.1640215\ttotal: 4.12s\tremaining: 1m 33s\n",
      "42:\tlearn: 0.1634221\ttotal: 4.21s\tremaining: 1m 33s\n",
      "43:\tlearn: 0.1627145\ttotal: 4.32s\tremaining: 1m 33s\n",
      "44:\tlearn: 0.1619436\ttotal: 4.42s\tremaining: 1m 33s\n",
      "45:\tlearn: 0.1610363\ttotal: 4.52s\tremaining: 1m 33s\n",
      "46:\tlearn: 0.1602128\ttotal: 4.62s\tremaining: 1m 33s\n",
      "47:\tlearn: 0.1594734\ttotal: 4.71s\tremaining: 1m 33s\n",
      "48:\tlearn: 0.1588850\ttotal: 4.81s\tremaining: 1m 33s\n",
      "49:\tlearn: 0.1583758\ttotal: 4.9s\tremaining: 1m 33s\n",
      "50:\tlearn: 0.1576319\ttotal: 5s\tremaining: 1m 32s\n",
      "51:\tlearn: 0.1572795\ttotal: 5.08s\tremaining: 1m 32s\n",
      "52:\tlearn: 0.1568771\ttotal: 5.18s\tremaining: 1m 32s\n",
      "53:\tlearn: 0.1563854\ttotal: 5.28s\tremaining: 1m 32s\n",
      "54:\tlearn: 0.1556445\ttotal: 5.37s\tremaining: 1m 32s\n",
      "55:\tlearn: 0.1550236\ttotal: 5.48s\tremaining: 1m 32s\n",
      "56:\tlearn: 0.1545078\ttotal: 5.59s\tremaining: 1m 32s\n",
      "57:\tlearn: 0.1541929\ttotal: 5.7s\tremaining: 1m 32s\n",
      "58:\tlearn: 0.1537567\ttotal: 5.8s\tremaining: 1m 32s\n",
      "59:\tlearn: 0.1531768\ttotal: 5.9s\tremaining: 1m 32s\n",
      "60:\tlearn: 0.1526770\ttotal: 6s\tremaining: 1m 32s\n",
      "61:\tlearn: 0.1523053\ttotal: 6.09s\tremaining: 1m 32s\n",
      "62:\tlearn: 0.1519712\ttotal: 6.18s\tremaining: 1m 31s\n",
      "63:\tlearn: 0.1514593\ttotal: 6.28s\tremaining: 1m 31s\n",
      "64:\tlearn: 0.1508909\ttotal: 6.37s\tremaining: 1m 31s\n",
      "65:\tlearn: 0.1505376\ttotal: 6.54s\tremaining: 1m 32s\n",
      "66:\tlearn: 0.1502094\ttotal: 6.66s\tremaining: 1m 32s\n",
      "67:\tlearn: 0.1497804\ttotal: 6.76s\tremaining: 1m 32s\n",
      "68:\tlearn: 0.1494810\ttotal: 6.86s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1490864\ttotal: 6.96s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1487769\ttotal: 7.05s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1485032\ttotal: 7.14s\tremaining: 1m 32s\n",
      "72:\tlearn: 0.1481410\ttotal: 7.25s\tremaining: 1m 32s\n",
      "73:\tlearn: 0.1478073\ttotal: 7.41s\tremaining: 1m 32s\n",
      "74:\tlearn: 0.1475518\ttotal: 7.5s\tremaining: 1m 32s\n",
      "75:\tlearn: 0.1472674\ttotal: 7.6s\tremaining: 1m 32s\n",
      "76:\tlearn: 0.1465009\ttotal: 7.68s\tremaining: 1m 32s\n",
      "77:\tlearn: 0.1461867\ttotal: 7.77s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1459362\ttotal: 7.86s\tremaining: 1m 31s\n",
      "79:\tlearn: 0.1455664\ttotal: 7.95s\tremaining: 1m 31s\n",
      "80:\tlearn: 0.1453026\ttotal: 8.12s\tremaining: 1m 32s\n",
      "81:\tlearn: 0.1448449\ttotal: 8.22s\tremaining: 1m 32s\n",
      "82:\tlearn: 0.1446525\ttotal: 8.31s\tremaining: 1m 31s\n",
      "83:\tlearn: 0.1444388\ttotal: 8.4s\tremaining: 1m 31s\n",
      "84:\tlearn: 0.1442546\ttotal: 8.5s\tremaining: 1m 31s\n",
      "85:\tlearn: 0.1439770\ttotal: 8.59s\tremaining: 1m 31s\n",
      "86:\tlearn: 0.1436316\ttotal: 8.68s\tremaining: 1m 31s\n",
      "87:\tlearn: 0.1433885\ttotal: 8.83s\tremaining: 1m 31s\n",
      "88:\tlearn: 0.1431889\ttotal: 8.92s\tremaining: 1m 31s\n",
      "89:\tlearn: 0.1428542\ttotal: 9.01s\tremaining: 1m 31s\n",
      "90:\tlearn: 0.1424933\ttotal: 9.11s\tremaining: 1m 30s\n",
      "91:\tlearn: 0.1422926\ttotal: 9.2s\tremaining: 1m 30s\n",
      "92:\tlearn: 0.1420365\ttotal: 9.29s\tremaining: 1m 30s\n",
      "93:\tlearn: 0.1418552\ttotal: 9.38s\tremaining: 1m 30s\n",
      "94:\tlearn: 0.1415318\ttotal: 9.48s\tremaining: 1m 30s\n",
      "95:\tlearn: 0.1411421\ttotal: 9.57s\tremaining: 1m 30s\n",
      "96:\tlearn: 0.1408045\ttotal: 9.67s\tremaining: 1m 29s\n",
      "97:\tlearn: 0.1405540\ttotal: 9.76s\tremaining: 1m 29s\n",
      "98:\tlearn: 0.1402427\ttotal: 9.86s\tremaining: 1m 29s\n",
      "99:\tlearn: 0.1398887\ttotal: 9.95s\tremaining: 1m 29s\n",
      "100:\tlearn: 0.1397203\ttotal: 10s\tremaining: 1m 29s\n",
      "101:\tlearn: 0.1394975\ttotal: 10.1s\tremaining: 1m 29s\n",
      "102:\tlearn: 0.1392873\ttotal: 10.2s\tremaining: 1m 29s\n",
      "103:\tlearn: 0.1389902\ttotal: 10.3s\tremaining: 1m 29s\n",
      "104:\tlearn: 0.1387788\ttotal: 10.4s\tremaining: 1m 28s\n",
      "105:\tlearn: 0.1384857\ttotal: 10.5s\tremaining: 1m 28s\n",
      "106:\tlearn: 0.1382386\ttotal: 10.6s\tremaining: 1m 28s\n",
      "107:\tlearn: 0.1380589\ttotal: 10.7s\tremaining: 1m 28s\n",
      "108:\tlearn: 0.1378432\ttotal: 10.8s\tremaining: 1m 28s\n",
      "109:\tlearn: 0.1376637\ttotal: 10.9s\tremaining: 1m 28s\n",
      "110:\tlearn: 0.1374519\ttotal: 11s\tremaining: 1m 28s\n",
      "111:\tlearn: 0.1372601\ttotal: 11.1s\tremaining: 1m 27s\n",
      "112:\tlearn: 0.1370462\ttotal: 11.2s\tremaining: 1m 27s\n",
      "113:\tlearn: 0.1368055\ttotal: 11.3s\tremaining: 1m 27s\n",
      "114:\tlearn: 0.1366886\ttotal: 11.4s\tremaining: 1m 27s\n",
      "115:\tlearn: 0.1365339\ttotal: 11.5s\tremaining: 1m 27s\n",
      "116:\tlearn: 0.1361956\ttotal: 11.6s\tremaining: 1m 27s\n",
      "117:\tlearn: 0.1360213\ttotal: 11.7s\tremaining: 1m 27s\n",
      "118:\tlearn: 0.1357781\ttotal: 11.8s\tremaining: 1m 27s\n",
      "119:\tlearn: 0.1355307\ttotal: 11.8s\tremaining: 1m 26s\n",
      "120:\tlearn: 0.1353288\ttotal: 11.9s\tremaining: 1m 26s\n",
      "121:\tlearn: 0.1351510\ttotal: 12s\tremaining: 1m 26s\n",
      "122:\tlearn: 0.1350057\ttotal: 12.1s\tremaining: 1m 26s\n",
      "123:\tlearn: 0.1347995\ttotal: 12.2s\tremaining: 1m 26s\n",
      "124:\tlearn: 0.1345595\ttotal: 12.3s\tremaining: 1m 26s\n",
      "125:\tlearn: 0.1344063\ttotal: 12.4s\tremaining: 1m 26s\n",
      "126:\tlearn: 0.1342242\ttotal: 12.5s\tremaining: 1m 25s\n",
      "127:\tlearn: 0.1340336\ttotal: 12.6s\tremaining: 1m 25s\n",
      "128:\tlearn: 0.1337690\ttotal: 12.7s\tremaining: 1m 25s\n",
      "129:\tlearn: 0.1335338\ttotal: 12.8s\tremaining: 1m 25s\n",
      "130:\tlearn: 0.1333892\ttotal: 12.9s\tremaining: 1m 25s\n",
      "131:\tlearn: 0.1332723\ttotal: 13s\tremaining: 1m 25s\n",
      "132:\tlearn: 0.1331502\ttotal: 13.1s\tremaining: 1m 25s\n",
      "133:\tlearn: 0.1330121\ttotal: 13.2s\tremaining: 1m 25s\n",
      "134:\tlearn: 0.1329061\ttotal: 13.2s\tremaining: 1m 24s\n",
      "135:\tlearn: 0.1327149\ttotal: 13.3s\tremaining: 1m 24s\n",
      "136:\tlearn: 0.1325452\ttotal: 13.4s\tremaining: 1m 24s\n",
      "137:\tlearn: 0.1323956\ttotal: 13.5s\tremaining: 1m 24s\n",
      "138:\tlearn: 0.1322082\ttotal: 13.6s\tremaining: 1m 24s\n",
      "139:\tlearn: 0.1320507\ttotal: 13.7s\tremaining: 1m 24s\n",
      "140:\tlearn: 0.1319117\ttotal: 13.8s\tremaining: 1m 24s\n",
      "141:\tlearn: 0.1317541\ttotal: 13.9s\tremaining: 1m 24s\n",
      "142:\tlearn: 0.1315899\ttotal: 14s\tremaining: 1m 23s\n",
      "143:\tlearn: 0.1313998\ttotal: 14.1s\tremaining: 1m 23s\n",
      "144:\tlearn: 0.1312715\ttotal: 14.2s\tremaining: 1m 23s\n",
      "145:\tlearn: 0.1311112\ttotal: 14.3s\tremaining: 1m 23s\n",
      "146:\tlearn: 0.1308869\ttotal: 14.4s\tremaining: 1m 23s\n",
      "147:\tlearn: 0.1306621\ttotal: 14.5s\tremaining: 1m 23s\n",
      "148:\tlearn: 0.1305288\ttotal: 14.6s\tremaining: 1m 23s\n",
      "149:\tlearn: 0.1304038\ttotal: 14.7s\tremaining: 1m 23s\n",
      "150:\tlearn: 0.1302783\ttotal: 14.8s\tremaining: 1m 23s\n",
      "151:\tlearn: 0.1301604\ttotal: 14.9s\tremaining: 1m 22s\n",
      "152:\tlearn: 0.1300270\ttotal: 15s\tremaining: 1m 22s\n",
      "153:\tlearn: 0.1299273\ttotal: 15s\tremaining: 1m 22s\n",
      "154:\tlearn: 0.1298137\ttotal: 15.1s\tremaining: 1m 22s\n",
      "155:\tlearn: 0.1297211\ttotal: 15.2s\tremaining: 1m 22s\n",
      "156:\tlearn: 0.1296072\ttotal: 15.3s\tremaining: 1m 22s\n",
      "157:\tlearn: 0.1295105\ttotal: 15.4s\tremaining: 1m 22s\n",
      "158:\tlearn: 0.1294048\ttotal: 15.5s\tremaining: 1m 22s\n",
      "159:\tlearn: 0.1292461\ttotal: 15.6s\tremaining: 1m 21s\n",
      "160:\tlearn: 0.1291276\ttotal: 15.7s\tremaining: 1m 21s\n",
      "161:\tlearn: 0.1289962\ttotal: 15.8s\tremaining: 1m 21s\n",
      "162:\tlearn: 0.1288591\ttotal: 15.9s\tremaining: 1m 21s\n",
      "163:\tlearn: 0.1287675\ttotal: 16s\tremaining: 1m 21s\n",
      "164:\tlearn: 0.1285956\ttotal: 16.1s\tremaining: 1m 21s\n",
      "165:\tlearn: 0.1284192\ttotal: 16.2s\tremaining: 1m 21s\n",
      "166:\tlearn: 0.1283228\ttotal: 16.3s\tremaining: 1m 21s\n",
      "167:\tlearn: 0.1281271\ttotal: 16.4s\tremaining: 1m 21s\n",
      "168:\tlearn: 0.1279557\ttotal: 16.5s\tremaining: 1m 20s\n",
      "169:\tlearn: 0.1278561\ttotal: 16.6s\tremaining: 1m 20s\n",
      "170:\tlearn: 0.1276495\ttotal: 16.7s\tremaining: 1m 20s\n",
      "171:\tlearn: 0.1275104\ttotal: 16.8s\tremaining: 1m 20s\n",
      "172:\tlearn: 0.1273983\ttotal: 16.8s\tremaining: 1m 20s\n",
      "173:\tlearn: 0.1272570\ttotal: 16.9s\tremaining: 1m 20s\n",
      "174:\tlearn: 0.1271269\ttotal: 17s\tremaining: 1m 20s\n",
      "175:\tlearn: 0.1270277\ttotal: 17.2s\tremaining: 1m 20s\n",
      "176:\tlearn: 0.1269040\ttotal: 17.3s\tremaining: 1m 20s\n",
      "177:\tlearn: 0.1267990\ttotal: 17.4s\tremaining: 1m 20s\n",
      "178:\tlearn: 0.1267441\ttotal: 17.5s\tremaining: 1m 20s\n",
      "179:\tlearn: 0.1265583\ttotal: 17.6s\tremaining: 1m 20s\n",
      "180:\tlearn: 0.1263702\ttotal: 17.7s\tremaining: 1m 20s\n",
      "181:\tlearn: 0.1262566\ttotal: 17.8s\tremaining: 1m 20s\n",
      "182:\tlearn: 0.1260798\ttotal: 17.9s\tremaining: 1m 19s\n",
      "183:\tlearn: 0.1259582\ttotal: 18s\tremaining: 1m 19s\n",
      "184:\tlearn: 0.1258243\ttotal: 18.1s\tremaining: 1m 19s\n",
      "185:\tlearn: 0.1257476\ttotal: 18.2s\tremaining: 1m 19s\n",
      "186:\tlearn: 0.1256174\ttotal: 18.3s\tremaining: 1m 19s\n",
      "187:\tlearn: 0.1254876\ttotal: 18.4s\tremaining: 1m 19s\n",
      "188:\tlearn: 0.1254090\ttotal: 18.5s\tremaining: 1m 19s\n",
      "189:\tlearn: 0.1253168\ttotal: 18.6s\tremaining: 1m 19s\n",
      "190:\tlearn: 0.1252451\ttotal: 18.7s\tremaining: 1m 19s\n",
      "191:\tlearn: 0.1251441\ttotal: 18.8s\tremaining: 1m 18s\n",
      "192:\tlearn: 0.1250396\ttotal: 18.9s\tremaining: 1m 18s\n",
      "193:\tlearn: 0.1249147\ttotal: 18.9s\tremaining: 1m 18s\n",
      "194:\tlearn: 0.1247739\ttotal: 19s\tremaining: 1m 18s\n",
      "195:\tlearn: 0.1245966\ttotal: 19.1s\tremaining: 1m 18s\n",
      "196:\tlearn: 0.1245060\ttotal: 19.2s\tremaining: 1m 18s\n",
      "197:\tlearn: 0.1243858\ttotal: 19.3s\tremaining: 1m 18s\n",
      "198:\tlearn: 0.1242927\ttotal: 19.4s\tremaining: 1m 18s\n",
      "199:\tlearn: 0.1241966\ttotal: 19.5s\tremaining: 1m 18s\n",
      "200:\tlearn: 0.1241192\ttotal: 19.6s\tremaining: 1m 17s\n",
      "201:\tlearn: 0.1239571\ttotal: 19.7s\tremaining: 1m 17s\n",
      "202:\tlearn: 0.1238523\ttotal: 19.8s\tremaining: 1m 17s\n",
      "203:\tlearn: 0.1237112\ttotal: 19.9s\tremaining: 1m 17s\n",
      "204:\tlearn: 0.1235913\ttotal: 20s\tremaining: 1m 17s\n",
      "205:\tlearn: 0.1235183\ttotal: 20.1s\tremaining: 1m 17s\n",
      "206:\tlearn: 0.1233837\ttotal: 20.2s\tremaining: 1m 17s\n",
      "207:\tlearn: 0.1233208\ttotal: 20.3s\tremaining: 1m 17s\n",
      "208:\tlearn: 0.1232357\ttotal: 20.4s\tremaining: 1m 17s\n",
      "209:\tlearn: 0.1231225\ttotal: 20.5s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1230434\ttotal: 20.5s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1229484\ttotal: 20.6s\tremaining: 1m 16s\n",
      "212:\tlearn: 0.1228616\ttotal: 20.7s\tremaining: 1m 16s\n",
      "213:\tlearn: 0.1227769\ttotal: 20.8s\tremaining: 1m 16s\n",
      "214:\tlearn: 0.1226802\ttotal: 20.9s\tremaining: 1m 16s\n",
      "215:\tlearn: 0.1226135\ttotal: 21s\tremaining: 1m 16s\n",
      "216:\tlearn: 0.1225393\ttotal: 21.1s\tremaining: 1m 16s\n",
      "217:\tlearn: 0.1223897\ttotal: 21.2s\tremaining: 1m 16s\n",
      "218:\tlearn: 0.1222908\ttotal: 21.3s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1221805\ttotal: 21.4s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1220267\ttotal: 21.5s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.1219132\ttotal: 21.6s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.1218110\ttotal: 21.7s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.1217200\ttotal: 21.8s\tremaining: 1m 15s\n",
      "224:\tlearn: 0.1215757\ttotal: 21.9s\tremaining: 1m 15s\n",
      "225:\tlearn: 0.1215034\ttotal: 22s\tremaining: 1m 15s\n",
      "226:\tlearn: 0.1213970\ttotal: 22s\tremaining: 1m 15s\n",
      "227:\tlearn: 0.1212845\ttotal: 22.1s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1211862\ttotal: 22.2s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1210999\ttotal: 22.3s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1210223\ttotal: 22.4s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1209428\ttotal: 22.5s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1208081\ttotal: 22.6s\tremaining: 1m 14s\n",
      "233:\tlearn: 0.1206547\ttotal: 22.7s\tremaining: 1m 14s\n",
      "234:\tlearn: 0.1205656\ttotal: 22.8s\tremaining: 1m 14s\n",
      "235:\tlearn: 0.1204955\ttotal: 22.9s\tremaining: 1m 14s\n",
      "236:\tlearn: 0.1203941\ttotal: 23s\tremaining: 1m 14s\n",
      "237:\tlearn: 0.1202786\ttotal: 23.1s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1201887\ttotal: 23.2s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1200797\ttotal: 23.3s\tremaining: 1m 13s\n",
      "240:\tlearn: 0.1200221\ttotal: 23.4s\tremaining: 1m 13s\n",
      "241:\tlearn: 0.1199261\ttotal: 23.5s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1198521\ttotal: 23.6s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1196802\ttotal: 23.7s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1196184\ttotal: 23.8s\tremaining: 1m 13s\n",
      "245:\tlearn: 0.1195320\ttotal: 23.9s\tremaining: 1m 13s\n",
      "246:\tlearn: 0.1194098\ttotal: 24s\tremaining: 1m 13s\n",
      "247:\tlearn: 0.1193075\ttotal: 24.1s\tremaining: 1m 12s\n",
      "248:\tlearn: 0.1192447\ttotal: 24.2s\tremaining: 1m 12s\n",
      "249:\tlearn: 0.1191508\ttotal: 24.3s\tremaining: 1m 12s\n",
      "250:\tlearn: 0.1190514\ttotal: 24.4s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.1189760\ttotal: 24.5s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.1187875\ttotal: 24.6s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.1186761\ttotal: 24.7s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.1186180\ttotal: 24.8s\tremaining: 1m 12s\n",
      "255:\tlearn: 0.1184723\ttotal: 24.9s\tremaining: 1m 12s\n",
      "256:\tlearn: 0.1183526\ttotal: 25s\tremaining: 1m 12s\n",
      "257:\tlearn: 0.1182457\ttotal: 25.1s\tremaining: 1m 12s\n",
      "258:\tlearn: 0.1181412\ttotal: 25.2s\tremaining: 1m 11s\n",
      "259:\tlearn: 0.1180766\ttotal: 25.2s\tremaining: 1m 11s\n",
      "260:\tlearn: 0.1179667\ttotal: 25.3s\tremaining: 1m 11s\n",
      "261:\tlearn: 0.1178890\ttotal: 25.4s\tremaining: 1m 11s\n",
      "262:\tlearn: 0.1177935\ttotal: 25.5s\tremaining: 1m 11s\n",
      "263:\tlearn: 0.1177041\ttotal: 25.6s\tremaining: 1m 11s\n",
      "264:\tlearn: 0.1176498\ttotal: 25.7s\tremaining: 1m 11s\n",
      "265:\tlearn: 0.1175866\ttotal: 25.8s\tremaining: 1m 11s\n",
      "266:\tlearn: 0.1175166\ttotal: 25.9s\tremaining: 1m 11s\n",
      "267:\tlearn: 0.1173913\ttotal: 26s\tremaining: 1m 11s\n",
      "268:\tlearn: 0.1173162\ttotal: 26.1s\tremaining: 1m 10s\n",
      "269:\tlearn: 0.1172632\ttotal: 26.2s\tremaining: 1m 10s\n",
      "270:\tlearn: 0.1171676\ttotal: 26.3s\tremaining: 1m 10s\n",
      "271:\tlearn: 0.1170599\ttotal: 26.4s\tremaining: 1m 10s\n",
      "272:\tlearn: 0.1169683\ttotal: 26.5s\tremaining: 1m 10s\n",
      "273:\tlearn: 0.1169000\ttotal: 26.6s\tremaining: 1m 10s\n",
      "274:\tlearn: 0.1168447\ttotal: 26.7s\tremaining: 1m 10s\n",
      "275:\tlearn: 0.1167840\ttotal: 26.8s\tremaining: 1m 10s\n",
      "276:\tlearn: 0.1166810\ttotal: 26.9s\tremaining: 1m 10s\n",
      "277:\tlearn: 0.1165916\ttotal: 27s\tremaining: 1m 10s\n",
      "278:\tlearn: 0.1165465\ttotal: 27.1s\tremaining: 1m 9s\n",
      "279:\tlearn: 0.1164246\ttotal: 27.2s\tremaining: 1m 9s\n",
      "280:\tlearn: 0.1163223\ttotal: 27.3s\tremaining: 1m 9s\n",
      "281:\tlearn: 0.1162536\ttotal: 27.4s\tremaining: 1m 9s\n",
      "282:\tlearn: 0.1161693\ttotal: 27.5s\tremaining: 1m 9s\n",
      "283:\tlearn: 0.1160696\ttotal: 27.5s\tremaining: 1m 9s\n",
      "284:\tlearn: 0.1159481\ttotal: 27.6s\tremaining: 1m 9s\n",
      "285:\tlearn: 0.1158446\ttotal: 27.8s\tremaining: 1m 9s\n",
      "286:\tlearn: 0.1158064\ttotal: 28s\tremaining: 1m 9s\n",
      "287:\tlearn: 0.1157317\ttotal: 28s\tremaining: 1m 9s\n",
      "288:\tlearn: 0.1156252\ttotal: 28.1s\tremaining: 1m 9s\n",
      "289:\tlearn: 0.1155437\ttotal: 28.2s\tremaining: 1m 9s\n",
      "290:\tlearn: 0.1154923\ttotal: 28.3s\tremaining: 1m 9s\n",
      "291:\tlearn: 0.1154226\ttotal: 28.4s\tremaining: 1m 8s\n",
      "292:\tlearn: 0.1153639\ttotal: 28.5s\tremaining: 1m 8s\n",
      "293:\tlearn: 0.1152655\ttotal: 28.6s\tremaining: 1m 8s\n",
      "294:\tlearn: 0.1152095\ttotal: 28.7s\tremaining: 1m 8s\n",
      "295:\tlearn: 0.1151149\ttotal: 28.8s\tremaining: 1m 8s\n",
      "296:\tlearn: 0.1150489\ttotal: 28.9s\tremaining: 1m 8s\n",
      "297:\tlearn: 0.1149891\ttotal: 29s\tremaining: 1m 8s\n",
      "298:\tlearn: 0.1148956\ttotal: 29.1s\tremaining: 1m 8s\n",
      "299:\tlearn: 0.1148263\ttotal: 29.2s\tremaining: 1m 8s\n",
      "300:\tlearn: 0.1147254\ttotal: 29.3s\tremaining: 1m 8s\n",
      "301:\tlearn: 0.1146407\ttotal: 29.4s\tremaining: 1m 7s\n",
      "302:\tlearn: 0.1145758\ttotal: 29.5s\tremaining: 1m 7s\n",
      "303:\tlearn: 0.1145075\ttotal: 29.6s\tremaining: 1m 7s\n",
      "304:\tlearn: 0.1144093\ttotal: 29.7s\tremaining: 1m 7s\n",
      "305:\tlearn: 0.1142887\ttotal: 29.8s\tremaining: 1m 7s\n",
      "306:\tlearn: 0.1141871\ttotal: 29.9s\tremaining: 1m 7s\n",
      "307:\tlearn: 0.1141307\ttotal: 30s\tremaining: 1m 7s\n",
      "308:\tlearn: 0.1140627\ttotal: 30s\tremaining: 1m 7s\n",
      "309:\tlearn: 0.1140052\ttotal: 30.1s\tremaining: 1m 7s\n",
      "310:\tlearn: 0.1139314\ttotal: 30.2s\tremaining: 1m 7s\n",
      "311:\tlearn: 0.1138762\ttotal: 30.3s\tremaining: 1m 6s\n",
      "312:\tlearn: 0.1138072\ttotal: 30.4s\tremaining: 1m 6s\n",
      "313:\tlearn: 0.1137323\ttotal: 30.5s\tremaining: 1m 6s\n",
      "314:\tlearn: 0.1136684\ttotal: 30.6s\tremaining: 1m 6s\n",
      "315:\tlearn: 0.1136112\ttotal: 30.7s\tremaining: 1m 6s\n",
      "316:\tlearn: 0.1135566\ttotal: 30.8s\tremaining: 1m 6s\n",
      "317:\tlearn: 0.1135028\ttotal: 30.9s\tremaining: 1m 6s\n",
      "318:\tlearn: 0.1134374\ttotal: 31s\tremaining: 1m 6s\n",
      "319:\tlearn: 0.1133728\ttotal: 31.1s\tremaining: 1m 6s\n",
      "320:\tlearn: 0.1132364\ttotal: 31.2s\tremaining: 1m 5s\n",
      "321:\tlearn: 0.1131448\ttotal: 31.3s\tremaining: 1m 5s\n",
      "322:\tlearn: 0.1130902\ttotal: 31.4s\tremaining: 1m 5s\n",
      "323:\tlearn: 0.1129988\ttotal: 31.5s\tremaining: 1m 5s\n",
      "324:\tlearn: 0.1129015\ttotal: 31.6s\tremaining: 1m 5s\n",
      "325:\tlearn: 0.1128635\ttotal: 31.7s\tremaining: 1m 5s\n",
      "326:\tlearn: 0.1128169\ttotal: 31.8s\tremaining: 1m 5s\n",
      "327:\tlearn: 0.1127460\ttotal: 31.9s\tremaining: 1m 5s\n",
      "328:\tlearn: 0.1126309\ttotal: 32s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1125885\ttotal: 32s\tremaining: 1m 5s\n",
      "330:\tlearn: 0.1125097\ttotal: 32.1s\tremaining: 1m 4s\n",
      "331:\tlearn: 0.1124096\ttotal: 32.2s\tremaining: 1m 4s\n",
      "332:\tlearn: 0.1123089\ttotal: 32.3s\tremaining: 1m 4s\n",
      "333:\tlearn: 0.1122214\ttotal: 32.4s\tremaining: 1m 4s\n",
      "334:\tlearn: 0.1121619\ttotal: 32.5s\tremaining: 1m 4s\n",
      "335:\tlearn: 0.1120752\ttotal: 32.6s\tremaining: 1m 4s\n",
      "336:\tlearn: 0.1119766\ttotal: 32.7s\tremaining: 1m 4s\n",
      "337:\tlearn: 0.1119360\ttotal: 32.8s\tremaining: 1m 4s\n",
      "338:\tlearn: 0.1118691\ttotal: 32.9s\tremaining: 1m 4s\n",
      "339:\tlearn: 0.1117882\ttotal: 33s\tremaining: 1m 4s\n",
      "340:\tlearn: 0.1117318\ttotal: 33.1s\tremaining: 1m 4s\n",
      "341:\tlearn: 0.1116664\ttotal: 33.2s\tremaining: 1m 3s\n",
      "342:\tlearn: 0.1116071\ttotal: 33.3s\tremaining: 1m 3s\n",
      "343:\tlearn: 0.1115561\ttotal: 33.4s\tremaining: 1m 3s\n",
      "344:\tlearn: 0.1114846\ttotal: 33.5s\tremaining: 1m 3s\n",
      "345:\tlearn: 0.1113984\ttotal: 33.6s\tremaining: 1m 3s\n",
      "346:\tlearn: 0.1113329\ttotal: 33.7s\tremaining: 1m 3s\n",
      "347:\tlearn: 0.1112619\ttotal: 33.8s\tremaining: 1m 3s\n",
      "348:\tlearn: 0.1112077\ttotal: 33.9s\tremaining: 1m 3s\n",
      "349:\tlearn: 0.1111519\ttotal: 34s\tremaining: 1m 3s\n",
      "350:\tlearn: 0.1111156\ttotal: 34.1s\tremaining: 1m 3s\n",
      "351:\tlearn: 0.1110531\ttotal: 34.2s\tremaining: 1m 2s\n",
      "352:\tlearn: 0.1109741\ttotal: 34.3s\tremaining: 1m 2s\n",
      "353:\tlearn: 0.1109330\ttotal: 34.4s\tremaining: 1m 2s\n",
      "354:\tlearn: 0.1108743\ttotal: 34.5s\tremaining: 1m 2s\n",
      "355:\tlearn: 0.1108165\ttotal: 34.6s\tremaining: 1m 2s\n",
      "356:\tlearn: 0.1107436\ttotal: 34.7s\tremaining: 1m 2s\n",
      "357:\tlearn: 0.1106966\ttotal: 34.8s\tremaining: 1m 2s\n",
      "358:\tlearn: 0.1106627\ttotal: 34.9s\tremaining: 1m 2s\n",
      "359:\tlearn: 0.1105864\ttotal: 34.9s\tremaining: 1m 2s\n",
      "360:\tlearn: 0.1105304\ttotal: 35s\tremaining: 1m 2s\n",
      "361:\tlearn: 0.1104290\ttotal: 35.1s\tremaining: 1m 1s\n",
      "362:\tlearn: 0.1103410\ttotal: 35.2s\tremaining: 1m 1s\n",
      "363:\tlearn: 0.1102466\ttotal: 35.3s\tremaining: 1m 1s\n",
      "364:\tlearn: 0.1102104\ttotal: 35.4s\tremaining: 1m 1s\n",
      "365:\tlearn: 0.1101551\ttotal: 35.5s\tremaining: 1m 1s\n",
      "366:\tlearn: 0.1101030\ttotal: 35.6s\tremaining: 1m 1s\n",
      "367:\tlearn: 0.1100458\ttotal: 35.7s\tremaining: 1m 1s\n",
      "368:\tlearn: 0.1099845\ttotal: 35.8s\tremaining: 1m 1s\n",
      "369:\tlearn: 0.1099090\ttotal: 35.9s\tremaining: 1m 1s\n",
      "370:\tlearn: 0.1098376\ttotal: 36s\tremaining: 1m 1s\n",
      "371:\tlearn: 0.1097221\ttotal: 36.1s\tremaining: 1m\n",
      "372:\tlearn: 0.1096605\ttotal: 36.2s\tremaining: 1m\n",
      "373:\tlearn: 0.1095837\ttotal: 36.3s\tremaining: 1m\n",
      "374:\tlearn: 0.1095105\ttotal: 36.4s\tremaining: 1m\n",
      "375:\tlearn: 0.1094312\ttotal: 36.5s\tremaining: 1m\n",
      "376:\tlearn: 0.1093434\ttotal: 36.6s\tremaining: 1m\n",
      "377:\tlearn: 0.1093040\ttotal: 36.7s\tremaining: 1m\n",
      "378:\tlearn: 0.1092519\ttotal: 36.8s\tremaining: 1m\n",
      "379:\tlearn: 0.1091940\ttotal: 36.9s\tremaining: 1m\n",
      "380:\tlearn: 0.1091011\ttotal: 37s\tremaining: 1m\n",
      "381:\tlearn: 0.1090519\ttotal: 37.1s\tremaining: 60s\n",
      "382:\tlearn: 0.1089955\ttotal: 37.2s\tremaining: 59.9s\n",
      "383:\tlearn: 0.1089547\ttotal: 37.3s\tremaining: 59.8s\n",
      "384:\tlearn: 0.1089092\ttotal: 37.4s\tremaining: 59.7s\n",
      "385:\tlearn: 0.1088552\ttotal: 37.4s\tremaining: 59.6s\n",
      "386:\tlearn: 0.1088072\ttotal: 37.5s\tremaining: 59.5s\n",
      "387:\tlearn: 0.1087356\ttotal: 37.6s\tremaining: 59.4s\n",
      "388:\tlearn: 0.1086892\ttotal: 37.7s\tremaining: 59.3s\n",
      "389:\tlearn: 0.1086363\ttotal: 37.8s\tremaining: 59.2s\n",
      "390:\tlearn: 0.1085912\ttotal: 37.9s\tremaining: 59.1s\n",
      "391:\tlearn: 0.1085322\ttotal: 38s\tremaining: 59s\n",
      "392:\tlearn: 0.1084876\ttotal: 38.1s\tremaining: 58.9s\n",
      "393:\tlearn: 0.1084022\ttotal: 38.2s\tremaining: 58.8s\n",
      "394:\tlearn: 0.1083265\ttotal: 38.3s\tremaining: 58.7s\n",
      "395:\tlearn: 0.1082353\ttotal: 38.4s\tremaining: 58.6s\n",
      "396:\tlearn: 0.1081728\ttotal: 38.6s\tremaining: 58.6s\n",
      "397:\tlearn: 0.1080952\ttotal: 38.7s\tremaining: 58.5s\n",
      "398:\tlearn: 0.1080419\ttotal: 38.8s\tremaining: 58.4s\n",
      "399:\tlearn: 0.1079772\ttotal: 38.9s\tremaining: 58.3s\n",
      "400:\tlearn: 0.1079106\ttotal: 39s\tremaining: 58.2s\n",
      "401:\tlearn: 0.1078669\ttotal: 39.1s\tremaining: 58.1s\n",
      "402:\tlearn: 0.1078238\ttotal: 39.2s\tremaining: 58s\n",
      "403:\tlearn: 0.1077651\ttotal: 39.3s\tremaining: 57.9s\n",
      "404:\tlearn: 0.1077040\ttotal: 39.4s\tremaining: 57.8s\n",
      "405:\tlearn: 0.1076433\ttotal: 39.5s\tremaining: 57.7s\n",
      "406:\tlearn: 0.1075618\ttotal: 39.6s\tremaining: 57.6s\n",
      "407:\tlearn: 0.1074988\ttotal: 39.7s\tremaining: 57.5s\n",
      "408:\tlearn: 0.1074053\ttotal: 39.7s\tremaining: 57.4s\n",
      "409:\tlearn: 0.1073068\ttotal: 39.8s\tremaining: 57.3s\n",
      "410:\tlearn: 0.1072657\ttotal: 39.9s\tremaining: 57.2s\n",
      "411:\tlearn: 0.1072315\ttotal: 40s\tremaining: 57.1s\n",
      "412:\tlearn: 0.1071882\ttotal: 40.1s\tremaining: 57s\n",
      "413:\tlearn: 0.1071472\ttotal: 40.2s\tremaining: 56.9s\n",
      "414:\tlearn: 0.1070933\ttotal: 40.3s\tremaining: 56.8s\n",
      "415:\tlearn: 0.1070432\ttotal: 40.4s\tremaining: 56.7s\n",
      "416:\tlearn: 0.1069935\ttotal: 40.5s\tremaining: 56.6s\n",
      "417:\tlearn: 0.1069595\ttotal: 40.6s\tremaining: 56.5s\n",
      "418:\tlearn: 0.1069269\ttotal: 40.7s\tremaining: 56.4s\n",
      "419:\tlearn: 0.1068526\ttotal: 40.8s\tremaining: 56.3s\n",
      "420:\tlearn: 0.1067973\ttotal: 40.9s\tremaining: 56.2s\n",
      "421:\tlearn: 0.1067752\ttotal: 41s\tremaining: 56.1s\n",
      "422:\tlearn: 0.1067318\ttotal: 41.1s\tremaining: 56s\n",
      "423:\tlearn: 0.1066570\ttotal: 41.2s\tremaining: 55.9s\n",
      "424:\tlearn: 0.1066336\ttotal: 41.3s\tremaining: 55.8s\n",
      "425:\tlearn: 0.1065610\ttotal: 41.4s\tremaining: 55.7s\n",
      "426:\tlearn: 0.1064957\ttotal: 41.5s\tremaining: 55.6s\n",
      "427:\tlearn: 0.1064288\ttotal: 41.6s\tremaining: 55.5s\n",
      "428:\tlearn: 0.1063885\ttotal: 41.6s\tremaining: 55.4s\n",
      "429:\tlearn: 0.1063123\ttotal: 41.7s\tremaining: 55.3s\n",
      "430:\tlearn: 0.1062341\ttotal: 41.8s\tremaining: 55.2s\n",
      "431:\tlearn: 0.1061859\ttotal: 41.9s\tremaining: 55.1s\n",
      "432:\tlearn: 0.1061336\ttotal: 42s\tremaining: 55s\n",
      "433:\tlearn: 0.1060379\ttotal: 42.1s\tremaining: 54.9s\n",
      "434:\tlearn: 0.1060070\ttotal: 42.2s\tremaining: 54.8s\n",
      "435:\tlearn: 0.1059620\ttotal: 42.3s\tremaining: 54.7s\n",
      "436:\tlearn: 0.1059318\ttotal: 42.4s\tremaining: 54.6s\n",
      "437:\tlearn: 0.1058861\ttotal: 42.5s\tremaining: 54.5s\n",
      "438:\tlearn: 0.1058443\ttotal: 42.6s\tremaining: 54.4s\n",
      "439:\tlearn: 0.1057875\ttotal: 42.7s\tremaining: 54.3s\n",
      "440:\tlearn: 0.1057515\ttotal: 42.8s\tremaining: 54.2s\n",
      "441:\tlearn: 0.1057092\ttotal: 42.9s\tremaining: 54.1s\n",
      "442:\tlearn: 0.1056275\ttotal: 43s\tremaining: 54s\n",
      "443:\tlearn: 0.1055677\ttotal: 43.1s\tremaining: 53.9s\n",
      "444:\tlearn: 0.1055492\ttotal: 43.2s\tremaining: 53.8s\n",
      "445:\tlearn: 0.1055123\ttotal: 43.3s\tremaining: 53.7s\n",
      "446:\tlearn: 0.1054823\ttotal: 43.4s\tremaining: 53.6s\n",
      "447:\tlearn: 0.1054491\ttotal: 43.5s\tremaining: 53.5s\n",
      "448:\tlearn: 0.1053506\ttotal: 43.6s\tremaining: 53.5s\n",
      "449:\tlearn: 0.1052901\ttotal: 43.7s\tremaining: 53.4s\n",
      "450:\tlearn: 0.1052523\ttotal: 43.8s\tremaining: 53.3s\n",
      "451:\tlearn: 0.1051563\ttotal: 43.9s\tremaining: 53.2s\n",
      "452:\tlearn: 0.1050992\ttotal: 44s\tremaining: 53.1s\n",
      "453:\tlearn: 0.1050467\ttotal: 44.1s\tremaining: 53s\n",
      "454:\tlearn: 0.1049697\ttotal: 44.2s\tremaining: 52.9s\n",
      "455:\tlearn: 0.1049340\ttotal: 44.3s\tremaining: 52.8s\n",
      "456:\tlearn: 0.1048861\ttotal: 44.4s\tremaining: 52.7s\n",
      "457:\tlearn: 0.1048527\ttotal: 44.5s\tremaining: 52.6s\n",
      "458:\tlearn: 0.1047802\ttotal: 44.6s\tremaining: 52.5s\n",
      "459:\tlearn: 0.1047423\ttotal: 44.7s\tremaining: 52.4s\n",
      "460:\tlearn: 0.1046720\ttotal: 44.8s\tremaining: 52.3s\n",
      "461:\tlearn: 0.1046226\ttotal: 44.9s\tremaining: 52.2s\n",
      "462:\tlearn: 0.1045529\ttotal: 45s\tremaining: 52.1s\n",
      "463:\tlearn: 0.1044806\ttotal: 45.1s\tremaining: 52.1s\n",
      "464:\tlearn: 0.1043959\ttotal: 45.2s\tremaining: 52s\n",
      "465:\tlearn: 0.1042882\ttotal: 45.3s\tremaining: 51.9s\n",
      "466:\tlearn: 0.1042310\ttotal: 45.4s\tremaining: 51.8s\n",
      "467:\tlearn: 0.1041251\ttotal: 45.5s\tremaining: 51.8s\n",
      "468:\tlearn: 0.1040247\ttotal: 45.6s\tremaining: 51.7s\n",
      "469:\tlearn: 0.1039779\ttotal: 45.7s\tremaining: 51.6s\n",
      "470:\tlearn: 0.1039398\ttotal: 45.8s\tremaining: 51.5s\n",
      "471:\tlearn: 0.1038936\ttotal: 45.9s\tremaining: 51.4s\n",
      "472:\tlearn: 0.1038063\ttotal: 46s\tremaining: 51.3s\n",
      "473:\tlearn: 0.1037577\ttotal: 46.2s\tremaining: 51.2s\n",
      "474:\tlearn: 0.1037177\ttotal: 46.3s\tremaining: 51.1s\n",
      "475:\tlearn: 0.1036616\ttotal: 46.4s\tremaining: 51s\n",
      "476:\tlearn: 0.1036321\ttotal: 46.5s\tremaining: 50.9s\n",
      "477:\tlearn: 0.1035682\ttotal: 46.6s\tremaining: 50.9s\n",
      "478:\tlearn: 0.1035216\ttotal: 46.7s\tremaining: 50.8s\n",
      "479:\tlearn: 0.1034574\ttotal: 46.8s\tremaining: 50.7s\n",
      "480:\tlearn: 0.1034118\ttotal: 46.9s\tremaining: 50.6s\n",
      "481:\tlearn: 0.1033725\ttotal: 47s\tremaining: 50.5s\n",
      "482:\tlearn: 0.1033055\ttotal: 47.1s\tremaining: 50.4s\n",
      "483:\tlearn: 0.1032163\ttotal: 47.2s\tremaining: 50.3s\n",
      "484:\tlearn: 0.1031632\ttotal: 47.3s\tremaining: 50.3s\n",
      "485:\tlearn: 0.1031129\ttotal: 47.5s\tremaining: 50.2s\n",
      "486:\tlearn: 0.1030485\ttotal: 47.6s\tremaining: 50.1s\n",
      "487:\tlearn: 0.1029897\ttotal: 47.7s\tremaining: 50s\n",
      "488:\tlearn: 0.1029513\ttotal: 47.8s\tremaining: 49.9s\n",
      "489:\tlearn: 0.1028164\ttotal: 47.9s\tremaining: 49.8s\n",
      "490:\tlearn: 0.1027818\ttotal: 48s\tremaining: 49.7s\n",
      "491:\tlearn: 0.1027290\ttotal: 48.1s\tremaining: 49.7s\n",
      "492:\tlearn: 0.1026487\ttotal: 48.2s\tremaining: 49.6s\n",
      "493:\tlearn: 0.1026069\ttotal: 48.3s\tremaining: 49.5s\n",
      "494:\tlearn: 0.1025419\ttotal: 48.4s\tremaining: 49.4s\n",
      "495:\tlearn: 0.1024861\ttotal: 48.5s\tremaining: 49.3s\n",
      "496:\tlearn: 0.1024461\ttotal: 48.6s\tremaining: 49.2s\n",
      "497:\tlearn: 0.1023815\ttotal: 48.7s\tremaining: 49.1s\n",
      "498:\tlearn: 0.1023297\ttotal: 48.9s\tremaining: 49.1s\n",
      "499:\tlearn: 0.1022593\ttotal: 49s\tremaining: 49s\n",
      "500:\tlearn: 0.1022286\ttotal: 49.1s\tremaining: 48.9s\n",
      "501:\tlearn: 0.1021687\ttotal: 49.2s\tremaining: 48.8s\n",
      "502:\tlearn: 0.1021339\ttotal: 49.3s\tremaining: 48.7s\n",
      "503:\tlearn: 0.1020858\ttotal: 49.4s\tremaining: 48.6s\n",
      "504:\tlearn: 0.1020536\ttotal: 49.5s\tremaining: 48.5s\n",
      "505:\tlearn: 0.1020113\ttotal: 49.6s\tremaining: 48.4s\n",
      "506:\tlearn: 0.1019590\ttotal: 49.7s\tremaining: 48.3s\n",
      "507:\tlearn: 0.1018748\ttotal: 49.8s\tremaining: 48.2s\n",
      "508:\tlearn: 0.1018161\ttotal: 49.9s\tremaining: 48.1s\n",
      "509:\tlearn: 0.1017445\ttotal: 50s\tremaining: 48.1s\n",
      "510:\tlearn: 0.1016810\ttotal: 50.2s\tremaining: 48s\n",
      "511:\tlearn: 0.1016089\ttotal: 50.3s\tremaining: 47.9s\n",
      "512:\tlearn: 0.1015713\ttotal: 50.4s\tremaining: 47.8s\n",
      "513:\tlearn: 0.1015362\ttotal: 50.5s\tremaining: 47.7s\n",
      "514:\tlearn: 0.1014996\ttotal: 50.6s\tremaining: 47.7s\n",
      "515:\tlearn: 0.1014597\ttotal: 50.7s\tremaining: 47.6s\n",
      "516:\tlearn: 0.1014275\ttotal: 50.8s\tremaining: 47.5s\n",
      "517:\tlearn: 0.1014170\ttotal: 50.9s\tremaining: 47.3s\n",
      "518:\tlearn: 0.1013824\ttotal: 51s\tremaining: 47.3s\n",
      "519:\tlearn: 0.1013222\ttotal: 51.1s\tremaining: 47.2s\n",
      "520:\tlearn: 0.1012755\ttotal: 51.2s\tremaining: 47.1s\n",
      "521:\tlearn: 0.1011839\ttotal: 51.3s\tremaining: 47s\n",
      "522:\tlearn: 0.1011128\ttotal: 51.4s\tremaining: 46.9s\n",
      "523:\tlearn: 0.1010742\ttotal: 51.5s\tremaining: 46.8s\n",
      "524:\tlearn: 0.1010050\ttotal: 51.6s\tremaining: 46.7s\n",
      "525:\tlearn: 0.1009357\ttotal: 51.7s\tremaining: 46.6s\n",
      "526:\tlearn: 0.1008995\ttotal: 51.8s\tremaining: 46.5s\n",
      "527:\tlearn: 0.1008382\ttotal: 51.9s\tremaining: 46.4s\n",
      "528:\tlearn: 0.1007782\ttotal: 52s\tremaining: 46.3s\n",
      "529:\tlearn: 0.1007341\ttotal: 52.1s\tremaining: 46.2s\n",
      "530:\tlearn: 0.1006717\ttotal: 52.2s\tremaining: 46.1s\n",
      "531:\tlearn: 0.1006349\ttotal: 52.3s\tremaining: 46s\n",
      "532:\tlearn: 0.1005962\ttotal: 52.4s\tremaining: 45.9s\n",
      "533:\tlearn: 0.1005529\ttotal: 52.5s\tremaining: 45.8s\n",
      "534:\tlearn: 0.1005067\ttotal: 52.6s\tremaining: 45.7s\n",
      "535:\tlearn: 0.1004736\ttotal: 52.7s\tremaining: 45.6s\n",
      "536:\tlearn: 0.1004253\ttotal: 52.8s\tremaining: 45.5s\n",
      "537:\tlearn: 0.1003952\ttotal: 52.9s\tremaining: 45.4s\n",
      "538:\tlearn: 0.1003680\ttotal: 53s\tremaining: 45.3s\n",
      "539:\tlearn: 0.1003096\ttotal: 53.1s\tremaining: 45.2s\n",
      "540:\tlearn: 0.1002832\ttotal: 53.2s\tremaining: 45.1s\n",
      "541:\tlearn: 0.1002196\ttotal: 53.2s\tremaining: 45s\n",
      "542:\tlearn: 0.1001721\ttotal: 53.3s\tremaining: 44.9s\n",
      "543:\tlearn: 0.1001298\ttotal: 53.4s\tremaining: 44.8s\n",
      "544:\tlearn: 0.1001006\ttotal: 53.5s\tremaining: 44.7s\n",
      "545:\tlearn: 0.1000774\ttotal: 53.6s\tremaining: 44.6s\n",
      "546:\tlearn: 0.1000347\ttotal: 53.7s\tremaining: 44.5s\n",
      "547:\tlearn: 0.1000184\ttotal: 53.8s\tremaining: 44.4s\n",
      "548:\tlearn: 0.0999327\ttotal: 53.9s\tremaining: 44.3s\n",
      "549:\tlearn: 0.0998931\ttotal: 54s\tremaining: 44.2s\n",
      "550:\tlearn: 0.0998526\ttotal: 54.1s\tremaining: 44.1s\n",
      "551:\tlearn: 0.0997918\ttotal: 54.2s\tremaining: 44s\n",
      "552:\tlearn: 0.0997313\ttotal: 54.3s\tremaining: 43.9s\n",
      "553:\tlearn: 0.0996972\ttotal: 54.4s\tremaining: 43.8s\n",
      "554:\tlearn: 0.0996578\ttotal: 54.5s\tremaining: 43.7s\n",
      "555:\tlearn: 0.0995861\ttotal: 54.6s\tremaining: 43.6s\n",
      "556:\tlearn: 0.0995600\ttotal: 54.7s\tremaining: 43.5s\n",
      "557:\tlearn: 0.0995316\ttotal: 54.8s\tremaining: 43.4s\n",
      "558:\tlearn: 0.0994770\ttotal: 54.9s\tremaining: 43.3s\n",
      "559:\tlearn: 0.0994026\ttotal: 55s\tremaining: 43.2s\n",
      "560:\tlearn: 0.0993559\ttotal: 55.1s\tremaining: 43.1s\n",
      "561:\tlearn: 0.0993146\ttotal: 55.2s\tremaining: 43s\n",
      "562:\tlearn: 0.0992730\ttotal: 55.3s\tremaining: 42.9s\n",
      "563:\tlearn: 0.0992537\ttotal: 55.4s\tremaining: 42.8s\n",
      "564:\tlearn: 0.0992030\ttotal: 55.5s\tremaining: 42.7s\n",
      "565:\tlearn: 0.0991652\ttotal: 55.6s\tremaining: 42.6s\n",
      "566:\tlearn: 0.0991110\ttotal: 55.7s\tremaining: 42.5s\n",
      "567:\tlearn: 0.0990691\ttotal: 55.8s\tremaining: 42.4s\n",
      "568:\tlearn: 0.0990244\ttotal: 55.8s\tremaining: 42.3s\n",
      "569:\tlearn: 0.0989606\ttotal: 55.9s\tremaining: 42.2s\n",
      "570:\tlearn: 0.0989046\ttotal: 56s\tremaining: 42.1s\n",
      "571:\tlearn: 0.0988791\ttotal: 56.1s\tremaining: 42s\n",
      "572:\tlearn: 0.0987839\ttotal: 56.3s\tremaining: 42s\n",
      "573:\tlearn: 0.0987060\ttotal: 56.4s\tremaining: 41.9s\n",
      "574:\tlearn: 0.0987045\ttotal: 56.5s\tremaining: 41.8s\n",
      "575:\tlearn: 0.0986104\ttotal: 56.6s\tremaining: 41.7s\n",
      "576:\tlearn: 0.0985745\ttotal: 56.7s\tremaining: 41.6s\n",
      "577:\tlearn: 0.0985396\ttotal: 56.8s\tremaining: 41.5s\n",
      "578:\tlearn: 0.0984767\ttotal: 56.9s\tremaining: 41.4s\n",
      "579:\tlearn: 0.0984393\ttotal: 57s\tremaining: 41.3s\n",
      "580:\tlearn: 0.0984321\ttotal: 57.1s\tremaining: 41.2s\n",
      "581:\tlearn: 0.0983673\ttotal: 57.2s\tremaining: 41.1s\n",
      "582:\tlearn: 0.0982753\ttotal: 57.3s\tremaining: 41s\n",
      "583:\tlearn: 0.0982333\ttotal: 57.4s\tremaining: 40.9s\n",
      "584:\tlearn: 0.0982015\ttotal: 57.5s\tremaining: 40.8s\n",
      "585:\tlearn: 0.0981375\ttotal: 57.6s\tremaining: 40.7s\n",
      "586:\tlearn: 0.0980923\ttotal: 57.7s\tremaining: 40.6s\n",
      "587:\tlearn: 0.0979703\ttotal: 57.8s\tremaining: 40.5s\n",
      "588:\tlearn: 0.0979366\ttotal: 57.9s\tremaining: 40.4s\n",
      "589:\tlearn: 0.0979099\ttotal: 58s\tremaining: 40.3s\n",
      "590:\tlearn: 0.0978675\ttotal: 58.1s\tremaining: 40.2s\n",
      "591:\tlearn: 0.0978423\ttotal: 58.2s\tremaining: 40.1s\n",
      "592:\tlearn: 0.0977861\ttotal: 58.3s\tremaining: 40s\n",
      "593:\tlearn: 0.0977394\ttotal: 58.4s\tremaining: 39.9s\n",
      "594:\tlearn: 0.0976892\ttotal: 58.5s\tremaining: 39.8s\n",
      "595:\tlearn: 0.0976653\ttotal: 58.6s\tremaining: 39.7s\n",
      "596:\tlearn: 0.0976280\ttotal: 58.7s\tremaining: 39.6s\n",
      "597:\tlearn: 0.0975611\ttotal: 58.8s\tremaining: 39.5s\n",
      "598:\tlearn: 0.0974966\ttotal: 58.9s\tremaining: 39.4s\n",
      "599:\tlearn: 0.0974552\ttotal: 59s\tremaining: 39.3s\n",
      "600:\tlearn: 0.0973875\ttotal: 59.1s\tremaining: 39.2s\n",
      "601:\tlearn: 0.0973265\ttotal: 59.2s\tremaining: 39.2s\n",
      "602:\tlearn: 0.0972902\ttotal: 59.4s\tremaining: 39.1s\n",
      "603:\tlearn: 0.0972698\ttotal: 59.5s\tremaining: 39s\n",
      "604:\tlearn: 0.0972168\ttotal: 59.6s\tremaining: 38.9s\n",
      "605:\tlearn: 0.0971434\ttotal: 59.7s\tremaining: 38.8s\n",
      "606:\tlearn: 0.0971099\ttotal: 59.8s\tremaining: 38.7s\n",
      "607:\tlearn: 0.0970572\ttotal: 59.9s\tremaining: 38.6s\n",
      "608:\tlearn: 0.0970238\ttotal: 1m\tremaining: 38.5s\n",
      "609:\tlearn: 0.0970195\ttotal: 1m\tremaining: 38.4s\n",
      "610:\tlearn: 0.0969825\ttotal: 1m\tremaining: 38.3s\n",
      "611:\tlearn: 0.0969221\ttotal: 1m\tremaining: 38.2s\n",
      "612:\tlearn: 0.0968616\ttotal: 1m\tremaining: 38.1s\n",
      "613:\tlearn: 0.0968193\ttotal: 1m\tremaining: 38s\n",
      "614:\tlearn: 0.0967837\ttotal: 1m\tremaining: 37.9s\n",
      "615:\tlearn: 0.0967517\ttotal: 1m\tremaining: 37.8s\n",
      "616:\tlearn: 0.0967063\ttotal: 1m\tremaining: 37.7s\n",
      "617:\tlearn: 0.0966762\ttotal: 1m\tremaining: 37.6s\n",
      "618:\tlearn: 0.0966525\ttotal: 1m\tremaining: 37.5s\n",
      "619:\tlearn: 0.0965922\ttotal: 1m 1s\tremaining: 37.4s\n",
      "620:\tlearn: 0.0965531\ttotal: 1m 1s\tremaining: 37.3s\n",
      "621:\tlearn: 0.0965160\ttotal: 1m 1s\tremaining: 37.2s\n",
      "622:\tlearn: 0.0964863\ttotal: 1m 1s\tremaining: 37.1s\n",
      "623:\tlearn: 0.0964282\ttotal: 1m 1s\tremaining: 37s\n",
      "624:\tlearn: 0.0963706\ttotal: 1m 1s\tremaining: 36.9s\n",
      "625:\tlearn: 0.0962877\ttotal: 1m 1s\tremaining: 36.8s\n",
      "626:\tlearn: 0.0962467\ttotal: 1m 1s\tremaining: 36.7s\n",
      "627:\tlearn: 0.0961915\ttotal: 1m 1s\tremaining: 36.6s\n",
      "628:\tlearn: 0.0961580\ttotal: 1m 1s\tremaining: 36.5s\n",
      "629:\tlearn: 0.0961169\ttotal: 1m 2s\tremaining: 36.4s\n",
      "630:\tlearn: 0.0960518\ttotal: 1m 2s\tremaining: 36.3s\n",
      "631:\tlearn: 0.0960081\ttotal: 1m 2s\tremaining: 36.2s\n",
      "632:\tlearn: 0.0959750\ttotal: 1m 2s\tremaining: 36.1s\n",
      "633:\tlearn: 0.0959415\ttotal: 1m 2s\tremaining: 36s\n",
      "634:\tlearn: 0.0958993\ttotal: 1m 2s\tremaining: 35.9s\n",
      "635:\tlearn: 0.0958602\ttotal: 1m 2s\tremaining: 35.8s\n",
      "636:\tlearn: 0.0958526\ttotal: 1m 2s\tremaining: 35.7s\n",
      "637:\tlearn: 0.0958146\ttotal: 1m 2s\tremaining: 35.6s\n",
      "638:\tlearn: 0.0957812\ttotal: 1m 2s\tremaining: 35.5s\n",
      "639:\tlearn: 0.0957375\ttotal: 1m 2s\tremaining: 35.4s\n",
      "640:\tlearn: 0.0956979\ttotal: 1m 3s\tremaining: 35.3s\n",
      "641:\tlearn: 0.0956417\ttotal: 1m 3s\tremaining: 35.2s\n",
      "642:\tlearn: 0.0956193\ttotal: 1m 3s\tremaining: 35.1s\n",
      "643:\tlearn: 0.0954900\ttotal: 1m 3s\tremaining: 35s\n",
      "644:\tlearn: 0.0954614\ttotal: 1m 3s\tremaining: 34.9s\n",
      "645:\tlearn: 0.0954090\ttotal: 1m 3s\tremaining: 34.8s\n",
      "646:\tlearn: 0.0953562\ttotal: 1m 3s\tremaining: 34.7s\n",
      "647:\tlearn: 0.0952877\ttotal: 1m 3s\tremaining: 34.6s\n",
      "648:\tlearn: 0.0952360\ttotal: 1m 3s\tremaining: 34.5s\n",
      "649:\tlearn: 0.0951742\ttotal: 1m 3s\tremaining: 34.4s\n",
      "650:\tlearn: 0.0951245\ttotal: 1m 4s\tremaining: 34.3s\n",
      "651:\tlearn: 0.0950830\ttotal: 1m 4s\tremaining: 34.2s\n",
      "652:\tlearn: 0.0950629\ttotal: 1m 4s\tremaining: 34.1s\n",
      "653:\tlearn: 0.0950083\ttotal: 1m 4s\tremaining: 34s\n",
      "654:\tlearn: 0.0949639\ttotal: 1m 4s\tremaining: 34s\n",
      "655:\tlearn: 0.0949304\ttotal: 1m 4s\tremaining: 33.9s\n",
      "656:\tlearn: 0.0948993\ttotal: 1m 4s\tremaining: 33.8s\n",
      "657:\tlearn: 0.0948420\ttotal: 1m 4s\tremaining: 33.7s\n",
      "658:\tlearn: 0.0947979\ttotal: 1m 4s\tremaining: 33.6s\n",
      "659:\tlearn: 0.0947978\ttotal: 1m 4s\tremaining: 33.5s\n",
      "660:\tlearn: 0.0947904\ttotal: 1m 5s\tremaining: 33.4s\n",
      "661:\tlearn: 0.0947683\ttotal: 1m 5s\tremaining: 33.3s\n",
      "662:\tlearn: 0.0947166\ttotal: 1m 5s\tremaining: 33.2s\n",
      "663:\tlearn: 0.0946892\ttotal: 1m 5s\tremaining: 33.1s\n",
      "664:\tlearn: 0.0946406\ttotal: 1m 5s\tremaining: 33s\n",
      "665:\tlearn: 0.0945721\ttotal: 1m 5s\tremaining: 32.9s\n",
      "666:\tlearn: 0.0945175\ttotal: 1m 5s\tremaining: 32.8s\n",
      "667:\tlearn: 0.0944683\ttotal: 1m 5s\tremaining: 32.7s\n",
      "668:\tlearn: 0.0944467\ttotal: 1m 5s\tremaining: 32.6s\n",
      "669:\tlearn: 0.0943961\ttotal: 1m 5s\tremaining: 32.5s\n",
      "670:\tlearn: 0.0943504\ttotal: 1m 6s\tremaining: 32.4s\n",
      "671:\tlearn: 0.0942559\ttotal: 1m 6s\tremaining: 32.3s\n",
      "672:\tlearn: 0.0942184\ttotal: 1m 6s\tremaining: 32.2s\n",
      "673:\tlearn: 0.0941736\ttotal: 1m 6s\tremaining: 32.1s\n",
      "674:\tlearn: 0.0940708\ttotal: 1m 6s\tremaining: 32s\n",
      "675:\tlearn: 0.0940202\ttotal: 1m 6s\tremaining: 31.9s\n",
      "676:\tlearn: 0.0939885\ttotal: 1m 6s\tremaining: 31.8s\n",
      "677:\tlearn: 0.0939372\ttotal: 1m 6s\tremaining: 31.7s\n",
      "678:\tlearn: 0.0939008\ttotal: 1m 6s\tremaining: 31.6s\n",
      "679:\tlearn: 0.0938618\ttotal: 1m 6s\tremaining: 31.5s\n",
      "680:\tlearn: 0.0937987\ttotal: 1m 7s\tremaining: 31.4s\n",
      "681:\tlearn: 0.0937435\ttotal: 1m 7s\tremaining: 31.3s\n",
      "682:\tlearn: 0.0937242\ttotal: 1m 7s\tremaining: 31.2s\n",
      "683:\tlearn: 0.0936932\ttotal: 1m 7s\tremaining: 31.1s\n",
      "684:\tlearn: 0.0936387\ttotal: 1m 7s\tremaining: 31s\n",
      "685:\tlearn: 0.0935697\ttotal: 1m 7s\tremaining: 30.9s\n",
      "686:\tlearn: 0.0935264\ttotal: 1m 7s\tremaining: 30.8s\n",
      "687:\tlearn: 0.0934636\ttotal: 1m 7s\tremaining: 30.7s\n",
      "688:\tlearn: 0.0934377\ttotal: 1m 7s\tremaining: 30.6s\n",
      "689:\tlearn: 0.0933903\ttotal: 1m 7s\tremaining: 30.5s\n",
      "690:\tlearn: 0.0933404\ttotal: 1m 7s\tremaining: 30.4s\n",
      "691:\tlearn: 0.0933071\ttotal: 1m 8s\tremaining: 30.3s\n",
      "692:\tlearn: 0.0932768\ttotal: 1m 8s\tremaining: 30.2s\n",
      "693:\tlearn: 0.0932143\ttotal: 1m 8s\tremaining: 30.1s\n",
      "694:\tlearn: 0.0931650\ttotal: 1m 8s\tremaining: 30s\n",
      "695:\tlearn: 0.0931434\ttotal: 1m 8s\tremaining: 29.9s\n",
      "696:\tlearn: 0.0930959\ttotal: 1m 8s\tremaining: 29.9s\n",
      "697:\tlearn: 0.0929880\ttotal: 1m 8s\tremaining: 29.8s\n",
      "698:\tlearn: 0.0929511\ttotal: 1m 8s\tremaining: 29.7s\n",
      "699:\tlearn: 0.0929026\ttotal: 1m 8s\tremaining: 29.6s\n",
      "700:\tlearn: 0.0928536\ttotal: 1m 9s\tremaining: 29.5s\n",
      "701:\tlearn: 0.0928037\ttotal: 1m 9s\tremaining: 29.4s\n",
      "702:\tlearn: 0.0927661\ttotal: 1m 9s\tremaining: 29.3s\n",
      "703:\tlearn: 0.0927254\ttotal: 1m 9s\tremaining: 29.2s\n",
      "704:\tlearn: 0.0927027\ttotal: 1m 9s\tremaining: 29.1s\n",
      "705:\tlearn: 0.0926579\ttotal: 1m 9s\tremaining: 29s\n",
      "706:\tlearn: 0.0925911\ttotal: 1m 9s\tremaining: 28.9s\n",
      "707:\tlearn: 0.0925088\ttotal: 1m 9s\tremaining: 28.8s\n",
      "708:\tlearn: 0.0924422\ttotal: 1m 10s\tremaining: 28.7s\n",
      "709:\tlearn: 0.0923786\ttotal: 1m 10s\tremaining: 28.6s\n",
      "710:\tlearn: 0.0923524\ttotal: 1m 10s\tremaining: 28.5s\n",
      "711:\tlearn: 0.0923065\ttotal: 1m 10s\tremaining: 28.4s\n",
      "712:\tlearn: 0.0922176\ttotal: 1m 10s\tremaining: 28.3s\n",
      "713:\tlearn: 0.0921936\ttotal: 1m 10s\tremaining: 28.2s\n",
      "714:\tlearn: 0.0921441\ttotal: 1m 10s\tremaining: 28.1s\n",
      "715:\tlearn: 0.0920978\ttotal: 1m 10s\tremaining: 28s\n",
      "716:\tlearn: 0.0920327\ttotal: 1m 10s\tremaining: 27.9s\n",
      "717:\tlearn: 0.0919831\ttotal: 1m 10s\tremaining: 27.8s\n",
      "718:\tlearn: 0.0919415\ttotal: 1m 11s\tremaining: 27.8s\n",
      "719:\tlearn: 0.0918931\ttotal: 1m 11s\tremaining: 27.7s\n",
      "720:\tlearn: 0.0918540\ttotal: 1m 11s\tremaining: 27.6s\n",
      "721:\tlearn: 0.0918127\ttotal: 1m 11s\tremaining: 27.5s\n",
      "722:\tlearn: 0.0917840\ttotal: 1m 11s\tremaining: 27.4s\n",
      "723:\tlearn: 0.0917657\ttotal: 1m 11s\tremaining: 27.3s\n",
      "724:\tlearn: 0.0917396\ttotal: 1m 11s\tremaining: 27.2s\n",
      "725:\tlearn: 0.0917157\ttotal: 1m 11s\tremaining: 27.1s\n",
      "726:\tlearn: 0.0916484\ttotal: 1m 11s\tremaining: 27s\n",
      "727:\tlearn: 0.0916033\ttotal: 1m 11s\tremaining: 26.9s\n",
      "728:\tlearn: 0.0915751\ttotal: 1m 12s\tremaining: 26.8s\n",
      "729:\tlearn: 0.0915510\ttotal: 1m 12s\tremaining: 26.7s\n",
      "730:\tlearn: 0.0915024\ttotal: 1m 12s\tremaining: 26.6s\n",
      "731:\tlearn: 0.0914671\ttotal: 1m 12s\tremaining: 26.5s\n",
      "732:\tlearn: 0.0914259\ttotal: 1m 12s\tremaining: 26.4s\n",
      "733:\tlearn: 0.0914209\ttotal: 1m 12s\tremaining: 26.3s\n",
      "734:\tlearn: 0.0913872\ttotal: 1m 12s\tremaining: 26.2s\n",
      "735:\tlearn: 0.0913615\ttotal: 1m 12s\tremaining: 26.1s\n",
      "736:\tlearn: 0.0913355\ttotal: 1m 12s\tremaining: 26s\n",
      "737:\tlearn: 0.0913114\ttotal: 1m 12s\tremaining: 25.9s\n",
      "738:\tlearn: 0.0912754\ttotal: 1m 13s\tremaining: 25.8s\n",
      "739:\tlearn: 0.0912499\ttotal: 1m 13s\tremaining: 25.7s\n",
      "740:\tlearn: 0.0912071\ttotal: 1m 13s\tremaining: 25.6s\n",
      "741:\tlearn: 0.0911646\ttotal: 1m 13s\tremaining: 25.5s\n",
      "742:\tlearn: 0.0911263\ttotal: 1m 13s\tremaining: 25.4s\n",
      "743:\tlearn: 0.0910837\ttotal: 1m 13s\tremaining: 25.3s\n",
      "744:\tlearn: 0.0910316\ttotal: 1m 13s\tremaining: 25.2s\n",
      "745:\tlearn: 0.0909896\ttotal: 1m 13s\tremaining: 25.1s\n",
      "746:\tlearn: 0.0909725\ttotal: 1m 13s\tremaining: 25s\n",
      "747:\tlearn: 0.0909478\ttotal: 1m 14s\tremaining: 24.9s\n",
      "748:\tlearn: 0.0909221\ttotal: 1m 14s\tremaining: 24.8s\n",
      "749:\tlearn: 0.0908869\ttotal: 1m 14s\tremaining: 24.8s\n",
      "750:\tlearn: 0.0908449\ttotal: 1m 14s\tremaining: 24.7s\n",
      "751:\tlearn: 0.0908228\ttotal: 1m 14s\tremaining: 24.6s\n",
      "752:\tlearn: 0.0907795\ttotal: 1m 14s\tremaining: 24.5s\n",
      "753:\tlearn: 0.0907704\ttotal: 1m 14s\tremaining: 24.4s\n",
      "754:\tlearn: 0.0907184\ttotal: 1m 14s\tremaining: 24.3s\n",
      "755:\tlearn: 0.0906947\ttotal: 1m 14s\tremaining: 24.2s\n",
      "756:\tlearn: 0.0906486\ttotal: 1m 15s\tremaining: 24.1s\n",
      "757:\tlearn: 0.0906171\ttotal: 1m 15s\tremaining: 24s\n",
      "758:\tlearn: 0.0905605\ttotal: 1m 15s\tremaining: 23.9s\n",
      "759:\tlearn: 0.0904890\ttotal: 1m 15s\tremaining: 23.8s\n",
      "760:\tlearn: 0.0904203\ttotal: 1m 15s\tremaining: 23.7s\n",
      "761:\tlearn: 0.0903949\ttotal: 1m 15s\tremaining: 23.6s\n",
      "762:\tlearn: 0.0903148\ttotal: 1m 15s\tremaining: 23.5s\n",
      "763:\tlearn: 0.0902233\ttotal: 1m 15s\tremaining: 23.4s\n",
      "764:\tlearn: 0.0901904\ttotal: 1m 15s\tremaining: 23.3s\n",
      "765:\tlearn: 0.0901655\ttotal: 1m 16s\tremaining: 23.2s\n",
      "766:\tlearn: 0.0901331\ttotal: 1m 16s\tremaining: 23.1s\n",
      "767:\tlearn: 0.0900723\ttotal: 1m 16s\tremaining: 23s\n",
      "768:\tlearn: 0.0900385\ttotal: 1m 16s\tremaining: 22.9s\n",
      "769:\tlearn: 0.0899908\ttotal: 1m 16s\tremaining: 22.8s\n",
      "770:\tlearn: 0.0899223\ttotal: 1m 16s\tremaining: 22.7s\n",
      "771:\tlearn: 0.0898896\ttotal: 1m 16s\tremaining: 22.6s\n",
      "772:\tlearn: 0.0898337\ttotal: 1m 16s\tremaining: 22.6s\n",
      "773:\tlearn: 0.0898015\ttotal: 1m 16s\tremaining: 22.5s\n",
      "774:\tlearn: 0.0897681\ttotal: 1m 17s\tremaining: 22.4s\n",
      "775:\tlearn: 0.0897276\ttotal: 1m 17s\tremaining: 22.3s\n",
      "776:\tlearn: 0.0896881\ttotal: 1m 17s\tremaining: 22.2s\n",
      "777:\tlearn: 0.0896332\ttotal: 1m 17s\tremaining: 22.1s\n",
      "778:\tlearn: 0.0895695\ttotal: 1m 17s\tremaining: 22s\n",
      "779:\tlearn: 0.0895234\ttotal: 1m 17s\tremaining: 21.9s\n",
      "780:\tlearn: 0.0894866\ttotal: 1m 17s\tremaining: 21.8s\n",
      "781:\tlearn: 0.0894560\ttotal: 1m 17s\tremaining: 21.7s\n",
      "782:\tlearn: 0.0894273\ttotal: 1m 17s\tremaining: 21.6s\n",
      "783:\tlearn: 0.0893932\ttotal: 1m 18s\tremaining: 21.5s\n",
      "784:\tlearn: 0.0893508\ttotal: 1m 18s\tremaining: 21.4s\n",
      "785:\tlearn: 0.0893131\ttotal: 1m 18s\tremaining: 21.3s\n",
      "786:\tlearn: 0.0892929\ttotal: 1m 18s\tremaining: 21.2s\n",
      "787:\tlearn: 0.0892542\ttotal: 1m 18s\tremaining: 21.1s\n",
      "788:\tlearn: 0.0892388\ttotal: 1m 18s\tremaining: 21s\n",
      "789:\tlearn: 0.0892172\ttotal: 1m 18s\tremaining: 20.9s\n",
      "790:\tlearn: 0.0891676\ttotal: 1m 18s\tremaining: 20.8s\n",
      "791:\tlearn: 0.0891521\ttotal: 1m 18s\tremaining: 20.7s\n",
      "792:\tlearn: 0.0891108\ttotal: 1m 18s\tremaining: 20.6s\n",
      "793:\tlearn: 0.0890796\ttotal: 1m 19s\tremaining: 20.5s\n",
      "794:\tlearn: 0.0890341\ttotal: 1m 19s\tremaining: 20.4s\n",
      "795:\tlearn: 0.0889726\ttotal: 1m 19s\tremaining: 20.3s\n",
      "796:\tlearn: 0.0889252\ttotal: 1m 19s\tremaining: 20.2s\n",
      "797:\tlearn: 0.0888970\ttotal: 1m 19s\tremaining: 20.1s\n",
      "798:\tlearn: 0.0888452\ttotal: 1m 19s\tremaining: 20s\n",
      "799:\tlearn: 0.0887619\ttotal: 1m 19s\tremaining: 19.9s\n",
      "800:\tlearn: 0.0887527\ttotal: 1m 19s\tremaining: 19.8s\n",
      "801:\tlearn: 0.0886950\ttotal: 1m 19s\tremaining: 19.7s\n",
      "802:\tlearn: 0.0886582\ttotal: 1m 20s\tremaining: 19.6s\n",
      "803:\tlearn: 0.0886219\ttotal: 1m 20s\tremaining: 19.5s\n",
      "804:\tlearn: 0.0885604\ttotal: 1m 20s\tremaining: 19.4s\n",
      "805:\tlearn: 0.0885246\ttotal: 1m 20s\tremaining: 19.3s\n",
      "806:\tlearn: 0.0884702\ttotal: 1m 20s\tremaining: 19.2s\n",
      "807:\tlearn: 0.0884335\ttotal: 1m 20s\tremaining: 19.1s\n",
      "808:\tlearn: 0.0883859\ttotal: 1m 20s\tremaining: 19s\n",
      "809:\tlearn: 0.0883515\ttotal: 1m 20s\tremaining: 18.9s\n",
      "810:\tlearn: 0.0883279\ttotal: 1m 20s\tremaining: 18.8s\n",
      "811:\tlearn: 0.0883022\ttotal: 1m 20s\tremaining: 18.7s\n",
      "812:\tlearn: 0.0882558\ttotal: 1m 20s\tremaining: 18.6s\n",
      "813:\tlearn: 0.0882056\ttotal: 1m 21s\tremaining: 18.5s\n",
      "814:\tlearn: 0.0881872\ttotal: 1m 21s\tremaining: 18.4s\n",
      "815:\tlearn: 0.0881719\ttotal: 1m 21s\tremaining: 18.3s\n",
      "816:\tlearn: 0.0881365\ttotal: 1m 21s\tremaining: 18.2s\n",
      "817:\tlearn: 0.0880918\ttotal: 1m 21s\tremaining: 18.1s\n",
      "818:\tlearn: 0.0880654\ttotal: 1m 21s\tremaining: 18s\n",
      "819:\tlearn: 0.0880242\ttotal: 1m 21s\tremaining: 17.9s\n",
      "820:\tlearn: 0.0879757\ttotal: 1m 21s\tremaining: 17.8s\n",
      "821:\tlearn: 0.0879375\ttotal: 1m 21s\tremaining: 17.7s\n",
      "822:\tlearn: 0.0878922\ttotal: 1m 21s\tremaining: 17.6s\n",
      "823:\tlearn: 0.0878533\ttotal: 1m 22s\tremaining: 17.5s\n",
      "824:\tlearn: 0.0878129\ttotal: 1m 22s\tremaining: 17.4s\n",
      "825:\tlearn: 0.0877690\ttotal: 1m 22s\tremaining: 17.3s\n",
      "826:\tlearn: 0.0877247\ttotal: 1m 22s\tremaining: 17.2s\n",
      "827:\tlearn: 0.0876759\ttotal: 1m 22s\tremaining: 17.1s\n",
      "828:\tlearn: 0.0876048\ttotal: 1m 22s\tremaining: 17s\n",
      "829:\tlearn: 0.0875982\ttotal: 1m 22s\tremaining: 16.9s\n",
      "830:\tlearn: 0.0875385\ttotal: 1m 22s\tremaining: 16.8s\n",
      "831:\tlearn: 0.0875069\ttotal: 1m 22s\tremaining: 16.7s\n",
      "832:\tlearn: 0.0874996\ttotal: 1m 22s\tremaining: 16.6s\n",
      "833:\tlearn: 0.0874893\ttotal: 1m 22s\tremaining: 16.5s\n",
      "834:\tlearn: 0.0874466\ttotal: 1m 23s\tremaining: 16.4s\n",
      "835:\tlearn: 0.0874111\ttotal: 1m 23s\tremaining: 16.3s\n",
      "836:\tlearn: 0.0873855\ttotal: 1m 23s\tremaining: 16.2s\n",
      "837:\tlearn: 0.0873502\ttotal: 1m 23s\tremaining: 16.1s\n",
      "838:\tlearn: 0.0873031\ttotal: 1m 23s\tremaining: 16s\n",
      "839:\tlearn: 0.0872634\ttotal: 1m 23s\tremaining: 15.9s\n",
      "840:\tlearn: 0.0871989\ttotal: 1m 23s\tremaining: 15.8s\n",
      "841:\tlearn: 0.0871655\ttotal: 1m 23s\tremaining: 15.7s\n",
      "842:\tlearn: 0.0871489\ttotal: 1m 23s\tremaining: 15.6s\n",
      "843:\tlearn: 0.0870871\ttotal: 1m 23s\tremaining: 15.5s\n",
      "844:\tlearn: 0.0870428\ttotal: 1m 24s\tremaining: 15.4s\n",
      "845:\tlearn: 0.0870028\ttotal: 1m 24s\tremaining: 15.3s\n",
      "846:\tlearn: 0.0869636\ttotal: 1m 24s\tremaining: 15.2s\n",
      "847:\tlearn: 0.0869377\ttotal: 1m 24s\tremaining: 15.1s\n",
      "848:\tlearn: 0.0869139\ttotal: 1m 24s\tremaining: 15s\n",
      "849:\tlearn: 0.0869065\ttotal: 1m 24s\tremaining: 14.9s\n",
      "850:\tlearn: 0.0868707\ttotal: 1m 24s\tremaining: 14.8s\n",
      "851:\tlearn: 0.0868534\ttotal: 1m 24s\tremaining: 14.7s\n",
      "852:\tlearn: 0.0867982\ttotal: 1m 24s\tremaining: 14.6s\n",
      "853:\tlearn: 0.0867629\ttotal: 1m 24s\tremaining: 14.5s\n",
      "854:\tlearn: 0.0867342\ttotal: 1m 25s\tremaining: 14.4s\n",
      "855:\tlearn: 0.0866834\ttotal: 1m 25s\tremaining: 14.3s\n",
      "856:\tlearn: 0.0866574\ttotal: 1m 25s\tremaining: 14.2s\n",
      "857:\tlearn: 0.0866377\ttotal: 1m 25s\tremaining: 14.1s\n",
      "858:\tlearn: 0.0866003\ttotal: 1m 25s\tremaining: 14s\n",
      "859:\tlearn: 0.0865713\ttotal: 1m 25s\tremaining: 13.9s\n",
      "860:\tlearn: 0.0865280\ttotal: 1m 25s\tremaining: 13.8s\n",
      "861:\tlearn: 0.0864837\ttotal: 1m 25s\tremaining: 13.7s\n",
      "862:\tlearn: 0.0864522\ttotal: 1m 25s\tremaining: 13.6s\n",
      "863:\tlearn: 0.0864126\ttotal: 1m 25s\tremaining: 13.5s\n",
      "864:\tlearn: 0.0863769\ttotal: 1m 25s\tremaining: 13.4s\n",
      "865:\tlearn: 0.0863151\ttotal: 1m 26s\tremaining: 13.3s\n",
      "866:\tlearn: 0.0862606\ttotal: 1m 26s\tremaining: 13.2s\n",
      "867:\tlearn: 0.0862213\ttotal: 1m 26s\tremaining: 13.1s\n",
      "868:\tlearn: 0.0862024\ttotal: 1m 26s\tremaining: 13s\n",
      "869:\tlearn: 0.0861501\ttotal: 1m 26s\tremaining: 12.9s\n",
      "870:\tlearn: 0.0861123\ttotal: 1m 26s\tremaining: 12.8s\n",
      "871:\tlearn: 0.0860864\ttotal: 1m 26s\tremaining: 12.7s\n",
      "872:\tlearn: 0.0860367\ttotal: 1m 26s\tremaining: 12.6s\n",
      "873:\tlearn: 0.0859837\ttotal: 1m 26s\tremaining: 12.5s\n",
      "874:\tlearn: 0.0859636\ttotal: 1m 26s\tremaining: 12.4s\n",
      "875:\tlearn: 0.0859343\ttotal: 1m 27s\tremaining: 12.3s\n",
      "876:\tlearn: 0.0858571\ttotal: 1m 27s\tremaining: 12.2s\n",
      "877:\tlearn: 0.0858371\ttotal: 1m 27s\tremaining: 12.1s\n",
      "878:\tlearn: 0.0857947\ttotal: 1m 27s\tremaining: 12s\n",
      "879:\tlearn: 0.0857689\ttotal: 1m 27s\tremaining: 11.9s\n",
      "880:\tlearn: 0.0857270\ttotal: 1m 27s\tremaining: 11.8s\n",
      "881:\tlearn: 0.0857049\ttotal: 1m 27s\tremaining: 11.7s\n",
      "882:\tlearn: 0.0856735\ttotal: 1m 27s\tremaining: 11.6s\n",
      "883:\tlearn: 0.0856233\ttotal: 1m 27s\tremaining: 11.5s\n",
      "884:\tlearn: 0.0855643\ttotal: 1m 27s\tremaining: 11.4s\n",
      "885:\tlearn: 0.0855360\ttotal: 1m 27s\tremaining: 11.3s\n",
      "886:\tlearn: 0.0855098\ttotal: 1m 28s\tremaining: 11.2s\n",
      "887:\tlearn: 0.0854904\ttotal: 1m 28s\tremaining: 11.1s\n",
      "888:\tlearn: 0.0854593\ttotal: 1m 28s\tremaining: 11s\n",
      "889:\tlearn: 0.0854361\ttotal: 1m 28s\tremaining: 10.9s\n",
      "890:\tlearn: 0.0853891\ttotal: 1m 28s\tremaining: 10.8s\n",
      "891:\tlearn: 0.0853455\ttotal: 1m 28s\tremaining: 10.7s\n",
      "892:\tlearn: 0.0852821\ttotal: 1m 28s\tremaining: 10.6s\n",
      "893:\tlearn: 0.0852635\ttotal: 1m 28s\tremaining: 10.5s\n",
      "894:\tlearn: 0.0852087\ttotal: 1m 28s\tremaining: 10.4s\n",
      "895:\tlearn: 0.0851357\ttotal: 1m 28s\tremaining: 10.3s\n",
      "896:\tlearn: 0.0850698\ttotal: 1m 29s\tremaining: 10.2s\n",
      "897:\tlearn: 0.0850258\ttotal: 1m 29s\tremaining: 10.1s\n",
      "898:\tlearn: 0.0849860\ttotal: 1m 29s\tremaining: 10s\n",
      "899:\tlearn: 0.0849355\ttotal: 1m 29s\tremaining: 9.92s\n",
      "900:\tlearn: 0.0848915\ttotal: 1m 29s\tremaining: 9.82s\n",
      "901:\tlearn: 0.0848527\ttotal: 1m 29s\tremaining: 9.72s\n",
      "902:\tlearn: 0.0848169\ttotal: 1m 29s\tremaining: 9.63s\n",
      "903:\tlearn: 0.0847838\ttotal: 1m 29s\tremaining: 9.53s\n",
      "904:\tlearn: 0.0847368\ttotal: 1m 29s\tremaining: 9.43s\n",
      "905:\tlearn: 0.0846891\ttotal: 1m 29s\tremaining: 9.33s\n",
      "906:\tlearn: 0.0846436\ttotal: 1m 30s\tremaining: 9.23s\n",
      "907:\tlearn: 0.0846121\ttotal: 1m 30s\tremaining: 9.13s\n",
      "908:\tlearn: 0.0845931\ttotal: 1m 30s\tremaining: 9.03s\n",
      "909:\tlearn: 0.0845158\ttotal: 1m 30s\tremaining: 8.94s\n",
      "910:\tlearn: 0.0844811\ttotal: 1m 30s\tremaining: 8.84s\n",
      "911:\tlearn: 0.0844382\ttotal: 1m 30s\tremaining: 8.74s\n",
      "912:\tlearn: 0.0844285\ttotal: 1m 30s\tremaining: 8.64s\n",
      "913:\tlearn: 0.0843971\ttotal: 1m 30s\tremaining: 8.54s\n",
      "914:\tlearn: 0.0843494\ttotal: 1m 30s\tremaining: 8.44s\n",
      "915:\tlearn: 0.0843141\ttotal: 1m 30s\tremaining: 8.34s\n",
      "916:\tlearn: 0.0842547\ttotal: 1m 31s\tremaining: 8.24s\n",
      "917:\tlearn: 0.0842140\ttotal: 1m 31s\tremaining: 8.14s\n",
      "918:\tlearn: 0.0841420\ttotal: 1m 31s\tremaining: 8.04s\n",
      "919:\tlearn: 0.0841131\ttotal: 1m 31s\tremaining: 7.94s\n",
      "920:\tlearn: 0.0840436\ttotal: 1m 31s\tremaining: 7.84s\n",
      "921:\tlearn: 0.0840373\ttotal: 1m 31s\tremaining: 7.75s\n",
      "922:\tlearn: 0.0840040\ttotal: 1m 31s\tremaining: 7.65s\n",
      "923:\tlearn: 0.0839728\ttotal: 1m 31s\tremaining: 7.55s\n",
      "924:\tlearn: 0.0839494\ttotal: 1m 31s\tremaining: 7.45s\n",
      "925:\tlearn: 0.0839308\ttotal: 1m 31s\tremaining: 7.35s\n",
      "926:\tlearn: 0.0838985\ttotal: 1m 32s\tremaining: 7.25s\n",
      "927:\tlearn: 0.0838924\ttotal: 1m 32s\tremaining: 7.15s\n",
      "928:\tlearn: 0.0838326\ttotal: 1m 32s\tremaining: 7.05s\n",
      "929:\tlearn: 0.0837899\ttotal: 1m 32s\tremaining: 6.95s\n",
      "930:\tlearn: 0.0837534\ttotal: 1m 32s\tremaining: 6.85s\n",
      "931:\tlearn: 0.0837308\ttotal: 1m 32s\tremaining: 6.75s\n",
      "932:\tlearn: 0.0836582\ttotal: 1m 32s\tremaining: 6.65s\n",
      "933:\tlearn: 0.0836322\ttotal: 1m 32s\tremaining: 6.55s\n",
      "934:\tlearn: 0.0836091\ttotal: 1m 32s\tremaining: 6.45s\n",
      "935:\tlearn: 0.0835631\ttotal: 1m 32s\tremaining: 6.35s\n",
      "936:\tlearn: 0.0835241\ttotal: 1m 32s\tremaining: 6.25s\n",
      "937:\tlearn: 0.0835185\ttotal: 1m 33s\tremaining: 6.15s\n",
      "938:\tlearn: 0.0834941\ttotal: 1m 33s\tremaining: 6.05s\n",
      "939:\tlearn: 0.0834744\ttotal: 1m 33s\tremaining: 5.95s\n",
      "940:\tlearn: 0.0834348\ttotal: 1m 33s\tremaining: 5.85s\n",
      "941:\tlearn: 0.0834199\ttotal: 1m 33s\tremaining: 5.75s\n",
      "942:\tlearn: 0.0833884\ttotal: 1m 33s\tremaining: 5.66s\n",
      "943:\tlearn: 0.0833762\ttotal: 1m 33s\tremaining: 5.56s\n",
      "944:\tlearn: 0.0833449\ttotal: 1m 33s\tremaining: 5.46s\n",
      "945:\tlearn: 0.0832930\ttotal: 1m 33s\tremaining: 5.36s\n",
      "946:\tlearn: 0.0832396\ttotal: 1m 33s\tremaining: 5.26s\n",
      "947:\tlearn: 0.0832043\ttotal: 1m 34s\tremaining: 5.16s\n",
      "948:\tlearn: 0.0831565\ttotal: 1m 34s\tremaining: 5.06s\n",
      "949:\tlearn: 0.0831350\ttotal: 1m 34s\tremaining: 4.96s\n",
      "950:\tlearn: 0.0831013\ttotal: 1m 34s\tremaining: 4.86s\n",
      "951:\tlearn: 0.0830370\ttotal: 1m 34s\tremaining: 4.76s\n",
      "952:\tlearn: 0.0829900\ttotal: 1m 34s\tremaining: 4.66s\n",
      "953:\tlearn: 0.0829587\ttotal: 1m 34s\tremaining: 4.56s\n",
      "954:\tlearn: 0.0829541\ttotal: 1m 34s\tremaining: 4.46s\n",
      "955:\tlearn: 0.0829360\ttotal: 1m 34s\tremaining: 4.36s\n",
      "956:\tlearn: 0.0828946\ttotal: 1m 34s\tremaining: 4.26s\n",
      "957:\tlearn: 0.0828632\ttotal: 1m 35s\tremaining: 4.17s\n",
      "958:\tlearn: 0.0828385\ttotal: 1m 35s\tremaining: 4.07s\n",
      "959:\tlearn: 0.0828076\ttotal: 1m 35s\tremaining: 3.97s\n",
      "960:\tlearn: 0.0827560\ttotal: 1m 35s\tremaining: 3.87s\n",
      "961:\tlearn: 0.0827265\ttotal: 1m 35s\tremaining: 3.77s\n",
      "962:\tlearn: 0.0826841\ttotal: 1m 35s\tremaining: 3.67s\n",
      "963:\tlearn: 0.0826125\ttotal: 1m 35s\tremaining: 3.57s\n",
      "964:\tlearn: 0.0825614\ttotal: 1m 35s\tremaining: 3.47s\n",
      "965:\tlearn: 0.0825101\ttotal: 1m 35s\tremaining: 3.37s\n",
      "966:\tlearn: 0.0824345\ttotal: 1m 35s\tremaining: 3.27s\n",
      "967:\tlearn: 0.0824062\ttotal: 1m 36s\tremaining: 3.17s\n",
      "968:\tlearn: 0.0823855\ttotal: 1m 36s\tremaining: 3.07s\n",
      "969:\tlearn: 0.0823408\ttotal: 1m 36s\tremaining: 2.98s\n",
      "970:\tlearn: 0.0822999\ttotal: 1m 36s\tremaining: 2.88s\n",
      "971:\tlearn: 0.0822614\ttotal: 1m 36s\tremaining: 2.78s\n",
      "972:\tlearn: 0.0822224\ttotal: 1m 36s\tremaining: 2.68s\n",
      "973:\tlearn: 0.0821842\ttotal: 1m 36s\tremaining: 2.58s\n",
      "974:\tlearn: 0.0821246\ttotal: 1m 36s\tremaining: 2.48s\n",
      "975:\tlearn: 0.0821012\ttotal: 1m 36s\tremaining: 2.38s\n",
      "976:\tlearn: 0.0820600\ttotal: 1m 36s\tremaining: 2.28s\n",
      "977:\tlearn: 0.0820165\ttotal: 1m 36s\tremaining: 2.18s\n",
      "978:\tlearn: 0.0819582\ttotal: 1m 37s\tremaining: 2.08s\n",
      "979:\tlearn: 0.0819294\ttotal: 1m 37s\tremaining: 1.98s\n",
      "980:\tlearn: 0.0819049\ttotal: 1m 37s\tremaining: 1.88s\n",
      "981:\tlearn: 0.0818808\ttotal: 1m 37s\tremaining: 1.78s\n",
      "982:\tlearn: 0.0818464\ttotal: 1m 37s\tremaining: 1.69s\n",
      "983:\tlearn: 0.0818416\ttotal: 1m 37s\tremaining: 1.59s\n",
      "984:\tlearn: 0.0818161\ttotal: 1m 37s\tremaining: 1.49s\n",
      "985:\tlearn: 0.0817881\ttotal: 1m 37s\tremaining: 1.39s\n",
      "986:\tlearn: 0.0817652\ttotal: 1m 37s\tremaining: 1.29s\n",
      "987:\tlearn: 0.0817244\ttotal: 1m 37s\tremaining: 1.19s\n",
      "988:\tlearn: 0.0817086\ttotal: 1m 38s\tremaining: 1.09s\n",
      "989:\tlearn: 0.0816760\ttotal: 1m 38s\tremaining: 992ms\n",
      "990:\tlearn: 0.0816094\ttotal: 1m 38s\tremaining: 892ms\n",
      "991:\tlearn: 0.0815574\ttotal: 1m 38s\tremaining: 793ms\n",
      "992:\tlearn: 0.0815480\ttotal: 1m 38s\tremaining: 694ms\n",
      "993:\tlearn: 0.0814742\ttotal: 1m 38s\tremaining: 595ms\n",
      "994:\tlearn: 0.0814373\ttotal: 1m 38s\tremaining: 496ms\n",
      "995:\tlearn: 0.0814186\ttotal: 1m 38s\tremaining: 397ms\n",
      "996:\tlearn: 0.0813767\ttotal: 1m 38s\tremaining: 297ms\n",
      "997:\tlearn: 0.0813466\ttotal: 1m 38s\tremaining: 198ms\n",
      "998:\tlearn: 0.0813239\ttotal: 1m 39s\tremaining: 99.1ms\n",
      "999:\tlearn: 0.0812905\ttotal: 1m 39s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5247142\ttotal: 96.5ms\tremaining: 1m 36s\n",
      "1:\tlearn: 0.4290319\ttotal: 194ms\tremaining: 1m 36s\n",
      "2:\tlearn: 0.3662552\ttotal: 287ms\tremaining: 1m 35s\n",
      "3:\tlearn: 0.3285835\ttotal: 380ms\tremaining: 1m 34s\n",
      "4:\tlearn: 0.2969836\ttotal: 491ms\tremaining: 1m 37s\n",
      "5:\tlearn: 0.2747354\ttotal: 583ms\tremaining: 1m 36s\n",
      "6:\tlearn: 0.2611399\ttotal: 676ms\tremaining: 1m 35s\n",
      "7:\tlearn: 0.2520892\ttotal: 765ms\tremaining: 1m 34s\n",
      "8:\tlearn: 0.2410174\ttotal: 866ms\tremaining: 1m 35s\n",
      "9:\tlearn: 0.2324716\ttotal: 960ms\tremaining: 1m 35s\n",
      "10:\tlearn: 0.2217792\ttotal: 1.05s\tremaining: 1m 34s\n",
      "11:\tlearn: 0.2160220\ttotal: 1.16s\tremaining: 1m 35s\n",
      "12:\tlearn: 0.2115084\ttotal: 1.24s\tremaining: 1m 34s\n",
      "13:\tlearn: 0.2060661\ttotal: 1.34s\tremaining: 1m 34s\n",
      "14:\tlearn: 0.2027406\ttotal: 1.43s\tremaining: 1m 33s\n",
      "15:\tlearn: 0.2006642\ttotal: 1.52s\tremaining: 1m 33s\n",
      "16:\tlearn: 0.1979357\ttotal: 1.61s\tremaining: 1m 33s\n",
      "17:\tlearn: 0.1952674\ttotal: 1.71s\tremaining: 1m 33s\n",
      "18:\tlearn: 0.1926978\ttotal: 1.8s\tremaining: 1m 33s\n",
      "19:\tlearn: 0.1907816\ttotal: 1.9s\tremaining: 1m 33s\n",
      "20:\tlearn: 0.1880369\ttotal: 1.99s\tremaining: 1m 32s\n",
      "21:\tlearn: 0.1866016\ttotal: 2.08s\tremaining: 1m 32s\n",
      "22:\tlearn: 0.1846220\ttotal: 2.18s\tremaining: 1m 32s\n",
      "23:\tlearn: 0.1823096\ttotal: 2.26s\tremaining: 1m 32s\n",
      "24:\tlearn: 0.1804778\ttotal: 2.35s\tremaining: 1m 31s\n",
      "25:\tlearn: 0.1793183\ttotal: 2.45s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1777770\ttotal: 2.55s\tremaining: 1m 31s\n",
      "27:\tlearn: 0.1764437\ttotal: 2.66s\tremaining: 1m 32s\n",
      "28:\tlearn: 0.1752544\ttotal: 2.76s\tremaining: 1m 32s\n",
      "29:\tlearn: 0.1742391\ttotal: 2.85s\tremaining: 1m 32s\n",
      "30:\tlearn: 0.1729539\ttotal: 2.96s\tremaining: 1m 32s\n",
      "31:\tlearn: 0.1712130\ttotal: 3.05s\tremaining: 1m 32s\n",
      "32:\tlearn: 0.1700782\ttotal: 3.14s\tremaining: 1m 31s\n",
      "33:\tlearn: 0.1691012\ttotal: 3.23s\tremaining: 1m 31s\n",
      "34:\tlearn: 0.1682603\ttotal: 3.33s\tremaining: 1m 31s\n",
      "35:\tlearn: 0.1673643\ttotal: 3.42s\tremaining: 1m 31s\n",
      "36:\tlearn: 0.1665535\ttotal: 3.52s\tremaining: 1m 31s\n",
      "37:\tlearn: 0.1656842\ttotal: 3.62s\tremaining: 1m 31s\n",
      "38:\tlearn: 0.1648665\ttotal: 3.71s\tremaining: 1m 31s\n",
      "39:\tlearn: 0.1633246\ttotal: 3.81s\tremaining: 1m 31s\n",
      "40:\tlearn: 0.1625337\ttotal: 3.9s\tremaining: 1m 31s\n",
      "41:\tlearn: 0.1618969\ttotal: 3.99s\tremaining: 1m 30s\n",
      "42:\tlearn: 0.1614084\ttotal: 4.08s\tremaining: 1m 30s\n",
      "43:\tlearn: 0.1609816\ttotal: 4.18s\tremaining: 1m 30s\n",
      "44:\tlearn: 0.1605900\ttotal: 4.27s\tremaining: 1m 30s\n",
      "45:\tlearn: 0.1600429\ttotal: 4.36s\tremaining: 1m 30s\n",
      "46:\tlearn: 0.1593968\ttotal: 4.46s\tremaining: 1m 30s\n",
      "47:\tlearn: 0.1589328\ttotal: 4.55s\tremaining: 1m 30s\n",
      "48:\tlearn: 0.1580936\ttotal: 4.68s\tremaining: 1m 30s\n",
      "49:\tlearn: 0.1572806\ttotal: 4.78s\tremaining: 1m 30s\n",
      "50:\tlearn: 0.1566601\ttotal: 4.88s\tremaining: 1m 30s\n",
      "51:\tlearn: 0.1559921\ttotal: 4.97s\tremaining: 1m 30s\n",
      "52:\tlearn: 0.1554978\ttotal: 5.07s\tremaining: 1m 30s\n",
      "53:\tlearn: 0.1549274\ttotal: 5.16s\tremaining: 1m 30s\n",
      "54:\tlearn: 0.1544377\ttotal: 5.26s\tremaining: 1m 30s\n",
      "55:\tlearn: 0.1537820\ttotal: 5.36s\tremaining: 1m 30s\n",
      "56:\tlearn: 0.1532621\ttotal: 5.45s\tremaining: 1m 30s\n",
      "57:\tlearn: 0.1529369\ttotal: 5.54s\tremaining: 1m 30s\n",
      "58:\tlearn: 0.1524606\ttotal: 5.64s\tremaining: 1m 29s\n",
      "59:\tlearn: 0.1521997\ttotal: 5.73s\tremaining: 1m 29s\n",
      "60:\tlearn: 0.1517807\ttotal: 5.82s\tremaining: 1m 29s\n",
      "61:\tlearn: 0.1512709\ttotal: 5.92s\tremaining: 1m 29s\n",
      "62:\tlearn: 0.1507872\ttotal: 6.01s\tremaining: 1m 29s\n",
      "63:\tlearn: 0.1502769\ttotal: 6.11s\tremaining: 1m 29s\n",
      "64:\tlearn: 0.1497003\ttotal: 6.21s\tremaining: 1m 29s\n",
      "65:\tlearn: 0.1493667\ttotal: 6.3s\tremaining: 1m 29s\n",
      "66:\tlearn: 0.1490641\ttotal: 6.39s\tremaining: 1m 29s\n",
      "67:\tlearn: 0.1487439\ttotal: 6.49s\tremaining: 1m 28s\n",
      "68:\tlearn: 0.1482392\ttotal: 6.58s\tremaining: 1m 28s\n",
      "69:\tlearn: 0.1478802\ttotal: 6.68s\tremaining: 1m 28s\n",
      "70:\tlearn: 0.1474656\ttotal: 6.78s\tremaining: 1m 28s\n",
      "71:\tlearn: 0.1472309\ttotal: 6.87s\tremaining: 1m 28s\n",
      "72:\tlearn: 0.1469294\ttotal: 6.97s\tremaining: 1m 28s\n",
      "73:\tlearn: 0.1466011\ttotal: 7.06s\tremaining: 1m 28s\n",
      "74:\tlearn: 0.1462527\ttotal: 7.16s\tremaining: 1m 28s\n",
      "75:\tlearn: 0.1459718\ttotal: 7.26s\tremaining: 1m 28s\n",
      "76:\tlearn: 0.1456238\ttotal: 7.35s\tremaining: 1m 28s\n",
      "77:\tlearn: 0.1453612\ttotal: 7.45s\tremaining: 1m 28s\n",
      "78:\tlearn: 0.1450347\ttotal: 7.55s\tremaining: 1m 27s\n",
      "79:\tlearn: 0.1447586\ttotal: 7.65s\tremaining: 1m 27s\n",
      "80:\tlearn: 0.1445564\ttotal: 7.74s\tremaining: 1m 27s\n",
      "81:\tlearn: 0.1442456\ttotal: 7.84s\tremaining: 1m 27s\n",
      "82:\tlearn: 0.1440459\ttotal: 7.93s\tremaining: 1m 27s\n",
      "83:\tlearn: 0.1436258\ttotal: 8.02s\tremaining: 1m 27s\n",
      "84:\tlearn: 0.1433357\ttotal: 8.12s\tremaining: 1m 27s\n",
      "85:\tlearn: 0.1430034\ttotal: 8.21s\tremaining: 1m 27s\n",
      "86:\tlearn: 0.1428288\ttotal: 8.3s\tremaining: 1m 27s\n",
      "87:\tlearn: 0.1425692\ttotal: 8.4s\tremaining: 1m 27s\n",
      "88:\tlearn: 0.1423890\ttotal: 8.49s\tremaining: 1m 26s\n",
      "89:\tlearn: 0.1421052\ttotal: 8.58s\tremaining: 1m 26s\n",
      "90:\tlearn: 0.1418699\ttotal: 8.67s\tremaining: 1m 26s\n",
      "91:\tlearn: 0.1415642\ttotal: 8.77s\tremaining: 1m 26s\n",
      "92:\tlearn: 0.1413346\ttotal: 8.86s\tremaining: 1m 26s\n",
      "93:\tlearn: 0.1411255\ttotal: 8.96s\tremaining: 1m 26s\n",
      "94:\tlearn: 0.1408647\ttotal: 9.09s\tremaining: 1m 26s\n",
      "95:\tlearn: 0.1406386\ttotal: 9.26s\tremaining: 1m 27s\n",
      "96:\tlearn: 0.1404526\ttotal: 9.36s\tremaining: 1m 27s\n",
      "97:\tlearn: 0.1403146\ttotal: 9.45s\tremaining: 1m 26s\n",
      "98:\tlearn: 0.1399154\ttotal: 9.54s\tremaining: 1m 26s\n",
      "99:\tlearn: 0.1396737\ttotal: 9.64s\tremaining: 1m 26s\n",
      "100:\tlearn: 0.1393900\ttotal: 9.73s\tremaining: 1m 26s\n",
      "101:\tlearn: 0.1390799\ttotal: 9.83s\tremaining: 1m 26s\n",
      "102:\tlearn: 0.1388730\ttotal: 9.92s\tremaining: 1m 26s\n",
      "103:\tlearn: 0.1386459\ttotal: 10s\tremaining: 1m 26s\n",
      "104:\tlearn: 0.1382075\ttotal: 10.1s\tremaining: 1m 26s\n",
      "105:\tlearn: 0.1380331\ttotal: 10.2s\tremaining: 1m 25s\n",
      "106:\tlearn: 0.1378826\ttotal: 10.3s\tremaining: 1m 25s\n",
      "107:\tlearn: 0.1376540\ttotal: 10.4s\tremaining: 1m 25s\n",
      "108:\tlearn: 0.1374066\ttotal: 10.5s\tremaining: 1m 25s\n",
      "109:\tlearn: 0.1371449\ttotal: 10.6s\tremaining: 1m 25s\n",
      "110:\tlearn: 0.1368167\ttotal: 10.7s\tremaining: 1m 25s\n",
      "111:\tlearn: 0.1366158\ttotal: 10.8s\tremaining: 1m 25s\n",
      "112:\tlearn: 0.1363906\ttotal: 10.9s\tremaining: 1m 25s\n",
      "113:\tlearn: 0.1362661\ttotal: 11s\tremaining: 1m 25s\n",
      "114:\tlearn: 0.1359261\ttotal: 11s\tremaining: 1m 24s\n",
      "115:\tlearn: 0.1356938\ttotal: 11.1s\tremaining: 1m 24s\n",
      "116:\tlearn: 0.1355054\ttotal: 11.2s\tremaining: 1m 24s\n",
      "117:\tlearn: 0.1352392\ttotal: 11.3s\tremaining: 1m 24s\n",
      "118:\tlearn: 0.1350467\ttotal: 11.4s\tremaining: 1m 24s\n",
      "119:\tlearn: 0.1348777\ttotal: 11.5s\tremaining: 1m 24s\n",
      "120:\tlearn: 0.1347154\ttotal: 11.6s\tremaining: 1m 24s\n",
      "121:\tlearn: 0.1345329\ttotal: 11.7s\tremaining: 1m 24s\n",
      "122:\tlearn: 0.1342786\ttotal: 11.8s\tremaining: 1m 24s\n",
      "123:\tlearn: 0.1341378\ttotal: 11.9s\tremaining: 1m 23s\n",
      "124:\tlearn: 0.1339675\ttotal: 12s\tremaining: 1m 23s\n",
      "125:\tlearn: 0.1337047\ttotal: 12.1s\tremaining: 1m 23s\n",
      "126:\tlearn: 0.1334960\ttotal: 12.2s\tremaining: 1m 23s\n",
      "127:\tlearn: 0.1332800\ttotal: 12.3s\tremaining: 1m 23s\n",
      "128:\tlearn: 0.1330483\ttotal: 12.4s\tremaining: 1m 23s\n",
      "129:\tlearn: 0.1327978\ttotal: 12.5s\tremaining: 1m 23s\n",
      "130:\tlearn: 0.1326519\ttotal: 12.6s\tremaining: 1m 23s\n",
      "131:\tlearn: 0.1324372\ttotal: 12.7s\tremaining: 1m 23s\n",
      "132:\tlearn: 0.1322280\ttotal: 12.8s\tremaining: 1m 23s\n",
      "133:\tlearn: 0.1320908\ttotal: 12.8s\tremaining: 1m 22s\n",
      "134:\tlearn: 0.1319449\ttotal: 13s\tremaining: 1m 23s\n",
      "135:\tlearn: 0.1317987\ttotal: 13.1s\tremaining: 1m 22s\n",
      "136:\tlearn: 0.1315724\ttotal: 13.2s\tremaining: 1m 22s\n",
      "137:\tlearn: 0.1313807\ttotal: 13.3s\tremaining: 1m 22s\n",
      "138:\tlearn: 0.1312960\ttotal: 13.4s\tremaining: 1m 22s\n",
      "139:\tlearn: 0.1310957\ttotal: 13.5s\tremaining: 1m 22s\n",
      "140:\tlearn: 0.1309753\ttotal: 13.6s\tremaining: 1m 22s\n",
      "141:\tlearn: 0.1307997\ttotal: 13.6s\tremaining: 1m 22s\n",
      "142:\tlearn: 0.1306800\ttotal: 13.7s\tremaining: 1m 22s\n",
      "143:\tlearn: 0.1305669\ttotal: 13.8s\tremaining: 1m 22s\n",
      "144:\tlearn: 0.1304110\ttotal: 13.9s\tremaining: 1m 22s\n",
      "145:\tlearn: 0.1302813\ttotal: 14s\tremaining: 1m 22s\n",
      "146:\tlearn: 0.1300555\ttotal: 14.1s\tremaining: 1m 21s\n",
      "147:\tlearn: 0.1299303\ttotal: 14.2s\tremaining: 1m 21s\n",
      "148:\tlearn: 0.1297095\ttotal: 14.3s\tremaining: 1m 21s\n",
      "149:\tlearn: 0.1295947\ttotal: 14.4s\tremaining: 1m 21s\n",
      "150:\tlearn: 0.1294462\ttotal: 14.5s\tremaining: 1m 21s\n",
      "151:\tlearn: 0.1292897\ttotal: 14.6s\tremaining: 1m 21s\n",
      "152:\tlearn: 0.1291531\ttotal: 14.7s\tremaining: 1m 21s\n",
      "153:\tlearn: 0.1289635\ttotal: 14.8s\tremaining: 1m 21s\n",
      "154:\tlearn: 0.1287964\ttotal: 14.9s\tremaining: 1m 21s\n",
      "155:\tlearn: 0.1286321\ttotal: 15s\tremaining: 1m 21s\n",
      "156:\tlearn: 0.1285308\ttotal: 15.1s\tremaining: 1m 21s\n",
      "157:\tlearn: 0.1284044\ttotal: 15.2s\tremaining: 1m 20s\n",
      "158:\tlearn: 0.1282614\ttotal: 15.3s\tremaining: 1m 20s\n",
      "159:\tlearn: 0.1281640\ttotal: 15.4s\tremaining: 1m 20s\n",
      "160:\tlearn: 0.1278987\ttotal: 15.5s\tremaining: 1m 20s\n",
      "161:\tlearn: 0.1277917\ttotal: 15.6s\tremaining: 1m 20s\n",
      "162:\tlearn: 0.1276562\ttotal: 15.7s\tremaining: 1m 20s\n",
      "163:\tlearn: 0.1275424\ttotal: 15.8s\tremaining: 1m 20s\n",
      "164:\tlearn: 0.1273804\ttotal: 15.9s\tremaining: 1m 20s\n",
      "165:\tlearn: 0.1272900\ttotal: 16s\tremaining: 1m 20s\n",
      "166:\tlearn: 0.1271990\ttotal: 16s\tremaining: 1m 20s\n",
      "167:\tlearn: 0.1270662\ttotal: 16.1s\tremaining: 1m 19s\n",
      "168:\tlearn: 0.1269807\ttotal: 16.2s\tremaining: 1m 19s\n",
      "169:\tlearn: 0.1268347\ttotal: 16.3s\tremaining: 1m 19s\n",
      "170:\tlearn: 0.1267478\ttotal: 16.4s\tremaining: 1m 19s\n",
      "171:\tlearn: 0.1265774\ttotal: 16.5s\tremaining: 1m 19s\n",
      "172:\tlearn: 0.1264081\ttotal: 16.6s\tremaining: 1m 19s\n",
      "173:\tlearn: 0.1262483\ttotal: 16.7s\tremaining: 1m 19s\n",
      "174:\tlearn: 0.1261476\ttotal: 16.8s\tremaining: 1m 19s\n",
      "175:\tlearn: 0.1259788\ttotal: 16.9s\tremaining: 1m 19s\n",
      "176:\tlearn: 0.1257798\ttotal: 17s\tremaining: 1m 19s\n",
      "177:\tlearn: 0.1256822\ttotal: 17.1s\tremaining: 1m 18s\n",
      "178:\tlearn: 0.1255589\ttotal: 17.2s\tremaining: 1m 18s\n",
      "179:\tlearn: 0.1254087\ttotal: 17.3s\tremaining: 1m 18s\n",
      "180:\tlearn: 0.1253234\ttotal: 17.4s\tremaining: 1m 18s\n",
      "181:\tlearn: 0.1251857\ttotal: 17.5s\tremaining: 1m 18s\n",
      "182:\tlearn: 0.1249929\ttotal: 17.6s\tremaining: 1m 18s\n",
      "183:\tlearn: 0.1248791\ttotal: 17.7s\tremaining: 1m 18s\n",
      "184:\tlearn: 0.1247574\ttotal: 17.8s\tremaining: 1m 18s\n",
      "185:\tlearn: 0.1246110\ttotal: 17.9s\tremaining: 1m 18s\n",
      "186:\tlearn: 0.1244564\ttotal: 18s\tremaining: 1m 18s\n",
      "187:\tlearn: 0.1243859\ttotal: 18.1s\tremaining: 1m 18s\n",
      "188:\tlearn: 0.1242768\ttotal: 18.2s\tremaining: 1m 18s\n",
      "189:\tlearn: 0.1241866\ttotal: 18.3s\tremaining: 1m 17s\n",
      "190:\tlearn: 0.1240816\ttotal: 18.4s\tremaining: 1m 17s\n",
      "191:\tlearn: 0.1239623\ttotal: 18.5s\tremaining: 1m 17s\n",
      "192:\tlearn: 0.1238748\ttotal: 18.6s\tremaining: 1m 17s\n",
      "193:\tlearn: 0.1237862\ttotal: 18.7s\tremaining: 1m 17s\n",
      "194:\tlearn: 0.1236979\ttotal: 18.7s\tremaining: 1m 17s\n",
      "195:\tlearn: 0.1235935\ttotal: 18.8s\tremaining: 1m 17s\n",
      "196:\tlearn: 0.1233832\ttotal: 18.9s\tremaining: 1m 17s\n",
      "197:\tlearn: 0.1232541\ttotal: 19s\tremaining: 1m 17s\n",
      "198:\tlearn: 0.1230974\ttotal: 19.1s\tremaining: 1m 17s\n",
      "199:\tlearn: 0.1230000\ttotal: 19.2s\tremaining: 1m 16s\n",
      "200:\tlearn: 0.1228879\ttotal: 19.3s\tremaining: 1m 16s\n",
      "201:\tlearn: 0.1227618\ttotal: 19.4s\tremaining: 1m 16s\n",
      "202:\tlearn: 0.1226634\ttotal: 19.5s\tremaining: 1m 16s\n",
      "203:\tlearn: 0.1225659\ttotal: 19.6s\tremaining: 1m 16s\n",
      "204:\tlearn: 0.1224876\ttotal: 19.7s\tremaining: 1m 16s\n",
      "205:\tlearn: 0.1224038\ttotal: 19.8s\tremaining: 1m 16s\n",
      "206:\tlearn: 0.1222986\ttotal: 19.9s\tremaining: 1m 16s\n",
      "207:\tlearn: 0.1221665\ttotal: 20.1s\tremaining: 1m 16s\n",
      "208:\tlearn: 0.1220021\ttotal: 20.2s\tremaining: 1m 16s\n",
      "209:\tlearn: 0.1218890\ttotal: 20.3s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1218059\ttotal: 20.4s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1217227\ttotal: 20.5s\tremaining: 1m 16s\n",
      "212:\tlearn: 0.1216273\ttotal: 20.6s\tremaining: 1m 16s\n",
      "213:\tlearn: 0.1214810\ttotal: 20.7s\tremaining: 1m 16s\n",
      "214:\tlearn: 0.1213770\ttotal: 20.8s\tremaining: 1m 15s\n",
      "215:\tlearn: 0.1212872\ttotal: 20.9s\tremaining: 1m 15s\n",
      "216:\tlearn: 0.1212118\ttotal: 21s\tremaining: 1m 15s\n",
      "217:\tlearn: 0.1211335\ttotal: 21.1s\tremaining: 1m 15s\n",
      "218:\tlearn: 0.1210574\ttotal: 21.2s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1208791\ttotal: 21.3s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1207165\ttotal: 21.4s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.1206384\ttotal: 21.5s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.1205391\ttotal: 21.6s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.1204514\ttotal: 21.7s\tremaining: 1m 15s\n",
      "224:\tlearn: 0.1203252\ttotal: 21.8s\tremaining: 1m 14s\n",
      "225:\tlearn: 0.1202140\ttotal: 21.9s\tremaining: 1m 14s\n",
      "226:\tlearn: 0.1201345\ttotal: 22s\tremaining: 1m 14s\n",
      "227:\tlearn: 0.1200165\ttotal: 22.1s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1199505\ttotal: 22.1s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1198620\ttotal: 22.2s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1197694\ttotal: 22.3s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1196419\ttotal: 22.4s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1195453\ttotal: 22.5s\tremaining: 1m 14s\n",
      "233:\tlearn: 0.1194415\ttotal: 22.6s\tremaining: 1m 14s\n",
      "234:\tlearn: 0.1193641\ttotal: 22.7s\tremaining: 1m 13s\n",
      "235:\tlearn: 0.1192952\ttotal: 22.8s\tremaining: 1m 13s\n",
      "236:\tlearn: 0.1191535\ttotal: 22.9s\tremaining: 1m 13s\n",
      "237:\tlearn: 0.1190850\ttotal: 23s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1189564\ttotal: 23.1s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1188672\ttotal: 23.2s\tremaining: 1m 13s\n",
      "240:\tlearn: 0.1188024\ttotal: 23.3s\tremaining: 1m 13s\n",
      "241:\tlearn: 0.1186929\ttotal: 23.4s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1186097\ttotal: 23.5s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1185232\ttotal: 23.6s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1183942\ttotal: 23.7s\tremaining: 1m 12s\n",
      "245:\tlearn: 0.1183201\ttotal: 23.8s\tremaining: 1m 12s\n",
      "246:\tlearn: 0.1182528\ttotal: 23.9s\tremaining: 1m 12s\n",
      "247:\tlearn: 0.1181763\ttotal: 24s\tremaining: 1m 12s\n",
      "248:\tlearn: 0.1181012\ttotal: 24.1s\tremaining: 1m 12s\n",
      "249:\tlearn: 0.1180398\ttotal: 24.2s\tremaining: 1m 12s\n",
      "250:\tlearn: 0.1179590\ttotal: 24.3s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.1178490\ttotal: 24.4s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.1177588\ttotal: 24.4s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.1176626\ttotal: 24.5s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.1175720\ttotal: 24.6s\tremaining: 1m 11s\n",
      "255:\tlearn: 0.1174761\ttotal: 24.7s\tremaining: 1m 11s\n",
      "256:\tlearn: 0.1173988\ttotal: 24.8s\tremaining: 1m 11s\n",
      "257:\tlearn: 0.1173485\ttotal: 24.9s\tremaining: 1m 11s\n",
      "258:\tlearn: 0.1173083\ttotal: 25s\tremaining: 1m 11s\n",
      "259:\tlearn: 0.1172119\ttotal: 25.1s\tremaining: 1m 11s\n",
      "260:\tlearn: 0.1170693\ttotal: 25.2s\tremaining: 1m 11s\n",
      "261:\tlearn: 0.1170081\ttotal: 25.3s\tremaining: 1m 11s\n",
      "262:\tlearn: 0.1169300\ttotal: 25.4s\tremaining: 1m 11s\n",
      "263:\tlearn: 0.1168331\ttotal: 25.5s\tremaining: 1m 11s\n",
      "264:\tlearn: 0.1167321\ttotal: 25.6s\tremaining: 1m 11s\n",
      "265:\tlearn: 0.1166709\ttotal: 25.7s\tremaining: 1m 10s\n",
      "266:\tlearn: 0.1165897\ttotal: 25.8s\tremaining: 1m 10s\n",
      "267:\tlearn: 0.1164707\ttotal: 25.9s\tremaining: 1m 10s\n",
      "268:\tlearn: 0.1163407\ttotal: 26s\tremaining: 1m 10s\n",
      "269:\tlearn: 0.1162691\ttotal: 26.1s\tremaining: 1m 10s\n",
      "270:\tlearn: 0.1161864\ttotal: 26.2s\tremaining: 1m 10s\n",
      "271:\tlearn: 0.1161165\ttotal: 26.3s\tremaining: 1m 10s\n",
      "272:\tlearn: 0.1160044\ttotal: 26.4s\tremaining: 1m 10s\n",
      "273:\tlearn: 0.1158990\ttotal: 26.5s\tremaining: 1m 10s\n",
      "274:\tlearn: 0.1158360\ttotal: 26.6s\tremaining: 1m 10s\n",
      "275:\tlearn: 0.1157836\ttotal: 26.7s\tremaining: 1m 10s\n",
      "276:\tlearn: 0.1156688\ttotal: 26.8s\tremaining: 1m 9s\n",
      "277:\tlearn: 0.1155881\ttotal: 26.9s\tremaining: 1m 9s\n",
      "278:\tlearn: 0.1154832\ttotal: 27s\tremaining: 1m 9s\n",
      "279:\tlearn: 0.1153797\ttotal: 27.1s\tremaining: 1m 9s\n",
      "280:\tlearn: 0.1152845\ttotal: 27.2s\tremaining: 1m 9s\n",
      "281:\tlearn: 0.1152215\ttotal: 27.3s\tremaining: 1m 9s\n",
      "282:\tlearn: 0.1151648\ttotal: 27.4s\tremaining: 1m 9s\n",
      "283:\tlearn: 0.1150675\ttotal: 27.5s\tremaining: 1m 9s\n",
      "284:\tlearn: 0.1149638\ttotal: 27.6s\tremaining: 1m 9s\n",
      "285:\tlearn: 0.1148778\ttotal: 27.7s\tremaining: 1m 9s\n",
      "286:\tlearn: 0.1148189\ttotal: 27.8s\tremaining: 1m 8s\n",
      "287:\tlearn: 0.1147735\ttotal: 27.8s\tremaining: 1m 8s\n",
      "288:\tlearn: 0.1147135\ttotal: 27.9s\tremaining: 1m 8s\n",
      "289:\tlearn: 0.1146410\ttotal: 28s\tremaining: 1m 8s\n",
      "290:\tlearn: 0.1145926\ttotal: 28.1s\tremaining: 1m 8s\n",
      "291:\tlearn: 0.1145223\ttotal: 28.2s\tremaining: 1m 8s\n",
      "292:\tlearn: 0.1144283\ttotal: 28.3s\tremaining: 1m 8s\n",
      "293:\tlearn: 0.1143778\ttotal: 28.4s\tremaining: 1m 8s\n",
      "294:\tlearn: 0.1142480\ttotal: 28.5s\tremaining: 1m 8s\n",
      "295:\tlearn: 0.1142257\ttotal: 28.6s\tremaining: 1m 8s\n",
      "296:\tlearn: 0.1141463\ttotal: 28.7s\tremaining: 1m 7s\n",
      "297:\tlearn: 0.1140559\ttotal: 28.8s\tremaining: 1m 7s\n",
      "298:\tlearn: 0.1140196\ttotal: 28.9s\tremaining: 1m 7s\n",
      "299:\tlearn: 0.1139539\ttotal: 29s\tremaining: 1m 7s\n",
      "300:\tlearn: 0.1138379\ttotal: 29.1s\tremaining: 1m 7s\n",
      "301:\tlearn: 0.1137363\ttotal: 29.2s\tremaining: 1m 7s\n",
      "302:\tlearn: 0.1136611\ttotal: 29.3s\tremaining: 1m 7s\n",
      "303:\tlearn: 0.1135326\ttotal: 29.4s\tremaining: 1m 7s\n",
      "304:\tlearn: 0.1134628\ttotal: 29.5s\tremaining: 1m 7s\n",
      "305:\tlearn: 0.1134153\ttotal: 29.6s\tremaining: 1m 7s\n",
      "306:\tlearn: 0.1133590\ttotal: 29.7s\tremaining: 1m 6s\n",
      "307:\tlearn: 0.1132577\ttotal: 29.8s\tremaining: 1m 6s\n",
      "308:\tlearn: 0.1131683\ttotal: 29.9s\tremaining: 1m 6s\n",
      "309:\tlearn: 0.1131015\ttotal: 30s\tremaining: 1m 6s\n",
      "310:\tlearn: 0.1130395\ttotal: 30.1s\tremaining: 1m 6s\n",
      "311:\tlearn: 0.1129564\ttotal: 30.1s\tremaining: 1m 6s\n",
      "312:\tlearn: 0.1128990\ttotal: 30.2s\tremaining: 1m 6s\n",
      "313:\tlearn: 0.1128224\ttotal: 30.3s\tremaining: 1m 6s\n",
      "314:\tlearn: 0.1127848\ttotal: 30.4s\tremaining: 1m 6s\n",
      "315:\tlearn: 0.1127165\ttotal: 30.5s\tremaining: 1m 6s\n",
      "316:\tlearn: 0.1126764\ttotal: 30.6s\tremaining: 1m 5s\n",
      "317:\tlearn: 0.1126074\ttotal: 30.8s\tremaining: 1m 5s\n",
      "318:\tlearn: 0.1125427\ttotal: 30.9s\tremaining: 1m 5s\n",
      "319:\tlearn: 0.1125312\ttotal: 31s\tremaining: 1m 5s\n",
      "320:\tlearn: 0.1124177\ttotal: 31.1s\tremaining: 1m 5s\n",
      "321:\tlearn: 0.1123354\ttotal: 31.2s\tremaining: 1m 5s\n",
      "322:\tlearn: 0.1122505\ttotal: 31.3s\tremaining: 1m 5s\n",
      "323:\tlearn: 0.1121877\ttotal: 31.4s\tremaining: 1m 5s\n",
      "324:\tlearn: 0.1121278\ttotal: 31.5s\tremaining: 1m 5s\n",
      "325:\tlearn: 0.1120654\ttotal: 31.6s\tremaining: 1m 5s\n",
      "326:\tlearn: 0.1119621\ttotal: 31.7s\tremaining: 1m 5s\n",
      "327:\tlearn: 0.1118909\ttotal: 31.8s\tremaining: 1m 5s\n",
      "328:\tlearn: 0.1117945\ttotal: 31.9s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1117305\ttotal: 32s\tremaining: 1m 4s\n",
      "330:\tlearn: 0.1116406\ttotal: 32.1s\tremaining: 1m 4s\n",
      "331:\tlearn: 0.1115625\ttotal: 32.2s\tremaining: 1m 4s\n",
      "332:\tlearn: 0.1114836\ttotal: 32.3s\tremaining: 1m 4s\n",
      "333:\tlearn: 0.1113890\ttotal: 32.4s\tremaining: 1m 4s\n",
      "334:\tlearn: 0.1113620\ttotal: 32.4s\tremaining: 1m 4s\n",
      "335:\tlearn: 0.1113127\ttotal: 32.5s\tremaining: 1m 4s\n",
      "336:\tlearn: 0.1112471\ttotal: 32.6s\tremaining: 1m 4s\n",
      "337:\tlearn: 0.1111673\ttotal: 32.8s\tremaining: 1m 4s\n",
      "338:\tlearn: 0.1111143\ttotal: 32.8s\tremaining: 1m 4s\n",
      "339:\tlearn: 0.1110427\ttotal: 32.9s\tremaining: 1m 3s\n",
      "340:\tlearn: 0.1109699\ttotal: 33s\tremaining: 1m 3s\n",
      "341:\tlearn: 0.1109105\ttotal: 33.1s\tremaining: 1m 3s\n",
      "342:\tlearn: 0.1108035\ttotal: 33.2s\tremaining: 1m 3s\n",
      "343:\tlearn: 0.1107678\ttotal: 33.3s\tremaining: 1m 3s\n",
      "344:\tlearn: 0.1107121\ttotal: 33.4s\tremaining: 1m 3s\n",
      "345:\tlearn: 0.1106216\ttotal: 33.5s\tremaining: 1m 3s\n",
      "346:\tlearn: 0.1105299\ttotal: 33.6s\tremaining: 1m 3s\n",
      "347:\tlearn: 0.1104339\ttotal: 33.7s\tremaining: 1m 3s\n",
      "348:\tlearn: 0.1103471\ttotal: 33.8s\tremaining: 1m 3s\n",
      "349:\tlearn: 0.1102855\ttotal: 33.9s\tremaining: 1m 3s\n",
      "350:\tlearn: 0.1102040\ttotal: 34s\tremaining: 1m 2s\n",
      "351:\tlearn: 0.1101308\ttotal: 34.1s\tremaining: 1m 2s\n",
      "352:\tlearn: 0.1100712\ttotal: 34.2s\tremaining: 1m 2s\n",
      "353:\tlearn: 0.1099928\ttotal: 34.3s\tremaining: 1m 2s\n",
      "354:\tlearn: 0.1099539\ttotal: 34.4s\tremaining: 1m 2s\n",
      "355:\tlearn: 0.1099014\ttotal: 34.5s\tremaining: 1m 2s\n",
      "356:\tlearn: 0.1098632\ttotal: 34.6s\tremaining: 1m 2s\n",
      "357:\tlearn: 0.1097862\ttotal: 34.7s\tremaining: 1m 2s\n",
      "358:\tlearn: 0.1097106\ttotal: 34.8s\tremaining: 1m 2s\n",
      "359:\tlearn: 0.1096077\ttotal: 34.9s\tremaining: 1m 2s\n",
      "360:\tlearn: 0.1095293\ttotal: 35s\tremaining: 1m 1s\n",
      "361:\tlearn: 0.1094603\ttotal: 35.1s\tremaining: 1m 1s\n",
      "362:\tlearn: 0.1093909\ttotal: 35.2s\tremaining: 1m 1s\n",
      "363:\tlearn: 0.1093445\ttotal: 35.3s\tremaining: 1m 1s\n",
      "364:\tlearn: 0.1092544\ttotal: 35.4s\tremaining: 1m 1s\n",
      "365:\tlearn: 0.1091960\ttotal: 35.5s\tremaining: 1m 1s\n",
      "366:\tlearn: 0.1091193\ttotal: 35.6s\tremaining: 1m 1s\n",
      "367:\tlearn: 0.1090714\ttotal: 35.7s\tremaining: 1m 1s\n",
      "368:\tlearn: 0.1090255\ttotal: 35.8s\tremaining: 1m 1s\n",
      "369:\tlearn: 0.1089614\ttotal: 35.9s\tremaining: 1m 1s\n",
      "370:\tlearn: 0.1088988\ttotal: 35.9s\tremaining: 1m\n",
      "371:\tlearn: 0.1088537\ttotal: 36s\tremaining: 1m\n",
      "372:\tlearn: 0.1088012\ttotal: 36.1s\tremaining: 1m\n",
      "373:\tlearn: 0.1087195\ttotal: 36.2s\tremaining: 1m\n",
      "374:\tlearn: 0.1086712\ttotal: 36.3s\tremaining: 1m\n",
      "375:\tlearn: 0.1085692\ttotal: 36.4s\tremaining: 1m\n",
      "376:\tlearn: 0.1085141\ttotal: 36.5s\tremaining: 1m\n",
      "377:\tlearn: 0.1084217\ttotal: 36.6s\tremaining: 1m\n",
      "378:\tlearn: 0.1083925\ttotal: 36.7s\tremaining: 1m\n",
      "379:\tlearn: 0.1083423\ttotal: 36.8s\tremaining: 1m\n",
      "380:\tlearn: 0.1082509\ttotal: 36.9s\tremaining: 60s\n",
      "381:\tlearn: 0.1081810\ttotal: 37s\tremaining: 59.9s\n",
      "382:\tlearn: 0.1080899\ttotal: 37.1s\tremaining: 59.8s\n",
      "383:\tlearn: 0.1080243\ttotal: 37.2s\tremaining: 59.7s\n",
      "384:\tlearn: 0.1079494\ttotal: 37.3s\tremaining: 59.6s\n",
      "385:\tlearn: 0.1079197\ttotal: 37.4s\tremaining: 59.5s\n",
      "386:\tlearn: 0.1078795\ttotal: 37.5s\tremaining: 59.4s\n",
      "387:\tlearn: 0.1078409\ttotal: 37.6s\tremaining: 59.3s\n",
      "388:\tlearn: 0.1077796\ttotal: 37.7s\tremaining: 59.2s\n",
      "389:\tlearn: 0.1077103\ttotal: 37.8s\tremaining: 59.1s\n",
      "390:\tlearn: 0.1076629\ttotal: 37.9s\tremaining: 59s\n",
      "391:\tlearn: 0.1076156\ttotal: 38s\tremaining: 58.9s\n",
      "392:\tlearn: 0.1075478\ttotal: 38.1s\tremaining: 58.8s\n",
      "393:\tlearn: 0.1074825\ttotal: 38.1s\tremaining: 58.7s\n",
      "394:\tlearn: 0.1074023\ttotal: 38.2s\tremaining: 58.6s\n",
      "395:\tlearn: 0.1073141\ttotal: 38.3s\tremaining: 58.5s\n",
      "396:\tlearn: 0.1071882\ttotal: 38.4s\tremaining: 58.4s\n",
      "397:\tlearn: 0.1071273\ttotal: 38.5s\tremaining: 58.3s\n",
      "398:\tlearn: 0.1070407\ttotal: 38.6s\tremaining: 58.2s\n",
      "399:\tlearn: 0.1069970\ttotal: 38.7s\tremaining: 58.1s\n",
      "400:\tlearn: 0.1069517\ttotal: 38.8s\tremaining: 58s\n",
      "401:\tlearn: 0.1069176\ttotal: 38.9s\tremaining: 57.9s\n",
      "402:\tlearn: 0.1068584\ttotal: 39s\tremaining: 57.8s\n",
      "403:\tlearn: 0.1068030\ttotal: 39.1s\tremaining: 57.7s\n",
      "404:\tlearn: 0.1067088\ttotal: 39.2s\tremaining: 57.6s\n",
      "405:\tlearn: 0.1066504\ttotal: 39.3s\tremaining: 57.5s\n",
      "406:\tlearn: 0.1066023\ttotal: 39.4s\tremaining: 57.4s\n",
      "407:\tlearn: 0.1065658\ttotal: 39.5s\tremaining: 57.3s\n",
      "408:\tlearn: 0.1065242\ttotal: 39.6s\tremaining: 57.2s\n",
      "409:\tlearn: 0.1064198\ttotal: 39.7s\tremaining: 57.1s\n",
      "410:\tlearn: 0.1063658\ttotal: 39.8s\tremaining: 57s\n",
      "411:\tlearn: 0.1063027\ttotal: 39.9s\tremaining: 56.9s\n",
      "412:\tlearn: 0.1062231\ttotal: 40s\tremaining: 56.8s\n",
      "413:\tlearn: 0.1061911\ttotal: 40.1s\tremaining: 56.7s\n",
      "414:\tlearn: 0.1061737\ttotal: 40.2s\tremaining: 56.6s\n",
      "415:\tlearn: 0.1061040\ttotal: 40.3s\tremaining: 56.5s\n",
      "416:\tlearn: 0.1060320\ttotal: 40.4s\tremaining: 56.4s\n",
      "417:\tlearn: 0.1059529\ttotal: 40.5s\tremaining: 56.4s\n",
      "418:\tlearn: 0.1058797\ttotal: 40.6s\tremaining: 56.3s\n",
      "419:\tlearn: 0.1058025\ttotal: 40.7s\tremaining: 56.2s\n",
      "420:\tlearn: 0.1057431\ttotal: 40.8s\tremaining: 56.1s\n",
      "421:\tlearn: 0.1056825\ttotal: 40.9s\tremaining: 56s\n",
      "422:\tlearn: 0.1056206\ttotal: 41s\tremaining: 55.9s\n",
      "423:\tlearn: 0.1055255\ttotal: 41.2s\tremaining: 55.9s\n",
      "424:\tlearn: 0.1054663\ttotal: 41.3s\tremaining: 55.8s\n",
      "425:\tlearn: 0.1054177\ttotal: 41.4s\tremaining: 55.7s\n",
      "426:\tlearn: 0.1053717\ttotal: 41.5s\tremaining: 55.6s\n",
      "427:\tlearn: 0.1053123\ttotal: 41.6s\tremaining: 55.5s\n",
      "428:\tlearn: 0.1052497\ttotal: 41.7s\tremaining: 55.4s\n",
      "429:\tlearn: 0.1052099\ttotal: 41.7s\tremaining: 55.3s\n",
      "430:\tlearn: 0.1051328\ttotal: 41.8s\tremaining: 55.2s\n",
      "431:\tlearn: 0.1050592\ttotal: 41.9s\tremaining: 55.1s\n",
      "432:\tlearn: 0.1050008\ttotal: 42s\tremaining: 55s\n",
      "433:\tlearn: 0.1049483\ttotal: 42.1s\tremaining: 54.9s\n",
      "434:\tlearn: 0.1049195\ttotal: 42.2s\tremaining: 54.8s\n",
      "435:\tlearn: 0.1048727\ttotal: 42.3s\tremaining: 54.7s\n",
      "436:\tlearn: 0.1047717\ttotal: 42.4s\tremaining: 54.6s\n",
      "437:\tlearn: 0.1047148\ttotal: 42.5s\tremaining: 54.5s\n",
      "438:\tlearn: 0.1046330\ttotal: 42.6s\tremaining: 54.5s\n",
      "439:\tlearn: 0.1045799\ttotal: 42.7s\tremaining: 54.4s\n",
      "440:\tlearn: 0.1045062\ttotal: 42.8s\tremaining: 54.3s\n",
      "441:\tlearn: 0.1044288\ttotal: 42.9s\tremaining: 54.2s\n",
      "442:\tlearn: 0.1043689\ttotal: 43s\tremaining: 54.1s\n",
      "443:\tlearn: 0.1042701\ttotal: 43.1s\tremaining: 54s\n",
      "444:\tlearn: 0.1041864\ttotal: 43.2s\tremaining: 53.9s\n",
      "445:\tlearn: 0.1041280\ttotal: 43.3s\tremaining: 53.8s\n",
      "446:\tlearn: 0.1040582\ttotal: 43.4s\tremaining: 53.7s\n",
      "447:\tlearn: 0.1039699\ttotal: 43.5s\tremaining: 53.6s\n",
      "448:\tlearn: 0.1038992\ttotal: 43.6s\tremaining: 53.5s\n",
      "449:\tlearn: 0.1038498\ttotal: 43.7s\tremaining: 53.4s\n",
      "450:\tlearn: 0.1037764\ttotal: 43.8s\tremaining: 53.3s\n",
      "451:\tlearn: 0.1037288\ttotal: 43.9s\tremaining: 53.2s\n",
      "452:\tlearn: 0.1036958\ttotal: 44s\tremaining: 53.1s\n",
      "453:\tlearn: 0.1036384\ttotal: 44.1s\tremaining: 53s\n",
      "454:\tlearn: 0.1035788\ttotal: 44.2s\tremaining: 52.9s\n",
      "455:\tlearn: 0.1035553\ttotal: 44.3s\tremaining: 52.8s\n",
      "456:\tlearn: 0.1034935\ttotal: 44.4s\tremaining: 52.7s\n",
      "457:\tlearn: 0.1034194\ttotal: 44.4s\tremaining: 52.6s\n",
      "458:\tlearn: 0.1033821\ttotal: 44.5s\tremaining: 52.5s\n",
      "459:\tlearn: 0.1033191\ttotal: 44.6s\tremaining: 52.4s\n",
      "460:\tlearn: 0.1032763\ttotal: 44.7s\tremaining: 52.3s\n",
      "461:\tlearn: 0.1032393\ttotal: 44.8s\tremaining: 52.2s\n",
      "462:\tlearn: 0.1031833\ttotal: 44.9s\tremaining: 52.1s\n",
      "463:\tlearn: 0.1030917\ttotal: 45s\tremaining: 52s\n",
      "464:\tlearn: 0.1030507\ttotal: 45.1s\tremaining: 51.9s\n",
      "465:\tlearn: 0.1029780\ttotal: 45.2s\tremaining: 51.8s\n",
      "466:\tlearn: 0.1029087\ttotal: 45.3s\tremaining: 51.7s\n",
      "467:\tlearn: 0.1028589\ttotal: 45.4s\tremaining: 51.6s\n",
      "468:\tlearn: 0.1027918\ttotal: 45.5s\tremaining: 51.5s\n",
      "469:\tlearn: 0.1027470\ttotal: 45.6s\tremaining: 51.4s\n",
      "470:\tlearn: 0.1026799\ttotal: 45.7s\tremaining: 51.3s\n",
      "471:\tlearn: 0.1026214\ttotal: 45.8s\tremaining: 51.2s\n",
      "472:\tlearn: 0.1025731\ttotal: 45.9s\tremaining: 51.1s\n",
      "473:\tlearn: 0.1024982\ttotal: 46s\tremaining: 51s\n",
      "474:\tlearn: 0.1024458\ttotal: 46.1s\tremaining: 50.9s\n",
      "475:\tlearn: 0.1024106\ttotal: 46.2s\tremaining: 50.8s\n",
      "476:\tlearn: 0.1023227\ttotal: 46.3s\tremaining: 50.7s\n",
      "477:\tlearn: 0.1022996\ttotal: 46.4s\tremaining: 50.6s\n",
      "478:\tlearn: 0.1021949\ttotal: 46.5s\tremaining: 50.5s\n",
      "479:\tlearn: 0.1021555\ttotal: 46.6s\tremaining: 50.4s\n",
      "480:\tlearn: 0.1020973\ttotal: 46.7s\tremaining: 50.3s\n",
      "481:\tlearn: 0.1020433\ttotal: 46.8s\tremaining: 50.2s\n",
      "482:\tlearn: 0.1020022\ttotal: 46.8s\tremaining: 50.1s\n",
      "483:\tlearn: 0.1019250\ttotal: 46.9s\tremaining: 50s\n",
      "484:\tlearn: 0.1018330\ttotal: 47s\tremaining: 50s\n",
      "485:\tlearn: 0.1017642\ttotal: 47.2s\tremaining: 49.9s\n",
      "486:\tlearn: 0.1017150\ttotal: 47.3s\tremaining: 49.8s\n",
      "487:\tlearn: 0.1016704\ttotal: 47.4s\tremaining: 49.7s\n",
      "488:\tlearn: 0.1016317\ttotal: 47.5s\tremaining: 49.6s\n",
      "489:\tlearn: 0.1015952\ttotal: 47.5s\tremaining: 49.5s\n",
      "490:\tlearn: 0.1015046\ttotal: 47.6s\tremaining: 49.4s\n",
      "491:\tlearn: 0.1014442\ttotal: 47.7s\tremaining: 49.3s\n",
      "492:\tlearn: 0.1013940\ttotal: 47.8s\tremaining: 49.2s\n",
      "493:\tlearn: 0.1013620\ttotal: 47.9s\tremaining: 49.1s\n",
      "494:\tlearn: 0.1013268\ttotal: 48s\tremaining: 49s\n",
      "495:\tlearn: 0.1012717\ttotal: 48.1s\tremaining: 48.9s\n",
      "496:\tlearn: 0.1012335\ttotal: 48.2s\tremaining: 48.8s\n",
      "497:\tlearn: 0.1011966\ttotal: 48.3s\tremaining: 48.7s\n",
      "498:\tlearn: 0.1011366\ttotal: 48.4s\tremaining: 48.6s\n",
      "499:\tlearn: 0.1010900\ttotal: 48.5s\tremaining: 48.5s\n",
      "500:\tlearn: 0.1010434\ttotal: 48.6s\tremaining: 48.4s\n",
      "501:\tlearn: 0.1010235\ttotal: 48.7s\tremaining: 48.3s\n",
      "502:\tlearn: 0.1010025\ttotal: 48.8s\tremaining: 48.2s\n",
      "503:\tlearn: 0.1009616\ttotal: 48.9s\tremaining: 48.1s\n",
      "504:\tlearn: 0.1008572\ttotal: 49s\tremaining: 48s\n",
      "505:\tlearn: 0.1007294\ttotal: 49.1s\tremaining: 47.9s\n",
      "506:\tlearn: 0.1006762\ttotal: 49.2s\tremaining: 47.8s\n",
      "507:\tlearn: 0.1005986\ttotal: 49.3s\tremaining: 47.7s\n",
      "508:\tlearn: 0.1005551\ttotal: 49.4s\tremaining: 47.6s\n",
      "509:\tlearn: 0.1005271\ttotal: 49.5s\tremaining: 47.5s\n",
      "510:\tlearn: 0.1004580\ttotal: 49.6s\tremaining: 47.4s\n",
      "511:\tlearn: 0.1004165\ttotal: 49.7s\tremaining: 47.3s\n",
      "512:\tlearn: 0.1003703\ttotal: 49.8s\tremaining: 47.2s\n",
      "513:\tlearn: 0.1003258\ttotal: 49.8s\tremaining: 47.1s\n",
      "514:\tlearn: 0.1002892\ttotal: 49.9s\tremaining: 47s\n",
      "515:\tlearn: 0.1002356\ttotal: 50.1s\tremaining: 47s\n",
      "516:\tlearn: 0.1001919\ttotal: 50.3s\tremaining: 46.9s\n",
      "517:\tlearn: 0.1001565\ttotal: 50.3s\tremaining: 46.8s\n",
      "518:\tlearn: 0.1001263\ttotal: 50.4s\tremaining: 46.7s\n",
      "519:\tlearn: 0.1000407\ttotal: 50.5s\tremaining: 46.6s\n",
      "520:\tlearn: 0.0999781\ttotal: 50.6s\tremaining: 46.6s\n",
      "521:\tlearn: 0.0999309\ttotal: 50.7s\tremaining: 46.4s\n",
      "522:\tlearn: 0.0998879\ttotal: 50.8s\tremaining: 46.3s\n",
      "523:\tlearn: 0.0998279\ttotal: 50.9s\tremaining: 46.3s\n",
      "524:\tlearn: 0.0997863\ttotal: 51s\tremaining: 46.2s\n",
      "525:\tlearn: 0.0997450\ttotal: 51.1s\tremaining: 46.1s\n",
      "526:\tlearn: 0.0996984\ttotal: 51.2s\tremaining: 46s\n",
      "527:\tlearn: 0.0996361\ttotal: 51.3s\tremaining: 45.9s\n",
      "528:\tlearn: 0.0995630\ttotal: 51.4s\tremaining: 45.8s\n",
      "529:\tlearn: 0.0994694\ttotal: 51.5s\tremaining: 45.7s\n",
      "530:\tlearn: 0.0994002\ttotal: 51.6s\tremaining: 45.6s\n",
      "531:\tlearn: 0.0993757\ttotal: 51.7s\tremaining: 45.5s\n",
      "532:\tlearn: 0.0993106\ttotal: 51.8s\tremaining: 45.4s\n",
      "533:\tlearn: 0.0992626\ttotal: 51.9s\tremaining: 45.3s\n",
      "534:\tlearn: 0.0992287\ttotal: 52s\tremaining: 45.2s\n",
      "535:\tlearn: 0.0991991\ttotal: 52.1s\tremaining: 45.1s\n",
      "536:\tlearn: 0.0991473\ttotal: 52.2s\tremaining: 45s\n",
      "537:\tlearn: 0.0990854\ttotal: 52.3s\tremaining: 44.9s\n",
      "538:\tlearn: 0.0990240\ttotal: 52.4s\tremaining: 44.8s\n",
      "539:\tlearn: 0.0989039\ttotal: 52.5s\tremaining: 44.7s\n",
      "540:\tlearn: 0.0988664\ttotal: 52.6s\tremaining: 44.6s\n",
      "541:\tlearn: 0.0988102\ttotal: 52.7s\tremaining: 44.5s\n",
      "542:\tlearn: 0.0987354\ttotal: 52.8s\tremaining: 44.4s\n",
      "543:\tlearn: 0.0986677\ttotal: 52.9s\tremaining: 44.3s\n",
      "544:\tlearn: 0.0986036\ttotal: 53s\tremaining: 44.2s\n",
      "545:\tlearn: 0.0985366\ttotal: 53.1s\tremaining: 44.1s\n",
      "546:\tlearn: 0.0984925\ttotal: 53.2s\tremaining: 44.1s\n",
      "547:\tlearn: 0.0984245\ttotal: 53.3s\tremaining: 44s\n",
      "548:\tlearn: 0.0983782\ttotal: 53.4s\tremaining: 43.9s\n",
      "549:\tlearn: 0.0983185\ttotal: 53.5s\tremaining: 43.8s\n",
      "550:\tlearn: 0.0982978\ttotal: 53.6s\tremaining: 43.7s\n",
      "551:\tlearn: 0.0982428\ttotal: 53.7s\tremaining: 43.6s\n",
      "552:\tlearn: 0.0982013\ttotal: 53.8s\tremaining: 43.5s\n",
      "553:\tlearn: 0.0981329\ttotal: 53.9s\tremaining: 43.4s\n",
      "554:\tlearn: 0.0980611\ttotal: 54s\tremaining: 43.3s\n",
      "555:\tlearn: 0.0980184\ttotal: 54.1s\tremaining: 43.2s\n",
      "556:\tlearn: 0.0979936\ttotal: 54.2s\tremaining: 43.1s\n",
      "557:\tlearn: 0.0979407\ttotal: 54.2s\tremaining: 43s\n",
      "558:\tlearn: 0.0979112\ttotal: 54.3s\tremaining: 42.9s\n",
      "559:\tlearn: 0.0978388\ttotal: 54.4s\tremaining: 42.8s\n",
      "560:\tlearn: 0.0977724\ttotal: 54.5s\tremaining: 42.7s\n",
      "561:\tlearn: 0.0977483\ttotal: 54.6s\tremaining: 42.6s\n",
      "562:\tlearn: 0.0977087\ttotal: 54.7s\tremaining: 42.5s\n",
      "563:\tlearn: 0.0976697\ttotal: 54.8s\tremaining: 42.4s\n",
      "564:\tlearn: 0.0976197\ttotal: 54.9s\tremaining: 42.3s\n",
      "565:\tlearn: 0.0975644\ttotal: 55s\tremaining: 42.2s\n",
      "566:\tlearn: 0.0975348\ttotal: 55.1s\tremaining: 42.1s\n",
      "567:\tlearn: 0.0975029\ttotal: 55.2s\tremaining: 42s\n",
      "568:\tlearn: 0.0974182\ttotal: 55.3s\tremaining: 41.9s\n",
      "569:\tlearn: 0.0973887\ttotal: 55.4s\tremaining: 41.8s\n",
      "570:\tlearn: 0.0973501\ttotal: 55.5s\tremaining: 41.7s\n",
      "571:\tlearn: 0.0973217\ttotal: 55.6s\tremaining: 41.6s\n",
      "572:\tlearn: 0.0972851\ttotal: 55.7s\tremaining: 41.5s\n",
      "573:\tlearn: 0.0972322\ttotal: 55.8s\tremaining: 41.4s\n",
      "574:\tlearn: 0.0971787\ttotal: 55.9s\tremaining: 41.3s\n",
      "575:\tlearn: 0.0971393\ttotal: 56s\tremaining: 41.2s\n",
      "576:\tlearn: 0.0971052\ttotal: 56.1s\tremaining: 41.1s\n",
      "577:\tlearn: 0.0970532\ttotal: 56.2s\tremaining: 41s\n",
      "578:\tlearn: 0.0970136\ttotal: 56.3s\tremaining: 40.9s\n",
      "579:\tlearn: 0.0970010\ttotal: 56.4s\tremaining: 40.8s\n",
      "580:\tlearn: 0.0969718\ttotal: 56.5s\tremaining: 40.7s\n",
      "581:\tlearn: 0.0969182\ttotal: 56.6s\tremaining: 40.6s\n",
      "582:\tlearn: 0.0968720\ttotal: 56.7s\tremaining: 40.5s\n",
      "583:\tlearn: 0.0968000\ttotal: 56.8s\tremaining: 40.4s\n",
      "584:\tlearn: 0.0967489\ttotal: 56.9s\tremaining: 40.3s\n",
      "585:\tlearn: 0.0966840\ttotal: 57s\tremaining: 40.3s\n",
      "586:\tlearn: 0.0966337\ttotal: 57.1s\tremaining: 40.2s\n",
      "587:\tlearn: 0.0965517\ttotal: 57.2s\tremaining: 40.1s\n",
      "588:\tlearn: 0.0965223\ttotal: 57.3s\tremaining: 40s\n",
      "589:\tlearn: 0.0964838\ttotal: 57.4s\tremaining: 39.9s\n",
      "590:\tlearn: 0.0964383\ttotal: 57.5s\tremaining: 39.8s\n",
      "591:\tlearn: 0.0964028\ttotal: 57.5s\tremaining: 39.7s\n",
      "592:\tlearn: 0.0963440\ttotal: 57.7s\tremaining: 39.6s\n",
      "593:\tlearn: 0.0963153\ttotal: 57.7s\tremaining: 39.5s\n",
      "594:\tlearn: 0.0962773\ttotal: 57.8s\tremaining: 39.4s\n",
      "595:\tlearn: 0.0962070\ttotal: 57.9s\tremaining: 39.3s\n",
      "596:\tlearn: 0.0961200\ttotal: 58s\tremaining: 39.2s\n",
      "597:\tlearn: 0.0960626\ttotal: 58.1s\tremaining: 39.1s\n",
      "598:\tlearn: 0.0960301\ttotal: 58.2s\tremaining: 39s\n",
      "599:\tlearn: 0.0960038\ttotal: 58.3s\tremaining: 38.9s\n",
      "600:\tlearn: 0.0959707\ttotal: 58.4s\tremaining: 38.8s\n",
      "601:\tlearn: 0.0959057\ttotal: 58.5s\tremaining: 38.7s\n",
      "602:\tlearn: 0.0958773\ttotal: 58.6s\tremaining: 38.6s\n",
      "603:\tlearn: 0.0958132\ttotal: 58.7s\tremaining: 38.5s\n",
      "604:\tlearn: 0.0957860\ttotal: 58.8s\tremaining: 38.4s\n",
      "605:\tlearn: 0.0957418\ttotal: 58.9s\tremaining: 38.3s\n",
      "606:\tlearn: 0.0956878\ttotal: 59s\tremaining: 38.2s\n",
      "607:\tlearn: 0.0956250\ttotal: 59.1s\tremaining: 38.1s\n",
      "608:\tlearn: 0.0955911\ttotal: 59.2s\tremaining: 38s\n",
      "609:\tlearn: 0.0955369\ttotal: 59.3s\tremaining: 37.9s\n",
      "610:\tlearn: 0.0954880\ttotal: 59.4s\tremaining: 37.8s\n",
      "611:\tlearn: 0.0954033\ttotal: 59.5s\tremaining: 37.7s\n",
      "612:\tlearn: 0.0953683\ttotal: 59.6s\tremaining: 37.6s\n",
      "613:\tlearn: 0.0953139\ttotal: 59.7s\tremaining: 37.5s\n",
      "614:\tlearn: 0.0952857\ttotal: 59.8s\tremaining: 37.5s\n",
      "615:\tlearn: 0.0952583\ttotal: 59.9s\tremaining: 37.4s\n",
      "616:\tlearn: 0.0951996\ttotal: 1m\tremaining: 37.3s\n",
      "617:\tlearn: 0.0951709\ttotal: 1m\tremaining: 37.2s\n",
      "618:\tlearn: 0.0951247\ttotal: 1m\tremaining: 37.1s\n",
      "619:\tlearn: 0.0950662\ttotal: 1m\tremaining: 37s\n",
      "620:\tlearn: 0.0950134\ttotal: 1m\tremaining: 36.9s\n",
      "621:\tlearn: 0.0949668\ttotal: 1m\tremaining: 36.8s\n",
      "622:\tlearn: 0.0948745\ttotal: 1m\tremaining: 36.7s\n",
      "623:\tlearn: 0.0948320\ttotal: 1m\tremaining: 36.6s\n",
      "624:\tlearn: 0.0947919\ttotal: 1m\tremaining: 36.5s\n",
      "625:\tlearn: 0.0947517\ttotal: 1m\tremaining: 36.4s\n",
      "626:\tlearn: 0.0946994\ttotal: 1m 1s\tremaining: 36.3s\n",
      "627:\tlearn: 0.0946438\ttotal: 1m 1s\tremaining: 36.2s\n",
      "628:\tlearn: 0.0946123\ttotal: 1m 1s\tremaining: 36.1s\n",
      "629:\tlearn: 0.0945857\ttotal: 1m 1s\tremaining: 36s\n",
      "630:\tlearn: 0.0945242\ttotal: 1m 1s\tremaining: 36s\n",
      "631:\tlearn: 0.0944519\ttotal: 1m 1s\tremaining: 35.9s\n",
      "632:\tlearn: 0.0943896\ttotal: 1m 1s\tremaining: 35.8s\n",
      "633:\tlearn: 0.0943680\ttotal: 1m 1s\tremaining: 35.7s\n",
      "634:\tlearn: 0.0943149\ttotal: 1m 1s\tremaining: 35.6s\n",
      "635:\tlearn: 0.0942781\ttotal: 1m 1s\tremaining: 35.5s\n",
      "636:\tlearn: 0.0942227\ttotal: 1m 2s\tremaining: 35.4s\n",
      "637:\tlearn: 0.0942062\ttotal: 1m 2s\tremaining: 35.3s\n",
      "638:\tlearn: 0.0941511\ttotal: 1m 2s\tremaining: 35.2s\n",
      "639:\tlearn: 0.0940965\ttotal: 1m 2s\tremaining: 35.1s\n",
      "640:\tlearn: 0.0939992\ttotal: 1m 2s\tremaining: 35s\n",
      "641:\tlearn: 0.0939727\ttotal: 1m 2s\tremaining: 34.9s\n",
      "642:\tlearn: 0.0938989\ttotal: 1m 2s\tremaining: 34.8s\n",
      "643:\tlearn: 0.0938695\ttotal: 1m 2s\tremaining: 34.7s\n",
      "644:\tlearn: 0.0938430\ttotal: 1m 2s\tremaining: 34.6s\n",
      "645:\tlearn: 0.0937603\ttotal: 1m 2s\tremaining: 34.5s\n",
      "646:\tlearn: 0.0937297\ttotal: 1m 3s\tremaining: 34.4s\n",
      "647:\tlearn: 0.0936739\ttotal: 1m 3s\tremaining: 34.3s\n",
      "648:\tlearn: 0.0935922\ttotal: 1m 3s\tremaining: 34.2s\n",
      "649:\tlearn: 0.0935634\ttotal: 1m 3s\tremaining: 34.1s\n",
      "650:\tlearn: 0.0935174\ttotal: 1m 3s\tremaining: 34s\n",
      "651:\tlearn: 0.0934939\ttotal: 1m 3s\tremaining: 33.9s\n",
      "652:\tlearn: 0.0934586\ttotal: 1m 3s\tremaining: 33.8s\n",
      "653:\tlearn: 0.0934270\ttotal: 1m 3s\tremaining: 33.7s\n",
      "654:\tlearn: 0.0933917\ttotal: 1m 3s\tremaining: 33.6s\n",
      "655:\tlearn: 0.0933731\ttotal: 1m 3s\tremaining: 33.5s\n",
      "656:\tlearn: 0.0933062\ttotal: 1m 3s\tremaining: 33.4s\n",
      "657:\tlearn: 0.0932697\ttotal: 1m 4s\tremaining: 33.3s\n",
      "658:\tlearn: 0.0931930\ttotal: 1m 4s\tremaining: 33.2s\n",
      "659:\tlearn: 0.0931703\ttotal: 1m 4s\tremaining: 33.1s\n",
      "660:\tlearn: 0.0931057\ttotal: 1m 4s\tremaining: 33s\n",
      "661:\tlearn: 0.0930534\ttotal: 1m 4s\tremaining: 32.9s\n",
      "662:\tlearn: 0.0929970\ttotal: 1m 4s\tremaining: 32.8s\n",
      "663:\tlearn: 0.0929380\ttotal: 1m 4s\tremaining: 32.7s\n",
      "664:\tlearn: 0.0929184\ttotal: 1m 4s\tremaining: 32.6s\n",
      "665:\tlearn: 0.0928565\ttotal: 1m 4s\tremaining: 32.5s\n",
      "666:\tlearn: 0.0928416\ttotal: 1m 4s\tremaining: 32.4s\n",
      "667:\tlearn: 0.0928150\ttotal: 1m 5s\tremaining: 32.3s\n",
      "668:\tlearn: 0.0927774\ttotal: 1m 5s\tremaining: 32.2s\n",
      "669:\tlearn: 0.0927526\ttotal: 1m 5s\tremaining: 32.1s\n",
      "670:\tlearn: 0.0927269\ttotal: 1m 5s\tremaining: 32s\n",
      "671:\tlearn: 0.0927023\ttotal: 1m 5s\tremaining: 31.9s\n",
      "672:\tlearn: 0.0926683\ttotal: 1m 5s\tremaining: 31.8s\n",
      "673:\tlearn: 0.0926427\ttotal: 1m 5s\tremaining: 31.7s\n",
      "674:\tlearn: 0.0925956\ttotal: 1m 5s\tremaining: 31.6s\n",
      "675:\tlearn: 0.0925817\ttotal: 1m 5s\tremaining: 31.5s\n",
      "676:\tlearn: 0.0925528\ttotal: 1m 5s\tremaining: 31.4s\n",
      "677:\tlearn: 0.0925189\ttotal: 1m 6s\tremaining: 31.3s\n",
      "678:\tlearn: 0.0924958\ttotal: 1m 6s\tremaining: 31.2s\n",
      "679:\tlearn: 0.0924574\ttotal: 1m 6s\tremaining: 31.1s\n",
      "680:\tlearn: 0.0924115\ttotal: 1m 6s\tremaining: 31.1s\n",
      "681:\tlearn: 0.0923862\ttotal: 1m 6s\tremaining: 30.9s\n",
      "682:\tlearn: 0.0923623\ttotal: 1m 6s\tremaining: 30.9s\n",
      "683:\tlearn: 0.0923244\ttotal: 1m 6s\tremaining: 30.8s\n",
      "684:\tlearn: 0.0922825\ttotal: 1m 6s\tremaining: 30.7s\n",
      "685:\tlearn: 0.0922624\ttotal: 1m 6s\tremaining: 30.6s\n",
      "686:\tlearn: 0.0922021\ttotal: 1m 6s\tremaining: 30.5s\n",
      "687:\tlearn: 0.0921669\ttotal: 1m 6s\tremaining: 30.4s\n",
      "688:\tlearn: 0.0921166\ttotal: 1m 7s\tremaining: 30.3s\n",
      "689:\tlearn: 0.0920537\ttotal: 1m 7s\tremaining: 30.2s\n",
      "690:\tlearn: 0.0920110\ttotal: 1m 7s\tremaining: 30.1s\n",
      "691:\tlearn: 0.0919552\ttotal: 1m 7s\tremaining: 30s\n",
      "692:\tlearn: 0.0919221\ttotal: 1m 7s\tremaining: 29.9s\n",
      "693:\tlearn: 0.0918869\ttotal: 1m 7s\tremaining: 29.8s\n",
      "694:\tlearn: 0.0918613\ttotal: 1m 7s\tremaining: 29.7s\n",
      "695:\tlearn: 0.0918101\ttotal: 1m 7s\tremaining: 29.6s\n",
      "696:\tlearn: 0.0917783\ttotal: 1m 7s\tremaining: 29.5s\n",
      "697:\tlearn: 0.0917265\ttotal: 1m 7s\tremaining: 29.4s\n",
      "698:\tlearn: 0.0916624\ttotal: 1m 8s\tremaining: 29.3s\n",
      "699:\tlearn: 0.0916257\ttotal: 1m 8s\tremaining: 29.2s\n",
      "700:\tlearn: 0.0915839\ttotal: 1m 8s\tremaining: 29.1s\n",
      "701:\tlearn: 0.0915477\ttotal: 1m 8s\tremaining: 29s\n",
      "702:\tlearn: 0.0915072\ttotal: 1m 8s\tremaining: 28.9s\n",
      "703:\tlearn: 0.0914788\ttotal: 1m 8s\tremaining: 28.8s\n",
      "704:\tlearn: 0.0914121\ttotal: 1m 8s\tremaining: 28.7s\n",
      "705:\tlearn: 0.0913583\ttotal: 1m 8s\tremaining: 28.6s\n",
      "706:\tlearn: 0.0912947\ttotal: 1m 8s\tremaining: 28.5s\n",
      "707:\tlearn: 0.0912508\ttotal: 1m 8s\tremaining: 28.4s\n",
      "708:\tlearn: 0.0912033\ttotal: 1m 8s\tremaining: 28.3s\n",
      "709:\tlearn: 0.0911463\ttotal: 1m 9s\tremaining: 28.2s\n",
      "710:\tlearn: 0.0911169\ttotal: 1m 9s\tremaining: 28.1s\n",
      "711:\tlearn: 0.0910756\ttotal: 1m 9s\tremaining: 28s\n",
      "712:\tlearn: 0.0910300\ttotal: 1m 9s\tremaining: 27.9s\n",
      "713:\tlearn: 0.0909858\ttotal: 1m 9s\tremaining: 27.8s\n",
      "714:\tlearn: 0.0909662\ttotal: 1m 9s\tremaining: 27.7s\n",
      "715:\tlearn: 0.0909228\ttotal: 1m 9s\tremaining: 27.6s\n",
      "716:\tlearn: 0.0908939\ttotal: 1m 9s\tremaining: 27.5s\n",
      "717:\tlearn: 0.0908362\ttotal: 1m 9s\tremaining: 27.4s\n",
      "718:\tlearn: 0.0907940\ttotal: 1m 9s\tremaining: 27.3s\n",
      "719:\tlearn: 0.0907799\ttotal: 1m 10s\tremaining: 27.2s\n",
      "720:\tlearn: 0.0907379\ttotal: 1m 10s\tremaining: 27.1s\n",
      "721:\tlearn: 0.0907042\ttotal: 1m 10s\tremaining: 27s\n",
      "722:\tlearn: 0.0906373\ttotal: 1m 10s\tremaining: 26.9s\n",
      "723:\tlearn: 0.0905966\ttotal: 1m 10s\tremaining: 26.8s\n",
      "724:\tlearn: 0.0905850\ttotal: 1m 10s\tremaining: 26.7s\n",
      "725:\tlearn: 0.0905540\ttotal: 1m 10s\tremaining: 26.7s\n",
      "726:\tlearn: 0.0905265\ttotal: 1m 10s\tremaining: 26.6s\n",
      "727:\tlearn: 0.0904739\ttotal: 1m 10s\tremaining: 26.5s\n",
      "728:\tlearn: 0.0904352\ttotal: 1m 11s\tremaining: 26.4s\n",
      "729:\tlearn: 0.0904147\ttotal: 1m 11s\tremaining: 26.3s\n",
      "730:\tlearn: 0.0903831\ttotal: 1m 11s\tremaining: 26.2s\n",
      "731:\tlearn: 0.0903431\ttotal: 1m 11s\tremaining: 26.1s\n",
      "732:\tlearn: 0.0902671\ttotal: 1m 11s\tremaining: 26s\n",
      "733:\tlearn: 0.0902373\ttotal: 1m 11s\tremaining: 25.9s\n",
      "734:\tlearn: 0.0902039\ttotal: 1m 11s\tremaining: 25.8s\n",
      "735:\tlearn: 0.0901708\ttotal: 1m 11s\tremaining: 25.7s\n",
      "736:\tlearn: 0.0901340\ttotal: 1m 11s\tremaining: 25.6s\n",
      "737:\tlearn: 0.0901062\ttotal: 1m 11s\tremaining: 25.5s\n",
      "738:\tlearn: 0.0900809\ttotal: 1m 11s\tremaining: 25.4s\n",
      "739:\tlearn: 0.0900472\ttotal: 1m 12s\tremaining: 25.3s\n",
      "740:\tlearn: 0.0900023\ttotal: 1m 12s\tremaining: 25.2s\n",
      "741:\tlearn: 0.0899825\ttotal: 1m 12s\tremaining: 25.1s\n",
      "742:\tlearn: 0.0899528\ttotal: 1m 12s\tremaining: 25s\n",
      "743:\tlearn: 0.0898788\ttotal: 1m 12s\tremaining: 24.9s\n",
      "744:\tlearn: 0.0898182\ttotal: 1m 12s\tremaining: 24.8s\n",
      "745:\tlearn: 0.0897689\ttotal: 1m 12s\tremaining: 24.7s\n",
      "746:\tlearn: 0.0897516\ttotal: 1m 12s\tremaining: 24.6s\n",
      "747:\tlearn: 0.0896990\ttotal: 1m 12s\tremaining: 24.5s\n",
      "748:\tlearn: 0.0896552\ttotal: 1m 12s\tremaining: 24.4s\n",
      "749:\tlearn: 0.0895864\ttotal: 1m 13s\tremaining: 24.3s\n",
      "750:\tlearn: 0.0895579\ttotal: 1m 13s\tremaining: 24.2s\n",
      "751:\tlearn: 0.0895101\ttotal: 1m 13s\tremaining: 24.1s\n",
      "752:\tlearn: 0.0894651\ttotal: 1m 13s\tremaining: 24.1s\n",
      "753:\tlearn: 0.0894205\ttotal: 1m 13s\tremaining: 24s\n",
      "754:\tlearn: 0.0893906\ttotal: 1m 13s\tremaining: 23.9s\n",
      "755:\tlearn: 0.0893472\ttotal: 1m 13s\tremaining: 23.8s\n",
      "756:\tlearn: 0.0893120\ttotal: 1m 13s\tremaining: 23.7s\n",
      "757:\tlearn: 0.0892566\ttotal: 1m 13s\tremaining: 23.6s\n",
      "758:\tlearn: 0.0891749\ttotal: 1m 13s\tremaining: 23.5s\n",
      "759:\tlearn: 0.0891449\ttotal: 1m 14s\tremaining: 23.4s\n",
      "760:\tlearn: 0.0891251\ttotal: 1m 14s\tremaining: 23.3s\n",
      "761:\tlearn: 0.0890641\ttotal: 1m 14s\tremaining: 23.2s\n",
      "762:\tlearn: 0.0890540\ttotal: 1m 14s\tremaining: 23.1s\n",
      "763:\tlearn: 0.0890391\ttotal: 1m 14s\tremaining: 23s\n",
      "764:\tlearn: 0.0890096\ttotal: 1m 14s\tremaining: 22.9s\n",
      "765:\tlearn: 0.0889963\ttotal: 1m 14s\tremaining: 22.8s\n",
      "766:\tlearn: 0.0889511\ttotal: 1m 14s\tremaining: 22.7s\n",
      "767:\tlearn: 0.0889281\ttotal: 1m 14s\tremaining: 22.6s\n",
      "768:\tlearn: 0.0888707\ttotal: 1m 14s\tremaining: 22.5s\n",
      "769:\tlearn: 0.0888094\ttotal: 1m 14s\tremaining: 22.4s\n",
      "770:\tlearn: 0.0887553\ttotal: 1m 15s\tremaining: 22.3s\n",
      "771:\tlearn: 0.0887528\ttotal: 1m 15s\tremaining: 22.2s\n",
      "772:\tlearn: 0.0887100\ttotal: 1m 15s\tremaining: 22.1s\n",
      "773:\tlearn: 0.0886850\ttotal: 1m 15s\tremaining: 22s\n",
      "774:\tlearn: 0.0886306\ttotal: 1m 15s\tremaining: 21.9s\n",
      "775:\tlearn: 0.0885762\ttotal: 1m 15s\tremaining: 21.8s\n",
      "776:\tlearn: 0.0885551\ttotal: 1m 15s\tremaining: 21.7s\n",
      "777:\tlearn: 0.0884973\ttotal: 1m 15s\tremaining: 21.6s\n",
      "778:\tlearn: 0.0884624\ttotal: 1m 15s\tremaining: 21.5s\n",
      "779:\tlearn: 0.0884329\ttotal: 1m 15s\tremaining: 21.4s\n",
      "780:\tlearn: 0.0883726\ttotal: 1m 16s\tremaining: 21.3s\n",
      "781:\tlearn: 0.0883501\ttotal: 1m 16s\tremaining: 21.2s\n",
      "782:\tlearn: 0.0883023\ttotal: 1m 16s\tremaining: 21.1s\n",
      "783:\tlearn: 0.0882461\ttotal: 1m 16s\tremaining: 21s\n",
      "784:\tlearn: 0.0882135\ttotal: 1m 16s\tremaining: 20.9s\n",
      "785:\tlearn: 0.0881875\ttotal: 1m 16s\tremaining: 20.8s\n",
      "786:\tlearn: 0.0881398\ttotal: 1m 16s\tremaining: 20.7s\n",
      "787:\tlearn: 0.0881252\ttotal: 1m 16s\tremaining: 20.6s\n",
      "788:\tlearn: 0.0881102\ttotal: 1m 16s\tremaining: 20.5s\n",
      "789:\tlearn: 0.0880761\ttotal: 1m 16s\tremaining: 20.4s\n",
      "790:\tlearn: 0.0880679\ttotal: 1m 16s\tremaining: 20.3s\n",
      "791:\tlearn: 0.0880031\ttotal: 1m 17s\tremaining: 20.2s\n",
      "792:\tlearn: 0.0879792\ttotal: 1m 17s\tremaining: 20.1s\n",
      "793:\tlearn: 0.0879500\ttotal: 1m 17s\tremaining: 20s\n",
      "794:\tlearn: 0.0879113\ttotal: 1m 17s\tremaining: 19.9s\n",
      "795:\tlearn: 0.0878496\ttotal: 1m 17s\tremaining: 19.9s\n",
      "796:\tlearn: 0.0878167\ttotal: 1m 17s\tremaining: 19.8s\n",
      "797:\tlearn: 0.0877706\ttotal: 1m 17s\tremaining: 19.7s\n",
      "798:\tlearn: 0.0877317\ttotal: 1m 17s\tremaining: 19.6s\n",
      "799:\tlearn: 0.0876804\ttotal: 1m 17s\tremaining: 19.5s\n",
      "800:\tlearn: 0.0875981\ttotal: 1m 17s\tremaining: 19.4s\n",
      "801:\tlearn: 0.0875397\ttotal: 1m 18s\tremaining: 19.3s\n",
      "802:\tlearn: 0.0875099\ttotal: 1m 18s\tremaining: 19.2s\n",
      "803:\tlearn: 0.0874639\ttotal: 1m 18s\tremaining: 19.1s\n",
      "804:\tlearn: 0.0874333\ttotal: 1m 18s\tremaining: 19s\n",
      "805:\tlearn: 0.0873800\ttotal: 1m 18s\tremaining: 18.9s\n",
      "806:\tlearn: 0.0873603\ttotal: 1m 18s\tremaining: 18.8s\n",
      "807:\tlearn: 0.0873175\ttotal: 1m 18s\tremaining: 18.7s\n",
      "808:\tlearn: 0.0872650\ttotal: 1m 18s\tremaining: 18.6s\n",
      "809:\tlearn: 0.0872146\ttotal: 1m 18s\tremaining: 18.5s\n",
      "810:\tlearn: 0.0871979\ttotal: 1m 18s\tremaining: 18.4s\n",
      "811:\tlearn: 0.0871585\ttotal: 1m 19s\tremaining: 18.3s\n",
      "812:\tlearn: 0.0871138\ttotal: 1m 19s\tremaining: 18.2s\n",
      "813:\tlearn: 0.0870591\ttotal: 1m 19s\tremaining: 18.1s\n",
      "814:\tlearn: 0.0869760\ttotal: 1m 19s\tremaining: 18s\n",
      "815:\tlearn: 0.0868909\ttotal: 1m 19s\tremaining: 17.9s\n",
      "816:\tlearn: 0.0868353\ttotal: 1m 19s\tremaining: 17.8s\n",
      "817:\tlearn: 0.0868092\ttotal: 1m 19s\tremaining: 17.7s\n",
      "818:\tlearn: 0.0867720\ttotal: 1m 19s\tremaining: 17.6s\n",
      "819:\tlearn: 0.0867048\ttotal: 1m 19s\tremaining: 17.5s\n",
      "820:\tlearn: 0.0866692\ttotal: 1m 19s\tremaining: 17.4s\n",
      "821:\tlearn: 0.0866411\ttotal: 1m 19s\tremaining: 17.3s\n",
      "822:\tlearn: 0.0866114\ttotal: 1m 20s\tremaining: 17.2s\n",
      "823:\tlearn: 0.0865458\ttotal: 1m 20s\tremaining: 17.1s\n",
      "824:\tlearn: 0.0865330\ttotal: 1m 20s\tremaining: 17s\n",
      "825:\tlearn: 0.0865094\ttotal: 1m 20s\tremaining: 16.9s\n",
      "826:\tlearn: 0.0864632\ttotal: 1m 20s\tremaining: 16.8s\n",
      "827:\tlearn: 0.0864417\ttotal: 1m 20s\tremaining: 16.7s\n",
      "828:\tlearn: 0.0863862\ttotal: 1m 20s\tremaining: 16.6s\n",
      "829:\tlearn: 0.0863553\ttotal: 1m 20s\tremaining: 16.5s\n",
      "830:\tlearn: 0.0863196\ttotal: 1m 20s\tremaining: 16.4s\n",
      "831:\tlearn: 0.0862798\ttotal: 1m 20s\tremaining: 16.3s\n",
      "832:\tlearn: 0.0862313\ttotal: 1m 21s\tremaining: 16.2s\n",
      "833:\tlearn: 0.0862040\ttotal: 1m 21s\tremaining: 16.1s\n",
      "834:\tlearn: 0.0861628\ttotal: 1m 21s\tremaining: 16.1s\n",
      "835:\tlearn: 0.0861286\ttotal: 1m 21s\tremaining: 16s\n",
      "836:\tlearn: 0.0860798\ttotal: 1m 21s\tremaining: 15.9s\n",
      "837:\tlearn: 0.0860447\ttotal: 1m 21s\tremaining: 15.8s\n",
      "838:\tlearn: 0.0860167\ttotal: 1m 21s\tremaining: 15.7s\n",
      "839:\tlearn: 0.0859995\ttotal: 1m 21s\tremaining: 15.6s\n",
      "840:\tlearn: 0.0859493\ttotal: 1m 21s\tremaining: 15.5s\n",
      "841:\tlearn: 0.0859006\ttotal: 1m 22s\tremaining: 15.4s\n",
      "842:\tlearn: 0.0858559\ttotal: 1m 22s\tremaining: 15.3s\n",
      "843:\tlearn: 0.0858183\ttotal: 1m 22s\tremaining: 15.2s\n",
      "844:\tlearn: 0.0857898\ttotal: 1m 22s\tremaining: 15.1s\n",
      "845:\tlearn: 0.0857495\ttotal: 1m 22s\tremaining: 15s\n",
      "846:\tlearn: 0.0857337\ttotal: 1m 22s\tremaining: 14.9s\n",
      "847:\tlearn: 0.0856939\ttotal: 1m 22s\tremaining: 14.8s\n",
      "848:\tlearn: 0.0856560\ttotal: 1m 22s\tremaining: 14.7s\n",
      "849:\tlearn: 0.0856257\ttotal: 1m 22s\tremaining: 14.6s\n",
      "850:\tlearn: 0.0855721\ttotal: 1m 22s\tremaining: 14.5s\n",
      "851:\tlearn: 0.0855038\ttotal: 1m 23s\tremaining: 14.4s\n",
      "852:\tlearn: 0.0854352\ttotal: 1m 23s\tremaining: 14.3s\n",
      "853:\tlearn: 0.0854168\ttotal: 1m 23s\tremaining: 14.2s\n",
      "854:\tlearn: 0.0853916\ttotal: 1m 23s\tremaining: 14.1s\n",
      "855:\tlearn: 0.0853702\ttotal: 1m 23s\tremaining: 14s\n",
      "856:\tlearn: 0.0853265\ttotal: 1m 23s\tremaining: 13.9s\n",
      "857:\tlearn: 0.0852338\ttotal: 1m 23s\tremaining: 13.8s\n",
      "858:\tlearn: 0.0852052\ttotal: 1m 23s\tremaining: 13.7s\n",
      "859:\tlearn: 0.0851692\ttotal: 1m 23s\tremaining: 13.6s\n",
      "860:\tlearn: 0.0851256\ttotal: 1m 23s\tremaining: 13.5s\n",
      "861:\tlearn: 0.0850633\ttotal: 1m 23s\tremaining: 13.4s\n",
      "862:\tlearn: 0.0850110\ttotal: 1m 24s\tremaining: 13.3s\n",
      "863:\tlearn: 0.0849932\ttotal: 1m 24s\tremaining: 13.2s\n",
      "864:\tlearn: 0.0849428\ttotal: 1m 24s\tremaining: 13.1s\n",
      "865:\tlearn: 0.0848883\ttotal: 1m 24s\tremaining: 13s\n",
      "866:\tlearn: 0.0848320\ttotal: 1m 24s\tremaining: 13s\n",
      "867:\tlearn: 0.0847992\ttotal: 1m 24s\tremaining: 12.9s\n",
      "868:\tlearn: 0.0847429\ttotal: 1m 24s\tremaining: 12.8s\n",
      "869:\tlearn: 0.0847029\ttotal: 1m 24s\tremaining: 12.7s\n",
      "870:\tlearn: 0.0846868\ttotal: 1m 24s\tremaining: 12.6s\n",
      "871:\tlearn: 0.0846543\ttotal: 1m 24s\tremaining: 12.5s\n",
      "872:\tlearn: 0.0846193\ttotal: 1m 24s\tremaining: 12.4s\n",
      "873:\tlearn: 0.0845928\ttotal: 1m 25s\tremaining: 12.3s\n",
      "874:\tlearn: 0.0845448\ttotal: 1m 25s\tremaining: 12.2s\n",
      "875:\tlearn: 0.0845223\ttotal: 1m 25s\tremaining: 12.1s\n",
      "876:\tlearn: 0.0844718\ttotal: 1m 25s\tremaining: 12s\n",
      "877:\tlearn: 0.0844383\ttotal: 1m 25s\tremaining: 11.9s\n",
      "878:\tlearn: 0.0844146\ttotal: 1m 25s\tremaining: 11.8s\n",
      "879:\tlearn: 0.0843985\ttotal: 1m 25s\tremaining: 11.7s\n",
      "880:\tlearn: 0.0843768\ttotal: 1m 25s\tremaining: 11.6s\n",
      "881:\tlearn: 0.0843109\ttotal: 1m 25s\tremaining: 11.5s\n",
      "882:\tlearn: 0.0842590\ttotal: 1m 25s\tremaining: 11.4s\n",
      "883:\tlearn: 0.0842166\ttotal: 1m 26s\tremaining: 11.3s\n",
      "884:\tlearn: 0.0841655\ttotal: 1m 26s\tremaining: 11.2s\n",
      "885:\tlearn: 0.0841257\ttotal: 1m 26s\tremaining: 11.1s\n",
      "886:\tlearn: 0.0840654\ttotal: 1m 26s\tremaining: 11s\n",
      "887:\tlearn: 0.0840329\ttotal: 1m 26s\tremaining: 10.9s\n",
      "888:\tlearn: 0.0840329\ttotal: 1m 26s\tremaining: 10.8s\n",
      "889:\tlearn: 0.0839770\ttotal: 1m 26s\tremaining: 10.7s\n",
      "890:\tlearn: 0.0839545\ttotal: 1m 26s\tremaining: 10.6s\n",
      "891:\tlearn: 0.0839030\ttotal: 1m 26s\tremaining: 10.5s\n",
      "892:\tlearn: 0.0838786\ttotal: 1m 26s\tremaining: 10.4s\n",
      "893:\tlearn: 0.0838238\ttotal: 1m 27s\tremaining: 10.3s\n",
      "894:\tlearn: 0.0838125\ttotal: 1m 27s\tremaining: 10.2s\n",
      "895:\tlearn: 0.0837976\ttotal: 1m 27s\tremaining: 10.1s\n",
      "896:\tlearn: 0.0837728\ttotal: 1m 27s\tremaining: 10s\n",
      "897:\tlearn: 0.0837276\ttotal: 1m 27s\tremaining: 9.93s\n",
      "898:\tlearn: 0.0836914\ttotal: 1m 27s\tremaining: 9.83s\n",
      "899:\tlearn: 0.0836363\ttotal: 1m 27s\tremaining: 9.73s\n",
      "900:\tlearn: 0.0836126\ttotal: 1m 27s\tremaining: 9.63s\n",
      "901:\tlearn: 0.0835736\ttotal: 1m 27s\tremaining: 9.54s\n",
      "902:\tlearn: 0.0835612\ttotal: 1m 27s\tremaining: 9.44s\n",
      "903:\tlearn: 0.0835279\ttotal: 1m 27s\tremaining: 9.34s\n",
      "904:\tlearn: 0.0834971\ttotal: 1m 28s\tremaining: 9.24s\n",
      "905:\tlearn: 0.0834487\ttotal: 1m 28s\tremaining: 9.15s\n",
      "906:\tlearn: 0.0834079\ttotal: 1m 28s\tremaining: 9.05s\n",
      "907:\tlearn: 0.0833896\ttotal: 1m 28s\tremaining: 8.95s\n",
      "908:\tlearn: 0.0833366\ttotal: 1m 28s\tremaining: 8.85s\n",
      "909:\tlearn: 0.0832746\ttotal: 1m 28s\tremaining: 8.76s\n",
      "910:\tlearn: 0.0832493\ttotal: 1m 28s\tremaining: 8.66s\n",
      "911:\tlearn: 0.0832261\ttotal: 1m 28s\tremaining: 8.56s\n",
      "912:\tlearn: 0.0831808\ttotal: 1m 28s\tremaining: 8.46s\n",
      "913:\tlearn: 0.0831697\ttotal: 1m 28s\tremaining: 8.37s\n",
      "914:\tlearn: 0.0831285\ttotal: 1m 29s\tremaining: 8.27s\n",
      "915:\tlearn: 0.0831185\ttotal: 1m 29s\tremaining: 8.17s\n",
      "916:\tlearn: 0.0830861\ttotal: 1m 29s\tremaining: 8.07s\n",
      "917:\tlearn: 0.0830597\ttotal: 1m 29s\tremaining: 7.97s\n",
      "918:\tlearn: 0.0830202\ttotal: 1m 29s\tremaining: 7.88s\n",
      "919:\tlearn: 0.0829947\ttotal: 1m 29s\tremaining: 7.78s\n",
      "920:\tlearn: 0.0829801\ttotal: 1m 29s\tremaining: 7.68s\n",
      "921:\tlearn: 0.0829598\ttotal: 1m 29s\tremaining: 7.58s\n",
      "922:\tlearn: 0.0829211\ttotal: 1m 29s\tremaining: 7.49s\n",
      "923:\tlearn: 0.0828566\ttotal: 1m 29s\tremaining: 7.39s\n",
      "924:\tlearn: 0.0828198\ttotal: 1m 29s\tremaining: 7.29s\n",
      "925:\tlearn: 0.0827822\ttotal: 1m 30s\tremaining: 7.2s\n",
      "926:\tlearn: 0.0827541\ttotal: 1m 30s\tremaining: 7.1s\n",
      "927:\tlearn: 0.0827390\ttotal: 1m 30s\tremaining: 7s\n",
      "928:\tlearn: 0.0826856\ttotal: 1m 30s\tremaining: 6.9s\n",
      "929:\tlearn: 0.0826460\ttotal: 1m 30s\tremaining: 6.8s\n",
      "930:\tlearn: 0.0825962\ttotal: 1m 30s\tremaining: 6.71s\n",
      "931:\tlearn: 0.0825322\ttotal: 1m 30s\tremaining: 6.61s\n",
      "932:\tlearn: 0.0824746\ttotal: 1m 30s\tremaining: 6.51s\n",
      "933:\tlearn: 0.0824422\ttotal: 1m 30s\tremaining: 6.42s\n",
      "934:\tlearn: 0.0823927\ttotal: 1m 30s\tremaining: 6.32s\n",
      "935:\tlearn: 0.0823597\ttotal: 1m 31s\tremaining: 6.22s\n",
      "936:\tlearn: 0.0823478\ttotal: 1m 31s\tremaining: 6.12s\n",
      "937:\tlearn: 0.0823014\ttotal: 1m 31s\tremaining: 6.03s\n",
      "938:\tlearn: 0.0822396\ttotal: 1m 31s\tremaining: 5.94s\n",
      "939:\tlearn: 0.0821560\ttotal: 1m 31s\tremaining: 5.84s\n",
      "940:\tlearn: 0.0821245\ttotal: 1m 31s\tremaining: 5.74s\n",
      "941:\tlearn: 0.0820913\ttotal: 1m 31s\tremaining: 5.64s\n",
      "942:\tlearn: 0.0820449\ttotal: 1m 31s\tremaining: 5.55s\n",
      "943:\tlearn: 0.0820052\ttotal: 1m 31s\tremaining: 5.45s\n",
      "944:\tlearn: 0.0819635\ttotal: 1m 32s\tremaining: 5.35s\n",
      "945:\tlearn: 0.0819127\ttotal: 1m 32s\tremaining: 5.26s\n",
      "946:\tlearn: 0.0818708\ttotal: 1m 32s\tremaining: 5.16s\n",
      "947:\tlearn: 0.0818362\ttotal: 1m 32s\tremaining: 5.06s\n",
      "948:\tlearn: 0.0818216\ttotal: 1m 32s\tremaining: 4.96s\n",
      "949:\tlearn: 0.0817785\ttotal: 1m 32s\tremaining: 4.87s\n",
      "950:\tlearn: 0.0817456\ttotal: 1m 32s\tremaining: 4.77s\n",
      "951:\tlearn: 0.0817131\ttotal: 1m 32s\tremaining: 4.67s\n",
      "952:\tlearn: 0.0816924\ttotal: 1m 32s\tremaining: 4.58s\n",
      "953:\tlearn: 0.0816717\ttotal: 1m 32s\tremaining: 4.48s\n",
      "954:\tlearn: 0.0816484\ttotal: 1m 32s\tremaining: 4.38s\n",
      "955:\tlearn: 0.0815842\ttotal: 1m 33s\tremaining: 4.28s\n",
      "956:\tlearn: 0.0815408\ttotal: 1m 33s\tremaining: 4.18s\n",
      "957:\tlearn: 0.0815180\ttotal: 1m 33s\tremaining: 4.09s\n",
      "958:\tlearn: 0.0814931\ttotal: 1m 33s\tremaining: 3.99s\n",
      "959:\tlearn: 0.0814700\ttotal: 1m 33s\tremaining: 3.89s\n",
      "960:\tlearn: 0.0814452\ttotal: 1m 33s\tremaining: 3.79s\n",
      "961:\tlearn: 0.0813897\ttotal: 1m 33s\tremaining: 3.7s\n",
      "962:\tlearn: 0.0813728\ttotal: 1m 33s\tremaining: 3.6s\n",
      "963:\tlearn: 0.0813506\ttotal: 1m 33s\tremaining: 3.5s\n",
      "964:\tlearn: 0.0813274\ttotal: 1m 33s\tremaining: 3.41s\n",
      "965:\tlearn: 0.0813172\ttotal: 1m 33s\tremaining: 3.31s\n",
      "966:\tlearn: 0.0812696\ttotal: 1m 34s\tremaining: 3.21s\n",
      "967:\tlearn: 0.0812535\ttotal: 1m 34s\tremaining: 3.11s\n",
      "968:\tlearn: 0.0812192\ttotal: 1m 34s\tremaining: 3.02s\n",
      "969:\tlearn: 0.0811859\ttotal: 1m 34s\tremaining: 2.92s\n",
      "970:\tlearn: 0.0811561\ttotal: 1m 34s\tremaining: 2.82s\n",
      "971:\tlearn: 0.0811038\ttotal: 1m 34s\tremaining: 2.72s\n",
      "972:\tlearn: 0.0810386\ttotal: 1m 34s\tremaining: 2.63s\n",
      "973:\tlearn: 0.0810154\ttotal: 1m 34s\tremaining: 2.53s\n",
      "974:\tlearn: 0.0809596\ttotal: 1m 34s\tremaining: 2.43s\n",
      "975:\tlearn: 0.0809224\ttotal: 1m 34s\tremaining: 2.33s\n",
      "976:\tlearn: 0.0808828\ttotal: 1m 35s\tremaining: 2.24s\n",
      "977:\tlearn: 0.0808228\ttotal: 1m 35s\tremaining: 2.14s\n",
      "978:\tlearn: 0.0807972\ttotal: 1m 35s\tremaining: 2.04s\n",
      "979:\tlearn: 0.0807559\ttotal: 1m 35s\tremaining: 1.95s\n",
      "980:\tlearn: 0.0807050\ttotal: 1m 35s\tremaining: 1.85s\n",
      "981:\tlearn: 0.0806688\ttotal: 1m 35s\tremaining: 1.75s\n",
      "982:\tlearn: 0.0806022\ttotal: 1m 35s\tremaining: 1.65s\n",
      "983:\tlearn: 0.0805649\ttotal: 1m 35s\tremaining: 1.56s\n",
      "984:\tlearn: 0.0805124\ttotal: 1m 35s\tremaining: 1.46s\n",
      "985:\tlearn: 0.0804644\ttotal: 1m 35s\tremaining: 1.36s\n",
      "986:\tlearn: 0.0804398\ttotal: 1m 36s\tremaining: 1.26s\n",
      "987:\tlearn: 0.0803963\ttotal: 1m 36s\tremaining: 1.17s\n",
      "988:\tlearn: 0.0803384\ttotal: 1m 36s\tremaining: 1.07s\n",
      "989:\tlearn: 0.0802596\ttotal: 1m 36s\tremaining: 973ms\n",
      "990:\tlearn: 0.0802293\ttotal: 1m 36s\tremaining: 876ms\n",
      "991:\tlearn: 0.0802097\ttotal: 1m 36s\tremaining: 778ms\n",
      "992:\tlearn: 0.0801766\ttotal: 1m 36s\tremaining: 681ms\n",
      "993:\tlearn: 0.0801464\ttotal: 1m 36s\tremaining: 584ms\n",
      "994:\tlearn: 0.0801145\ttotal: 1m 36s\tremaining: 487ms\n",
      "995:\tlearn: 0.0800984\ttotal: 1m 36s\tremaining: 389ms\n",
      "996:\tlearn: 0.0800487\ttotal: 1m 37s\tremaining: 292ms\n",
      "997:\tlearn: 0.0800181\ttotal: 1m 37s\tremaining: 195ms\n",
      "998:\tlearn: 0.0799809\ttotal: 1m 37s\tremaining: 97.3ms\n",
      "999:\tlearn: 0.0799424\ttotal: 1m 37s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5202879\ttotal: 95ms\tremaining: 1m 34s\n",
      "1:\tlearn: 0.4233036\ttotal: 194ms\tremaining: 1m 36s\n",
      "2:\tlearn: 0.3584844\ttotal: 290ms\tremaining: 1m 36s\n",
      "3:\tlearn: 0.3223838\ttotal: 385ms\tremaining: 1m 35s\n",
      "4:\tlearn: 0.2949049\ttotal: 486ms\tremaining: 1m 36s\n",
      "5:\tlearn: 0.2779582\ttotal: 573ms\tremaining: 1m 34s\n",
      "6:\tlearn: 0.2574987\ttotal: 665ms\tremaining: 1m 34s\n",
      "7:\tlearn: 0.2471196\ttotal: 763ms\tremaining: 1m 34s\n",
      "8:\tlearn: 0.2386418\ttotal: 854ms\tremaining: 1m 34s\n",
      "9:\tlearn: 0.2260424\ttotal: 947ms\tremaining: 1m 33s\n",
      "10:\tlearn: 0.2174175\ttotal: 1.04s\tremaining: 1m 33s\n",
      "11:\tlearn: 0.2134280\ttotal: 1.13s\tremaining: 1m 33s\n",
      "12:\tlearn: 0.2101899\ttotal: 1.22s\tremaining: 1m 32s\n",
      "13:\tlearn: 0.2047555\ttotal: 1.31s\tremaining: 1m 32s\n",
      "14:\tlearn: 0.2024618\ttotal: 1.41s\tremaining: 1m 32s\n",
      "15:\tlearn: 0.2004523\ttotal: 1.51s\tremaining: 1m 32s\n",
      "16:\tlearn: 0.1979402\ttotal: 1.6s\tremaining: 1m 32s\n",
      "17:\tlearn: 0.1941637\ttotal: 1.7s\tremaining: 1m 32s\n",
      "18:\tlearn: 0.1920484\ttotal: 1.79s\tremaining: 1m 32s\n",
      "19:\tlearn: 0.1900425\ttotal: 1.89s\tremaining: 1m 32s\n",
      "20:\tlearn: 0.1873963\ttotal: 1.99s\tremaining: 1m 32s\n",
      "21:\tlearn: 0.1861834\ttotal: 2.08s\tremaining: 1m 32s\n",
      "22:\tlearn: 0.1846827\ttotal: 2.17s\tremaining: 1m 32s\n",
      "23:\tlearn: 0.1824957\ttotal: 2.27s\tremaining: 1m 32s\n",
      "24:\tlearn: 0.1809935\ttotal: 2.36s\tremaining: 1m 32s\n",
      "25:\tlearn: 0.1796033\ttotal: 2.45s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1781335\ttotal: 2.54s\tremaining: 1m 31s\n",
      "27:\tlearn: 0.1757106\ttotal: 2.63s\tremaining: 1m 31s\n",
      "28:\tlearn: 0.1745203\ttotal: 2.74s\tremaining: 1m 31s\n",
      "29:\tlearn: 0.1732687\ttotal: 2.83s\tremaining: 1m 31s\n",
      "30:\tlearn: 0.1717565\ttotal: 2.93s\tremaining: 1m 31s\n",
      "31:\tlearn: 0.1710146\ttotal: 3.03s\tremaining: 1m 31s\n",
      "32:\tlearn: 0.1699484\ttotal: 3.12s\tremaining: 1m 31s\n",
      "33:\tlearn: 0.1686906\ttotal: 3.21s\tremaining: 1m 31s\n",
      "34:\tlearn: 0.1680522\ttotal: 3.31s\tremaining: 1m 31s\n",
      "35:\tlearn: 0.1672166\ttotal: 3.4s\tremaining: 1m 31s\n",
      "36:\tlearn: 0.1664976\ttotal: 3.5s\tremaining: 1m 30s\n",
      "37:\tlearn: 0.1654004\ttotal: 3.6s\tremaining: 1m 31s\n",
      "38:\tlearn: 0.1646917\ttotal: 3.69s\tremaining: 1m 30s\n",
      "39:\tlearn: 0.1634072\ttotal: 3.78s\tremaining: 1m 30s\n",
      "40:\tlearn: 0.1625236\ttotal: 3.87s\tremaining: 1m 30s\n",
      "41:\tlearn: 0.1618867\ttotal: 3.96s\tremaining: 1m 30s\n",
      "42:\tlearn: 0.1612370\ttotal: 4.06s\tremaining: 1m 30s\n",
      "43:\tlearn: 0.1605362\ttotal: 4.16s\tremaining: 1m 30s\n",
      "44:\tlearn: 0.1599859\ttotal: 4.25s\tremaining: 1m 30s\n",
      "45:\tlearn: 0.1592201\ttotal: 4.34s\tremaining: 1m 30s\n",
      "46:\tlearn: 0.1587677\ttotal: 4.46s\tremaining: 1m 30s\n",
      "47:\tlearn: 0.1581722\ttotal: 4.55s\tremaining: 1m 30s\n",
      "48:\tlearn: 0.1573938\ttotal: 4.65s\tremaining: 1m 30s\n",
      "49:\tlearn: 0.1566059\ttotal: 4.75s\tremaining: 1m 30s\n",
      "50:\tlearn: 0.1557287\ttotal: 4.84s\tremaining: 1m 30s\n",
      "51:\tlearn: 0.1551930\ttotal: 4.94s\tremaining: 1m 30s\n",
      "52:\tlearn: 0.1546719\ttotal: 5.04s\tremaining: 1m 29s\n",
      "53:\tlearn: 0.1542653\ttotal: 5.13s\tremaining: 1m 29s\n",
      "54:\tlearn: 0.1538120\ttotal: 5.23s\tremaining: 1m 29s\n",
      "55:\tlearn: 0.1534961\ttotal: 5.32s\tremaining: 1m 29s\n",
      "56:\tlearn: 0.1527439\ttotal: 5.42s\tremaining: 1m 29s\n",
      "57:\tlearn: 0.1520798\ttotal: 5.51s\tremaining: 1m 29s\n",
      "58:\tlearn: 0.1517715\ttotal: 5.61s\tremaining: 1m 29s\n",
      "59:\tlearn: 0.1514190\ttotal: 5.7s\tremaining: 1m 29s\n",
      "60:\tlearn: 0.1510939\ttotal: 5.79s\tremaining: 1m 29s\n",
      "61:\tlearn: 0.1507517\ttotal: 5.89s\tremaining: 1m 29s\n",
      "62:\tlearn: 0.1504627\ttotal: 5.98s\tremaining: 1m 28s\n",
      "63:\tlearn: 0.1501569\ttotal: 6.07s\tremaining: 1m 28s\n",
      "64:\tlearn: 0.1497491\ttotal: 6.17s\tremaining: 1m 28s\n",
      "65:\tlearn: 0.1493173\ttotal: 6.26s\tremaining: 1m 28s\n",
      "66:\tlearn: 0.1488112\ttotal: 6.36s\tremaining: 1m 28s\n",
      "67:\tlearn: 0.1485700\ttotal: 6.45s\tremaining: 1m 28s\n",
      "68:\tlearn: 0.1482102\ttotal: 6.55s\tremaining: 1m 28s\n",
      "69:\tlearn: 0.1476959\ttotal: 6.63s\tremaining: 1m 28s\n",
      "70:\tlearn: 0.1471820\ttotal: 6.73s\tremaining: 1m 28s\n",
      "71:\tlearn: 0.1468548\ttotal: 6.83s\tremaining: 1m 28s\n",
      "72:\tlearn: 0.1463526\ttotal: 6.93s\tremaining: 1m 27s\n",
      "73:\tlearn: 0.1460063\ttotal: 7.03s\tremaining: 1m 27s\n",
      "74:\tlearn: 0.1456899\ttotal: 7.12s\tremaining: 1m 27s\n",
      "75:\tlearn: 0.1453766\ttotal: 7.21s\tremaining: 1m 27s\n",
      "76:\tlearn: 0.1450181\ttotal: 7.31s\tremaining: 1m 27s\n",
      "77:\tlearn: 0.1447886\ttotal: 7.4s\tremaining: 1m 27s\n",
      "78:\tlearn: 0.1444716\ttotal: 7.5s\tremaining: 1m 27s\n",
      "79:\tlearn: 0.1442052\ttotal: 7.59s\tremaining: 1m 27s\n",
      "80:\tlearn: 0.1438696\ttotal: 7.69s\tremaining: 1m 27s\n",
      "81:\tlearn: 0.1436511\ttotal: 7.78s\tremaining: 1m 27s\n",
      "82:\tlearn: 0.1433436\ttotal: 7.88s\tremaining: 1m 27s\n",
      "83:\tlearn: 0.1429985\ttotal: 7.98s\tremaining: 1m 27s\n",
      "84:\tlearn: 0.1427558\ttotal: 8.07s\tremaining: 1m 26s\n",
      "85:\tlearn: 0.1424134\ttotal: 8.16s\tremaining: 1m 26s\n",
      "86:\tlearn: 0.1422554\ttotal: 8.26s\tremaining: 1m 26s\n",
      "87:\tlearn: 0.1419037\ttotal: 8.35s\tremaining: 1m 26s\n",
      "88:\tlearn: 0.1416295\ttotal: 8.54s\tremaining: 1m 27s\n",
      "89:\tlearn: 0.1414106\ttotal: 8.65s\tremaining: 1m 27s\n",
      "90:\tlearn: 0.1410792\ttotal: 8.75s\tremaining: 1m 27s\n",
      "91:\tlearn: 0.1408227\ttotal: 8.85s\tremaining: 1m 27s\n",
      "92:\tlearn: 0.1405724\ttotal: 8.94s\tremaining: 1m 27s\n",
      "93:\tlearn: 0.1404036\ttotal: 9.06s\tremaining: 1m 27s\n",
      "94:\tlearn: 0.1400766\ttotal: 9.15s\tremaining: 1m 27s\n",
      "95:\tlearn: 0.1397773\ttotal: 9.24s\tremaining: 1m 27s\n",
      "96:\tlearn: 0.1393887\ttotal: 9.35s\tremaining: 1m 27s\n",
      "97:\tlearn: 0.1392482\ttotal: 9.44s\tremaining: 1m 26s\n",
      "98:\tlearn: 0.1388778\ttotal: 9.53s\tremaining: 1m 26s\n",
      "99:\tlearn: 0.1386668\ttotal: 9.63s\tremaining: 1m 26s\n",
      "100:\tlearn: 0.1383983\ttotal: 9.73s\tremaining: 1m 26s\n",
      "101:\tlearn: 0.1381434\ttotal: 9.82s\tremaining: 1m 26s\n",
      "102:\tlearn: 0.1377997\ttotal: 9.91s\tremaining: 1m 26s\n",
      "103:\tlearn: 0.1375880\ttotal: 10s\tremaining: 1m 26s\n",
      "104:\tlearn: 0.1374422\ttotal: 10.1s\tremaining: 1m 26s\n",
      "105:\tlearn: 0.1372619\ttotal: 10.2s\tremaining: 1m 26s\n",
      "106:\tlearn: 0.1369503\ttotal: 10.3s\tremaining: 1m 25s\n",
      "107:\tlearn: 0.1366777\ttotal: 10.4s\tremaining: 1m 25s\n",
      "108:\tlearn: 0.1363947\ttotal: 10.5s\tremaining: 1m 25s\n",
      "109:\tlearn: 0.1362098\ttotal: 10.6s\tremaining: 1m 25s\n",
      "110:\tlearn: 0.1359351\ttotal: 10.7s\tremaining: 1m 25s\n",
      "111:\tlearn: 0.1357820\ttotal: 10.8s\tremaining: 1m 25s\n",
      "112:\tlearn: 0.1355856\ttotal: 10.9s\tremaining: 1m 25s\n",
      "113:\tlearn: 0.1354284\ttotal: 11s\tremaining: 1m 25s\n",
      "114:\tlearn: 0.1352133\ttotal: 11.1s\tremaining: 1m 25s\n",
      "115:\tlearn: 0.1349720\ttotal: 11.2s\tremaining: 1m 25s\n",
      "116:\tlearn: 0.1347276\ttotal: 11.3s\tremaining: 1m 24s\n",
      "117:\tlearn: 0.1345699\ttotal: 11.4s\tremaining: 1m 24s\n",
      "118:\tlearn: 0.1344476\ttotal: 11.5s\tremaining: 1m 24s\n",
      "119:\tlearn: 0.1342577\ttotal: 11.6s\tremaining: 1m 24s\n",
      "120:\tlearn: 0.1341392\ttotal: 11.6s\tremaining: 1m 24s\n",
      "121:\tlearn: 0.1339629\ttotal: 11.7s\tremaining: 1m 24s\n",
      "122:\tlearn: 0.1337657\ttotal: 11.8s\tremaining: 1m 24s\n",
      "123:\tlearn: 0.1335919\ttotal: 11.9s\tremaining: 1m 24s\n",
      "124:\tlearn: 0.1334004\ttotal: 12s\tremaining: 1m 24s\n",
      "125:\tlearn: 0.1332171\ttotal: 12.1s\tremaining: 1m 24s\n",
      "126:\tlearn: 0.1330186\ttotal: 12.2s\tremaining: 1m 24s\n",
      "127:\tlearn: 0.1328003\ttotal: 12.3s\tremaining: 1m 24s\n",
      "128:\tlearn: 0.1326064\ttotal: 12.5s\tremaining: 1m 24s\n",
      "129:\tlearn: 0.1324480\ttotal: 12.6s\tremaining: 1m 24s\n",
      "130:\tlearn: 0.1323101\ttotal: 12.7s\tremaining: 1m 23s\n",
      "131:\tlearn: 0.1322080\ttotal: 12.8s\tremaining: 1m 23s\n",
      "132:\tlearn: 0.1319428\ttotal: 12.8s\tremaining: 1m 23s\n",
      "133:\tlearn: 0.1318032\ttotal: 12.9s\tremaining: 1m 23s\n",
      "134:\tlearn: 0.1316087\ttotal: 13s\tremaining: 1m 23s\n",
      "135:\tlearn: 0.1314734\ttotal: 13.1s\tremaining: 1m 23s\n",
      "136:\tlearn: 0.1313295\ttotal: 13.2s\tremaining: 1m 23s\n",
      "137:\tlearn: 0.1311329\ttotal: 13.3s\tremaining: 1m 23s\n",
      "138:\tlearn: 0.1309771\ttotal: 13.4s\tremaining: 1m 23s\n",
      "139:\tlearn: 0.1307498\ttotal: 13.5s\tremaining: 1m 22s\n",
      "140:\tlearn: 0.1304703\ttotal: 13.6s\tremaining: 1m 23s\n",
      "141:\tlearn: 0.1303216\ttotal: 13.7s\tremaining: 1m 22s\n",
      "142:\tlearn: 0.1301026\ttotal: 13.8s\tremaining: 1m 22s\n",
      "143:\tlearn: 0.1298645\ttotal: 13.9s\tremaining: 1m 22s\n",
      "144:\tlearn: 0.1296306\ttotal: 14s\tremaining: 1m 22s\n",
      "145:\tlearn: 0.1295015\ttotal: 14.1s\tremaining: 1m 22s\n",
      "146:\tlearn: 0.1293417\ttotal: 14.2s\tremaining: 1m 22s\n",
      "147:\tlearn: 0.1291925\ttotal: 14.3s\tremaining: 1m 22s\n",
      "148:\tlearn: 0.1290488\ttotal: 14.4s\tremaining: 1m 22s\n",
      "149:\tlearn: 0.1288731\ttotal: 14.5s\tremaining: 1m 22s\n",
      "150:\tlearn: 0.1287745\ttotal: 14.6s\tremaining: 1m 21s\n",
      "151:\tlearn: 0.1286384\ttotal: 14.7s\tremaining: 1m 21s\n",
      "152:\tlearn: 0.1285143\ttotal: 14.8s\tremaining: 1m 21s\n",
      "153:\tlearn: 0.1283022\ttotal: 14.9s\tremaining: 1m 21s\n",
      "154:\tlearn: 0.1280641\ttotal: 15s\tremaining: 1m 21s\n",
      "155:\tlearn: 0.1279204\ttotal: 15.1s\tremaining: 1m 21s\n",
      "156:\tlearn: 0.1277804\ttotal: 15.2s\tremaining: 1m 21s\n",
      "157:\tlearn: 0.1276370\ttotal: 15.3s\tremaining: 1m 21s\n",
      "158:\tlearn: 0.1275417\ttotal: 15.4s\tremaining: 1m 21s\n",
      "159:\tlearn: 0.1273958\ttotal: 15.4s\tremaining: 1m 21s\n",
      "160:\tlearn: 0.1272892\ttotal: 15.5s\tremaining: 1m 21s\n",
      "161:\tlearn: 0.1271030\ttotal: 15.7s\tremaining: 1m 20s\n",
      "162:\tlearn: 0.1270077\ttotal: 15.7s\tremaining: 1m 20s\n",
      "163:\tlearn: 0.1268669\ttotal: 15.8s\tremaining: 1m 20s\n",
      "164:\tlearn: 0.1267244\ttotal: 15.9s\tremaining: 1m 20s\n",
      "165:\tlearn: 0.1266211\ttotal: 16s\tremaining: 1m 20s\n",
      "166:\tlearn: 0.1264340\ttotal: 16.1s\tremaining: 1m 20s\n",
      "167:\tlearn: 0.1262935\ttotal: 16.2s\tremaining: 1m 20s\n",
      "168:\tlearn: 0.1261899\ttotal: 16.3s\tremaining: 1m 20s\n",
      "169:\tlearn: 0.1260848\ttotal: 16.4s\tremaining: 1m 20s\n",
      "170:\tlearn: 0.1259992\ttotal: 16.5s\tremaining: 1m 20s\n",
      "171:\tlearn: 0.1258176\ttotal: 16.6s\tremaining: 1m 19s\n",
      "172:\tlearn: 0.1256927\ttotal: 16.7s\tremaining: 1m 19s\n",
      "173:\tlearn: 0.1255513\ttotal: 16.8s\tremaining: 1m 19s\n",
      "174:\tlearn: 0.1254354\ttotal: 16.9s\tremaining: 1m 19s\n",
      "175:\tlearn: 0.1252639\ttotal: 17s\tremaining: 1m 19s\n",
      "176:\tlearn: 0.1251941\ttotal: 17.1s\tremaining: 1m 19s\n",
      "177:\tlearn: 0.1251105\ttotal: 17.2s\tremaining: 1m 19s\n",
      "178:\tlearn: 0.1249704\ttotal: 17.3s\tremaining: 1m 19s\n",
      "179:\tlearn: 0.1248085\ttotal: 17.4s\tremaining: 1m 19s\n",
      "180:\tlearn: 0.1246514\ttotal: 17.5s\tremaining: 1m 19s\n",
      "181:\tlearn: 0.1245686\ttotal: 17.5s\tremaining: 1m 18s\n",
      "182:\tlearn: 0.1244722\ttotal: 17.6s\tremaining: 1m 18s\n",
      "183:\tlearn: 0.1243482\ttotal: 17.7s\tremaining: 1m 18s\n",
      "184:\tlearn: 0.1242375\ttotal: 17.8s\tremaining: 1m 18s\n",
      "185:\tlearn: 0.1241323\ttotal: 18s\tremaining: 1m 18s\n",
      "186:\tlearn: 0.1239512\ttotal: 18.1s\tremaining: 1m 18s\n",
      "187:\tlearn: 0.1238090\ttotal: 18.2s\tremaining: 1m 18s\n",
      "188:\tlearn: 0.1236879\ttotal: 18.3s\tremaining: 1m 18s\n",
      "189:\tlearn: 0.1235623\ttotal: 18.3s\tremaining: 1m 18s\n",
      "190:\tlearn: 0.1234524\ttotal: 18.4s\tremaining: 1m 18s\n",
      "191:\tlearn: 0.1233731\ttotal: 18.5s\tremaining: 1m 18s\n",
      "192:\tlearn: 0.1232854\ttotal: 18.6s\tremaining: 1m 17s\n",
      "193:\tlearn: 0.1232227\ttotal: 18.7s\tremaining: 1m 17s\n",
      "194:\tlearn: 0.1231062\ttotal: 18.8s\tremaining: 1m 17s\n",
      "195:\tlearn: 0.1229690\ttotal: 19s\tremaining: 1m 18s\n",
      "196:\tlearn: 0.1228751\ttotal: 19.1s\tremaining: 1m 17s\n",
      "197:\tlearn: 0.1227930\ttotal: 19.2s\tremaining: 1m 17s\n",
      "198:\tlearn: 0.1227288\ttotal: 19.3s\tremaining: 1m 17s\n",
      "199:\tlearn: 0.1225978\ttotal: 19.4s\tremaining: 1m 17s\n",
      "200:\tlearn: 0.1224208\ttotal: 19.5s\tremaining: 1m 17s\n",
      "201:\tlearn: 0.1223269\ttotal: 19.6s\tremaining: 1m 17s\n",
      "202:\tlearn: 0.1222359\ttotal: 19.7s\tremaining: 1m 17s\n",
      "203:\tlearn: 0.1221697\ttotal: 19.8s\tremaining: 1m 17s\n",
      "204:\tlearn: 0.1220919\ttotal: 19.9s\tremaining: 1m 17s\n",
      "205:\tlearn: 0.1220124\ttotal: 20s\tremaining: 1m 17s\n",
      "206:\tlearn: 0.1219422\ttotal: 20.1s\tremaining: 1m 17s\n",
      "207:\tlearn: 0.1218490\ttotal: 20.2s\tremaining: 1m 16s\n",
      "208:\tlearn: 0.1217777\ttotal: 20.3s\tremaining: 1m 16s\n",
      "209:\tlearn: 0.1216703\ttotal: 20.4s\tremaining: 1m 16s\n",
      "210:\tlearn: 0.1215809\ttotal: 20.5s\tremaining: 1m 16s\n",
      "211:\tlearn: 0.1215300\ttotal: 20.6s\tremaining: 1m 16s\n",
      "212:\tlearn: 0.1214200\ttotal: 20.7s\tremaining: 1m 16s\n",
      "213:\tlearn: 0.1213492\ttotal: 20.8s\tremaining: 1m 16s\n",
      "214:\tlearn: 0.1212171\ttotal: 20.9s\tremaining: 1m 16s\n",
      "215:\tlearn: 0.1211333\ttotal: 20.9s\tremaining: 1m 16s\n",
      "216:\tlearn: 0.1209849\ttotal: 21s\tremaining: 1m 15s\n",
      "217:\tlearn: 0.1208524\ttotal: 21.1s\tremaining: 1m 15s\n",
      "218:\tlearn: 0.1207570\ttotal: 21.2s\tremaining: 1m 15s\n",
      "219:\tlearn: 0.1206696\ttotal: 21.3s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.1205814\ttotal: 21.4s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.1205084\ttotal: 21.5s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.1204526\ttotal: 21.6s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.1203039\ttotal: 21.7s\tremaining: 1m 15s\n",
      "224:\tlearn: 0.1201944\ttotal: 21.8s\tremaining: 1m 15s\n",
      "225:\tlearn: 0.1201260\ttotal: 21.9s\tremaining: 1m 15s\n",
      "226:\tlearn: 0.1200675\ttotal: 22s\tremaining: 1m 14s\n",
      "227:\tlearn: 0.1199782\ttotal: 22.1s\tremaining: 1m 14s\n",
      "228:\tlearn: 0.1198787\ttotal: 22.2s\tremaining: 1m 14s\n",
      "229:\tlearn: 0.1197470\ttotal: 22.3s\tremaining: 1m 14s\n",
      "230:\tlearn: 0.1195795\ttotal: 22.4s\tremaining: 1m 14s\n",
      "231:\tlearn: 0.1195441\ttotal: 22.5s\tremaining: 1m 14s\n",
      "232:\tlearn: 0.1194321\ttotal: 22.6s\tremaining: 1m 14s\n",
      "233:\tlearn: 0.1193388\ttotal: 22.7s\tremaining: 1m 14s\n",
      "234:\tlearn: 0.1192196\ttotal: 22.8s\tremaining: 1m 14s\n",
      "235:\tlearn: 0.1191617\ttotal: 22.9s\tremaining: 1m 14s\n",
      "236:\tlearn: 0.1190994\ttotal: 23s\tremaining: 1m 13s\n",
      "237:\tlearn: 0.1190339\ttotal: 23.1s\tremaining: 1m 13s\n",
      "238:\tlearn: 0.1189300\ttotal: 23.2s\tremaining: 1m 13s\n",
      "239:\tlearn: 0.1188731\ttotal: 23.3s\tremaining: 1m 13s\n",
      "240:\tlearn: 0.1187465\ttotal: 23.4s\tremaining: 1m 13s\n",
      "241:\tlearn: 0.1185917\ttotal: 23.5s\tremaining: 1m 13s\n",
      "242:\tlearn: 0.1184509\ttotal: 23.6s\tremaining: 1m 13s\n",
      "243:\tlearn: 0.1183171\ttotal: 23.7s\tremaining: 1m 13s\n",
      "244:\tlearn: 0.1182474\ttotal: 23.8s\tremaining: 1m 13s\n",
      "245:\tlearn: 0.1181438\ttotal: 23.9s\tremaining: 1m 13s\n",
      "246:\tlearn: 0.1180360\ttotal: 24s\tremaining: 1m 13s\n",
      "247:\tlearn: 0.1179527\ttotal: 24.1s\tremaining: 1m 13s\n",
      "248:\tlearn: 0.1178847\ttotal: 24.2s\tremaining: 1m 12s\n",
      "249:\tlearn: 0.1177990\ttotal: 24.3s\tremaining: 1m 12s\n",
      "250:\tlearn: 0.1177112\ttotal: 24.4s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.1176156\ttotal: 24.5s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.1175522\ttotal: 24.6s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.1174925\ttotal: 24.7s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.1173682\ttotal: 24.8s\tremaining: 1m 12s\n",
      "255:\tlearn: 0.1172844\ttotal: 24.8s\tremaining: 1m 12s\n",
      "256:\tlearn: 0.1172253\ttotal: 24.9s\tremaining: 1m 12s\n",
      "257:\tlearn: 0.1171145\ttotal: 25s\tremaining: 1m 12s\n",
      "258:\tlearn: 0.1169486\ttotal: 25.1s\tremaining: 1m 11s\n",
      "259:\tlearn: 0.1168518\ttotal: 25.3s\tremaining: 1m 11s\n",
      "260:\tlearn: 0.1167141\ttotal: 25.4s\tremaining: 1m 11s\n",
      "261:\tlearn: 0.1166056\ttotal: 25.5s\tremaining: 1m 11s\n",
      "262:\tlearn: 0.1165067\ttotal: 25.6s\tremaining: 1m 11s\n",
      "263:\tlearn: 0.1163971\ttotal: 25.7s\tremaining: 1m 11s\n",
      "264:\tlearn: 0.1163305\ttotal: 25.8s\tremaining: 1m 11s\n",
      "265:\tlearn: 0.1162906\ttotal: 25.9s\tremaining: 1m 11s\n",
      "266:\tlearn: 0.1161653\ttotal: 26s\tremaining: 1m 11s\n",
      "267:\tlearn: 0.1161059\ttotal: 26s\tremaining: 1m 11s\n",
      "268:\tlearn: 0.1160543\ttotal: 26.1s\tremaining: 1m 11s\n",
      "269:\tlearn: 0.1159719\ttotal: 26.2s\tremaining: 1m 10s\n",
      "270:\tlearn: 0.1159439\ttotal: 26.3s\tremaining: 1m 10s\n",
      "271:\tlearn: 0.1158694\ttotal: 26.4s\tremaining: 1m 10s\n",
      "272:\tlearn: 0.1157780\ttotal: 26.5s\tremaining: 1m 10s\n",
      "273:\tlearn: 0.1156777\ttotal: 26.6s\tremaining: 1m 10s\n",
      "274:\tlearn: 0.1155581\ttotal: 26.7s\tremaining: 1m 10s\n",
      "275:\tlearn: 0.1154564\ttotal: 26.8s\tremaining: 1m 10s\n",
      "276:\tlearn: 0.1153883\ttotal: 26.9s\tremaining: 1m 10s\n",
      "277:\tlearn: 0.1153035\ttotal: 27s\tremaining: 1m 10s\n",
      "278:\tlearn: 0.1152216\ttotal: 27.1s\tremaining: 1m 10s\n",
      "279:\tlearn: 0.1151446\ttotal: 27.2s\tremaining: 1m 9s\n",
      "280:\tlearn: 0.1150910\ttotal: 27.3s\tremaining: 1m 9s\n",
      "281:\tlearn: 0.1149703\ttotal: 27.4s\tremaining: 1m 9s\n",
      "282:\tlearn: 0.1149283\ttotal: 27.5s\tremaining: 1m 9s\n",
      "283:\tlearn: 0.1148753\ttotal: 27.6s\tremaining: 1m 9s\n",
      "284:\tlearn: 0.1148054\ttotal: 27.7s\tremaining: 1m 9s\n",
      "285:\tlearn: 0.1147372\ttotal: 27.8s\tremaining: 1m 9s\n",
      "286:\tlearn: 0.1146632\ttotal: 27.9s\tremaining: 1m 9s\n",
      "287:\tlearn: 0.1145697\ttotal: 28s\tremaining: 1m 9s\n",
      "288:\tlearn: 0.1144803\ttotal: 28.1s\tremaining: 1m 9s\n",
      "289:\tlearn: 0.1143799\ttotal: 28.2s\tremaining: 1m 8s\n",
      "290:\tlearn: 0.1142951\ttotal: 28.3s\tremaining: 1m 8s\n",
      "291:\tlearn: 0.1141846\ttotal: 28.4s\tremaining: 1m 8s\n",
      "292:\tlearn: 0.1141160\ttotal: 28.5s\tremaining: 1m 8s\n",
      "293:\tlearn: 0.1140380\ttotal: 28.6s\tremaining: 1m 8s\n",
      "294:\tlearn: 0.1139657\ttotal: 28.7s\tremaining: 1m 8s\n",
      "295:\tlearn: 0.1139092\ttotal: 28.8s\tremaining: 1m 8s\n",
      "296:\tlearn: 0.1138104\ttotal: 29s\tremaining: 1m 8s\n",
      "297:\tlearn: 0.1137226\ttotal: 29.1s\tremaining: 1m 8s\n",
      "298:\tlearn: 0.1136778\ttotal: 29.2s\tremaining: 1m 8s\n",
      "299:\tlearn: 0.1136180\ttotal: 29.3s\tremaining: 1m 8s\n",
      "300:\tlearn: 0.1135102\ttotal: 29.4s\tremaining: 1m 8s\n",
      "301:\tlearn: 0.1134408\ttotal: 29.5s\tremaining: 1m 8s\n",
      "302:\tlearn: 0.1133739\ttotal: 29.6s\tremaining: 1m 8s\n",
      "303:\tlearn: 0.1132882\ttotal: 29.7s\tremaining: 1m 8s\n",
      "304:\tlearn: 0.1132080\ttotal: 29.8s\tremaining: 1m 7s\n",
      "305:\tlearn: 0.1131298\ttotal: 29.9s\tremaining: 1m 7s\n",
      "306:\tlearn: 0.1130438\ttotal: 30s\tremaining: 1m 7s\n",
      "307:\tlearn: 0.1129823\ttotal: 30.1s\tremaining: 1m 7s\n",
      "308:\tlearn: 0.1129176\ttotal: 30.2s\tremaining: 1m 7s\n",
      "309:\tlearn: 0.1128365\ttotal: 30.4s\tremaining: 1m 7s\n",
      "310:\tlearn: 0.1127855\ttotal: 30.5s\tremaining: 1m 7s\n",
      "311:\tlearn: 0.1127301\ttotal: 30.6s\tremaining: 1m 7s\n",
      "312:\tlearn: 0.1126855\ttotal: 30.7s\tremaining: 1m 7s\n",
      "313:\tlearn: 0.1126111\ttotal: 30.8s\tremaining: 1m 7s\n",
      "314:\tlearn: 0.1125252\ttotal: 30.9s\tremaining: 1m 7s\n",
      "315:\tlearn: 0.1124428\ttotal: 31s\tremaining: 1m 7s\n",
      "316:\tlearn: 0.1123970\ttotal: 31.1s\tremaining: 1m 6s\n",
      "317:\tlearn: 0.1122772\ttotal: 31.2s\tremaining: 1m 6s\n",
      "318:\tlearn: 0.1121640\ttotal: 31.3s\tremaining: 1m 6s\n",
      "319:\tlearn: 0.1120998\ttotal: 31.4s\tremaining: 1m 6s\n",
      "320:\tlearn: 0.1120326\ttotal: 31.5s\tremaining: 1m 6s\n",
      "321:\tlearn: 0.1119446\ttotal: 31.6s\tremaining: 1m 6s\n",
      "322:\tlearn: 0.1118592\ttotal: 31.7s\tremaining: 1m 6s\n",
      "323:\tlearn: 0.1118031\ttotal: 31.8s\tremaining: 1m 6s\n",
      "324:\tlearn: 0.1117431\ttotal: 31.9s\tremaining: 1m 6s\n",
      "325:\tlearn: 0.1116729\ttotal: 32s\tremaining: 1m 6s\n",
      "326:\tlearn: 0.1115677\ttotal: 32.1s\tremaining: 1m 6s\n",
      "327:\tlearn: 0.1115249\ttotal: 32.2s\tremaining: 1m 6s\n",
      "328:\tlearn: 0.1114121\ttotal: 32.3s\tremaining: 1m 5s\n",
      "329:\tlearn: 0.1113030\ttotal: 32.4s\tremaining: 1m 5s\n",
      "330:\tlearn: 0.1112434\ttotal: 32.5s\tremaining: 1m 5s\n",
      "331:\tlearn: 0.1111463\ttotal: 32.7s\tremaining: 1m 5s\n",
      "332:\tlearn: 0.1111192\ttotal: 32.8s\tremaining: 1m 5s\n",
      "333:\tlearn: 0.1110348\ttotal: 32.9s\tremaining: 1m 5s\n",
      "334:\tlearn: 0.1109454\ttotal: 33s\tremaining: 1m 5s\n",
      "335:\tlearn: 0.1108782\ttotal: 33.1s\tremaining: 1m 5s\n",
      "336:\tlearn: 0.1107875\ttotal: 33.2s\tremaining: 1m 5s\n",
      "337:\tlearn: 0.1106851\ttotal: 33.3s\tremaining: 1m 5s\n",
      "338:\tlearn: 0.1105861\ttotal: 33.4s\tremaining: 1m 5s\n",
      "339:\tlearn: 0.1105106\ttotal: 33.5s\tremaining: 1m 5s\n",
      "340:\tlearn: 0.1104357\ttotal: 33.6s\tremaining: 1m 4s\n",
      "341:\tlearn: 0.1103875\ttotal: 33.7s\tremaining: 1m 4s\n",
      "342:\tlearn: 0.1103168\ttotal: 33.8s\tremaining: 1m 4s\n",
      "343:\tlearn: 0.1102018\ttotal: 33.9s\tremaining: 1m 4s\n",
      "344:\tlearn: 0.1101294\ttotal: 34s\tremaining: 1m 4s\n",
      "345:\tlearn: 0.1100577\ttotal: 34.1s\tremaining: 1m 4s\n",
      "346:\tlearn: 0.1099968\ttotal: 34.2s\tremaining: 1m 4s\n",
      "347:\tlearn: 0.1099362\ttotal: 34.3s\tremaining: 1m 4s\n",
      "348:\tlearn: 0.1098737\ttotal: 34.4s\tremaining: 1m 4s\n",
      "349:\tlearn: 0.1098092\ttotal: 34.5s\tremaining: 1m 4s\n",
      "350:\tlearn: 0.1097635\ttotal: 34.6s\tremaining: 1m 4s\n",
      "351:\tlearn: 0.1097189\ttotal: 34.7s\tremaining: 1m 3s\n",
      "352:\tlearn: 0.1096532\ttotal: 34.8s\tremaining: 1m 3s\n",
      "353:\tlearn: 0.1095847\ttotal: 34.9s\tremaining: 1m 3s\n",
      "354:\tlearn: 0.1095200\ttotal: 35s\tremaining: 1m 3s\n",
      "355:\tlearn: 0.1094191\ttotal: 35.1s\tremaining: 1m 3s\n",
      "356:\tlearn: 0.1093657\ttotal: 35.2s\tremaining: 1m 3s\n",
      "357:\tlearn: 0.1093055\ttotal: 35.5s\tremaining: 1m 3s\n",
      "358:\tlearn: 0.1092443\ttotal: 35.6s\tremaining: 1m 3s\n",
      "359:\tlearn: 0.1092005\ttotal: 35.8s\tremaining: 1m 3s\n",
      "360:\tlearn: 0.1091248\ttotal: 35.9s\tremaining: 1m 3s\n",
      "361:\tlearn: 0.1090493\ttotal: 36s\tremaining: 1m 3s\n",
      "362:\tlearn: 0.1090259\ttotal: 36s\tremaining: 1m 3s\n",
      "363:\tlearn: 0.1089916\ttotal: 36.1s\tremaining: 1m 3s\n",
      "364:\tlearn: 0.1088839\ttotal: 36.2s\tremaining: 1m 3s\n",
      "365:\tlearn: 0.1087817\ttotal: 36.4s\tremaining: 1m 2s\n",
      "366:\tlearn: 0.1087012\ttotal: 36.5s\tremaining: 1m 2s\n",
      "367:\tlearn: 0.1086499\ttotal: 36.6s\tremaining: 1m 2s\n",
      "368:\tlearn: 0.1085936\ttotal: 36.7s\tremaining: 1m 2s\n",
      "369:\tlearn: 0.1085467\ttotal: 36.8s\tremaining: 1m 2s\n",
      "370:\tlearn: 0.1084983\ttotal: 36.9s\tremaining: 1m 2s\n",
      "371:\tlearn: 0.1084301\ttotal: 37s\tremaining: 1m 2s\n",
      "372:\tlearn: 0.1083620\ttotal: 37.1s\tremaining: 1m 2s\n",
      "373:\tlearn: 0.1083142\ttotal: 37.2s\tremaining: 1m 2s\n",
      "374:\tlearn: 0.1082587\ttotal: 37.3s\tremaining: 1m 2s\n",
      "375:\tlearn: 0.1082202\ttotal: 37.4s\tremaining: 1m 2s\n",
      "376:\tlearn: 0.1081757\ttotal: 37.5s\tremaining: 1m 1s\n",
      "377:\tlearn: 0.1081284\ttotal: 37.6s\tremaining: 1m 1s\n",
      "378:\tlearn: 0.1080837\ttotal: 37.7s\tremaining: 1m 1s\n",
      "379:\tlearn: 0.1080105\ttotal: 37.8s\tremaining: 1m 1s\n",
      "380:\tlearn: 0.1079709\ttotal: 37.9s\tremaining: 1m 1s\n",
      "381:\tlearn: 0.1078845\ttotal: 38s\tremaining: 1m 1s\n",
      "382:\tlearn: 0.1078084\ttotal: 38.1s\tremaining: 1m 1s\n",
      "383:\tlearn: 0.1077160\ttotal: 38.2s\tremaining: 1m 1s\n",
      "384:\tlearn: 0.1076457\ttotal: 38.3s\tremaining: 1m 1s\n",
      "385:\tlearn: 0.1075944\ttotal: 38.4s\tremaining: 1m 1s\n",
      "386:\tlearn: 0.1075337\ttotal: 38.5s\tremaining: 1m 1s\n",
      "387:\tlearn: 0.1074716\ttotal: 38.6s\tremaining: 1m\n",
      "388:\tlearn: 0.1074126\ttotal: 38.7s\tremaining: 1m\n",
      "389:\tlearn: 0.1073613\ttotal: 38.8s\tremaining: 1m\n",
      "390:\tlearn: 0.1072946\ttotal: 39s\tremaining: 1m\n",
      "391:\tlearn: 0.1071978\ttotal: 39.1s\tremaining: 1m\n",
      "392:\tlearn: 0.1071514\ttotal: 39.2s\tremaining: 1m\n",
      "393:\tlearn: 0.1070677\ttotal: 39.3s\tremaining: 1m\n",
      "394:\tlearn: 0.1069788\ttotal: 39.4s\tremaining: 1m\n",
      "395:\tlearn: 0.1069103\ttotal: 39.5s\tremaining: 1m\n",
      "396:\tlearn: 0.1068366\ttotal: 39.6s\tremaining: 1m\n",
      "397:\tlearn: 0.1067630\ttotal: 39.7s\tremaining: 1m\n",
      "398:\tlearn: 0.1066849\ttotal: 39.8s\tremaining: 1m\n",
      "399:\tlearn: 0.1066019\ttotal: 40s\tremaining: 59.9s\n",
      "400:\tlearn: 0.1065090\ttotal: 40.1s\tremaining: 59.8s\n",
      "401:\tlearn: 0.1064396\ttotal: 40.1s\tremaining: 59.7s\n",
      "402:\tlearn: 0.1063973\ttotal: 40.3s\tremaining: 59.6s\n",
      "403:\tlearn: 0.1063481\ttotal: 40.3s\tremaining: 59.5s\n",
      "404:\tlearn: 0.1062770\ttotal: 40.4s\tremaining: 59.4s\n",
      "405:\tlearn: 0.1062303\ttotal: 40.5s\tremaining: 59.3s\n",
      "406:\tlearn: 0.1061704\ttotal: 40.6s\tremaining: 59.2s\n",
      "407:\tlearn: 0.1061245\ttotal: 40.7s\tremaining: 59.1s\n",
      "408:\tlearn: 0.1060988\ttotal: 40.9s\tremaining: 59.1s\n",
      "409:\tlearn: 0.1060247\ttotal: 41s\tremaining: 59s\n",
      "410:\tlearn: 0.1059980\ttotal: 41.1s\tremaining: 58.9s\n",
      "411:\tlearn: 0.1059226\ttotal: 41.2s\tremaining: 58.8s\n",
      "412:\tlearn: 0.1058734\ttotal: 41.3s\tremaining: 58.8s\n",
      "413:\tlearn: 0.1058276\ttotal: 41.5s\tremaining: 58.7s\n",
      "414:\tlearn: 0.1057688\ttotal: 41.6s\tremaining: 58.6s\n",
      "415:\tlearn: 0.1057226\ttotal: 41.7s\tremaining: 58.5s\n",
      "416:\tlearn: 0.1056749\ttotal: 41.8s\tremaining: 58.4s\n",
      "417:\tlearn: 0.1056240\ttotal: 41.9s\tremaining: 58.3s\n",
      "418:\tlearn: 0.1055933\ttotal: 42s\tremaining: 58.3s\n",
      "419:\tlearn: 0.1055730\ttotal: 42.2s\tremaining: 58.3s\n",
      "420:\tlearn: 0.1055468\ttotal: 42.3s\tremaining: 58.2s\n",
      "421:\tlearn: 0.1054895\ttotal: 42.4s\tremaining: 58.1s\n",
      "422:\tlearn: 0.1054299\ttotal: 42.5s\tremaining: 58s\n",
      "423:\tlearn: 0.1053506\ttotal: 42.6s\tremaining: 57.9s\n",
      "424:\tlearn: 0.1052914\ttotal: 42.7s\tremaining: 57.8s\n",
      "425:\tlearn: 0.1052305\ttotal: 42.8s\tremaining: 57.7s\n",
      "426:\tlearn: 0.1051949\ttotal: 42.9s\tremaining: 57.6s\n",
      "427:\tlearn: 0.1051494\ttotal: 43s\tremaining: 57.5s\n",
      "428:\tlearn: 0.1051117\ttotal: 43.1s\tremaining: 57.4s\n",
      "429:\tlearn: 0.1050474\ttotal: 43.2s\tremaining: 57.3s\n",
      "430:\tlearn: 0.1049814\ttotal: 43.3s\tremaining: 57.2s\n",
      "431:\tlearn: 0.1049244\ttotal: 43.5s\tremaining: 57.1s\n",
      "432:\tlearn: 0.1048423\ttotal: 43.6s\tremaining: 57s\n",
      "433:\tlearn: 0.1047374\ttotal: 43.7s\tremaining: 56.9s\n",
      "434:\tlearn: 0.1046912\ttotal: 43.7s\tremaining: 56.8s\n",
      "435:\tlearn: 0.1046606\ttotal: 43.8s\tremaining: 56.7s\n",
      "436:\tlearn: 0.1046121\ttotal: 43.9s\tremaining: 56.6s\n",
      "437:\tlearn: 0.1045662\ttotal: 44s\tremaining: 56.5s\n",
      "438:\tlearn: 0.1045301\ttotal: 44.1s\tremaining: 56.4s\n",
      "439:\tlearn: 0.1044950\ttotal: 44.2s\tremaining: 56.3s\n",
      "440:\tlearn: 0.1044465\ttotal: 44.3s\tremaining: 56.2s\n",
      "441:\tlearn: 0.1043102\ttotal: 44.4s\tremaining: 56.1s\n",
      "442:\tlearn: 0.1042005\ttotal: 44.5s\tremaining: 56s\n",
      "443:\tlearn: 0.1041277\ttotal: 44.6s\tremaining: 55.9s\n",
      "444:\tlearn: 0.1040635\ttotal: 44.7s\tremaining: 55.8s\n",
      "445:\tlearn: 0.1039909\ttotal: 44.8s\tremaining: 55.7s\n",
      "446:\tlearn: 0.1039576\ttotal: 44.9s\tremaining: 55.6s\n",
      "447:\tlearn: 0.1039084\ttotal: 45s\tremaining: 55.5s\n",
      "448:\tlearn: 0.1038744\ttotal: 45.1s\tremaining: 55.4s\n",
      "449:\tlearn: 0.1038494\ttotal: 45.2s\tremaining: 55.2s\n",
      "450:\tlearn: 0.1037860\ttotal: 45.3s\tremaining: 55.1s\n",
      "451:\tlearn: 0.1037561\ttotal: 45.4s\tremaining: 55s\n",
      "452:\tlearn: 0.1037093\ttotal: 45.5s\tremaining: 54.9s\n",
      "453:\tlearn: 0.1036297\ttotal: 45.6s\tremaining: 54.8s\n",
      "454:\tlearn: 0.1035982\ttotal: 45.7s\tremaining: 54.7s\n",
      "455:\tlearn: 0.1035144\ttotal: 45.8s\tremaining: 54.7s\n",
      "456:\tlearn: 0.1034706\ttotal: 46s\tremaining: 54.6s\n",
      "457:\tlearn: 0.1034171\ttotal: 46.1s\tremaining: 54.5s\n",
      "458:\tlearn: 0.1033626\ttotal: 46.2s\tremaining: 54.5s\n",
      "459:\tlearn: 0.1032723\ttotal: 46.3s\tremaining: 54.4s\n",
      "460:\tlearn: 0.1031601\ttotal: 46.4s\tremaining: 54.3s\n",
      "461:\tlearn: 0.1031226\ttotal: 46.5s\tremaining: 54.2s\n",
      "462:\tlearn: 0.1030834\ttotal: 46.6s\tremaining: 54.1s\n",
      "463:\tlearn: 0.1030446\ttotal: 46.8s\tremaining: 54.1s\n",
      "464:\tlearn: 0.1029850\ttotal: 46.9s\tremaining: 54s\n",
      "465:\tlearn: 0.1028897\ttotal: 47.1s\tremaining: 54s\n",
      "466:\tlearn: 0.1028639\ttotal: 47.3s\tremaining: 54s\n",
      "467:\tlearn: 0.1028516\ttotal: 47.4s\tremaining: 53.9s\n",
      "468:\tlearn: 0.1028094\ttotal: 47.5s\tremaining: 53.8s\n",
      "469:\tlearn: 0.1027897\ttotal: 47.6s\tremaining: 53.7s\n",
      "470:\tlearn: 0.1027187\ttotal: 47.8s\tremaining: 53.6s\n",
      "471:\tlearn: 0.1026495\ttotal: 47.9s\tremaining: 53.6s\n",
      "472:\tlearn: 0.1025851\ttotal: 48s\tremaining: 53.5s\n",
      "473:\tlearn: 0.1025098\ttotal: 48.2s\tremaining: 53.5s\n",
      "474:\tlearn: 0.1024712\ttotal: 48.3s\tremaining: 53.4s\n",
      "475:\tlearn: 0.1024305\ttotal: 48.4s\tremaining: 53.3s\n",
      "476:\tlearn: 0.1024087\ttotal: 48.5s\tremaining: 53.1s\n",
      "477:\tlearn: 0.1023255\ttotal: 48.6s\tremaining: 53s\n",
      "478:\tlearn: 0.1022617\ttotal: 48.7s\tremaining: 53s\n",
      "479:\tlearn: 0.1022078\ttotal: 48.8s\tremaining: 52.9s\n",
      "480:\tlearn: 0.1021759\ttotal: 48.9s\tremaining: 52.8s\n",
      "481:\tlearn: 0.1021272\ttotal: 49s\tremaining: 52.7s\n",
      "482:\tlearn: 0.1020570\ttotal: 49.1s\tremaining: 52.6s\n",
      "483:\tlearn: 0.1020120\ttotal: 49.2s\tremaining: 52.5s\n",
      "484:\tlearn: 0.1019934\ttotal: 49.3s\tremaining: 52.4s\n",
      "485:\tlearn: 0.1019420\ttotal: 49.5s\tremaining: 52.3s\n",
      "486:\tlearn: 0.1018721\ttotal: 49.6s\tremaining: 52.2s\n",
      "487:\tlearn: 0.1018246\ttotal: 49.7s\tremaining: 52.1s\n",
      "488:\tlearn: 0.1017632\ttotal: 49.8s\tremaining: 52s\n",
      "489:\tlearn: 0.1016866\ttotal: 49.9s\tremaining: 51.9s\n",
      "490:\tlearn: 0.1016198\ttotal: 50s\tremaining: 51.8s\n",
      "491:\tlearn: 0.1015667\ttotal: 50.1s\tremaining: 51.7s\n",
      "492:\tlearn: 0.1015109\ttotal: 50.2s\tremaining: 51.7s\n",
      "493:\tlearn: 0.1014447\ttotal: 50.3s\tremaining: 51.6s\n",
      "494:\tlearn: 0.1013670\ttotal: 50.4s\tremaining: 51.5s\n",
      "495:\tlearn: 0.1013284\ttotal: 50.5s\tremaining: 51.4s\n",
      "496:\tlearn: 0.1012985\ttotal: 50.6s\tremaining: 51.3s\n",
      "497:\tlearn: 0.1012371\ttotal: 50.8s\tremaining: 51.2s\n",
      "498:\tlearn: 0.1011709\ttotal: 50.9s\tremaining: 51.1s\n",
      "499:\tlearn: 0.1011129\ttotal: 51s\tremaining: 51s\n",
      "500:\tlearn: 0.1010682\ttotal: 51.1s\tremaining: 50.9s\n",
      "501:\tlearn: 0.1010568\ttotal: 51.3s\tremaining: 50.8s\n",
      "502:\tlearn: 0.1009981\ttotal: 51.4s\tremaining: 50.8s\n",
      "503:\tlearn: 0.1009505\ttotal: 51.5s\tremaining: 50.7s\n",
      "504:\tlearn: 0.1009093\ttotal: 51.6s\tremaining: 50.6s\n",
      "505:\tlearn: 0.1008739\ttotal: 51.7s\tremaining: 50.5s\n",
      "506:\tlearn: 0.1008095\ttotal: 51.8s\tremaining: 50.4s\n",
      "507:\tlearn: 0.1007630\ttotal: 51.9s\tremaining: 50.3s\n",
      "508:\tlearn: 0.1007177\ttotal: 52s\tremaining: 50.2s\n",
      "509:\tlearn: 0.1006971\ttotal: 52.2s\tremaining: 50.1s\n",
      "510:\tlearn: 0.1006391\ttotal: 52.3s\tremaining: 50.1s\n",
      "511:\tlearn: 0.1006089\ttotal: 52.4s\tremaining: 50s\n",
      "512:\tlearn: 0.1005440\ttotal: 52.5s\tremaining: 49.8s\n",
      "513:\tlearn: 0.1005097\ttotal: 52.6s\tremaining: 49.8s\n",
      "514:\tlearn: 0.1004431\ttotal: 52.7s\tremaining: 49.7s\n",
      "515:\tlearn: 0.1004166\ttotal: 52.8s\tremaining: 49.6s\n",
      "516:\tlearn: 0.1003514\ttotal: 52.9s\tremaining: 49.5s\n",
      "517:\tlearn: 0.1002334\ttotal: 53s\tremaining: 49.4s\n",
      "518:\tlearn: 0.1001722\ttotal: 53.1s\tremaining: 49.3s\n",
      "519:\tlearn: 0.1001560\ttotal: 53.2s\tremaining: 49.1s\n",
      "520:\tlearn: 0.1001316\ttotal: 53.4s\tremaining: 49.1s\n",
      "521:\tlearn: 0.1000732\ttotal: 53.5s\tremaining: 48.9s\n",
      "522:\tlearn: 0.0999689\ttotal: 53.5s\tremaining: 48.8s\n",
      "523:\tlearn: 0.0999286\ttotal: 53.7s\tremaining: 48.7s\n",
      "524:\tlearn: 0.0998779\ttotal: 53.8s\tremaining: 48.6s\n",
      "525:\tlearn: 0.0998419\ttotal: 53.9s\tremaining: 48.6s\n",
      "526:\tlearn: 0.0998039\ttotal: 54s\tremaining: 48.5s\n",
      "527:\tlearn: 0.0997711\ttotal: 54.1s\tremaining: 48.3s\n",
      "528:\tlearn: 0.0997213\ttotal: 54.2s\tremaining: 48.3s\n",
      "529:\tlearn: 0.0996611\ttotal: 54.3s\tremaining: 48.1s\n",
      "530:\tlearn: 0.0996231\ttotal: 54.4s\tremaining: 48s\n",
      "531:\tlearn: 0.0995576\ttotal: 54.5s\tremaining: 47.9s\n",
      "532:\tlearn: 0.0994983\ttotal: 54.6s\tremaining: 47.9s\n",
      "533:\tlearn: 0.0994420\ttotal: 54.7s\tremaining: 47.8s\n",
      "534:\tlearn: 0.0993978\ttotal: 54.9s\tremaining: 47.7s\n",
      "535:\tlearn: 0.0993676\ttotal: 55s\tremaining: 47.6s\n",
      "536:\tlearn: 0.0993292\ttotal: 55.1s\tremaining: 47.5s\n",
      "537:\tlearn: 0.0992897\ttotal: 55.3s\tremaining: 47.5s\n",
      "538:\tlearn: 0.0992303\ttotal: 55.4s\tremaining: 47.4s\n",
      "539:\tlearn: 0.0991846\ttotal: 55.5s\tremaining: 47.3s\n",
      "540:\tlearn: 0.0991548\ttotal: 55.6s\tremaining: 47.2s\n",
      "541:\tlearn: 0.0990973\ttotal: 55.7s\tremaining: 47.1s\n",
      "542:\tlearn: 0.0990487\ttotal: 55.8s\tremaining: 47s\n",
      "543:\tlearn: 0.0990165\ttotal: 55.9s\tremaining: 46.8s\n",
      "544:\tlearn: 0.0989818\ttotal: 56s\tremaining: 46.7s\n",
      "545:\tlearn: 0.0989544\ttotal: 56.1s\tremaining: 46.6s\n",
      "546:\tlearn: 0.0989144\ttotal: 56.2s\tremaining: 46.6s\n",
      "547:\tlearn: 0.0988441\ttotal: 56.4s\tremaining: 46.5s\n",
      "548:\tlearn: 0.0988188\ttotal: 56.5s\tremaining: 46.4s\n",
      "549:\tlearn: 0.0987599\ttotal: 56.6s\tremaining: 46.3s\n",
      "550:\tlearn: 0.0987347\ttotal: 56.7s\tremaining: 46.2s\n",
      "551:\tlearn: 0.0986112\ttotal: 56.8s\tremaining: 46.1s\n",
      "552:\tlearn: 0.0985955\ttotal: 56.9s\tremaining: 46s\n",
      "553:\tlearn: 0.0985364\ttotal: 57s\tremaining: 45.9s\n",
      "554:\tlearn: 0.0984992\ttotal: 57.2s\tremaining: 45.8s\n",
      "555:\tlearn: 0.0984520\ttotal: 57.3s\tremaining: 45.8s\n",
      "556:\tlearn: 0.0984267\ttotal: 57.4s\tremaining: 45.7s\n",
      "557:\tlearn: 0.0983984\ttotal: 57.5s\tremaining: 45.6s\n",
      "558:\tlearn: 0.0983543\ttotal: 57.6s\tremaining: 45.5s\n",
      "559:\tlearn: 0.0982917\ttotal: 57.7s\tremaining: 45.3s\n",
      "560:\tlearn: 0.0982316\ttotal: 57.8s\tremaining: 45.2s\n",
      "561:\tlearn: 0.0982021\ttotal: 57.9s\tremaining: 45.1s\n",
      "562:\tlearn: 0.0981403\ttotal: 58s\tremaining: 45s\n",
      "563:\tlearn: 0.0980819\ttotal: 58.1s\tremaining: 44.9s\n",
      "564:\tlearn: 0.0980419\ttotal: 58.2s\tremaining: 44.8s\n",
      "565:\tlearn: 0.0980127\ttotal: 58.3s\tremaining: 44.7s\n",
      "566:\tlearn: 0.0979841\ttotal: 58.5s\tremaining: 44.6s\n",
      "567:\tlearn: 0.0979551\ttotal: 58.6s\tremaining: 44.5s\n",
      "568:\tlearn: 0.0978722\ttotal: 58.7s\tremaining: 44.5s\n",
      "569:\tlearn: 0.0978135\ttotal: 58.9s\tremaining: 44.4s\n",
      "570:\tlearn: 0.0977486\ttotal: 59.1s\tremaining: 44.4s\n",
      "571:\tlearn: 0.0976903\ttotal: 59.2s\tremaining: 44.3s\n",
      "572:\tlearn: 0.0976470\ttotal: 59.3s\tremaining: 44.2s\n",
      "573:\tlearn: 0.0976066\ttotal: 59.4s\tremaining: 44.1s\n",
      "574:\tlearn: 0.0975362\ttotal: 59.5s\tremaining: 44s\n",
      "575:\tlearn: 0.0975175\ttotal: 59.6s\tremaining: 43.9s\n",
      "576:\tlearn: 0.0974623\ttotal: 59.7s\tremaining: 43.8s\n",
      "577:\tlearn: 0.0974023\ttotal: 59.8s\tremaining: 43.7s\n",
      "578:\tlearn: 0.0973777\ttotal: 59.9s\tremaining: 43.6s\n",
      "579:\tlearn: 0.0973283\ttotal: 1m\tremaining: 43.5s\n",
      "580:\tlearn: 0.0972728\ttotal: 1m\tremaining: 43.4s\n",
      "581:\tlearn: 0.0972431\ttotal: 1m\tremaining: 43.2s\n",
      "582:\tlearn: 0.0971697\ttotal: 1m\tremaining: 43.2s\n",
      "583:\tlearn: 0.0971254\ttotal: 1m\tremaining: 43.1s\n",
      "584:\tlearn: 0.0970966\ttotal: 1m\tremaining: 43s\n",
      "585:\tlearn: 0.0970490\ttotal: 1m\tremaining: 42.8s\n",
      "586:\tlearn: 0.0970250\ttotal: 1m\tremaining: 42.7s\n",
      "587:\tlearn: 0.0969928\ttotal: 1m\tremaining: 42.6s\n",
      "588:\tlearn: 0.0969317\ttotal: 1m\tremaining: 42.5s\n",
      "589:\tlearn: 0.0968890\ttotal: 1m 1s\tremaining: 42.4s\n",
      "590:\tlearn: 0.0968087\ttotal: 1m 1s\tremaining: 42.3s\n",
      "591:\tlearn: 0.0967654\ttotal: 1m 1s\tremaining: 42.2s\n",
      "592:\tlearn: 0.0966826\ttotal: 1m 1s\tremaining: 42.1s\n",
      "593:\tlearn: 0.0966534\ttotal: 1m 1s\tremaining: 42s\n",
      "594:\tlearn: 0.0966093\ttotal: 1m 1s\tremaining: 41.9s\n",
      "595:\tlearn: 0.0965423\ttotal: 1m 1s\tremaining: 41.8s\n",
      "596:\tlearn: 0.0964747\ttotal: 1m 1s\tremaining: 41.7s\n",
      "597:\tlearn: 0.0964195\ttotal: 1m 1s\tremaining: 41.6s\n",
      "598:\tlearn: 0.0964055\ttotal: 1m 1s\tremaining: 41.5s\n",
      "599:\tlearn: 0.0963747\ttotal: 1m 2s\tremaining: 41.4s\n",
      "600:\tlearn: 0.0963286\ttotal: 1m 2s\tremaining: 41.3s\n",
      "601:\tlearn: 0.0962854\ttotal: 1m 2s\tremaining: 41.2s\n",
      "602:\tlearn: 0.0962474\ttotal: 1m 2s\tremaining: 41.1s\n",
      "603:\tlearn: 0.0962329\ttotal: 1m 2s\tremaining: 41s\n",
      "604:\tlearn: 0.0962036\ttotal: 1m 2s\tremaining: 40.9s\n",
      "605:\tlearn: 0.0961761\ttotal: 1m 2s\tremaining: 40.8s\n",
      "606:\tlearn: 0.0961306\ttotal: 1m 2s\tremaining: 40.7s\n",
      "607:\tlearn: 0.0961111\ttotal: 1m 3s\tremaining: 40.6s\n",
      "608:\tlearn: 0.0960691\ttotal: 1m 3s\tremaining: 40.5s\n",
      "609:\tlearn: 0.0960163\ttotal: 1m 3s\tremaining: 40.4s\n",
      "610:\tlearn: 0.0959849\ttotal: 1m 3s\tremaining: 40.3s\n",
      "611:\tlearn: 0.0959745\ttotal: 1m 3s\tremaining: 40.2s\n",
      "612:\tlearn: 0.0959069\ttotal: 1m 3s\tremaining: 40.1s\n",
      "613:\tlearn: 0.0958691\ttotal: 1m 3s\tremaining: 40s\n",
      "614:\tlearn: 0.0958413\ttotal: 1m 3s\tremaining: 39.9s\n",
      "615:\tlearn: 0.0958053\ttotal: 1m 3s\tremaining: 39.8s\n",
      "616:\tlearn: 0.0957898\ttotal: 1m 4s\tremaining: 39.7s\n",
      "617:\tlearn: 0.0957515\ttotal: 1m 4s\tremaining: 39.7s\n",
      "618:\tlearn: 0.0957222\ttotal: 1m 4s\tremaining: 39.5s\n",
      "619:\tlearn: 0.0956704\ttotal: 1m 4s\tremaining: 39.4s\n",
      "620:\tlearn: 0.0956502\ttotal: 1m 4s\tremaining: 39.3s\n",
      "621:\tlearn: 0.0956242\ttotal: 1m 4s\tremaining: 39.2s\n",
      "622:\tlearn: 0.0955402\ttotal: 1m 4s\tremaining: 39.1s\n",
      "623:\tlearn: 0.0954731\ttotal: 1m 4s\tremaining: 39s\n",
      "624:\tlearn: 0.0953690\ttotal: 1m 4s\tremaining: 38.9s\n",
      "625:\tlearn: 0.0953135\ttotal: 1m 4s\tremaining: 38.8s\n",
      "626:\tlearn: 0.0953015\ttotal: 1m 5s\tremaining: 38.7s\n",
      "627:\tlearn: 0.0952752\ttotal: 1m 5s\tremaining: 38.6s\n",
      "628:\tlearn: 0.0952290\ttotal: 1m 5s\tremaining: 38.5s\n",
      "629:\tlearn: 0.0951999\ttotal: 1m 5s\tremaining: 38.4s\n",
      "630:\tlearn: 0.0951619\ttotal: 1m 5s\tremaining: 38.3s\n",
      "631:\tlearn: 0.0951113\ttotal: 1m 5s\tremaining: 38.2s\n",
      "632:\tlearn: 0.0950917\ttotal: 1m 5s\tremaining: 38.1s\n",
      "633:\tlearn: 0.0950475\ttotal: 1m 5s\tremaining: 38s\n",
      "634:\tlearn: 0.0950097\ttotal: 1m 5s\tremaining: 37.8s\n",
      "635:\tlearn: 0.0949405\ttotal: 1m 5s\tremaining: 37.7s\n",
      "636:\tlearn: 0.0948975\ttotal: 1m 6s\tremaining: 37.6s\n",
      "637:\tlearn: 0.0948659\ttotal: 1m 6s\tremaining: 37.5s\n",
      "638:\tlearn: 0.0948211\ttotal: 1m 6s\tremaining: 37.4s\n",
      "639:\tlearn: 0.0947552\ttotal: 1m 6s\tremaining: 37.3s\n",
      "640:\tlearn: 0.0946921\ttotal: 1m 6s\tremaining: 37.2s\n",
      "641:\tlearn: 0.0946100\ttotal: 1m 6s\tremaining: 37.1s\n",
      "642:\tlearn: 0.0945603\ttotal: 1m 6s\tremaining: 37s\n",
      "643:\tlearn: 0.0944899\ttotal: 1m 6s\tremaining: 36.9s\n",
      "644:\tlearn: 0.0944635\ttotal: 1m 6s\tremaining: 36.8s\n",
      "645:\tlearn: 0.0943775\ttotal: 1m 6s\tremaining: 36.7s\n",
      "646:\tlearn: 0.0943328\ttotal: 1m 7s\tremaining: 36.6s\n",
      "647:\tlearn: 0.0942536\ttotal: 1m 7s\tremaining: 36.5s\n",
      "648:\tlearn: 0.0942012\ttotal: 1m 7s\tremaining: 36.4s\n",
      "649:\tlearn: 0.0941685\ttotal: 1m 7s\tremaining: 36.3s\n",
      "650:\tlearn: 0.0941108\ttotal: 1m 7s\tremaining: 36.2s\n",
      "651:\tlearn: 0.0941067\ttotal: 1m 7s\tremaining: 36.1s\n",
      "652:\tlearn: 0.0940960\ttotal: 1m 7s\tremaining: 36s\n",
      "653:\tlearn: 0.0940707\ttotal: 1m 7s\tremaining: 35.9s\n",
      "654:\tlearn: 0.0940020\ttotal: 1m 8s\tremaining: 35.8s\n",
      "655:\tlearn: 0.0939653\ttotal: 1m 8s\tremaining: 35.7s\n",
      "656:\tlearn: 0.0939446\ttotal: 1m 8s\tremaining: 35.6s\n",
      "657:\tlearn: 0.0939106\ttotal: 1m 8s\tremaining: 35.5s\n",
      "658:\tlearn: 0.0938682\ttotal: 1m 8s\tremaining: 35.4s\n",
      "659:\tlearn: 0.0938046\ttotal: 1m 8s\tremaining: 35.3s\n",
      "660:\tlearn: 0.0937536\ttotal: 1m 8s\tremaining: 35.2s\n",
      "661:\tlearn: 0.0936947\ttotal: 1m 8s\tremaining: 35.1s\n",
      "662:\tlearn: 0.0936237\ttotal: 1m 8s\tremaining: 35s\n",
      "663:\tlearn: 0.0935985\ttotal: 1m 8s\tremaining: 34.9s\n",
      "664:\tlearn: 0.0935660\ttotal: 1m 9s\tremaining: 34.8s\n",
      "665:\tlearn: 0.0935289\ttotal: 1m 9s\tremaining: 34.7s\n",
      "666:\tlearn: 0.0935121\ttotal: 1m 9s\tremaining: 34.6s\n",
      "667:\tlearn: 0.0934773\ttotal: 1m 9s\tremaining: 34.5s\n",
      "668:\tlearn: 0.0934054\ttotal: 1m 9s\tremaining: 34.4s\n",
      "669:\tlearn: 0.0933405\ttotal: 1m 9s\tremaining: 34.3s\n",
      "670:\tlearn: 0.0933001\ttotal: 1m 9s\tremaining: 34.2s\n",
      "671:\tlearn: 0.0932363\ttotal: 1m 9s\tremaining: 34.1s\n",
      "672:\tlearn: 0.0932198\ttotal: 1m 10s\tremaining: 34s\n",
      "673:\tlearn: 0.0931718\ttotal: 1m 10s\tremaining: 33.9s\n",
      "674:\tlearn: 0.0931233\ttotal: 1m 10s\tremaining: 33.8s\n",
      "675:\tlearn: 0.0930840\ttotal: 1m 10s\tremaining: 33.7s\n",
      "676:\tlearn: 0.0930522\ttotal: 1m 10s\tremaining: 33.6s\n",
      "677:\tlearn: 0.0930260\ttotal: 1m 10s\tremaining: 33.5s\n",
      "678:\tlearn: 0.0929311\ttotal: 1m 10s\tremaining: 33.4s\n",
      "679:\tlearn: 0.0928908\ttotal: 1m 10s\tremaining: 33.3s\n",
      "680:\tlearn: 0.0928066\ttotal: 1m 10s\tremaining: 33.2s\n",
      "681:\tlearn: 0.0927777\ttotal: 1m 11s\tremaining: 33.1s\n",
      "682:\tlearn: 0.0927311\ttotal: 1m 11s\tremaining: 33s\n",
      "683:\tlearn: 0.0926918\ttotal: 1m 11s\tremaining: 32.9s\n",
      "684:\tlearn: 0.0926583\ttotal: 1m 11s\tremaining: 32.8s\n",
      "685:\tlearn: 0.0926200\ttotal: 1m 11s\tremaining: 32.8s\n",
      "686:\tlearn: 0.0925984\ttotal: 1m 11s\tremaining: 32.7s\n",
      "687:\tlearn: 0.0925639\ttotal: 1m 11s\tremaining: 32.5s\n",
      "688:\tlearn: 0.0925054\ttotal: 1m 11s\tremaining: 32.4s\n",
      "689:\tlearn: 0.0924581\ttotal: 1m 11s\tremaining: 32.3s\n",
      "690:\tlearn: 0.0924097\ttotal: 1m 12s\tremaining: 32.2s\n",
      "691:\tlearn: 0.0923712\ttotal: 1m 12s\tremaining: 32.1s\n",
      "692:\tlearn: 0.0923367\ttotal: 1m 12s\tremaining: 32s\n",
      "693:\tlearn: 0.0922838\ttotal: 1m 12s\tremaining: 31.9s\n",
      "694:\tlearn: 0.0922674\ttotal: 1m 12s\tremaining: 31.8s\n",
      "695:\tlearn: 0.0922156\ttotal: 1m 12s\tremaining: 31.7s\n",
      "696:\tlearn: 0.0921529\ttotal: 1m 12s\tremaining: 31.6s\n",
      "697:\tlearn: 0.0921155\ttotal: 1m 12s\tremaining: 31.5s\n",
      "698:\tlearn: 0.0920815\ttotal: 1m 12s\tremaining: 31.4s\n",
      "699:\tlearn: 0.0920465\ttotal: 1m 12s\tremaining: 31.3s\n",
      "700:\tlearn: 0.0920050\ttotal: 1m 13s\tremaining: 31.2s\n",
      "701:\tlearn: 0.0919578\ttotal: 1m 13s\tremaining: 31.1s\n",
      "702:\tlearn: 0.0918957\ttotal: 1m 13s\tremaining: 31s\n",
      "703:\tlearn: 0.0918069\ttotal: 1m 13s\tremaining: 30.9s\n",
      "704:\tlearn: 0.0917625\ttotal: 1m 13s\tremaining: 30.8s\n",
      "705:\tlearn: 0.0917528\ttotal: 1m 13s\tremaining: 30.6s\n",
      "706:\tlearn: 0.0917013\ttotal: 1m 13s\tremaining: 30.6s\n",
      "707:\tlearn: 0.0916161\ttotal: 1m 14s\tremaining: 30.5s\n",
      "708:\tlearn: 0.0915640\ttotal: 1m 14s\tremaining: 30.4s\n",
      "709:\tlearn: 0.0915297\ttotal: 1m 14s\tremaining: 30.3s\n",
      "710:\tlearn: 0.0914605\ttotal: 1m 14s\tremaining: 30.2s\n",
      "711:\tlearn: 0.0914177\ttotal: 1m 14s\tremaining: 30.2s\n",
      "712:\tlearn: 0.0913685\ttotal: 1m 14s\tremaining: 30.1s\n",
      "713:\tlearn: 0.0913253\ttotal: 1m 14s\tremaining: 30s\n",
      "714:\tlearn: 0.0912628\ttotal: 1m 14s\tremaining: 29.9s\n",
      "715:\tlearn: 0.0912335\ttotal: 1m 15s\tremaining: 29.8s\n",
      "716:\tlearn: 0.0912103\ttotal: 1m 15s\tremaining: 29.7s\n",
      "717:\tlearn: 0.0911808\ttotal: 1m 15s\tremaining: 29.6s\n",
      "718:\tlearn: 0.0911409\ttotal: 1m 15s\tremaining: 29.4s\n",
      "719:\tlearn: 0.0910928\ttotal: 1m 15s\tremaining: 29.3s\n",
      "720:\tlearn: 0.0910531\ttotal: 1m 15s\tremaining: 29.2s\n",
      "721:\tlearn: 0.0910265\ttotal: 1m 15s\tremaining: 29.1s\n",
      "722:\tlearn: 0.0910051\ttotal: 1m 15s\tremaining: 29s\n",
      "723:\tlearn: 0.0909816\ttotal: 1m 15s\tremaining: 28.9s\n",
      "724:\tlearn: 0.0909676\ttotal: 1m 15s\tremaining: 28.8s\n",
      "725:\tlearn: 0.0909414\ttotal: 1m 16s\tremaining: 28.7s\n",
      "726:\tlearn: 0.0908701\ttotal: 1m 16s\tremaining: 28.6s\n",
      "727:\tlearn: 0.0908116\ttotal: 1m 16s\tremaining: 28.5s\n",
      "728:\tlearn: 0.0907559\ttotal: 1m 16s\tremaining: 28.4s\n",
      "729:\tlearn: 0.0907224\ttotal: 1m 16s\tremaining: 28.3s\n",
      "730:\tlearn: 0.0907091\ttotal: 1m 16s\tremaining: 28.2s\n",
      "731:\tlearn: 0.0906616\ttotal: 1m 16s\tremaining: 28.1s\n",
      "732:\tlearn: 0.0906132\ttotal: 1m 16s\tremaining: 28s\n",
      "733:\tlearn: 0.0905816\ttotal: 1m 16s\tremaining: 27.9s\n",
      "734:\tlearn: 0.0905536\ttotal: 1m 17s\tremaining: 27.8s\n",
      "735:\tlearn: 0.0904917\ttotal: 1m 17s\tremaining: 27.7s\n",
      "736:\tlearn: 0.0904597\ttotal: 1m 17s\tremaining: 27.6s\n",
      "737:\tlearn: 0.0904320\ttotal: 1m 17s\tremaining: 27.5s\n",
      "738:\tlearn: 0.0903927\ttotal: 1m 17s\tremaining: 27.4s\n",
      "739:\tlearn: 0.0903545\ttotal: 1m 17s\tremaining: 27.3s\n",
      "740:\tlearn: 0.0902901\ttotal: 1m 17s\tremaining: 27.2s\n",
      "741:\tlearn: 0.0902699\ttotal: 1m 17s\tremaining: 27.1s\n",
      "742:\tlearn: 0.0902449\ttotal: 1m 18s\tremaining: 27s\n",
      "743:\tlearn: 0.0901986\ttotal: 1m 18s\tremaining: 26.9s\n",
      "744:\tlearn: 0.0901728\ttotal: 1m 18s\tremaining: 26.8s\n",
      "745:\tlearn: 0.0901181\ttotal: 1m 18s\tremaining: 26.7s\n",
      "746:\tlearn: 0.0900709\ttotal: 1m 18s\tremaining: 26.6s\n",
      "747:\tlearn: 0.0900304\ttotal: 1m 18s\tremaining: 26.5s\n",
      "748:\tlearn: 0.0899971\ttotal: 1m 18s\tremaining: 26.4s\n",
      "749:\tlearn: 0.0899646\ttotal: 1m 18s\tremaining: 26.3s\n",
      "750:\tlearn: 0.0899158\ttotal: 1m 18s\tremaining: 26.2s\n",
      "751:\tlearn: 0.0898911\ttotal: 1m 19s\tremaining: 26.1s\n",
      "752:\tlearn: 0.0897966\ttotal: 1m 19s\tremaining: 26s\n",
      "753:\tlearn: 0.0897492\ttotal: 1m 19s\tremaining: 25.8s\n",
      "754:\tlearn: 0.0897102\ttotal: 1m 19s\tremaining: 25.7s\n",
      "755:\tlearn: 0.0896679\ttotal: 1m 19s\tremaining: 25.6s\n",
      "756:\tlearn: 0.0896491\ttotal: 1m 19s\tremaining: 25.5s\n",
      "757:\tlearn: 0.0896233\ttotal: 1m 19s\tremaining: 25.4s\n",
      "758:\tlearn: 0.0895850\ttotal: 1m 19s\tremaining: 25.3s\n",
      "759:\tlearn: 0.0895459\ttotal: 1m 19s\tremaining: 25.2s\n",
      "760:\tlearn: 0.0895293\ttotal: 1m 19s\tremaining: 25.1s\n",
      "761:\tlearn: 0.0894688\ttotal: 1m 19s\tremaining: 25s\n",
      "762:\tlearn: 0.0894685\ttotal: 1m 20s\tremaining: 24.9s\n",
      "763:\tlearn: 0.0894477\ttotal: 1m 20s\tremaining: 24.8s\n",
      "764:\tlearn: 0.0893886\ttotal: 1m 20s\tremaining: 24.7s\n",
      "765:\tlearn: 0.0893262\ttotal: 1m 20s\tremaining: 24.6s\n",
      "766:\tlearn: 0.0892941\ttotal: 1m 20s\tremaining: 24.5s\n",
      "767:\tlearn: 0.0892204\ttotal: 1m 20s\tremaining: 24.4s\n",
      "768:\tlearn: 0.0891957\ttotal: 1m 20s\tremaining: 24.3s\n",
      "769:\tlearn: 0.0891777\ttotal: 1m 20s\tremaining: 24.2s\n",
      "770:\tlearn: 0.0891554\ttotal: 1m 20s\tremaining: 24s\n",
      "771:\tlearn: 0.0890855\ttotal: 1m 21s\tremaining: 23.9s\n",
      "772:\tlearn: 0.0890434\ttotal: 1m 21s\tremaining: 23.8s\n",
      "773:\tlearn: 0.0889934\ttotal: 1m 21s\tremaining: 23.7s\n",
      "774:\tlearn: 0.0889568\ttotal: 1m 21s\tremaining: 23.6s\n",
      "775:\tlearn: 0.0889091\ttotal: 1m 21s\tremaining: 23.5s\n",
      "776:\tlearn: 0.0888714\ttotal: 1m 21s\tremaining: 23.4s\n",
      "777:\tlearn: 0.0888332\ttotal: 1m 21s\tremaining: 23.3s\n",
      "778:\tlearn: 0.0887842\ttotal: 1m 21s\tremaining: 23.2s\n",
      "779:\tlearn: 0.0887516\ttotal: 1m 21s\tremaining: 23.1s\n",
      "780:\tlearn: 0.0887378\ttotal: 1m 21s\tremaining: 23s\n",
      "781:\tlearn: 0.0886768\ttotal: 1m 22s\tremaining: 22.9s\n",
      "782:\tlearn: 0.0886270\ttotal: 1m 22s\tremaining: 22.8s\n",
      "783:\tlearn: 0.0885925\ttotal: 1m 22s\tremaining: 22.6s\n",
      "784:\tlearn: 0.0885284\ttotal: 1m 22s\tremaining: 22.5s\n",
      "785:\tlearn: 0.0885122\ttotal: 1m 22s\tremaining: 22.4s\n",
      "786:\tlearn: 0.0884830\ttotal: 1m 22s\tremaining: 22.3s\n",
      "787:\tlearn: 0.0884510\ttotal: 1m 22s\tremaining: 22.2s\n",
      "788:\tlearn: 0.0884095\ttotal: 1m 22s\tremaining: 22.1s\n",
      "789:\tlearn: 0.0883806\ttotal: 1m 22s\tremaining: 22s\n",
      "790:\tlearn: 0.0883319\ttotal: 1m 22s\tremaining: 21.9s\n",
      "791:\tlearn: 0.0883054\ttotal: 1m 22s\tremaining: 21.8s\n",
      "792:\tlearn: 0.0882745\ttotal: 1m 23s\tremaining: 21.7s\n",
      "793:\tlearn: 0.0881953\ttotal: 1m 23s\tremaining: 21.6s\n",
      "794:\tlearn: 0.0881480\ttotal: 1m 23s\tremaining: 21.5s\n",
      "795:\tlearn: 0.0881202\ttotal: 1m 23s\tremaining: 21.4s\n",
      "796:\tlearn: 0.0880851\ttotal: 1m 23s\tremaining: 21.3s\n",
      "797:\tlearn: 0.0880665\ttotal: 1m 23s\tremaining: 21.2s\n",
      "798:\tlearn: 0.0880256\ttotal: 1m 23s\tremaining: 21.1s\n",
      "799:\tlearn: 0.0879920\ttotal: 1m 24s\tremaining: 21s\n",
      "800:\tlearn: 0.0879332\ttotal: 1m 24s\tremaining: 20.9s\n",
      "801:\tlearn: 0.0878767\ttotal: 1m 24s\tremaining: 20.8s\n",
      "802:\tlearn: 0.0878582\ttotal: 1m 24s\tremaining: 20.7s\n",
      "803:\tlearn: 0.0878028\ttotal: 1m 24s\tremaining: 20.6s\n",
      "804:\tlearn: 0.0877544\ttotal: 1m 24s\tremaining: 20.5s\n",
      "805:\tlearn: 0.0877127\ttotal: 1m 24s\tremaining: 20.4s\n",
      "806:\tlearn: 0.0876856\ttotal: 1m 24s\tremaining: 20.3s\n",
      "807:\tlearn: 0.0876560\ttotal: 1m 24s\tremaining: 20.2s\n",
      "808:\tlearn: 0.0876057\ttotal: 1m 25s\tremaining: 20.1s\n",
      "809:\tlearn: 0.0875698\ttotal: 1m 25s\tremaining: 20s\n",
      "810:\tlearn: 0.0875404\ttotal: 1m 25s\tremaining: 19.9s\n",
      "811:\tlearn: 0.0875089\ttotal: 1m 25s\tremaining: 19.8s\n",
      "812:\tlearn: 0.0874707\ttotal: 1m 25s\tremaining: 19.6s\n",
      "813:\tlearn: 0.0874318\ttotal: 1m 25s\tremaining: 19.5s\n",
      "814:\tlearn: 0.0873839\ttotal: 1m 25s\tremaining: 19.4s\n",
      "815:\tlearn: 0.0873508\ttotal: 1m 25s\tremaining: 19.3s\n",
      "816:\tlearn: 0.0873012\ttotal: 1m 25s\tremaining: 19.2s\n",
      "817:\tlearn: 0.0872633\ttotal: 1m 25s\tremaining: 19.1s\n",
      "818:\tlearn: 0.0872307\ttotal: 1m 25s\tremaining: 19s\n",
      "819:\tlearn: 0.0872026\ttotal: 1m 26s\tremaining: 18.9s\n",
      "820:\tlearn: 0.0871680\ttotal: 1m 26s\tremaining: 18.8s\n",
      "821:\tlearn: 0.0871111\ttotal: 1m 26s\tremaining: 18.7s\n",
      "822:\tlearn: 0.0870565\ttotal: 1m 26s\tremaining: 18.6s\n",
      "823:\tlearn: 0.0870229\ttotal: 1m 26s\tremaining: 18.5s\n",
      "824:\tlearn: 0.0869691\ttotal: 1m 26s\tremaining: 18.4s\n",
      "825:\tlearn: 0.0869314\ttotal: 1m 26s\tremaining: 18.3s\n",
      "826:\tlearn: 0.0869090\ttotal: 1m 26s\tremaining: 18.2s\n",
      "827:\tlearn: 0.0868684\ttotal: 1m 26s\tremaining: 18.1s\n",
      "828:\tlearn: 0.0868413\ttotal: 1m 27s\tremaining: 18s\n",
      "829:\tlearn: 0.0867739\ttotal: 1m 27s\tremaining: 17.8s\n",
      "830:\tlearn: 0.0867357\ttotal: 1m 27s\tremaining: 17.7s\n",
      "831:\tlearn: 0.0867078\ttotal: 1m 27s\tremaining: 17.6s\n",
      "832:\tlearn: 0.0866788\ttotal: 1m 27s\tremaining: 17.5s\n",
      "833:\tlearn: 0.0866262\ttotal: 1m 27s\tremaining: 17.4s\n",
      "834:\tlearn: 0.0865770\ttotal: 1m 27s\tremaining: 17.3s\n",
      "835:\tlearn: 0.0865532\ttotal: 1m 27s\tremaining: 17.2s\n",
      "836:\tlearn: 0.0865147\ttotal: 1m 27s\tremaining: 17.1s\n",
      "837:\tlearn: 0.0864291\ttotal: 1m 27s\tremaining: 17s\n",
      "838:\tlearn: 0.0863785\ttotal: 1m 28s\tremaining: 16.9s\n",
      "839:\tlearn: 0.0863620\ttotal: 1m 28s\tremaining: 16.8s\n",
      "840:\tlearn: 0.0863212\ttotal: 1m 28s\tremaining: 16.7s\n",
      "841:\tlearn: 0.0862502\ttotal: 1m 28s\tremaining: 16.6s\n",
      "842:\tlearn: 0.0862121\ttotal: 1m 28s\tremaining: 16.5s\n",
      "843:\tlearn: 0.0861792\ttotal: 1m 28s\tremaining: 16.4s\n",
      "844:\tlearn: 0.0861377\ttotal: 1m 28s\tremaining: 16.3s\n",
      "845:\tlearn: 0.0860605\ttotal: 1m 28s\tremaining: 16.2s\n",
      "846:\tlearn: 0.0859945\ttotal: 1m 28s\tremaining: 16.1s\n",
      "847:\tlearn: 0.0859622\ttotal: 1m 29s\tremaining: 16s\n",
      "848:\tlearn: 0.0859259\ttotal: 1m 29s\tremaining: 15.9s\n",
      "849:\tlearn: 0.0858951\ttotal: 1m 29s\tremaining: 15.8s\n",
      "850:\tlearn: 0.0858548\ttotal: 1m 29s\tremaining: 15.7s\n",
      "851:\tlearn: 0.0858097\ttotal: 1m 29s\tremaining: 15.6s\n",
      "852:\tlearn: 0.0857899\ttotal: 1m 29s\tremaining: 15.5s\n",
      "853:\tlearn: 0.0857700\ttotal: 1m 30s\tremaining: 15.4s\n",
      "854:\tlearn: 0.0857642\ttotal: 1m 30s\tremaining: 15.3s\n",
      "855:\tlearn: 0.0857464\ttotal: 1m 30s\tremaining: 15.2s\n",
      "856:\tlearn: 0.0857145\ttotal: 1m 30s\tremaining: 15.1s\n",
      "857:\tlearn: 0.0856779\ttotal: 1m 30s\tremaining: 15s\n",
      "858:\tlearn: 0.0856487\ttotal: 1m 30s\tremaining: 14.9s\n",
      "859:\tlearn: 0.0856165\ttotal: 1m 30s\tremaining: 14.8s\n",
      "860:\tlearn: 0.0855713\ttotal: 1m 30s\tremaining: 14.6s\n",
      "861:\tlearn: 0.0855261\ttotal: 1m 30s\tremaining: 14.5s\n",
      "862:\tlearn: 0.0854616\ttotal: 1m 30s\tremaining: 14.4s\n",
      "863:\tlearn: 0.0854125\ttotal: 1m 31s\tremaining: 14.3s\n",
      "864:\tlearn: 0.0853790\ttotal: 1m 31s\tremaining: 14.2s\n",
      "865:\tlearn: 0.0853309\ttotal: 1m 31s\tremaining: 14.1s\n",
      "866:\tlearn: 0.0852617\ttotal: 1m 31s\tremaining: 14s\n",
      "867:\tlearn: 0.0852362\ttotal: 1m 31s\tremaining: 13.9s\n",
      "868:\tlearn: 0.0851707\ttotal: 1m 31s\tremaining: 13.8s\n",
      "869:\tlearn: 0.0851354\ttotal: 1m 31s\tremaining: 13.7s\n",
      "870:\tlearn: 0.0851178\ttotal: 1m 31s\tremaining: 13.6s\n",
      "871:\tlearn: 0.0850674\ttotal: 1m 31s\tremaining: 13.5s\n",
      "872:\tlearn: 0.0850531\ttotal: 1m 32s\tremaining: 13.4s\n",
      "873:\tlearn: 0.0850319\ttotal: 1m 32s\tremaining: 13.3s\n",
      "874:\tlearn: 0.0850172\ttotal: 1m 32s\tremaining: 13.2s\n",
      "875:\tlearn: 0.0849881\ttotal: 1m 32s\tremaining: 13.1s\n",
      "876:\tlearn: 0.0849788\ttotal: 1m 32s\tremaining: 13s\n",
      "877:\tlearn: 0.0849525\ttotal: 1m 32s\tremaining: 12.9s\n",
      "878:\tlearn: 0.0849083\ttotal: 1m 32s\tremaining: 12.8s\n",
      "879:\tlearn: 0.0848459\ttotal: 1m 32s\tremaining: 12.7s\n",
      "880:\tlearn: 0.0848094\ttotal: 1m 32s\tremaining: 12.6s\n",
      "881:\tlearn: 0.0847855\ttotal: 1m 33s\tremaining: 12.4s\n",
      "882:\tlearn: 0.0847424\ttotal: 1m 33s\tremaining: 12.3s\n",
      "883:\tlearn: 0.0847184\ttotal: 1m 33s\tremaining: 12.2s\n",
      "884:\tlearn: 0.0846537\ttotal: 1m 33s\tremaining: 12.2s\n",
      "885:\tlearn: 0.0846005\ttotal: 1m 33s\tremaining: 12.1s\n",
      "886:\tlearn: 0.0846004\ttotal: 1m 33s\tremaining: 12s\n",
      "887:\tlearn: 0.0845753\ttotal: 1m 33s\tremaining: 11.8s\n",
      "888:\tlearn: 0.0845222\ttotal: 1m 34s\tremaining: 11.7s\n",
      "889:\tlearn: 0.0844958\ttotal: 1m 34s\tremaining: 11.6s\n",
      "890:\tlearn: 0.0844726\ttotal: 1m 34s\tremaining: 11.5s\n",
      "891:\tlearn: 0.0844472\ttotal: 1m 34s\tremaining: 11.4s\n",
      "892:\tlearn: 0.0844280\ttotal: 1m 34s\tremaining: 11.3s\n",
      "893:\tlearn: 0.0844021\ttotal: 1m 34s\tremaining: 11.2s\n",
      "894:\tlearn: 0.0843710\ttotal: 1m 34s\tremaining: 11.1s\n",
      "895:\tlearn: 0.0843454\ttotal: 1m 34s\tremaining: 11s\n",
      "896:\tlearn: 0.0843021\ttotal: 1m 34s\tremaining: 10.9s\n",
      "897:\tlearn: 0.0842736\ttotal: 1m 34s\tremaining: 10.8s\n",
      "898:\tlearn: 0.0842391\ttotal: 1m 35s\tremaining: 10.7s\n",
      "899:\tlearn: 0.0842048\ttotal: 1m 35s\tremaining: 10.6s\n",
      "900:\tlearn: 0.0841616\ttotal: 1m 35s\tremaining: 10.5s\n",
      "901:\tlearn: 0.0840625\ttotal: 1m 35s\tremaining: 10.4s\n",
      "902:\tlearn: 0.0840008\ttotal: 1m 35s\tremaining: 10.3s\n",
      "903:\tlearn: 0.0839645\ttotal: 1m 35s\tremaining: 10.2s\n",
      "904:\tlearn: 0.0839448\ttotal: 1m 35s\tremaining: 10s\n",
      "905:\tlearn: 0.0838825\ttotal: 1m 35s\tremaining: 9.94s\n",
      "906:\tlearn: 0.0838384\ttotal: 1m 35s\tremaining: 9.83s\n",
      "907:\tlearn: 0.0838068\ttotal: 1m 35s\tremaining: 9.73s\n",
      "908:\tlearn: 0.0837793\ttotal: 1m 36s\tremaining: 9.62s\n",
      "909:\tlearn: 0.0837165\ttotal: 1m 36s\tremaining: 9.52s\n",
      "910:\tlearn: 0.0836649\ttotal: 1m 36s\tremaining: 9.41s\n",
      "911:\tlearn: 0.0836462\ttotal: 1m 36s\tremaining: 9.3s\n",
      "912:\tlearn: 0.0836235\ttotal: 1m 36s\tremaining: 9.2s\n",
      "913:\tlearn: 0.0835971\ttotal: 1m 36s\tremaining: 9.09s\n",
      "914:\tlearn: 0.0835806\ttotal: 1m 36s\tremaining: 8.98s\n",
      "915:\tlearn: 0.0835506\ttotal: 1m 36s\tremaining: 8.88s\n",
      "916:\tlearn: 0.0835352\ttotal: 1m 36s\tremaining: 8.77s\n",
      "917:\tlearn: 0.0835104\ttotal: 1m 37s\tremaining: 8.66s\n",
      "918:\tlearn: 0.0834614\ttotal: 1m 37s\tremaining: 8.56s\n",
      "919:\tlearn: 0.0834295\ttotal: 1m 37s\tremaining: 8.45s\n",
      "920:\tlearn: 0.0833724\ttotal: 1m 37s\tremaining: 8.35s\n",
      "921:\tlearn: 0.0833408\ttotal: 1m 37s\tremaining: 8.24s\n",
      "922:\tlearn: 0.0833221\ttotal: 1m 37s\tremaining: 8.14s\n",
      "923:\tlearn: 0.0832632\ttotal: 1m 37s\tremaining: 8.03s\n",
      "924:\tlearn: 0.0832081\ttotal: 1m 37s\tremaining: 7.93s\n",
      "925:\tlearn: 0.0831443\ttotal: 1m 37s\tremaining: 7.82s\n",
      "926:\tlearn: 0.0831193\ttotal: 1m 37s\tremaining: 7.71s\n",
      "927:\tlearn: 0.0831108\ttotal: 1m 38s\tremaining: 7.61s\n",
      "928:\tlearn: 0.0830449\ttotal: 1m 38s\tremaining: 7.5s\n",
      "929:\tlearn: 0.0830224\ttotal: 1m 38s\tremaining: 7.4s\n",
      "930:\tlearn: 0.0829839\ttotal: 1m 38s\tremaining: 7.29s\n",
      "931:\tlearn: 0.0829338\ttotal: 1m 38s\tremaining: 7.19s\n",
      "932:\tlearn: 0.0829068\ttotal: 1m 38s\tremaining: 7.08s\n",
      "933:\tlearn: 0.0828895\ttotal: 1m 38s\tremaining: 6.97s\n",
      "934:\tlearn: 0.0828461\ttotal: 1m 38s\tremaining: 6.87s\n",
      "935:\tlearn: 0.0828058\ttotal: 1m 38s\tremaining: 6.76s\n",
      "936:\tlearn: 0.0827554\ttotal: 1m 38s\tremaining: 6.65s\n",
      "937:\tlearn: 0.0827414\ttotal: 1m 39s\tremaining: 6.55s\n",
      "938:\tlearn: 0.0826730\ttotal: 1m 39s\tremaining: 6.44s\n",
      "939:\tlearn: 0.0826427\ttotal: 1m 39s\tremaining: 6.33s\n",
      "940:\tlearn: 0.0825755\ttotal: 1m 39s\tremaining: 6.23s\n",
      "941:\tlearn: 0.0825289\ttotal: 1m 39s\tremaining: 6.12s\n",
      "942:\tlearn: 0.0825020\ttotal: 1m 39s\tremaining: 6.02s\n",
      "943:\tlearn: 0.0824464\ttotal: 1m 39s\tremaining: 5.91s\n",
      "944:\tlearn: 0.0823814\ttotal: 1m 39s\tremaining: 5.8s\n",
      "945:\tlearn: 0.0823327\ttotal: 1m 39s\tremaining: 5.7s\n",
      "946:\tlearn: 0.0823176\ttotal: 1m 39s\tremaining: 5.59s\n",
      "947:\tlearn: 0.0822751\ttotal: 1m 40s\tremaining: 5.49s\n",
      "948:\tlearn: 0.0822603\ttotal: 1m 40s\tremaining: 5.38s\n",
      "949:\tlearn: 0.0822381\ttotal: 1m 40s\tremaining: 5.27s\n",
      "950:\tlearn: 0.0822162\ttotal: 1m 40s\tremaining: 5.17s\n",
      "951:\tlearn: 0.0821859\ttotal: 1m 40s\tremaining: 5.06s\n",
      "952:\tlearn: 0.0821718\ttotal: 1m 40s\tremaining: 4.96s\n",
      "953:\tlearn: 0.0820853\ttotal: 1m 40s\tremaining: 4.86s\n",
      "954:\tlearn: 0.0820641\ttotal: 1m 40s\tremaining: 4.75s\n",
      "955:\tlearn: 0.0820000\ttotal: 1m 40s\tremaining: 4.65s\n",
      "956:\tlearn: 0.0819605\ttotal: 1m 41s\tremaining: 4.54s\n",
      "957:\tlearn: 0.0819225\ttotal: 1m 41s\tremaining: 4.43s\n",
      "958:\tlearn: 0.0818876\ttotal: 1m 41s\tremaining: 4.33s\n",
      "959:\tlearn: 0.0818305\ttotal: 1m 41s\tremaining: 4.22s\n",
      "960:\tlearn: 0.0817802\ttotal: 1m 41s\tremaining: 4.12s\n",
      "961:\tlearn: 0.0817356\ttotal: 1m 41s\tremaining: 4.01s\n",
      "962:\tlearn: 0.0816880\ttotal: 1m 41s\tremaining: 3.9s\n",
      "963:\tlearn: 0.0816655\ttotal: 1m 41s\tremaining: 3.8s\n",
      "964:\tlearn: 0.0816360\ttotal: 1m 41s\tremaining: 3.69s\n",
      "965:\tlearn: 0.0816063\ttotal: 1m 41s\tremaining: 3.59s\n",
      "966:\tlearn: 0.0815848\ttotal: 1m 42s\tremaining: 3.48s\n",
      "967:\tlearn: 0.0815504\ttotal: 1m 42s\tremaining: 3.38s\n",
      "968:\tlearn: 0.0814867\ttotal: 1m 42s\tremaining: 3.27s\n",
      "969:\tlearn: 0.0814561\ttotal: 1m 42s\tremaining: 3.16s\n",
      "970:\tlearn: 0.0814161\ttotal: 1m 42s\tremaining: 3.06s\n",
      "971:\tlearn: 0.0813838\ttotal: 1m 42s\tremaining: 2.95s\n",
      "972:\tlearn: 0.0813570\ttotal: 1m 42s\tremaining: 2.85s\n",
      "973:\tlearn: 0.0813233\ttotal: 1m 42s\tremaining: 2.74s\n",
      "974:\tlearn: 0.0812977\ttotal: 1m 42s\tremaining: 2.63s\n",
      "975:\tlearn: 0.0812669\ttotal: 1m 42s\tremaining: 2.53s\n",
      "976:\tlearn: 0.0812301\ttotal: 1m 42s\tremaining: 2.42s\n",
      "977:\tlearn: 0.0812177\ttotal: 1m 43s\tremaining: 2.32s\n",
      "978:\tlearn: 0.0812135\ttotal: 1m 43s\tremaining: 2.21s\n",
      "979:\tlearn: 0.0812043\ttotal: 1m 43s\tremaining: 2.11s\n",
      "980:\tlearn: 0.0811982\ttotal: 1m 43s\tremaining: 2s\n",
      "981:\tlearn: 0.0811786\ttotal: 1m 43s\tremaining: 1.9s\n",
      "982:\tlearn: 0.0811494\ttotal: 1m 43s\tremaining: 1.79s\n",
      "983:\tlearn: 0.0810933\ttotal: 1m 43s\tremaining: 1.68s\n",
      "984:\tlearn: 0.0810540\ttotal: 1m 43s\tremaining: 1.58s\n",
      "985:\tlearn: 0.0809934\ttotal: 1m 43s\tremaining: 1.47s\n",
      "986:\tlearn: 0.0809544\ttotal: 1m 43s\tremaining: 1.37s\n",
      "987:\tlearn: 0.0809314\ttotal: 1m 43s\tremaining: 1.26s\n",
      "988:\tlearn: 0.0809095\ttotal: 1m 44s\tremaining: 1.16s\n",
      "989:\tlearn: 0.0808853\ttotal: 1m 44s\tremaining: 1.05s\n",
      "990:\tlearn: 0.0808420\ttotal: 1m 44s\tremaining: 947ms\n",
      "991:\tlearn: 0.0808417\ttotal: 1m 44s\tremaining: 841ms\n",
      "992:\tlearn: 0.0808080\ttotal: 1m 44s\tremaining: 736ms\n",
      "993:\tlearn: 0.0807731\ttotal: 1m 44s\tremaining: 631ms\n",
      "994:\tlearn: 0.0807251\ttotal: 1m 44s\tremaining: 526ms\n",
      "995:\tlearn: 0.0806791\ttotal: 1m 44s\tremaining: 420ms\n",
      "996:\tlearn: 0.0806544\ttotal: 1m 44s\tremaining: 315ms\n",
      "997:\tlearn: 0.0806219\ttotal: 1m 44s\tremaining: 210ms\n",
      "998:\tlearn: 0.0806011\ttotal: 1m 44s\tremaining: 105ms\n",
      "999:\tlearn: 0.0805827\ttotal: 1m 45s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5293080\ttotal: 98.4ms\tremaining: 1m 38s\n",
      "1:\tlearn: 0.4226092\ttotal: 201ms\tremaining: 1m 40s\n",
      "2:\tlearn: 0.3612002\ttotal: 293ms\tremaining: 1m 37s\n",
      "3:\tlearn: 0.3216669\ttotal: 383ms\tremaining: 1m 35s\n",
      "4:\tlearn: 0.2950487\ttotal: 476ms\tremaining: 1m 34s\n",
      "5:\tlearn: 0.2777141\ttotal: 567ms\tremaining: 1m 34s\n",
      "6:\tlearn: 0.2619102\ttotal: 659ms\tremaining: 1m 33s\n",
      "7:\tlearn: 0.2515128\ttotal: 749ms\tremaining: 1m 32s\n",
      "8:\tlearn: 0.2357007\ttotal: 841ms\tremaining: 1m 32s\n",
      "9:\tlearn: 0.2275091\ttotal: 935ms\tremaining: 1m 32s\n",
      "10:\tlearn: 0.2214348\ttotal: 1.02s\tremaining: 1m 32s\n",
      "11:\tlearn: 0.2165170\ttotal: 1.12s\tremaining: 1m 32s\n",
      "12:\tlearn: 0.2126396\ttotal: 1.21s\tremaining: 1m 32s\n",
      "13:\tlearn: 0.2098227\ttotal: 1.32s\tremaining: 1m 32s\n",
      "14:\tlearn: 0.2062856\ttotal: 1.42s\tremaining: 1m 32s\n",
      "15:\tlearn: 0.2000575\ttotal: 1.51s\tremaining: 1m 32s\n",
      "16:\tlearn: 0.1960049\ttotal: 1.6s\tremaining: 1m 32s\n",
      "17:\tlearn: 0.1934664\ttotal: 1.69s\tremaining: 1m 32s\n",
      "18:\tlearn: 0.1911542\ttotal: 1.78s\tremaining: 1m 32s\n",
      "19:\tlearn: 0.1882423\ttotal: 1.87s\tremaining: 1m 31s\n",
      "20:\tlearn: 0.1866385\ttotal: 1.97s\tremaining: 1m 31s\n",
      "21:\tlearn: 0.1845704\ttotal: 2.06s\tremaining: 1m 31s\n",
      "22:\tlearn: 0.1832431\ttotal: 2.15s\tremaining: 1m 31s\n",
      "23:\tlearn: 0.1821054\ttotal: 2.25s\tremaining: 1m 31s\n",
      "24:\tlearn: 0.1802291\ttotal: 2.33s\tremaining: 1m 31s\n",
      "25:\tlearn: 0.1788500\ttotal: 2.43s\tremaining: 1m 31s\n",
      "26:\tlearn: 0.1771895\ttotal: 2.52s\tremaining: 1m 30s\n",
      "27:\tlearn: 0.1761279\ttotal: 2.61s\tremaining: 1m 30s\n",
      "28:\tlearn: 0.1744508\ttotal: 2.7s\tremaining: 1m 30s\n",
      "29:\tlearn: 0.1732665\ttotal: 2.8s\tremaining: 1m 30s\n",
      "30:\tlearn: 0.1719429\ttotal: 2.89s\tremaining: 1m 30s\n",
      "31:\tlearn: 0.1710706\ttotal: 2.98s\tremaining: 1m 30s\n",
      "32:\tlearn: 0.1693279\ttotal: 3.07s\tremaining: 1m 30s\n",
      "33:\tlearn: 0.1680570\ttotal: 3.17s\tremaining: 1m 29s\n",
      "34:\tlearn: 0.1669637\ttotal: 3.26s\tremaining: 1m 29s\n",
      "35:\tlearn: 0.1660921\ttotal: 3.35s\tremaining: 1m 29s\n",
      "36:\tlearn: 0.1651614\ttotal: 3.44s\tremaining: 1m 29s\n",
      "37:\tlearn: 0.1642093\ttotal: 3.54s\tremaining: 1m 29s\n",
      "38:\tlearn: 0.1633595\ttotal: 3.63s\tremaining: 1m 29s\n",
      "39:\tlearn: 0.1624492\ttotal: 3.72s\tremaining: 1m 29s\n",
      "40:\tlearn: 0.1617956\ttotal: 3.81s\tremaining: 1m 29s\n",
      "41:\tlearn: 0.1609815\ttotal: 3.9s\tremaining: 1m 29s\n",
      "42:\tlearn: 0.1601256\ttotal: 4s\tremaining: 1m 28s\n",
      "43:\tlearn: 0.1592379\ttotal: 4.09s\tremaining: 1m 28s\n",
      "44:\tlearn: 0.1586588\ttotal: 4.19s\tremaining: 1m 28s\n",
      "45:\tlearn: 0.1581984\ttotal: 4.28s\tremaining: 1m 28s\n",
      "46:\tlearn: 0.1577032\ttotal: 4.37s\tremaining: 1m 28s\n",
      "47:\tlearn: 0.1572804\ttotal: 4.47s\tremaining: 1m 28s\n",
      "48:\tlearn: 0.1566725\ttotal: 4.55s\tremaining: 1m 28s\n",
      "49:\tlearn: 0.1560189\ttotal: 4.65s\tremaining: 1m 28s\n",
      "50:\tlearn: 0.1554191\ttotal: 4.75s\tremaining: 1m 28s\n",
      "51:\tlearn: 0.1550057\ttotal: 4.84s\tremaining: 1m 28s\n",
      "52:\tlearn: 0.1544170\ttotal: 4.93s\tremaining: 1m 28s\n",
      "53:\tlearn: 0.1536813\ttotal: 5.02s\tremaining: 1m 27s\n",
      "54:\tlearn: 0.1531630\ttotal: 5.11s\tremaining: 1m 27s\n",
      "55:\tlearn: 0.1526945\ttotal: 5.21s\tremaining: 1m 27s\n",
      "56:\tlearn: 0.1522736\ttotal: 5.3s\tremaining: 1m 27s\n",
      "57:\tlearn: 0.1517863\ttotal: 5.39s\tremaining: 1m 27s\n",
      "58:\tlearn: 0.1514114\ttotal: 5.48s\tremaining: 1m 27s\n",
      "59:\tlearn: 0.1507929\ttotal: 5.58s\tremaining: 1m 27s\n",
      "60:\tlearn: 0.1503935\ttotal: 5.67s\tremaining: 1m 27s\n",
      "61:\tlearn: 0.1499971\ttotal: 5.77s\tremaining: 1m 27s\n",
      "62:\tlearn: 0.1494896\ttotal: 5.86s\tremaining: 1m 27s\n",
      "63:\tlearn: 0.1487996\ttotal: 5.95s\tremaining: 1m 27s\n",
      "64:\tlearn: 0.1483984\ttotal: 6.05s\tremaining: 1m 27s\n",
      "65:\tlearn: 0.1481213\ttotal: 6.14s\tremaining: 1m 26s\n",
      "66:\tlearn: 0.1478019\ttotal: 6.23s\tremaining: 1m 26s\n",
      "67:\tlearn: 0.1474684\ttotal: 6.32s\tremaining: 1m 26s\n",
      "68:\tlearn: 0.1471235\ttotal: 6.41s\tremaining: 1m 26s\n",
      "69:\tlearn: 0.1467792\ttotal: 6.51s\tremaining: 1m 26s\n",
      "70:\tlearn: 0.1463975\ttotal: 6.6s\tremaining: 1m 26s\n",
      "71:\tlearn: 0.1461927\ttotal: 6.69s\tremaining: 1m 26s\n",
      "72:\tlearn: 0.1457639\ttotal: 6.79s\tremaining: 1m 26s\n",
      "73:\tlearn: 0.1454365\ttotal: 6.88s\tremaining: 1m 26s\n",
      "74:\tlearn: 0.1450070\ttotal: 6.99s\tremaining: 1m 26s\n",
      "75:\tlearn: 0.1447017\ttotal: 7.08s\tremaining: 1m 26s\n",
      "76:\tlearn: 0.1443045\ttotal: 7.18s\tremaining: 1m 26s\n",
      "77:\tlearn: 0.1440663\ttotal: 7.27s\tremaining: 1m 25s\n",
      "78:\tlearn: 0.1438511\ttotal: 7.36s\tremaining: 1m 25s\n",
      "79:\tlearn: 0.1436094\ttotal: 7.46s\tremaining: 1m 25s\n",
      "80:\tlearn: 0.1433231\ttotal: 7.55s\tremaining: 1m 25s\n",
      "81:\tlearn: 0.1430424\ttotal: 7.64s\tremaining: 1m 25s\n",
      "82:\tlearn: 0.1427550\ttotal: 7.73s\tremaining: 1m 25s\n",
      "83:\tlearn: 0.1425547\ttotal: 7.82s\tremaining: 1m 25s\n",
      "84:\tlearn: 0.1423532\ttotal: 7.91s\tremaining: 1m 25s\n",
      "85:\tlearn: 0.1420926\ttotal: 8.01s\tremaining: 1m 25s\n",
      "86:\tlearn: 0.1418380\ttotal: 8.1s\tremaining: 1m 24s\n",
      "87:\tlearn: 0.1414727\ttotal: 8.19s\tremaining: 1m 24s\n",
      "88:\tlearn: 0.1412386\ttotal: 8.28s\tremaining: 1m 24s\n",
      "89:\tlearn: 0.1410639\ttotal: 8.37s\tremaining: 1m 24s\n",
      "90:\tlearn: 0.1408371\ttotal: 8.46s\tremaining: 1m 24s\n",
      "91:\tlearn: 0.1406135\ttotal: 8.56s\tremaining: 1m 24s\n",
      "92:\tlearn: 0.1402551\ttotal: 8.65s\tremaining: 1m 24s\n",
      "93:\tlearn: 0.1398866\ttotal: 8.74s\tremaining: 1m 24s\n",
      "94:\tlearn: 0.1397079\ttotal: 8.83s\tremaining: 1m 24s\n",
      "95:\tlearn: 0.1394619\ttotal: 8.92s\tremaining: 1m 24s\n",
      "96:\tlearn: 0.1391811\ttotal: 9.02s\tremaining: 1m 23s\n",
      "97:\tlearn: 0.1387832\ttotal: 9.11s\tremaining: 1m 23s\n",
      "98:\tlearn: 0.1385119\ttotal: 9.19s\tremaining: 1m 23s\n",
      "99:\tlearn: 0.1381249\ttotal: 9.29s\tremaining: 1m 23s\n",
      "100:\tlearn: 0.1378867\ttotal: 9.38s\tremaining: 1m 23s\n",
      "101:\tlearn: 0.1376255\ttotal: 9.47s\tremaining: 1m 23s\n",
      "102:\tlearn: 0.1373538\ttotal: 9.56s\tremaining: 1m 23s\n",
      "103:\tlearn: 0.1371233\ttotal: 9.65s\tremaining: 1m 23s\n",
      "104:\tlearn: 0.1369557\ttotal: 9.74s\tremaining: 1m 23s\n",
      "105:\tlearn: 0.1366956\ttotal: 9.83s\tremaining: 1m 22s\n",
      "106:\tlearn: 0.1364532\ttotal: 10s\tremaining: 1m 23s\n",
      "107:\tlearn: 0.1361191\ttotal: 10.1s\tremaining: 1m 23s\n",
      "108:\tlearn: 0.1359297\ttotal: 10.2s\tremaining: 1m 23s\n",
      "109:\tlearn: 0.1357129\ttotal: 10.3s\tremaining: 1m 23s\n",
      "110:\tlearn: 0.1355331\ttotal: 10.4s\tremaining: 1m 23s\n",
      "111:\tlearn: 0.1353467\ttotal: 10.5s\tremaining: 1m 23s\n",
      "112:\tlearn: 0.1351601\ttotal: 10.6s\tremaining: 1m 23s\n",
      "113:\tlearn: 0.1349837\ttotal: 10.7s\tremaining: 1m 23s\n",
      "114:\tlearn: 0.1348412\ttotal: 10.8s\tremaining: 1m 23s\n",
      "115:\tlearn: 0.1346663\ttotal: 10.9s\tremaining: 1m 23s\n",
      "116:\tlearn: 0.1343817\ttotal: 11s\tremaining: 1m 23s\n",
      "117:\tlearn: 0.1341648\ttotal: 11.1s\tremaining: 1m 23s\n",
      "118:\tlearn: 0.1339557\ttotal: 11.2s\tremaining: 1m 22s\n",
      "119:\tlearn: 0.1337692\ttotal: 11.3s\tremaining: 1m 22s\n",
      "120:\tlearn: 0.1335241\ttotal: 11.4s\tremaining: 1m 22s\n",
      "121:\tlearn: 0.1330884\ttotal: 11.5s\tremaining: 1m 22s\n",
      "122:\tlearn: 0.1328285\ttotal: 11.6s\tremaining: 1m 22s\n",
      "123:\tlearn: 0.1326496\ttotal: 11.7s\tremaining: 1m 22s\n",
      "124:\tlearn: 0.1324967\ttotal: 11.8s\tremaining: 1m 22s\n",
      "125:\tlearn: 0.1323372\ttotal: 11.9s\tremaining: 1m 22s\n",
      "126:\tlearn: 0.1321751\ttotal: 12s\tremaining: 1m 22s\n",
      "127:\tlearn: 0.1319166\ttotal: 12.1s\tremaining: 1m 22s\n",
      "128:\tlearn: 0.1317441\ttotal: 12.2s\tremaining: 1m 22s\n",
      "129:\tlearn: 0.1315653\ttotal: 12.3s\tremaining: 1m 22s\n",
      "130:\tlearn: 0.1313062\ttotal: 12.5s\tremaining: 1m 22s\n",
      "131:\tlearn: 0.1310611\ttotal: 12.6s\tremaining: 1m 22s\n",
      "132:\tlearn: 0.1308715\ttotal: 12.6s\tremaining: 1m 22s\n",
      "133:\tlearn: 0.1307184\ttotal: 12.7s\tremaining: 1m 22s\n",
      "134:\tlearn: 0.1306044\ttotal: 12.8s\tremaining: 1m 22s\n",
      "135:\tlearn: 0.1304553\ttotal: 12.9s\tremaining: 1m 22s\n",
      "136:\tlearn: 0.1303426\ttotal: 13s\tremaining: 1m 21s\n",
      "137:\tlearn: 0.1301554\ttotal: 13.1s\tremaining: 1m 21s\n",
      "138:\tlearn: 0.1300372\ttotal: 13.2s\tremaining: 1m 21s\n",
      "139:\tlearn: 0.1299172\ttotal: 13.3s\tremaining: 1m 21s\n",
      "140:\tlearn: 0.1297603\ttotal: 13.4s\tremaining: 1m 21s\n",
      "141:\tlearn: 0.1296765\ttotal: 13.5s\tremaining: 1m 21s\n",
      "142:\tlearn: 0.1295244\ttotal: 13.6s\tremaining: 1m 21s\n",
      "143:\tlearn: 0.1293954\ttotal: 13.7s\tremaining: 1m 21s\n",
      "144:\tlearn: 0.1292827\ttotal: 13.7s\tremaining: 1m 21s\n",
      "145:\tlearn: 0.1290705\ttotal: 13.8s\tremaining: 1m 20s\n",
      "146:\tlearn: 0.1288874\ttotal: 13.9s\tremaining: 1m 20s\n",
      "147:\tlearn: 0.1286961\ttotal: 14s\tremaining: 1m 20s\n",
      "148:\tlearn: 0.1284669\ttotal: 14.1s\tremaining: 1m 20s\n",
      "149:\tlearn: 0.1283067\ttotal: 14.2s\tremaining: 1m 20s\n",
      "150:\tlearn: 0.1281617\ttotal: 14.3s\tremaining: 1m 20s\n",
      "151:\tlearn: 0.1279691\ttotal: 14.4s\tremaining: 1m 20s\n",
      "152:\tlearn: 0.1278535\ttotal: 14.5s\tremaining: 1m 20s\n",
      "153:\tlearn: 0.1277367\ttotal: 14.6s\tremaining: 1m 20s\n",
      "154:\tlearn: 0.1275999\ttotal: 14.7s\tremaining: 1m 20s\n",
      "155:\tlearn: 0.1274381\ttotal: 14.8s\tremaining: 1m 19s\n",
      "156:\tlearn: 0.1273420\ttotal: 14.9s\tremaining: 1m 19s\n",
      "157:\tlearn: 0.1272095\ttotal: 15s\tremaining: 1m 19s\n",
      "158:\tlearn: 0.1270823\ttotal: 15.1s\tremaining: 1m 19s\n",
      "159:\tlearn: 0.1269454\ttotal: 15.2s\tremaining: 1m 19s\n",
      "160:\tlearn: 0.1267693\ttotal: 15.2s\tremaining: 1m 19s\n",
      "161:\tlearn: 0.1266322\ttotal: 15.3s\tremaining: 1m 19s\n",
      "162:\tlearn: 0.1264930\ttotal: 15.4s\tremaining: 1m 19s\n",
      "163:\tlearn: 0.1263851\ttotal: 15.5s\tremaining: 1m 19s\n",
      "164:\tlearn: 0.1263081\ttotal: 15.6s\tremaining: 1m 19s\n",
      "165:\tlearn: 0.1261634\ttotal: 15.7s\tremaining: 1m 18s\n",
      "166:\tlearn: 0.1260117\ttotal: 15.8s\tremaining: 1m 18s\n",
      "167:\tlearn: 0.1259086\ttotal: 15.9s\tremaining: 1m 18s\n",
      "168:\tlearn: 0.1257956\ttotal: 16s\tremaining: 1m 18s\n",
      "169:\tlearn: 0.1256874\ttotal: 16.1s\tremaining: 1m 18s\n",
      "170:\tlearn: 0.1255701\ttotal: 16.2s\tremaining: 1m 18s\n",
      "171:\tlearn: 0.1254256\ttotal: 16.3s\tremaining: 1m 18s\n",
      "172:\tlearn: 0.1253388\ttotal: 16.4s\tremaining: 1m 18s\n",
      "173:\tlearn: 0.1252366\ttotal: 16.5s\tremaining: 1m 18s\n",
      "174:\tlearn: 0.1251333\ttotal: 16.6s\tremaining: 1m 18s\n",
      "175:\tlearn: 0.1250125\ttotal: 16.7s\tremaining: 1m 17s\n",
      "176:\tlearn: 0.1248992\ttotal: 16.7s\tremaining: 1m 17s\n",
      "177:\tlearn: 0.1247949\ttotal: 16.8s\tremaining: 1m 17s\n",
      "178:\tlearn: 0.1247006\ttotal: 16.9s\tremaining: 1m 17s\n",
      "179:\tlearn: 0.1245899\ttotal: 17s\tremaining: 1m 17s\n",
      "180:\tlearn: 0.1244511\ttotal: 17.1s\tremaining: 1m 17s\n",
      "181:\tlearn: 0.1243299\ttotal: 17.2s\tremaining: 1m 17s\n",
      "182:\tlearn: 0.1242695\ttotal: 17.3s\tremaining: 1m 17s\n",
      "183:\tlearn: 0.1241739\ttotal: 17.4s\tremaining: 1m 17s\n",
      "184:\tlearn: 0.1239830\ttotal: 17.5s\tremaining: 1m 17s\n",
      "185:\tlearn: 0.1239137\ttotal: 17.6s\tremaining: 1m 17s\n",
      "186:\tlearn: 0.1238218\ttotal: 17.7s\tremaining: 1m 16s\n",
      "187:\tlearn: 0.1236566\ttotal: 17.8s\tremaining: 1m 16s\n",
      "188:\tlearn: 0.1235604\ttotal: 17.9s\tremaining: 1m 16s\n",
      "189:\tlearn: 0.1234640\ttotal: 18s\tremaining: 1m 16s\n",
      "190:\tlearn: 0.1233674\ttotal: 18.1s\tremaining: 1m 16s\n",
      "191:\tlearn: 0.1232854\ttotal: 18.2s\tremaining: 1m 16s\n",
      "192:\tlearn: 0.1231753\ttotal: 18.3s\tremaining: 1m 16s\n",
      "193:\tlearn: 0.1230735\ttotal: 18.4s\tremaining: 1m 16s\n",
      "194:\tlearn: 0.1229752\ttotal: 18.5s\tremaining: 1m 16s\n",
      "195:\tlearn: 0.1228621\ttotal: 18.5s\tremaining: 1m 16s\n",
      "196:\tlearn: 0.1227566\ttotal: 18.6s\tremaining: 1m 15s\n",
      "197:\tlearn: 0.1226199\ttotal: 18.7s\tremaining: 1m 15s\n",
      "198:\tlearn: 0.1225385\ttotal: 18.8s\tremaining: 1m 15s\n",
      "199:\tlearn: 0.1223389\ttotal: 18.9s\tremaining: 1m 15s\n",
      "200:\tlearn: 0.1222637\ttotal: 19s\tremaining: 1m 15s\n",
      "201:\tlearn: 0.1221273\ttotal: 19.1s\tremaining: 1m 15s\n",
      "202:\tlearn: 0.1220487\ttotal: 19.2s\tremaining: 1m 15s\n",
      "203:\tlearn: 0.1219758\ttotal: 19.3s\tremaining: 1m 15s\n",
      "204:\tlearn: 0.1218384\ttotal: 19.4s\tremaining: 1m 15s\n",
      "205:\tlearn: 0.1217301\ttotal: 19.5s\tremaining: 1m 15s\n",
      "206:\tlearn: 0.1216044\ttotal: 19.6s\tremaining: 1m 14s\n",
      "207:\tlearn: 0.1214759\ttotal: 19.7s\tremaining: 1m 14s\n",
      "208:\tlearn: 0.1214465\ttotal: 19.7s\tremaining: 1m 14s\n",
      "209:\tlearn: 0.1213383\ttotal: 19.8s\tremaining: 1m 14s\n",
      "210:\tlearn: 0.1212023\ttotal: 20s\tremaining: 1m 14s\n",
      "211:\tlearn: 0.1211154\ttotal: 20.1s\tremaining: 1m 14s\n",
      "212:\tlearn: 0.1210364\ttotal: 20.2s\tremaining: 1m 14s\n",
      "213:\tlearn: 0.1209798\ttotal: 20.3s\tremaining: 1m 14s\n",
      "214:\tlearn: 0.1208472\ttotal: 20.4s\tremaining: 1m 14s\n",
      "215:\tlearn: 0.1207502\ttotal: 20.5s\tremaining: 1m 14s\n",
      "216:\tlearn: 0.1206316\ttotal: 20.6s\tremaining: 1m 14s\n",
      "217:\tlearn: 0.1205820\ttotal: 20.7s\tremaining: 1m 14s\n",
      "218:\tlearn: 0.1205096\ttotal: 20.8s\tremaining: 1m 14s\n",
      "219:\tlearn: 0.1204042\ttotal: 20.9s\tremaining: 1m 13s\n",
      "220:\tlearn: 0.1202602\ttotal: 21s\tremaining: 1m 13s\n",
      "221:\tlearn: 0.1201899\ttotal: 21.1s\tremaining: 1m 13s\n",
      "222:\tlearn: 0.1201110\ttotal: 21.1s\tremaining: 1m 13s\n",
      "223:\tlearn: 0.1200338\ttotal: 21.2s\tremaining: 1m 13s\n",
      "224:\tlearn: 0.1199118\ttotal: 21.3s\tremaining: 1m 13s\n",
      "225:\tlearn: 0.1198088\ttotal: 21.4s\tremaining: 1m 13s\n",
      "226:\tlearn: 0.1197030\ttotal: 21.5s\tremaining: 1m 13s\n",
      "227:\tlearn: 0.1196269\ttotal: 21.6s\tremaining: 1m 13s\n",
      "228:\tlearn: 0.1195353\ttotal: 21.7s\tremaining: 1m 13s\n",
      "229:\tlearn: 0.1194716\ttotal: 21.8s\tremaining: 1m 12s\n",
      "230:\tlearn: 0.1192973\ttotal: 21.9s\tremaining: 1m 12s\n",
      "231:\tlearn: 0.1192011\ttotal: 22s\tremaining: 1m 12s\n",
      "232:\tlearn: 0.1190930\ttotal: 22.1s\tremaining: 1m 12s\n",
      "233:\tlearn: 0.1190215\ttotal: 22.2s\tremaining: 1m 12s\n",
      "234:\tlearn: 0.1189324\ttotal: 22.3s\tremaining: 1m 12s\n",
      "235:\tlearn: 0.1188551\ttotal: 22.4s\tremaining: 1m 12s\n",
      "236:\tlearn: 0.1187111\ttotal: 22.5s\tremaining: 1m 12s\n",
      "237:\tlearn: 0.1186098\ttotal: 22.6s\tremaining: 1m 12s\n",
      "238:\tlearn: 0.1184877\ttotal: 22.6s\tremaining: 1m 12s\n",
      "239:\tlearn: 0.1184049\ttotal: 22.7s\tremaining: 1m 12s\n",
      "240:\tlearn: 0.1183113\ttotal: 22.8s\tremaining: 1m 11s\n",
      "241:\tlearn: 0.1182756\ttotal: 22.9s\tremaining: 1m 11s\n",
      "242:\tlearn: 0.1181876\ttotal: 23s\tremaining: 1m 11s\n",
      "243:\tlearn: 0.1181325\ttotal: 23.1s\tremaining: 1m 11s\n",
      "244:\tlearn: 0.1180662\ttotal: 23.2s\tremaining: 1m 11s\n",
      "245:\tlearn: 0.1179074\ttotal: 23.3s\tremaining: 1m 11s\n",
      "246:\tlearn: 0.1177943\ttotal: 23.4s\tremaining: 1m 11s\n",
      "247:\tlearn: 0.1177028\ttotal: 23.5s\tremaining: 1m 11s\n",
      "248:\tlearn: 0.1176175\ttotal: 23.6s\tremaining: 1m 11s\n",
      "249:\tlearn: 0.1175514\ttotal: 23.7s\tremaining: 1m 10s\n",
      "250:\tlearn: 0.1174594\ttotal: 23.8s\tremaining: 1m 10s\n",
      "251:\tlearn: 0.1173926\ttotal: 23.9s\tremaining: 1m 10s\n",
      "252:\tlearn: 0.1173197\ttotal: 23.9s\tremaining: 1m 10s\n",
      "253:\tlearn: 0.1172508\ttotal: 24s\tremaining: 1m 10s\n",
      "254:\tlearn: 0.1171741\ttotal: 24.1s\tremaining: 1m 10s\n",
      "255:\tlearn: 0.1171083\ttotal: 24.2s\tremaining: 1m 10s\n",
      "256:\tlearn: 0.1170511\ttotal: 24.3s\tremaining: 1m 10s\n",
      "257:\tlearn: 0.1169564\ttotal: 24.4s\tremaining: 1m 10s\n",
      "258:\tlearn: 0.1168809\ttotal: 24.5s\tremaining: 1m 10s\n",
      "259:\tlearn: 0.1167987\ttotal: 24.6s\tremaining: 1m 9s\n",
      "260:\tlearn: 0.1166938\ttotal: 24.7s\tremaining: 1m 9s\n",
      "261:\tlearn: 0.1165997\ttotal: 24.8s\tremaining: 1m 9s\n",
      "262:\tlearn: 0.1165177\ttotal: 24.9s\tremaining: 1m 9s\n",
      "263:\tlearn: 0.1164810\ttotal: 25s\tremaining: 1m 9s\n",
      "264:\tlearn: 0.1163665\ttotal: 25.1s\tremaining: 1m 9s\n",
      "265:\tlearn: 0.1162853\ttotal: 25.2s\tremaining: 1m 9s\n",
      "266:\tlearn: 0.1162034\ttotal: 25.2s\tremaining: 1m 9s\n",
      "267:\tlearn: 0.1161514\ttotal: 25.4s\tremaining: 1m 9s\n",
      "268:\tlearn: 0.1160851\ttotal: 25.4s\tremaining: 1m 9s\n",
      "269:\tlearn: 0.1159893\ttotal: 25.5s\tremaining: 1m 9s\n",
      "270:\tlearn: 0.1159016\ttotal: 25.6s\tremaining: 1m 8s\n",
      "271:\tlearn: 0.1158227\ttotal: 25.7s\tremaining: 1m 8s\n",
      "272:\tlearn: 0.1157314\ttotal: 25.8s\tremaining: 1m 8s\n",
      "273:\tlearn: 0.1156703\ttotal: 25.9s\tremaining: 1m 8s\n",
      "274:\tlearn: 0.1156180\ttotal: 26s\tremaining: 1m 8s\n",
      "275:\tlearn: 0.1155099\ttotal: 26.1s\tremaining: 1m 8s\n",
      "276:\tlearn: 0.1154311\ttotal: 26.2s\tremaining: 1m 8s\n",
      "277:\tlearn: 0.1153263\ttotal: 26.3s\tremaining: 1m 8s\n",
      "278:\tlearn: 0.1152737\ttotal: 26.4s\tremaining: 1m 8s\n",
      "279:\tlearn: 0.1152080\ttotal: 26.5s\tremaining: 1m 8s\n",
      "280:\tlearn: 0.1151511\ttotal: 26.6s\tremaining: 1m 7s\n",
      "281:\tlearn: 0.1151016\ttotal: 26.7s\tremaining: 1m 7s\n",
      "282:\tlearn: 0.1150278\ttotal: 26.8s\tremaining: 1m 7s\n",
      "283:\tlearn: 0.1149628\ttotal: 26.9s\tremaining: 1m 7s\n",
      "284:\tlearn: 0.1149150\ttotal: 27s\tremaining: 1m 7s\n",
      "285:\tlearn: 0.1148546\ttotal: 27.1s\tremaining: 1m 7s\n",
      "286:\tlearn: 0.1147745\ttotal: 27.2s\tremaining: 1m 7s\n",
      "287:\tlearn: 0.1147125\ttotal: 27.3s\tremaining: 1m 7s\n",
      "288:\tlearn: 0.1146376\ttotal: 27.4s\tremaining: 1m 7s\n",
      "289:\tlearn: 0.1145708\ttotal: 27.5s\tremaining: 1m 7s\n",
      "290:\tlearn: 0.1145111\ttotal: 27.6s\tremaining: 1m 7s\n",
      "291:\tlearn: 0.1144546\ttotal: 27.7s\tremaining: 1m 7s\n",
      "292:\tlearn: 0.1144333\ttotal: 27.8s\tremaining: 1m 6s\n",
      "293:\tlearn: 0.1143392\ttotal: 27.9s\tremaining: 1m 6s\n",
      "294:\tlearn: 0.1142644\ttotal: 28s\tremaining: 1m 6s\n",
      "295:\tlearn: 0.1141265\ttotal: 28.1s\tremaining: 1m 6s\n",
      "296:\tlearn: 0.1140634\ttotal: 28.2s\tremaining: 1m 6s\n",
      "297:\tlearn: 0.1139845\ttotal: 28.3s\tremaining: 1m 6s\n",
      "298:\tlearn: 0.1139178\ttotal: 28.4s\tremaining: 1m 6s\n",
      "299:\tlearn: 0.1137370\ttotal: 28.5s\tremaining: 1m 6s\n",
      "300:\tlearn: 0.1136113\ttotal: 28.6s\tremaining: 1m 6s\n",
      "301:\tlearn: 0.1134880\ttotal: 28.7s\tremaining: 1m 6s\n",
      "302:\tlearn: 0.1133748\ttotal: 28.9s\tremaining: 1m 6s\n",
      "303:\tlearn: 0.1133330\ttotal: 29s\tremaining: 1m 6s\n",
      "304:\tlearn: 0.1132257\ttotal: 29.1s\tremaining: 1m 6s\n",
      "305:\tlearn: 0.1131578\ttotal: 29.2s\tremaining: 1m 6s\n",
      "306:\tlearn: 0.1130941\ttotal: 29.3s\tremaining: 1m 6s\n",
      "307:\tlearn: 0.1130344\ttotal: 29.4s\tremaining: 1m 6s\n",
      "308:\tlearn: 0.1129543\ttotal: 29.5s\tremaining: 1m 5s\n",
      "309:\tlearn: 0.1128784\ttotal: 29.6s\tremaining: 1m 5s\n",
      "310:\tlearn: 0.1128084\ttotal: 29.7s\tremaining: 1m 5s\n",
      "311:\tlearn: 0.1126932\ttotal: 29.8s\tremaining: 1m 5s\n",
      "312:\tlearn: 0.1126262\ttotal: 29.9s\tremaining: 1m 5s\n",
      "313:\tlearn: 0.1125723\ttotal: 30s\tremaining: 1m 5s\n",
      "314:\tlearn: 0.1125279\ttotal: 30.1s\tremaining: 1m 5s\n",
      "315:\tlearn: 0.1124599\ttotal: 30.2s\tremaining: 1m 5s\n",
      "316:\tlearn: 0.1124078\ttotal: 30.2s\tremaining: 1m 5s\n",
      "317:\tlearn: 0.1123591\ttotal: 30.3s\tremaining: 1m 5s\n",
      "318:\tlearn: 0.1122762\ttotal: 30.5s\tremaining: 1m 5s\n",
      "319:\tlearn: 0.1122053\ttotal: 30.6s\tremaining: 1m 5s\n",
      "320:\tlearn: 0.1121076\ttotal: 30.7s\tremaining: 1m 4s\n",
      "321:\tlearn: 0.1120445\ttotal: 30.8s\tremaining: 1m 4s\n",
      "322:\tlearn: 0.1120054\ttotal: 30.9s\tremaining: 1m 4s\n",
      "323:\tlearn: 0.1118892\ttotal: 31s\tremaining: 1m 4s\n",
      "324:\tlearn: 0.1118260\ttotal: 31.1s\tremaining: 1m 4s\n",
      "325:\tlearn: 0.1117205\ttotal: 31.2s\tremaining: 1m 4s\n",
      "326:\tlearn: 0.1116737\ttotal: 31.3s\tremaining: 1m 4s\n",
      "327:\tlearn: 0.1116085\ttotal: 31.4s\tremaining: 1m 4s\n",
      "328:\tlearn: 0.1115578\ttotal: 31.4s\tremaining: 1m 4s\n",
      "329:\tlearn: 0.1115093\ttotal: 31.5s\tremaining: 1m 4s\n",
      "330:\tlearn: 0.1114633\ttotal: 31.6s\tremaining: 1m 3s\n",
      "331:\tlearn: 0.1113739\ttotal: 31.7s\tremaining: 1m 3s\n",
      "332:\tlearn: 0.1113105\ttotal: 31.8s\tremaining: 1m 3s\n",
      "333:\tlearn: 0.1112233\ttotal: 31.9s\tremaining: 1m 3s\n",
      "334:\tlearn: 0.1111515\ttotal: 32s\tremaining: 1m 3s\n",
      "335:\tlearn: 0.1110813\ttotal: 32.1s\tremaining: 1m 3s\n",
      "336:\tlearn: 0.1110320\ttotal: 32.2s\tremaining: 1m 3s\n",
      "337:\tlearn: 0.1109610\ttotal: 32.3s\tremaining: 1m 3s\n",
      "338:\tlearn: 0.1108938\ttotal: 32.4s\tremaining: 1m 3s\n",
      "339:\tlearn: 0.1108286\ttotal: 32.5s\tremaining: 1m 3s\n",
      "340:\tlearn: 0.1107233\ttotal: 32.6s\tremaining: 1m 3s\n",
      "341:\tlearn: 0.1106457\ttotal: 32.7s\tremaining: 1m 2s\n",
      "342:\tlearn: 0.1105966\ttotal: 32.8s\tremaining: 1m 2s\n",
      "343:\tlearn: 0.1105263\ttotal: 32.9s\tremaining: 1m 2s\n",
      "344:\tlearn: 0.1104527\ttotal: 33s\tremaining: 1m 2s\n",
      "345:\tlearn: 0.1103891\ttotal: 33.1s\tremaining: 1m 2s\n",
      "346:\tlearn: 0.1103163\ttotal: 33.2s\tremaining: 1m 2s\n",
      "347:\tlearn: 0.1102808\ttotal: 33.3s\tremaining: 1m 2s\n",
      "348:\tlearn: 0.1102423\ttotal: 33.4s\tremaining: 1m 2s\n",
      "349:\tlearn: 0.1101703\ttotal: 33.5s\tremaining: 1m 2s\n",
      "350:\tlearn: 0.1101254\ttotal: 33.7s\tremaining: 1m 2s\n",
      "351:\tlearn: 0.1100886\ttotal: 33.8s\tremaining: 1m 2s\n",
      "352:\tlearn: 0.1100420\ttotal: 33.9s\tremaining: 1m 2s\n",
      "353:\tlearn: 0.1099837\ttotal: 34s\tremaining: 1m 1s\n",
      "354:\tlearn: 0.1099467\ttotal: 34.1s\tremaining: 1m 1s\n",
      "355:\tlearn: 0.1098952\ttotal: 34.1s\tremaining: 1m 1s\n",
      "356:\tlearn: 0.1098048\ttotal: 34.2s\tremaining: 1m 1s\n",
      "357:\tlearn: 0.1097712\ttotal: 34.3s\tremaining: 1m 1s\n",
      "358:\tlearn: 0.1097309\ttotal: 34.4s\tremaining: 1m 1s\n",
      "359:\tlearn: 0.1096630\ttotal: 34.5s\tremaining: 1m 1s\n",
      "360:\tlearn: 0.1096172\ttotal: 34.6s\tremaining: 1m 1s\n",
      "361:\tlearn: 0.1095028\ttotal: 34.7s\tremaining: 1m 1s\n",
      "362:\tlearn: 0.1094185\ttotal: 34.8s\tremaining: 1m 1s\n",
      "363:\tlearn: 0.1093354\ttotal: 34.9s\tremaining: 1m 1s\n",
      "364:\tlearn: 0.1092819\ttotal: 35s\tremaining: 1m\n",
      "365:\tlearn: 0.1092520\ttotal: 35.1s\tremaining: 1m\n",
      "366:\tlearn: 0.1091941\ttotal: 35.2s\tremaining: 1m\n",
      "367:\tlearn: 0.1090997\ttotal: 35.3s\tremaining: 1m\n",
      "368:\tlearn: 0.1090030\ttotal: 35.4s\tremaining: 1m\n",
      "369:\tlearn: 0.1089431\ttotal: 35.5s\tremaining: 1m\n",
      "370:\tlearn: 0.1088689\ttotal: 35.6s\tremaining: 1m\n",
      "371:\tlearn: 0.1088227\ttotal: 35.7s\tremaining: 1m\n",
      "372:\tlearn: 0.1087925\ttotal: 35.8s\tremaining: 1m\n",
      "373:\tlearn: 0.1087366\ttotal: 35.9s\tremaining: 1m\n",
      "374:\tlearn: 0.1086408\ttotal: 36s\tremaining: 1m\n",
      "375:\tlearn: 0.1085843\ttotal: 36.1s\tremaining: 60s\n",
      "376:\tlearn: 0.1085542\ttotal: 36.3s\tremaining: 59.9s\n",
      "377:\tlearn: 0.1085018\ttotal: 36.4s\tremaining: 59.8s\n",
      "378:\tlearn: 0.1084417\ttotal: 36.5s\tremaining: 59.7s\n",
      "379:\tlearn: 0.1084013\ttotal: 36.6s\tremaining: 59.6s\n",
      "380:\tlearn: 0.1083488\ttotal: 36.7s\tremaining: 59.6s\n",
      "381:\tlearn: 0.1082773\ttotal: 36.8s\tremaining: 59.5s\n",
      "382:\tlearn: 0.1082036\ttotal: 36.9s\tremaining: 59.4s\n",
      "383:\tlearn: 0.1081227\ttotal: 37s\tremaining: 59.3s\n",
      "384:\tlearn: 0.1080644\ttotal: 37.1s\tremaining: 59.2s\n",
      "385:\tlearn: 0.1079755\ttotal: 37.2s\tremaining: 59.1s\n",
      "386:\tlearn: 0.1079134\ttotal: 37.3s\tremaining: 59s\n",
      "387:\tlearn: 0.1078013\ttotal: 37.4s\tremaining: 59s\n",
      "388:\tlearn: 0.1077567\ttotal: 37.5s\tremaining: 58.9s\n",
      "389:\tlearn: 0.1077248\ttotal: 37.6s\tremaining: 58.8s\n",
      "390:\tlearn: 0.1076103\ttotal: 37.7s\tremaining: 58.7s\n",
      "391:\tlearn: 0.1075638\ttotal: 37.8s\tremaining: 58.6s\n",
      "392:\tlearn: 0.1075302\ttotal: 37.9s\tremaining: 58.5s\n",
      "393:\tlearn: 0.1074846\ttotal: 38s\tremaining: 58.4s\n",
      "394:\tlearn: 0.1074461\ttotal: 38.1s\tremaining: 58.4s\n",
      "395:\tlearn: 0.1073669\ttotal: 38.2s\tremaining: 58.3s\n",
      "396:\tlearn: 0.1073201\ttotal: 38.4s\tremaining: 58.4s\n",
      "397:\tlearn: 0.1072879\ttotal: 38.5s\tremaining: 58.3s\n",
      "398:\tlearn: 0.1071965\ttotal: 38.6s\tremaining: 58.2s\n",
      "399:\tlearn: 0.1071592\ttotal: 38.8s\tremaining: 58.2s\n",
      "400:\tlearn: 0.1071179\ttotal: 38.9s\tremaining: 58.1s\n",
      "401:\tlearn: 0.1070539\ttotal: 39s\tremaining: 58s\n",
      "402:\tlearn: 0.1069601\ttotal: 39.1s\tremaining: 57.9s\n",
      "403:\tlearn: 0.1068853\ttotal: 39.2s\tremaining: 57.8s\n",
      "404:\tlearn: 0.1068303\ttotal: 39.3s\tremaining: 57.7s\n",
      "405:\tlearn: 0.1067474\ttotal: 39.4s\tremaining: 57.6s\n",
      "406:\tlearn: 0.1066801\ttotal: 39.5s\tremaining: 57.5s\n",
      "407:\tlearn: 0.1066423\ttotal: 39.6s\tremaining: 57.4s\n",
      "408:\tlearn: 0.1066235\ttotal: 39.7s\tremaining: 57.3s\n",
      "409:\tlearn: 0.1065066\ttotal: 39.8s\tremaining: 57.2s\n",
      "410:\tlearn: 0.1064913\ttotal: 39.9s\tremaining: 57.1s\n",
      "411:\tlearn: 0.1064562\ttotal: 40s\tremaining: 57s\n",
      "412:\tlearn: 0.1064314\ttotal: 40s\tremaining: 56.9s\n",
      "413:\tlearn: 0.1063857\ttotal: 40.1s\tremaining: 56.8s\n",
      "414:\tlearn: 0.1063456\ttotal: 40.2s\tremaining: 56.7s\n",
      "415:\tlearn: 0.1062877\ttotal: 40.3s\tremaining: 56.6s\n",
      "416:\tlearn: 0.1062467\ttotal: 40.4s\tremaining: 56.5s\n",
      "417:\tlearn: 0.1061965\ttotal: 40.5s\tremaining: 56.4s\n",
      "418:\tlearn: 0.1061784\ttotal: 40.6s\tremaining: 56.3s\n",
      "419:\tlearn: 0.1060951\ttotal: 40.7s\tremaining: 56.2s\n",
      "420:\tlearn: 0.1060324\ttotal: 40.8s\tremaining: 56.1s\n",
      "421:\tlearn: 0.1059398\ttotal: 40.9s\tremaining: 56s\n",
      "422:\tlearn: 0.1058560\ttotal: 41s\tremaining: 55.9s\n",
      "423:\tlearn: 0.1058071\ttotal: 41.1s\tremaining: 55.8s\n",
      "424:\tlearn: 0.1057371\ttotal: 41.2s\tremaining: 55.7s\n",
      "425:\tlearn: 0.1056639\ttotal: 41.3s\tremaining: 55.6s\n",
      "426:\tlearn: 0.1055931\ttotal: 41.4s\tremaining: 55.6s\n",
      "427:\tlearn: 0.1055291\ttotal: 41.5s\tremaining: 55.5s\n",
      "428:\tlearn: 0.1054750\ttotal: 41.6s\tremaining: 55.4s\n",
      "429:\tlearn: 0.1053635\ttotal: 41.7s\tremaining: 55.3s\n",
      "430:\tlearn: 0.1053158\ttotal: 41.8s\tremaining: 55.2s\n",
      "431:\tlearn: 0.1052378\ttotal: 41.9s\tremaining: 55.1s\n",
      "432:\tlearn: 0.1051864\ttotal: 42s\tremaining: 55s\n",
      "433:\tlearn: 0.1051508\ttotal: 42.1s\tremaining: 54.9s\n",
      "434:\tlearn: 0.1051010\ttotal: 42.2s\tremaining: 54.8s\n",
      "435:\tlearn: 0.1050489\ttotal: 42.3s\tremaining: 54.7s\n",
      "436:\tlearn: 0.1049741\ttotal: 42.4s\tremaining: 54.6s\n",
      "437:\tlearn: 0.1049191\ttotal: 42.5s\tremaining: 54.5s\n",
      "438:\tlearn: 0.1048927\ttotal: 42.6s\tremaining: 54.4s\n",
      "439:\tlearn: 0.1048550\ttotal: 42.7s\tremaining: 54.4s\n",
      "440:\tlearn: 0.1047888\ttotal: 42.8s\tremaining: 54.3s\n",
      "441:\tlearn: 0.1047504\ttotal: 42.9s\tremaining: 54.2s\n",
      "442:\tlearn: 0.1046754\ttotal: 43s\tremaining: 54.1s\n",
      "443:\tlearn: 0.1045911\ttotal: 43.1s\tremaining: 54s\n",
      "444:\tlearn: 0.1045551\ttotal: 43.2s\tremaining: 53.9s\n",
      "445:\tlearn: 0.1044934\ttotal: 43.3s\tremaining: 53.8s\n",
      "446:\tlearn: 0.1044475\ttotal: 43.4s\tremaining: 53.7s\n",
      "447:\tlearn: 0.1043463\ttotal: 43.5s\tremaining: 53.6s\n",
      "448:\tlearn: 0.1042640\ttotal: 43.6s\tremaining: 53.5s\n",
      "449:\tlearn: 0.1042148\ttotal: 43.7s\tremaining: 53.5s\n",
      "450:\tlearn: 0.1041485\ttotal: 43.9s\tremaining: 53.4s\n",
      "451:\tlearn: 0.1040685\ttotal: 44s\tremaining: 53.3s\n",
      "452:\tlearn: 0.1040208\ttotal: 44.1s\tremaining: 53.2s\n",
      "453:\tlearn: 0.1039811\ttotal: 44.2s\tremaining: 53.1s\n",
      "454:\tlearn: 0.1039018\ttotal: 44.3s\tremaining: 53s\n",
      "455:\tlearn: 0.1038572\ttotal: 44.4s\tremaining: 52.9s\n",
      "456:\tlearn: 0.1038081\ttotal: 44.5s\tremaining: 52.8s\n",
      "457:\tlearn: 0.1037531\ttotal: 44.6s\tremaining: 52.7s\n",
      "458:\tlearn: 0.1037260\ttotal: 44.7s\tremaining: 52.6s\n",
      "459:\tlearn: 0.1036971\ttotal: 44.8s\tremaining: 52.5s\n",
      "460:\tlearn: 0.1036528\ttotal: 44.8s\tremaining: 52.4s\n",
      "461:\tlearn: 0.1036320\ttotal: 44.9s\tremaining: 52.3s\n",
      "462:\tlearn: 0.1035635\ttotal: 45s\tremaining: 52.2s\n",
      "463:\tlearn: 0.1035107\ttotal: 45.1s\tremaining: 52.1s\n",
      "464:\tlearn: 0.1034669\ttotal: 45.2s\tremaining: 52s\n",
      "465:\tlearn: 0.1034280\ttotal: 45.3s\tremaining: 51.9s\n",
      "466:\tlearn: 0.1033663\ttotal: 45.4s\tremaining: 51.8s\n",
      "467:\tlearn: 0.1033017\ttotal: 45.5s\tremaining: 51.7s\n",
      "468:\tlearn: 0.1032614\ttotal: 45.6s\tremaining: 51.6s\n",
      "469:\tlearn: 0.1032239\ttotal: 45.7s\tremaining: 51.5s\n",
      "470:\tlearn: 0.1031724\ttotal: 45.8s\tremaining: 51.4s\n",
      "471:\tlearn: 0.1030746\ttotal: 45.9s\tremaining: 51.3s\n",
      "472:\tlearn: 0.1030428\ttotal: 46s\tremaining: 51.2s\n",
      "473:\tlearn: 0.1029877\ttotal: 46.1s\tremaining: 51.1s\n",
      "474:\tlearn: 0.1029318\ttotal: 46.2s\tremaining: 51s\n",
      "475:\tlearn: 0.1028847\ttotal: 46.3s\tremaining: 50.9s\n",
      "476:\tlearn: 0.1028298\ttotal: 46.4s\tremaining: 50.8s\n",
      "477:\tlearn: 0.1027916\ttotal: 46.5s\tremaining: 50.8s\n",
      "478:\tlearn: 0.1027150\ttotal: 46.6s\tremaining: 50.7s\n",
      "479:\tlearn: 0.1026756\ttotal: 46.7s\tremaining: 50.6s\n",
      "480:\tlearn: 0.1025992\ttotal: 46.8s\tremaining: 50.5s\n",
      "481:\tlearn: 0.1025550\ttotal: 46.9s\tremaining: 50.4s\n",
      "482:\tlearn: 0.1025212\ttotal: 47s\tremaining: 50.3s\n",
      "483:\tlearn: 0.1024716\ttotal: 47.1s\tremaining: 50.2s\n",
      "484:\tlearn: 0.1024474\ttotal: 47.2s\tremaining: 50.1s\n",
      "485:\tlearn: 0.1023890\ttotal: 47.3s\tremaining: 50s\n",
      "486:\tlearn: 0.1023603\ttotal: 47.4s\tremaining: 49.9s\n",
      "487:\tlearn: 0.1023297\ttotal: 47.5s\tremaining: 49.8s\n",
      "488:\tlearn: 0.1022846\ttotal: 47.6s\tremaining: 49.7s\n",
      "489:\tlearn: 0.1022208\ttotal: 47.7s\tremaining: 49.6s\n",
      "490:\tlearn: 0.1021503\ttotal: 47.8s\tremaining: 49.5s\n",
      "491:\tlearn: 0.1021112\ttotal: 48s\tremaining: 49.5s\n",
      "492:\tlearn: 0.1020492\ttotal: 48.1s\tremaining: 49.4s\n",
      "493:\tlearn: 0.1019931\ttotal: 48.2s\tremaining: 49.3s\n",
      "494:\tlearn: 0.1019424\ttotal: 48.3s\tremaining: 49.2s\n",
      "495:\tlearn: 0.1019062\ttotal: 48.4s\tremaining: 49.1s\n",
      "496:\tlearn: 0.1018349\ttotal: 48.5s\tremaining: 49s\n",
      "497:\tlearn: 0.1017839\ttotal: 48.6s\tremaining: 48.9s\n",
      "498:\tlearn: 0.1017580\ttotal: 48.6s\tremaining: 48.8s\n",
      "499:\tlearn: 0.1016973\ttotal: 48.7s\tremaining: 48.7s\n",
      "500:\tlearn: 0.1016730\ttotal: 48.8s\tremaining: 48.6s\n",
      "501:\tlearn: 0.1016296\ttotal: 48.9s\tremaining: 48.5s\n",
      "502:\tlearn: 0.1015374\ttotal: 49s\tremaining: 48.4s\n",
      "503:\tlearn: 0.1014544\ttotal: 49.1s\tremaining: 48.3s\n",
      "504:\tlearn: 0.1013913\ttotal: 49.2s\tremaining: 48.2s\n",
      "505:\tlearn: 0.1012892\ttotal: 49.3s\tremaining: 48.1s\n",
      "506:\tlearn: 0.1012481\ttotal: 49.4s\tremaining: 48s\n",
      "507:\tlearn: 0.1012192\ttotal: 49.5s\tremaining: 47.9s\n",
      "508:\tlearn: 0.1011686\ttotal: 49.6s\tremaining: 47.8s\n",
      "509:\tlearn: 0.1011408\ttotal: 49.7s\tremaining: 47.7s\n",
      "510:\tlearn: 0.1010682\ttotal: 49.8s\tremaining: 47.6s\n",
      "511:\tlearn: 0.1010534\ttotal: 49.9s\tremaining: 47.6s\n",
      "512:\tlearn: 0.1010077\ttotal: 50s\tremaining: 47.5s\n",
      "513:\tlearn: 0.1009761\ttotal: 50.1s\tremaining: 47.4s\n",
      "514:\tlearn: 0.1009368\ttotal: 50.2s\tremaining: 47.3s\n",
      "515:\tlearn: 0.1009237\ttotal: 50.3s\tremaining: 47.2s\n",
      "516:\tlearn: 0.1008866\ttotal: 50.4s\tremaining: 47.1s\n",
      "517:\tlearn: 0.1008028\ttotal: 50.5s\tremaining: 47s\n",
      "518:\tlearn: 0.1007303\ttotal: 50.6s\tremaining: 46.9s\n",
      "519:\tlearn: 0.1007040\ttotal: 50.7s\tremaining: 46.8s\n",
      "520:\tlearn: 0.1006621\ttotal: 50.8s\tremaining: 46.7s\n",
      "521:\tlearn: 0.1006128\ttotal: 50.9s\tremaining: 46.6s\n",
      "522:\tlearn: 0.1005540\ttotal: 50.9s\tremaining: 46.5s\n",
      "523:\tlearn: 0.1005105\ttotal: 51s\tremaining: 46.4s\n",
      "524:\tlearn: 0.1004374\ttotal: 51.1s\tremaining: 46.3s\n",
      "525:\tlearn: 0.1004318\ttotal: 51.2s\tremaining: 46.2s\n",
      "526:\tlearn: 0.1003750\ttotal: 51.3s\tremaining: 46.1s\n",
      "527:\tlearn: 0.1003359\ttotal: 51.4s\tremaining: 46s\n",
      "528:\tlearn: 0.1002816\ttotal: 51.5s\tremaining: 45.9s\n",
      "529:\tlearn: 0.1002137\ttotal: 51.6s\tremaining: 45.8s\n",
      "530:\tlearn: 0.1001839\ttotal: 51.7s\tremaining: 45.7s\n",
      "531:\tlearn: 0.1001427\ttotal: 51.8s\tremaining: 45.6s\n",
      "532:\tlearn: 0.1000927\ttotal: 51.9s\tremaining: 45.5s\n",
      "533:\tlearn: 0.0999905\ttotal: 52s\tremaining: 45.4s\n",
      "534:\tlearn: 0.0999561\ttotal: 52.1s\tremaining: 45.3s\n",
      "535:\tlearn: 0.0999040\ttotal: 52.2s\tremaining: 45.2s\n",
      "536:\tlearn: 0.0998768\ttotal: 52.3s\tremaining: 45.1s\n",
      "537:\tlearn: 0.0998195\ttotal: 52.4s\tremaining: 45s\n",
      "538:\tlearn: 0.0997722\ttotal: 52.5s\tremaining: 44.9s\n",
      "539:\tlearn: 0.0997335\ttotal: 52.6s\tremaining: 44.8s\n",
      "540:\tlearn: 0.0997046\ttotal: 52.7s\tremaining: 44.7s\n",
      "541:\tlearn: 0.0996783\ttotal: 52.8s\tremaining: 44.6s\n",
      "542:\tlearn: 0.0996293\ttotal: 52.9s\tremaining: 44.5s\n",
      "543:\tlearn: 0.0995853\ttotal: 53s\tremaining: 44.4s\n",
      "544:\tlearn: 0.0995386\ttotal: 53.1s\tremaining: 44.3s\n",
      "545:\tlearn: 0.0995058\ttotal: 53.2s\tremaining: 44.2s\n",
      "546:\tlearn: 0.0994871\ttotal: 53.3s\tremaining: 44.1s\n",
      "547:\tlearn: 0.0994099\ttotal: 53.4s\tremaining: 44s\n",
      "548:\tlearn: 0.0993441\ttotal: 53.5s\tremaining: 43.9s\n",
      "549:\tlearn: 0.0993177\ttotal: 53.6s\tremaining: 43.8s\n",
      "550:\tlearn: 0.0993028\ttotal: 53.7s\tremaining: 43.7s\n",
      "551:\tlearn: 0.0992690\ttotal: 53.7s\tremaining: 43.6s\n",
      "552:\tlearn: 0.0992325\ttotal: 53.8s\tremaining: 43.5s\n",
      "553:\tlearn: 0.0991663\ttotal: 53.9s\tremaining: 43.4s\n",
      "554:\tlearn: 0.0991475\ttotal: 54s\tremaining: 43.3s\n",
      "555:\tlearn: 0.0990822\ttotal: 54.1s\tremaining: 43.2s\n",
      "556:\tlearn: 0.0990461\ttotal: 54.2s\tremaining: 43.1s\n",
      "557:\tlearn: 0.0989893\ttotal: 54.3s\tremaining: 43s\n",
      "558:\tlearn: 0.0989361\ttotal: 54.4s\tremaining: 42.9s\n",
      "559:\tlearn: 0.0989028\ttotal: 54.5s\tremaining: 42.8s\n",
      "560:\tlearn: 0.0988305\ttotal: 54.6s\tremaining: 42.7s\n",
      "561:\tlearn: 0.0988035\ttotal: 54.7s\tremaining: 42.6s\n",
      "562:\tlearn: 0.0987792\ttotal: 54.8s\tremaining: 42.5s\n",
      "563:\tlearn: 0.0987621\ttotal: 54.9s\tremaining: 42.4s\n",
      "564:\tlearn: 0.0987517\ttotal: 55s\tremaining: 42.3s\n",
      "565:\tlearn: 0.0987091\ttotal: 55.1s\tremaining: 42.2s\n",
      "566:\tlearn: 0.0986804\ttotal: 55.2s\tremaining: 42.1s\n",
      "567:\tlearn: 0.0986537\ttotal: 55.2s\tremaining: 42s\n",
      "568:\tlearn: 0.0986251\ttotal: 55.3s\tremaining: 41.9s\n",
      "569:\tlearn: 0.0985748\ttotal: 55.4s\tremaining: 41.8s\n",
      "570:\tlearn: 0.0985562\ttotal: 55.5s\tremaining: 41.7s\n",
      "571:\tlearn: 0.0984506\ttotal: 55.7s\tremaining: 41.6s\n",
      "572:\tlearn: 0.0984172\ttotal: 55.7s\tremaining: 41.5s\n",
      "573:\tlearn: 0.0983520\ttotal: 55.8s\tremaining: 41.4s\n",
      "574:\tlearn: 0.0983206\ttotal: 55.9s\tremaining: 41.3s\n",
      "575:\tlearn: 0.0982872\ttotal: 56s\tremaining: 41.2s\n",
      "576:\tlearn: 0.0982513\ttotal: 56.1s\tremaining: 41.1s\n",
      "577:\tlearn: 0.0981712\ttotal: 56.2s\tremaining: 41s\n",
      "578:\tlearn: 0.0981257\ttotal: 56.3s\tremaining: 41s\n",
      "579:\tlearn: 0.0981121\ttotal: 56.4s\tremaining: 40.9s\n",
      "580:\tlearn: 0.0980585\ttotal: 56.5s\tremaining: 40.8s\n",
      "581:\tlearn: 0.0980263\ttotal: 56.6s\tremaining: 40.7s\n",
      "582:\tlearn: 0.0979503\ttotal: 56.7s\tremaining: 40.6s\n",
      "583:\tlearn: 0.0979414\ttotal: 56.8s\tremaining: 40.5s\n",
      "584:\tlearn: 0.0978379\ttotal: 56.9s\tremaining: 40.4s\n",
      "585:\tlearn: 0.0977656\ttotal: 57s\tremaining: 40.3s\n",
      "586:\tlearn: 0.0977241\ttotal: 57.1s\tremaining: 40.2s\n",
      "587:\tlearn: 0.0976563\ttotal: 57.2s\tremaining: 40.1s\n",
      "588:\tlearn: 0.0975434\ttotal: 57.4s\tremaining: 40s\n",
      "589:\tlearn: 0.0974875\ttotal: 57.5s\tremaining: 39.9s\n",
      "590:\tlearn: 0.0974629\ttotal: 57.6s\tremaining: 39.9s\n",
      "591:\tlearn: 0.0974390\ttotal: 57.8s\tremaining: 39.8s\n",
      "592:\tlearn: 0.0973693\ttotal: 57.9s\tremaining: 39.7s\n",
      "593:\tlearn: 0.0973420\ttotal: 58s\tremaining: 39.7s\n",
      "594:\tlearn: 0.0972974\ttotal: 58.1s\tremaining: 39.6s\n",
      "595:\tlearn: 0.0972551\ttotal: 58.2s\tremaining: 39.5s\n",
      "596:\tlearn: 0.0972309\ttotal: 58.4s\tremaining: 39.4s\n",
      "597:\tlearn: 0.0971103\ttotal: 58.5s\tremaining: 39.3s\n",
      "598:\tlearn: 0.0970680\ttotal: 58.6s\tremaining: 39.2s\n",
      "599:\tlearn: 0.0970216\ttotal: 58.7s\tremaining: 39.1s\n",
      "600:\tlearn: 0.0969742\ttotal: 58.8s\tremaining: 39s\n",
      "601:\tlearn: 0.0969316\ttotal: 58.9s\tremaining: 39s\n",
      "602:\tlearn: 0.0968708\ttotal: 59s\tremaining: 38.9s\n",
      "603:\tlearn: 0.0968278\ttotal: 59.1s\tremaining: 38.8s\n",
      "604:\tlearn: 0.0967694\ttotal: 59.2s\tremaining: 38.7s\n",
      "605:\tlearn: 0.0966912\ttotal: 59.3s\tremaining: 38.6s\n",
      "606:\tlearn: 0.0966511\ttotal: 59.5s\tremaining: 38.5s\n",
      "607:\tlearn: 0.0965963\ttotal: 59.6s\tremaining: 38.4s\n",
      "608:\tlearn: 0.0965827\ttotal: 59.7s\tremaining: 38.3s\n",
      "609:\tlearn: 0.0965159\ttotal: 59.8s\tremaining: 38.2s\n",
      "610:\tlearn: 0.0965085\ttotal: 59.9s\tremaining: 38.1s\n",
      "611:\tlearn: 0.0964577\ttotal: 1m\tremaining: 38.1s\n",
      "612:\tlearn: 0.0963905\ttotal: 1m\tremaining: 38s\n",
      "613:\tlearn: 0.0963673\ttotal: 1m\tremaining: 37.9s\n",
      "614:\tlearn: 0.0962877\ttotal: 1m\tremaining: 37.8s\n",
      "615:\tlearn: 0.0961573\ttotal: 1m\tremaining: 37.7s\n",
      "616:\tlearn: 0.0961160\ttotal: 1m\tremaining: 37.6s\n",
      "617:\tlearn: 0.0960643\ttotal: 1m\tremaining: 37.5s\n",
      "618:\tlearn: 0.0960372\ttotal: 1m\tremaining: 37.4s\n",
      "619:\tlearn: 0.0960212\ttotal: 1m\tremaining: 37.3s\n",
      "620:\tlearn: 0.0959451\ttotal: 1m 1s\tremaining: 37.2s\n",
      "621:\tlearn: 0.0959269\ttotal: 1m 1s\tremaining: 37.1s\n",
      "622:\tlearn: 0.0958899\ttotal: 1m 1s\tremaining: 37s\n",
      "623:\tlearn: 0.0958619\ttotal: 1m 1s\tremaining: 36.9s\n",
      "624:\tlearn: 0.0958235\ttotal: 1m 1s\tremaining: 36.9s\n",
      "625:\tlearn: 0.0957403\ttotal: 1m 1s\tremaining: 36.8s\n",
      "626:\tlearn: 0.0957176\ttotal: 1m 1s\tremaining: 36.7s\n",
      "627:\tlearn: 0.0956489\ttotal: 1m 1s\tremaining: 36.6s\n",
      "628:\tlearn: 0.0956266\ttotal: 1m 1s\tremaining: 36.5s\n",
      "629:\tlearn: 0.0956038\ttotal: 1m 1s\tremaining: 36.4s\n",
      "630:\tlearn: 0.0955628\ttotal: 1m 2s\tremaining: 36.3s\n",
      "631:\tlearn: 0.0955313\ttotal: 1m 2s\tremaining: 36.2s\n",
      "632:\tlearn: 0.0955062\ttotal: 1m 2s\tremaining: 36.1s\n",
      "633:\tlearn: 0.0954144\ttotal: 1m 2s\tremaining: 36s\n",
      "634:\tlearn: 0.0953805\ttotal: 1m 2s\tremaining: 35.9s\n",
      "635:\tlearn: 0.0953088\ttotal: 1m 2s\tremaining: 35.8s\n",
      "636:\tlearn: 0.0952221\ttotal: 1m 2s\tremaining: 35.7s\n",
      "637:\tlearn: 0.0951921\ttotal: 1m 2s\tremaining: 35.6s\n",
      "638:\tlearn: 0.0951351\ttotal: 1m 2s\tremaining: 35.5s\n",
      "639:\tlearn: 0.0951131\ttotal: 1m 2s\tremaining: 35.4s\n",
      "640:\tlearn: 0.0950508\ttotal: 1m 3s\tremaining: 35.3s\n",
      "641:\tlearn: 0.0950064\ttotal: 1m 3s\tremaining: 35.2s\n",
      "642:\tlearn: 0.0949571\ttotal: 1m 3s\tremaining: 35.1s\n",
      "643:\tlearn: 0.0949087\ttotal: 1m 3s\tremaining: 35s\n",
      "644:\tlearn: 0.0948705\ttotal: 1m 3s\tremaining: 34.9s\n",
      "645:\tlearn: 0.0948489\ttotal: 1m 3s\tremaining: 34.8s\n",
      "646:\tlearn: 0.0948146\ttotal: 1m 3s\tremaining: 34.7s\n",
      "647:\tlearn: 0.0947810\ttotal: 1m 3s\tremaining: 34.6s\n",
      "648:\tlearn: 0.0947070\ttotal: 1m 3s\tremaining: 34.5s\n",
      "649:\tlearn: 0.0946656\ttotal: 1m 3s\tremaining: 34.4s\n",
      "650:\tlearn: 0.0946089\ttotal: 1m 4s\tremaining: 34.4s\n",
      "651:\tlearn: 0.0945647\ttotal: 1m 4s\tremaining: 34.3s\n",
      "652:\tlearn: 0.0945413\ttotal: 1m 4s\tremaining: 34.2s\n",
      "653:\tlearn: 0.0944801\ttotal: 1m 4s\tremaining: 34.1s\n",
      "654:\tlearn: 0.0944455\ttotal: 1m 4s\tremaining: 34s\n",
      "655:\tlearn: 0.0944225\ttotal: 1m 4s\tremaining: 33.9s\n",
      "656:\tlearn: 0.0943631\ttotal: 1m 4s\tremaining: 33.8s\n",
      "657:\tlearn: 0.0942919\ttotal: 1m 4s\tremaining: 33.8s\n",
      "658:\tlearn: 0.0942482\ttotal: 1m 5s\tremaining: 33.7s\n",
      "659:\tlearn: 0.0942140\ttotal: 1m 5s\tremaining: 33.7s\n",
      "660:\tlearn: 0.0941845\ttotal: 1m 5s\tremaining: 33.6s\n",
      "661:\tlearn: 0.0941224\ttotal: 1m 5s\tremaining: 33.5s\n",
      "662:\tlearn: 0.0940840\ttotal: 1m 5s\tremaining: 33.4s\n",
      "663:\tlearn: 0.0940324\ttotal: 1m 5s\tremaining: 33.4s\n",
      "664:\tlearn: 0.0939911\ttotal: 1m 6s\tremaining: 33.3s\n",
      "665:\tlearn: 0.0939438\ttotal: 1m 6s\tremaining: 33.2s\n",
      "666:\tlearn: 0.0938898\ttotal: 1m 6s\tremaining: 33.1s\n",
      "667:\tlearn: 0.0938441\ttotal: 1m 6s\tremaining: 33s\n",
      "668:\tlearn: 0.0937765\ttotal: 1m 6s\tremaining: 32.9s\n",
      "669:\tlearn: 0.0937306\ttotal: 1m 6s\tremaining: 32.8s\n",
      "670:\tlearn: 0.0936673\ttotal: 1m 6s\tremaining: 32.7s\n",
      "671:\tlearn: 0.0936161\ttotal: 1m 6s\tremaining: 32.6s\n",
      "672:\tlearn: 0.0935879\ttotal: 1m 6s\tremaining: 32.5s\n",
      "673:\tlearn: 0.0935363\ttotal: 1m 6s\tremaining: 32.4s\n",
      "674:\tlearn: 0.0934636\ttotal: 1m 7s\tremaining: 32.3s\n",
      "675:\tlearn: 0.0933994\ttotal: 1m 7s\tremaining: 32.3s\n",
      "676:\tlearn: 0.0933838\ttotal: 1m 7s\tremaining: 32.2s\n",
      "677:\tlearn: 0.0933621\ttotal: 1m 7s\tremaining: 32.1s\n",
      "678:\tlearn: 0.0933423\ttotal: 1m 7s\tremaining: 32s\n",
      "679:\tlearn: 0.0933231\ttotal: 1m 7s\tremaining: 31.8s\n",
      "680:\tlearn: 0.0932979\ttotal: 1m 7s\tremaining: 31.7s\n",
      "681:\tlearn: 0.0932553\ttotal: 1m 7s\tremaining: 31.6s\n",
      "682:\tlearn: 0.0932169\ttotal: 1m 7s\tremaining: 31.6s\n",
      "683:\tlearn: 0.0931764\ttotal: 1m 8s\tremaining: 31.5s\n",
      "684:\tlearn: 0.0931129\ttotal: 1m 8s\tremaining: 31.4s\n",
      "685:\tlearn: 0.0930674\ttotal: 1m 8s\tremaining: 31.3s\n",
      "686:\tlearn: 0.0930299\ttotal: 1m 8s\tremaining: 31.2s\n",
      "687:\tlearn: 0.0929974\ttotal: 1m 8s\tremaining: 31.1s\n",
      "688:\tlearn: 0.0929297\ttotal: 1m 8s\tremaining: 31s\n",
      "689:\tlearn: 0.0928640\ttotal: 1m 8s\tremaining: 30.9s\n",
      "690:\tlearn: 0.0927864\ttotal: 1m 8s\tremaining: 30.8s\n",
      "691:\tlearn: 0.0927324\ttotal: 1m 8s\tremaining: 30.7s\n",
      "692:\tlearn: 0.0926281\ttotal: 1m 9s\tremaining: 30.6s\n",
      "693:\tlearn: 0.0925821\ttotal: 1m 9s\tremaining: 30.5s\n",
      "694:\tlearn: 0.0925086\ttotal: 1m 9s\tremaining: 30.4s\n",
      "695:\tlearn: 0.0924994\ttotal: 1m 9s\tremaining: 30.3s\n",
      "696:\tlearn: 0.0924720\ttotal: 1m 9s\tremaining: 30.2s\n",
      "697:\tlearn: 0.0924472\ttotal: 1m 9s\tremaining: 30.1s\n",
      "698:\tlearn: 0.0923819\ttotal: 1m 9s\tremaining: 30s\n",
      "699:\tlearn: 0.0923160\ttotal: 1m 9s\tremaining: 29.9s\n",
      "700:\tlearn: 0.0922752\ttotal: 1m 9s\tremaining: 29.8s\n",
      "701:\tlearn: 0.0922348\ttotal: 1m 10s\tremaining: 29.8s\n",
      "702:\tlearn: 0.0922010\ttotal: 1m 10s\tremaining: 29.7s\n",
      "703:\tlearn: 0.0921669\ttotal: 1m 10s\tremaining: 29.6s\n",
      "704:\tlearn: 0.0921345\ttotal: 1m 10s\tremaining: 29.5s\n",
      "705:\tlearn: 0.0921017\ttotal: 1m 10s\tremaining: 29.4s\n",
      "706:\tlearn: 0.0920149\ttotal: 1m 10s\tremaining: 29.3s\n",
      "707:\tlearn: 0.0919751\ttotal: 1m 10s\tremaining: 29.2s\n",
      "708:\tlearn: 0.0919389\ttotal: 1m 10s\tremaining: 29.1s\n",
      "709:\tlearn: 0.0919046\ttotal: 1m 10s\tremaining: 29s\n",
      "710:\tlearn: 0.0918628\ttotal: 1m 11s\tremaining: 28.9s\n",
      "711:\tlearn: 0.0918104\ttotal: 1m 11s\tremaining: 28.8s\n",
      "712:\tlearn: 0.0917984\ttotal: 1m 11s\tremaining: 28.7s\n",
      "713:\tlearn: 0.0917321\ttotal: 1m 11s\tremaining: 28.6s\n",
      "714:\tlearn: 0.0916685\ttotal: 1m 11s\tremaining: 28.5s\n",
      "715:\tlearn: 0.0916258\ttotal: 1m 11s\tremaining: 28.4s\n",
      "716:\tlearn: 0.0915967\ttotal: 1m 11s\tremaining: 28.3s\n",
      "717:\tlearn: 0.0915568\ttotal: 1m 11s\tremaining: 28.2s\n",
      "718:\tlearn: 0.0915062\ttotal: 1m 11s\tremaining: 28.1s\n",
      "719:\tlearn: 0.0914726\ttotal: 1m 11s\tremaining: 28s\n",
      "720:\tlearn: 0.0914258\ttotal: 1m 12s\tremaining: 27.9s\n",
      "721:\tlearn: 0.0913843\ttotal: 1m 12s\tremaining: 27.8s\n",
      "722:\tlearn: 0.0913567\ttotal: 1m 12s\tremaining: 27.7s\n",
      "723:\tlearn: 0.0913292\ttotal: 1m 12s\tremaining: 27.6s\n",
      "724:\tlearn: 0.0912796\ttotal: 1m 12s\tremaining: 27.5s\n",
      "725:\tlearn: 0.0912382\ttotal: 1m 12s\tremaining: 27.4s\n",
      "726:\tlearn: 0.0911798\ttotal: 1m 12s\tremaining: 27.3s\n",
      "727:\tlearn: 0.0911469\ttotal: 1m 12s\tremaining: 27.2s\n",
      "728:\tlearn: 0.0911090\ttotal: 1m 12s\tremaining: 27.1s\n",
      "729:\tlearn: 0.0910748\ttotal: 1m 12s\tremaining: 27s\n",
      "730:\tlearn: 0.0910558\ttotal: 1m 13s\tremaining: 26.9s\n",
      "731:\tlearn: 0.0909910\ttotal: 1m 13s\tremaining: 26.8s\n",
      "732:\tlearn: 0.0909494\ttotal: 1m 13s\tremaining: 26.7s\n",
      "733:\tlearn: 0.0909179\ttotal: 1m 13s\tremaining: 26.6s\n",
      "734:\tlearn: 0.0908905\ttotal: 1m 13s\tremaining: 26.5s\n",
      "735:\tlearn: 0.0908560\ttotal: 1m 13s\tremaining: 26.4s\n",
      "736:\tlearn: 0.0908359\ttotal: 1m 13s\tremaining: 26.3s\n",
      "737:\tlearn: 0.0907446\ttotal: 1m 13s\tremaining: 26.2s\n",
      "738:\tlearn: 0.0906729\ttotal: 1m 13s\tremaining: 26.1s\n",
      "739:\tlearn: 0.0906044\ttotal: 1m 13s\tremaining: 26s\n",
      "740:\tlearn: 0.0905760\ttotal: 1m 14s\tremaining: 25.9s\n",
      "741:\tlearn: 0.0905101\ttotal: 1m 14s\tremaining: 25.8s\n",
      "742:\tlearn: 0.0904943\ttotal: 1m 14s\tremaining: 25.7s\n",
      "743:\tlearn: 0.0904286\ttotal: 1m 14s\tremaining: 25.6s\n",
      "744:\tlearn: 0.0903989\ttotal: 1m 14s\tremaining: 25.5s\n",
      "745:\tlearn: 0.0903438\ttotal: 1m 14s\tremaining: 25.4s\n",
      "746:\tlearn: 0.0903049\ttotal: 1m 14s\tremaining: 25.3s\n",
      "747:\tlearn: 0.0902721\ttotal: 1m 14s\tremaining: 25.2s\n",
      "748:\tlearn: 0.0902352\ttotal: 1m 14s\tremaining: 25.1s\n",
      "749:\tlearn: 0.0902066\ttotal: 1m 14s\tremaining: 25s\n",
      "750:\tlearn: 0.0901766\ttotal: 1m 15s\tremaining: 24.9s\n",
      "751:\tlearn: 0.0901602\ttotal: 1m 15s\tremaining: 24.8s\n",
      "752:\tlearn: 0.0901231\ttotal: 1m 15s\tremaining: 24.7s\n",
      "753:\tlearn: 0.0900813\ttotal: 1m 15s\tremaining: 24.6s\n",
      "754:\tlearn: 0.0900389\ttotal: 1m 15s\tremaining: 24.5s\n",
      "755:\tlearn: 0.0900041\ttotal: 1m 15s\tremaining: 24.4s\n",
      "756:\tlearn: 0.0899332\ttotal: 1m 15s\tremaining: 24.3s\n",
      "757:\tlearn: 0.0898727\ttotal: 1m 15s\tremaining: 24.2s\n",
      "758:\tlearn: 0.0898283\ttotal: 1m 15s\tremaining: 24.1s\n",
      "759:\tlearn: 0.0897384\ttotal: 1m 16s\tremaining: 24s\n",
      "760:\tlearn: 0.0897060\ttotal: 1m 16s\tremaining: 23.9s\n",
      "761:\tlearn: 0.0896537\ttotal: 1m 16s\tremaining: 23.8s\n",
      "762:\tlearn: 0.0896179\ttotal: 1m 16s\tremaining: 23.7s\n",
      "763:\tlearn: 0.0895797\ttotal: 1m 16s\tremaining: 23.6s\n",
      "764:\tlearn: 0.0895372\ttotal: 1m 16s\tremaining: 23.5s\n",
      "765:\tlearn: 0.0894911\ttotal: 1m 16s\tremaining: 23.4s\n",
      "766:\tlearn: 0.0894464\ttotal: 1m 16s\tremaining: 23.3s\n",
      "767:\tlearn: 0.0893723\ttotal: 1m 16s\tremaining: 23.2s\n",
      "768:\tlearn: 0.0893388\ttotal: 1m 16s\tremaining: 23.1s\n",
      "769:\tlearn: 0.0893195\ttotal: 1m 17s\tremaining: 23s\n",
      "770:\tlearn: 0.0892972\ttotal: 1m 17s\tremaining: 23s\n",
      "771:\tlearn: 0.0892447\ttotal: 1m 17s\tremaining: 22.9s\n",
      "772:\tlearn: 0.0891960\ttotal: 1m 17s\tremaining: 22.8s\n",
      "773:\tlearn: 0.0891510\ttotal: 1m 17s\tremaining: 22.7s\n",
      "774:\tlearn: 0.0891178\ttotal: 1m 17s\tremaining: 22.6s\n",
      "775:\tlearn: 0.0890949\ttotal: 1m 17s\tremaining: 22.5s\n",
      "776:\tlearn: 0.0890297\ttotal: 1m 17s\tremaining: 22.4s\n",
      "777:\tlearn: 0.0890042\ttotal: 1m 18s\tremaining: 22.3s\n",
      "778:\tlearn: 0.0889720\ttotal: 1m 18s\tremaining: 22.2s\n",
      "779:\tlearn: 0.0889279\ttotal: 1m 18s\tremaining: 22.1s\n",
      "780:\tlearn: 0.0889090\ttotal: 1m 18s\tremaining: 22s\n",
      "781:\tlearn: 0.0888743\ttotal: 1m 18s\tremaining: 21.9s\n",
      "782:\tlearn: 0.0888166\ttotal: 1m 18s\tremaining: 21.8s\n",
      "783:\tlearn: 0.0887855\ttotal: 1m 18s\tremaining: 21.7s\n",
      "784:\tlearn: 0.0887002\ttotal: 1m 18s\tremaining: 21.6s\n",
      "785:\tlearn: 0.0886396\ttotal: 1m 18s\tremaining: 21.5s\n",
      "786:\tlearn: 0.0886031\ttotal: 1m 18s\tremaining: 21.4s\n",
      "787:\tlearn: 0.0885483\ttotal: 1m 19s\tremaining: 21.3s\n",
      "788:\tlearn: 0.0885104\ttotal: 1m 19s\tremaining: 21.2s\n",
      "789:\tlearn: 0.0884591\ttotal: 1m 19s\tremaining: 21.1s\n",
      "790:\tlearn: 0.0883924\ttotal: 1m 19s\tremaining: 21s\n",
      "791:\tlearn: 0.0883578\ttotal: 1m 19s\tremaining: 20.9s\n",
      "792:\tlearn: 0.0883255\ttotal: 1m 19s\tremaining: 20.8s\n",
      "793:\tlearn: 0.0882980\ttotal: 1m 19s\tremaining: 20.7s\n",
      "794:\tlearn: 0.0882737\ttotal: 1m 19s\tremaining: 20.6s\n",
      "795:\tlearn: 0.0882487\ttotal: 1m 19s\tremaining: 20.5s\n",
      "796:\tlearn: 0.0882237\ttotal: 1m 19s\tremaining: 20.4s\n",
      "797:\tlearn: 0.0881952\ttotal: 1m 20s\tremaining: 20.3s\n",
      "798:\tlearn: 0.0881393\ttotal: 1m 20s\tremaining: 20.2s\n",
      "799:\tlearn: 0.0881120\ttotal: 1m 20s\tremaining: 20.1s\n",
      "800:\tlearn: 0.0880834\ttotal: 1m 20s\tremaining: 20s\n",
      "801:\tlearn: 0.0880024\ttotal: 1m 20s\tremaining: 19.9s\n",
      "802:\tlearn: 0.0879396\ttotal: 1m 20s\tremaining: 19.8s\n",
      "803:\tlearn: 0.0878938\ttotal: 1m 20s\tremaining: 19.7s\n",
      "804:\tlearn: 0.0878195\ttotal: 1m 20s\tremaining: 19.6s\n",
      "805:\tlearn: 0.0877955\ttotal: 1m 20s\tremaining: 19.5s\n",
      "806:\tlearn: 0.0877662\ttotal: 1m 20s\tremaining: 19.4s\n",
      "807:\tlearn: 0.0877360\ttotal: 1m 21s\tremaining: 19.3s\n",
      "808:\tlearn: 0.0876972\ttotal: 1m 21s\tremaining: 19.1s\n",
      "809:\tlearn: 0.0876571\ttotal: 1m 21s\tremaining: 19.1s\n",
      "810:\tlearn: 0.0876159\ttotal: 1m 21s\tremaining: 19s\n",
      "811:\tlearn: 0.0875820\ttotal: 1m 21s\tremaining: 18.9s\n",
      "812:\tlearn: 0.0875401\ttotal: 1m 21s\tremaining: 18.8s\n",
      "813:\tlearn: 0.0874877\ttotal: 1m 21s\tremaining: 18.7s\n",
      "814:\tlearn: 0.0874424\ttotal: 1m 21s\tremaining: 18.6s\n",
      "815:\tlearn: 0.0873857\ttotal: 1m 21s\tremaining: 18.4s\n",
      "816:\tlearn: 0.0873551\ttotal: 1m 21s\tremaining: 18.4s\n",
      "817:\tlearn: 0.0872936\ttotal: 1m 22s\tremaining: 18.2s\n",
      "818:\tlearn: 0.0872572\ttotal: 1m 22s\tremaining: 18.1s\n",
      "819:\tlearn: 0.0872200\ttotal: 1m 22s\tremaining: 18s\n",
      "820:\tlearn: 0.0871789\ttotal: 1m 22s\tremaining: 17.9s\n",
      "821:\tlearn: 0.0871428\ttotal: 1m 22s\tremaining: 17.8s\n",
      "822:\tlearn: 0.0871019\ttotal: 1m 22s\tremaining: 17.7s\n",
      "823:\tlearn: 0.0870568\ttotal: 1m 22s\tremaining: 17.6s\n",
      "824:\tlearn: 0.0870013\ttotal: 1m 22s\tremaining: 17.5s\n",
      "825:\tlearn: 0.0869530\ttotal: 1m 22s\tremaining: 17.4s\n",
      "826:\tlearn: 0.0869444\ttotal: 1m 22s\tremaining: 17.3s\n",
      "827:\tlearn: 0.0869086\ttotal: 1m 23s\tremaining: 17.2s\n",
      "828:\tlearn: 0.0868362\ttotal: 1m 23s\tremaining: 17.1s\n",
      "829:\tlearn: 0.0868088\ttotal: 1m 23s\tremaining: 17s\n",
      "830:\tlearn: 0.0867672\ttotal: 1m 23s\tremaining: 16.9s\n",
      "831:\tlearn: 0.0867454\ttotal: 1m 23s\tremaining: 16.8s\n",
      "832:\tlearn: 0.0867097\ttotal: 1m 23s\tremaining: 16.7s\n",
      "833:\tlearn: 0.0866451\ttotal: 1m 23s\tremaining: 16.6s\n",
      "834:\tlearn: 0.0866212\ttotal: 1m 23s\tremaining: 16.5s\n",
      "835:\tlearn: 0.0865805\ttotal: 1m 23s\tremaining: 16.4s\n",
      "836:\tlearn: 0.0865529\ttotal: 1m 23s\tremaining: 16.3s\n",
      "837:\tlearn: 0.0865431\ttotal: 1m 24s\tremaining: 16.2s\n",
      "838:\tlearn: 0.0865103\ttotal: 1m 24s\tremaining: 16.1s\n",
      "839:\tlearn: 0.0864270\ttotal: 1m 24s\tremaining: 16s\n",
      "840:\tlearn: 0.0864025\ttotal: 1m 24s\tremaining: 15.9s\n",
      "841:\tlearn: 0.0863457\ttotal: 1m 24s\tremaining: 15.8s\n",
      "842:\tlearn: 0.0863275\ttotal: 1m 24s\tremaining: 15.7s\n",
      "843:\tlearn: 0.0862603\ttotal: 1m 24s\tremaining: 15.6s\n",
      "844:\tlearn: 0.0861880\ttotal: 1m 24s\tremaining: 15.5s\n",
      "845:\tlearn: 0.0861077\ttotal: 1m 24s\tremaining: 15.4s\n",
      "846:\tlearn: 0.0860761\ttotal: 1m 24s\tremaining: 15.3s\n",
      "847:\tlearn: 0.0860405\ttotal: 1m 25s\tremaining: 15.2s\n",
      "848:\tlearn: 0.0859882\ttotal: 1m 25s\tremaining: 15.1s\n",
      "849:\tlearn: 0.0859589\ttotal: 1m 25s\tremaining: 15s\n",
      "850:\tlearn: 0.0859103\ttotal: 1m 25s\tremaining: 14.9s\n",
      "851:\tlearn: 0.0858831\ttotal: 1m 25s\tremaining: 14.8s\n",
      "852:\tlearn: 0.0858224\ttotal: 1m 25s\tremaining: 14.7s\n",
      "853:\tlearn: 0.0857839\ttotal: 1m 25s\tremaining: 14.6s\n",
      "854:\tlearn: 0.0857589\ttotal: 1m 25s\tremaining: 14.5s\n",
      "855:\tlearn: 0.0857267\ttotal: 1m 25s\tremaining: 14.4s\n",
      "856:\tlearn: 0.0857018\ttotal: 1m 25s\tremaining: 14.3s\n",
      "857:\tlearn: 0.0856595\ttotal: 1m 26s\tremaining: 14.2s\n",
      "858:\tlearn: 0.0856181\ttotal: 1m 26s\tremaining: 14.1s\n",
      "859:\tlearn: 0.0856014\ttotal: 1m 26s\tremaining: 14s\n",
      "860:\tlearn: 0.0855674\ttotal: 1m 26s\tremaining: 13.9s\n",
      "861:\tlearn: 0.0855170\ttotal: 1m 26s\tremaining: 13.8s\n",
      "862:\tlearn: 0.0854602\ttotal: 1m 26s\tremaining: 13.7s\n",
      "863:\tlearn: 0.0854427\ttotal: 1m 26s\tremaining: 13.6s\n",
      "864:\tlearn: 0.0854134\ttotal: 1m 26s\tremaining: 13.5s\n",
      "865:\tlearn: 0.0853725\ttotal: 1m 26s\tremaining: 13.4s\n",
      "866:\tlearn: 0.0853364\ttotal: 1m 27s\tremaining: 13.3s\n",
      "867:\tlearn: 0.0852941\ttotal: 1m 27s\tremaining: 13.2s\n",
      "868:\tlearn: 0.0852605\ttotal: 1m 27s\tremaining: 13.1s\n",
      "869:\tlearn: 0.0852235\ttotal: 1m 27s\tremaining: 13s\n",
      "870:\tlearn: 0.0851936\ttotal: 1m 27s\tremaining: 12.9s\n",
      "871:\tlearn: 0.0851690\ttotal: 1m 27s\tremaining: 12.8s\n",
      "872:\tlearn: 0.0851465\ttotal: 1m 27s\tremaining: 12.7s\n",
      "873:\tlearn: 0.0850782\ttotal: 1m 27s\tremaining: 12.6s\n",
      "874:\tlearn: 0.0850567\ttotal: 1m 27s\tremaining: 12.5s\n",
      "875:\tlearn: 0.0849950\ttotal: 1m 27s\tremaining: 12.4s\n",
      "876:\tlearn: 0.0849626\ttotal: 1m 27s\tremaining: 12.3s\n",
      "877:\tlearn: 0.0849561\ttotal: 1m 28s\tremaining: 12.2s\n",
      "878:\tlearn: 0.0849285\ttotal: 1m 28s\tremaining: 12.1s\n",
      "879:\tlearn: 0.0848829\ttotal: 1m 28s\tremaining: 12s\n",
      "880:\tlearn: 0.0848566\ttotal: 1m 28s\tremaining: 11.9s\n",
      "881:\tlearn: 0.0848007\ttotal: 1m 28s\tremaining: 11.8s\n",
      "882:\tlearn: 0.0847570\ttotal: 1m 28s\tremaining: 11.7s\n",
      "883:\tlearn: 0.0846999\ttotal: 1m 28s\tremaining: 11.6s\n",
      "884:\tlearn: 0.0846185\ttotal: 1m 28s\tremaining: 11.5s\n",
      "885:\tlearn: 0.0845726\ttotal: 1m 28s\tremaining: 11.4s\n",
      "886:\tlearn: 0.0844910\ttotal: 1m 29s\tremaining: 11.3s\n",
      "887:\tlearn: 0.0844294\ttotal: 1m 29s\tremaining: 11.2s\n",
      "888:\tlearn: 0.0843752\ttotal: 1m 29s\tremaining: 11.1s\n",
      "889:\tlearn: 0.0843264\ttotal: 1m 29s\tremaining: 11s\n",
      "890:\tlearn: 0.0842859\ttotal: 1m 29s\tremaining: 10.9s\n",
      "891:\tlearn: 0.0842577\ttotal: 1m 29s\tremaining: 10.8s\n",
      "892:\tlearn: 0.0842115\ttotal: 1m 29s\tremaining: 10.7s\n",
      "893:\tlearn: 0.0841797\ttotal: 1m 29s\tremaining: 10.6s\n",
      "894:\tlearn: 0.0841453\ttotal: 1m 29s\tremaining: 10.5s\n",
      "895:\tlearn: 0.0840404\ttotal: 1m 29s\tremaining: 10.4s\n",
      "896:\tlearn: 0.0839890\ttotal: 1m 30s\tremaining: 10.3s\n",
      "897:\tlearn: 0.0839578\ttotal: 1m 30s\tremaining: 10.2s\n",
      "898:\tlearn: 0.0838954\ttotal: 1m 30s\tremaining: 10.1s\n",
      "899:\tlearn: 0.0838740\ttotal: 1m 30s\tremaining: 10s\n",
      "900:\tlearn: 0.0838322\ttotal: 1m 30s\tremaining: 9.95s\n",
      "901:\tlearn: 0.0838022\ttotal: 1m 30s\tremaining: 9.85s\n",
      "902:\tlearn: 0.0837675\ttotal: 1m 30s\tremaining: 9.75s\n",
      "903:\tlearn: 0.0837451\ttotal: 1m 30s\tremaining: 9.65s\n",
      "904:\tlearn: 0.0836728\ttotal: 1m 31s\tremaining: 9.56s\n",
      "905:\tlearn: 0.0836009\ttotal: 1m 31s\tremaining: 9.46s\n",
      "906:\tlearn: 0.0835525\ttotal: 1m 31s\tremaining: 9.36s\n",
      "907:\tlearn: 0.0835280\ttotal: 1m 31s\tremaining: 9.26s\n",
      "908:\tlearn: 0.0835077\ttotal: 1m 31s\tremaining: 9.17s\n",
      "909:\tlearn: 0.0834770\ttotal: 1m 31s\tremaining: 9.07s\n",
      "910:\tlearn: 0.0834229\ttotal: 1m 31s\tremaining: 8.97s\n",
      "911:\tlearn: 0.0833853\ttotal: 1m 31s\tremaining: 8.87s\n",
      "912:\tlearn: 0.0833685\ttotal: 1m 32s\tremaining: 8.78s\n",
      "913:\tlearn: 0.0833314\ttotal: 1m 32s\tremaining: 8.68s\n",
      "914:\tlearn: 0.0833075\ttotal: 1m 32s\tremaining: 8.58s\n",
      "915:\tlearn: 0.0832663\ttotal: 1m 32s\tremaining: 8.48s\n",
      "916:\tlearn: 0.0832285\ttotal: 1m 32s\tremaining: 8.38s\n",
      "917:\tlearn: 0.0832214\ttotal: 1m 32s\tremaining: 8.28s\n",
      "918:\tlearn: 0.0831569\ttotal: 1m 32s\tremaining: 8.19s\n",
      "919:\tlearn: 0.0831292\ttotal: 1m 33s\tremaining: 8.09s\n",
      "920:\tlearn: 0.0830751\ttotal: 1m 33s\tremaining: 7.99s\n",
      "921:\tlearn: 0.0830366\ttotal: 1m 33s\tremaining: 7.89s\n",
      "922:\tlearn: 0.0829828\ttotal: 1m 33s\tremaining: 7.79s\n",
      "923:\tlearn: 0.0829475\ttotal: 1m 33s\tremaining: 7.69s\n",
      "924:\tlearn: 0.0829125\ttotal: 1m 33s\tremaining: 7.59s\n",
      "925:\tlearn: 0.0828846\ttotal: 1m 33s\tremaining: 7.49s\n",
      "926:\tlearn: 0.0828432\ttotal: 1m 33s\tremaining: 7.38s\n",
      "927:\tlearn: 0.0828129\ttotal: 1m 33s\tremaining: 7.28s\n",
      "928:\tlearn: 0.0827569\ttotal: 1m 33s\tremaining: 7.18s\n",
      "929:\tlearn: 0.0827399\ttotal: 1m 34s\tremaining: 7.08s\n",
      "930:\tlearn: 0.0827009\ttotal: 1m 34s\tremaining: 6.98s\n",
      "931:\tlearn: 0.0826543\ttotal: 1m 34s\tremaining: 6.88s\n",
      "932:\tlearn: 0.0825962\ttotal: 1m 34s\tremaining: 6.78s\n",
      "933:\tlearn: 0.0825492\ttotal: 1m 34s\tremaining: 6.67s\n",
      "934:\tlearn: 0.0825237\ttotal: 1m 34s\tremaining: 6.57s\n",
      "935:\tlearn: 0.0824958\ttotal: 1m 34s\tremaining: 6.47s\n",
      "936:\tlearn: 0.0824930\ttotal: 1m 34s\tremaining: 6.37s\n",
      "937:\tlearn: 0.0824412\ttotal: 1m 34s\tremaining: 6.27s\n",
      "938:\tlearn: 0.0824207\ttotal: 1m 34s\tremaining: 6.17s\n",
      "939:\tlearn: 0.0823809\ttotal: 1m 35s\tremaining: 6.07s\n",
      "940:\tlearn: 0.0823627\ttotal: 1m 35s\tremaining: 5.97s\n",
      "941:\tlearn: 0.0823346\ttotal: 1m 35s\tremaining: 5.86s\n",
      "942:\tlearn: 0.0823283\ttotal: 1m 35s\tremaining: 5.76s\n",
      "943:\tlearn: 0.0823137\ttotal: 1m 35s\tremaining: 5.66s\n",
      "944:\tlearn: 0.0822494\ttotal: 1m 35s\tremaining: 5.56s\n",
      "945:\tlearn: 0.0822158\ttotal: 1m 35s\tremaining: 5.46s\n",
      "946:\tlearn: 0.0821810\ttotal: 1m 35s\tremaining: 5.36s\n",
      "947:\tlearn: 0.0820871\ttotal: 1m 35s\tremaining: 5.26s\n",
      "948:\tlearn: 0.0820617\ttotal: 1m 35s\tremaining: 5.16s\n",
      "949:\tlearn: 0.0820359\ttotal: 1m 36s\tremaining: 5.06s\n",
      "950:\tlearn: 0.0820041\ttotal: 1m 36s\tremaining: 4.96s\n",
      "951:\tlearn: 0.0819751\ttotal: 1m 36s\tremaining: 4.86s\n",
      "952:\tlearn: 0.0819550\ttotal: 1m 36s\tremaining: 4.76s\n",
      "953:\tlearn: 0.0819304\ttotal: 1m 36s\tremaining: 4.66s\n",
      "954:\tlearn: 0.0819123\ttotal: 1m 36s\tremaining: 4.55s\n",
      "955:\tlearn: 0.0818755\ttotal: 1m 36s\tremaining: 4.45s\n",
      "956:\tlearn: 0.0818373\ttotal: 1m 36s\tremaining: 4.35s\n",
      "957:\tlearn: 0.0818215\ttotal: 1m 36s\tremaining: 4.25s\n",
      "958:\tlearn: 0.0817790\ttotal: 1m 37s\tremaining: 4.15s\n",
      "959:\tlearn: 0.0817632\ttotal: 1m 37s\tremaining: 4.05s\n",
      "960:\tlearn: 0.0817088\ttotal: 1m 37s\tremaining: 3.95s\n",
      "961:\tlearn: 0.0816807\ttotal: 1m 37s\tremaining: 3.85s\n",
      "962:\tlearn: 0.0816051\ttotal: 1m 37s\tremaining: 3.75s\n",
      "963:\tlearn: 0.0815750\ttotal: 1m 37s\tremaining: 3.64s\n",
      "964:\tlearn: 0.0815597\ttotal: 1m 37s\tremaining: 3.54s\n",
      "965:\tlearn: 0.0815406\ttotal: 1m 37s\tremaining: 3.44s\n",
      "966:\tlearn: 0.0815017\ttotal: 1m 37s\tremaining: 3.34s\n",
      "967:\tlearn: 0.0814761\ttotal: 1m 38s\tremaining: 3.24s\n",
      "968:\tlearn: 0.0814455\ttotal: 1m 38s\tremaining: 3.14s\n",
      "969:\tlearn: 0.0814048\ttotal: 1m 38s\tremaining: 3.04s\n",
      "970:\tlearn: 0.0813825\ttotal: 1m 38s\tremaining: 2.94s\n",
      "971:\tlearn: 0.0813423\ttotal: 1m 38s\tremaining: 2.83s\n",
      "972:\tlearn: 0.0812952\ttotal: 1m 38s\tremaining: 2.73s\n",
      "973:\tlearn: 0.0812902\ttotal: 1m 38s\tremaining: 2.63s\n",
      "974:\tlearn: 0.0812830\ttotal: 1m 38s\tremaining: 2.53s\n",
      "975:\tlearn: 0.0812421\ttotal: 1m 38s\tremaining: 2.43s\n",
      "976:\tlearn: 0.0812048\ttotal: 1m 38s\tremaining: 2.33s\n",
      "977:\tlearn: 0.0811698\ttotal: 1m 38s\tremaining: 2.23s\n",
      "978:\tlearn: 0.0811248\ttotal: 1m 39s\tremaining: 2.13s\n",
      "979:\tlearn: 0.0810942\ttotal: 1m 39s\tremaining: 2.02s\n",
      "980:\tlearn: 0.0810742\ttotal: 1m 39s\tremaining: 1.92s\n",
      "981:\tlearn: 0.0810276\ttotal: 1m 39s\tremaining: 1.82s\n",
      "982:\tlearn: 0.0810020\ttotal: 1m 39s\tremaining: 1.72s\n",
      "983:\tlearn: 0.0809661\ttotal: 1m 39s\tremaining: 1.62s\n",
      "984:\tlearn: 0.0809486\ttotal: 1m 39s\tremaining: 1.52s\n",
      "985:\tlearn: 0.0809220\ttotal: 1m 39s\tremaining: 1.42s\n",
      "986:\tlearn: 0.0808962\ttotal: 1m 39s\tremaining: 1.31s\n",
      "987:\tlearn: 0.0808706\ttotal: 1m 39s\tremaining: 1.21s\n",
      "988:\tlearn: 0.0808463\ttotal: 1m 40s\tremaining: 1.11s\n",
      "989:\tlearn: 0.0808337\ttotal: 1m 40s\tremaining: 1.01s\n",
      "990:\tlearn: 0.0807922\ttotal: 1m 40s\tremaining: 911ms\n",
      "991:\tlearn: 0.0807349\ttotal: 1m 40s\tremaining: 809ms\n",
      "992:\tlearn: 0.0806940\ttotal: 1m 40s\tremaining: 708ms\n",
      "993:\tlearn: 0.0806379\ttotal: 1m 40s\tremaining: 607ms\n",
      "994:\tlearn: 0.0806054\ttotal: 1m 40s\tremaining: 506ms\n",
      "995:\tlearn: 0.0805683\ttotal: 1m 40s\tremaining: 405ms\n",
      "996:\tlearn: 0.0805168\ttotal: 1m 40s\tremaining: 303ms\n",
      "997:\tlearn: 0.0804996\ttotal: 1m 40s\tremaining: 202ms\n",
      "998:\tlearn: 0.0804480\ttotal: 1m 41s\tremaining: 101ms\n",
      "999:\tlearn: 0.0804174\ttotal: 1m 41s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/Projets/Machine-learning/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:241: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5268607\ttotal: 109ms\tremaining: 1m 48s\n",
      "1:\tlearn: 0.4269820\ttotal: 221ms\tremaining: 1m 50s\n",
      "2:\tlearn: 0.3681616\ttotal: 318ms\tremaining: 1m 45s\n",
      "3:\tlearn: 0.3268981\ttotal: 417ms\tremaining: 1m 43s\n",
      "4:\tlearn: 0.3002497\ttotal: 527ms\tremaining: 1m 44s\n",
      "5:\tlearn: 0.2817856\ttotal: 623ms\tremaining: 1m 43s\n",
      "6:\tlearn: 0.2573488\ttotal: 725ms\tremaining: 1m 42s\n",
      "7:\tlearn: 0.2462725\ttotal: 814ms\tremaining: 1m 40s\n",
      "8:\tlearn: 0.2380101\ttotal: 906ms\tremaining: 1m 39s\n",
      "9:\tlearn: 0.2280518\ttotal: 1s\tremaining: 1m 39s\n",
      "10:\tlearn: 0.2231006\ttotal: 1.1s\tremaining: 1m 39s\n",
      "11:\tlearn: 0.2143295\ttotal: 1.2s\tremaining: 1m 38s\n",
      "12:\tlearn: 0.2105775\ttotal: 1.29s\tremaining: 1m 38s\n",
      "13:\tlearn: 0.2044587\ttotal: 1.39s\tremaining: 1m 38s\n",
      "14:\tlearn: 0.2017606\ttotal: 1.49s\tremaining: 1m 38s\n",
      "15:\tlearn: 0.1984549\ttotal: 1.59s\tremaining: 1m 38s\n",
      "16:\tlearn: 0.1960099\ttotal: 1.7s\tremaining: 1m 38s\n",
      "17:\tlearn: 0.1939983\ttotal: 1.79s\tremaining: 1m 37s\n",
      "18:\tlearn: 0.1920979\ttotal: 1.89s\tremaining: 1m 37s\n",
      "19:\tlearn: 0.1889975\ttotal: 1.99s\tremaining: 1m 37s\n",
      "20:\tlearn: 0.1862246\ttotal: 2.08s\tremaining: 1m 37s\n",
      "21:\tlearn: 0.1850184\ttotal: 2.18s\tremaining: 1m 37s\n",
      "22:\tlearn: 0.1833773\ttotal: 2.29s\tremaining: 1m 37s\n",
      "23:\tlearn: 0.1822404\ttotal: 2.38s\tremaining: 1m 37s\n",
      "24:\tlearn: 0.1809510\ttotal: 2.48s\tremaining: 1m 36s\n",
      "25:\tlearn: 0.1791770\ttotal: 2.59s\tremaining: 1m 36s\n",
      "26:\tlearn: 0.1780129\ttotal: 2.69s\tremaining: 1m 36s\n",
      "27:\tlearn: 0.1759416\ttotal: 2.79s\tremaining: 1m 36s\n",
      "28:\tlearn: 0.1747437\ttotal: 2.9s\tremaining: 1m 37s\n",
      "29:\tlearn: 0.1736159\ttotal: 3.02s\tremaining: 1m 37s\n",
      "30:\tlearn: 0.1726243\ttotal: 3.11s\tremaining: 1m 37s\n",
      "31:\tlearn: 0.1717540\ttotal: 3.21s\tremaining: 1m 37s\n",
      "32:\tlearn: 0.1707693\ttotal: 3.31s\tremaining: 1m 36s\n",
      "33:\tlearn: 0.1697840\ttotal: 3.41s\tremaining: 1m 36s\n",
      "34:\tlearn: 0.1690638\ttotal: 3.5s\tremaining: 1m 36s\n",
      "35:\tlearn: 0.1680833\ttotal: 3.6s\tremaining: 1m 36s\n",
      "36:\tlearn: 0.1669650\ttotal: 3.71s\tremaining: 1m 36s\n",
      "37:\tlearn: 0.1661950\ttotal: 3.8s\tremaining: 1m 36s\n",
      "38:\tlearn: 0.1648763\ttotal: 3.9s\tremaining: 1m 36s\n",
      "39:\tlearn: 0.1636315\ttotal: 3.98s\tremaining: 1m 35s\n",
      "40:\tlearn: 0.1630486\ttotal: 4.08s\tremaining: 1m 35s\n",
      "41:\tlearn: 0.1624553\ttotal: 4.18s\tremaining: 1m 35s\n",
      "42:\tlearn: 0.1615738\ttotal: 4.29s\tremaining: 1m 35s\n",
      "43:\tlearn: 0.1611372\ttotal: 4.39s\tremaining: 1m 35s\n",
      "44:\tlearn: 0.1604625\ttotal: 4.5s\tremaining: 1m 35s\n",
      "45:\tlearn: 0.1599535\ttotal: 4.59s\tremaining: 1m 35s\n",
      "46:\tlearn: 0.1593361\ttotal: 4.69s\tremaining: 1m 35s\n",
      "47:\tlearn: 0.1588303\ttotal: 4.79s\tremaining: 1m 35s\n",
      "48:\tlearn: 0.1581380\ttotal: 4.89s\tremaining: 1m 35s\n",
      "49:\tlearn: 0.1571699\ttotal: 4.99s\tremaining: 1m 34s\n",
      "50:\tlearn: 0.1567341\ttotal: 5.09s\tremaining: 1m 34s\n",
      "51:\tlearn: 0.1562317\ttotal: 5.19s\tremaining: 1m 34s\n",
      "52:\tlearn: 0.1556617\ttotal: 5.29s\tremaining: 1m 34s\n",
      "53:\tlearn: 0.1550293\ttotal: 5.39s\tremaining: 1m 34s\n",
      "54:\tlearn: 0.1545285\ttotal: 5.49s\tremaining: 1m 34s\n",
      "55:\tlearn: 0.1538878\ttotal: 5.59s\tremaining: 1m 34s\n",
      "56:\tlearn: 0.1534715\ttotal: 5.7s\tremaining: 1m 34s\n",
      "57:\tlearn: 0.1530334\ttotal: 5.81s\tremaining: 1m 34s\n",
      "58:\tlearn: 0.1525355\ttotal: 5.92s\tremaining: 1m 34s\n",
      "59:\tlearn: 0.1520597\ttotal: 6.02s\tremaining: 1m 34s\n",
      "60:\tlearn: 0.1516275\ttotal: 6.12s\tremaining: 1m 34s\n",
      "61:\tlearn: 0.1512854\ttotal: 6.22s\tremaining: 1m 34s\n",
      "62:\tlearn: 0.1506867\ttotal: 6.31s\tremaining: 1m 33s\n",
      "63:\tlearn: 0.1503658\ttotal: 6.41s\tremaining: 1m 33s\n",
      "64:\tlearn: 0.1500679\ttotal: 6.5s\tremaining: 1m 33s\n",
      "65:\tlearn: 0.1498187\ttotal: 6.6s\tremaining: 1m 33s\n",
      "66:\tlearn: 0.1494450\ttotal: 6.69s\tremaining: 1m 33s\n",
      "67:\tlearn: 0.1491698\ttotal: 6.79s\tremaining: 1m 33s\n",
      "68:\tlearn: 0.1489159\ttotal: 6.88s\tremaining: 1m 32s\n",
      "69:\tlearn: 0.1485396\ttotal: 6.98s\tremaining: 1m 32s\n",
      "70:\tlearn: 0.1481494\ttotal: 7.07s\tremaining: 1m 32s\n",
      "71:\tlearn: 0.1478616\ttotal: 7.16s\tremaining: 1m 32s\n",
      "72:\tlearn: 0.1474092\ttotal: 7.26s\tremaining: 1m 32s\n",
      "73:\tlearn: 0.1470613\ttotal: 7.35s\tremaining: 1m 31s\n",
      "74:\tlearn: 0.1466039\ttotal: 7.46s\tremaining: 1m 31s\n",
      "75:\tlearn: 0.1462449\ttotal: 7.57s\tremaining: 1m 32s\n",
      "76:\tlearn: 0.1459978\ttotal: 7.67s\tremaining: 1m 31s\n",
      "77:\tlearn: 0.1455905\ttotal: 7.78s\tremaining: 1m 31s\n",
      "78:\tlearn: 0.1452998\ttotal: 7.88s\tremaining: 1m 31s\n",
      "79:\tlearn: 0.1448523\ttotal: 7.98s\tremaining: 1m 31s\n",
      "80:\tlearn: 0.1446336\ttotal: 8.07s\tremaining: 1m 31s\n",
      "81:\tlearn: 0.1443779\ttotal: 8.17s\tremaining: 1m 31s\n",
      "82:\tlearn: 0.1441293\ttotal: 8.26s\tremaining: 1m 31s\n",
      "83:\tlearn: 0.1436414\ttotal: 8.36s\tremaining: 1m 31s\n",
      "84:\tlearn: 0.1431684\ttotal: 8.46s\tremaining: 1m 31s\n",
      "85:\tlearn: 0.1428985\ttotal: 8.57s\tremaining: 1m 31s\n",
      "86:\tlearn: 0.1426856\ttotal: 8.67s\tremaining: 1m 30s\n",
      "87:\tlearn: 0.1424852\ttotal: 8.76s\tremaining: 1m 30s\n",
      "88:\tlearn: 0.1422724\ttotal: 8.87s\tremaining: 1m 30s\n",
      "89:\tlearn: 0.1420304\ttotal: 8.97s\tremaining: 1m 30s\n",
      "90:\tlearn: 0.1416389\ttotal: 9.07s\tremaining: 1m 30s\n",
      "91:\tlearn: 0.1413915\ttotal: 9.17s\tremaining: 1m 30s\n",
      "92:\tlearn: 0.1411444\ttotal: 9.27s\tremaining: 1m 30s\n",
      "93:\tlearn: 0.1409195\ttotal: 9.37s\tremaining: 1m 30s\n",
      "94:\tlearn: 0.1405878\ttotal: 9.46s\tremaining: 1m 30s\n",
      "95:\tlearn: 0.1403854\ttotal: 9.56s\tremaining: 1m 30s\n",
      "96:\tlearn: 0.1400603\ttotal: 9.65s\tremaining: 1m 29s\n",
      "97:\tlearn: 0.1397621\ttotal: 9.75s\tremaining: 1m 29s\n",
      "98:\tlearn: 0.1394985\ttotal: 9.94s\tremaining: 1m 30s\n",
      "99:\tlearn: 0.1391571\ttotal: 10.1s\tremaining: 1m 30s\n",
      "100:\tlearn: 0.1388610\ttotal: 10.2s\tremaining: 1m 30s\n",
      "101:\tlearn: 0.1386445\ttotal: 10.2s\tremaining: 1m 30s\n",
      "102:\tlearn: 0.1384397\ttotal: 10.3s\tremaining: 1m 30s\n",
      "103:\tlearn: 0.1381609\ttotal: 10.4s\tremaining: 1m 29s\n",
      "104:\tlearn: 0.1379193\ttotal: 10.5s\tremaining: 1m 29s\n",
      "105:\tlearn: 0.1376623\ttotal: 10.6s\tremaining: 1m 29s\n",
      "106:\tlearn: 0.1374115\ttotal: 10.7s\tremaining: 1m 29s\n",
      "107:\tlearn: 0.1372035\ttotal: 10.8s\tremaining: 1m 29s\n",
      "108:\tlearn: 0.1369530\ttotal: 10.9s\tremaining: 1m 29s\n",
      "109:\tlearn: 0.1367694\ttotal: 11s\tremaining: 1m 29s\n",
      "110:\tlearn: 0.1365778\ttotal: 11.1s\tremaining: 1m 29s\n",
      "111:\tlearn: 0.1363358\ttotal: 11.2s\tremaining: 1m 29s\n",
      "112:\tlearn: 0.1361752\ttotal: 11.3s\tremaining: 1m 29s\n",
      "113:\tlearn: 0.1359588\ttotal: 11.4s\tremaining: 1m 28s\n",
      "114:\tlearn: 0.1357564\ttotal: 11.5s\tremaining: 1m 28s\n",
      "115:\tlearn: 0.1355401\ttotal: 11.6s\tremaining: 1m 28s\n",
      "116:\tlearn: 0.1353664\ttotal: 11.7s\tremaining: 1m 28s\n",
      "117:\tlearn: 0.1352324\ttotal: 11.8s\tremaining: 1m 28s\n",
      "118:\tlearn: 0.1350380\ttotal: 11.9s\tremaining: 1m 28s\n",
      "119:\tlearn: 0.1348003\ttotal: 12s\tremaining: 1m 28s\n",
      "120:\tlearn: 0.1346391\ttotal: 12.1s\tremaining: 1m 28s\n",
      "121:\tlearn: 0.1344749\ttotal: 12.3s\tremaining: 1m 28s\n",
      "122:\tlearn: 0.1342808\ttotal: 12.4s\tremaining: 1m 28s\n",
      "123:\tlearn: 0.1340956\ttotal: 12.5s\tremaining: 1m 27s\n",
      "124:\tlearn: 0.1337636\ttotal: 12.5s\tremaining: 1m 27s\n",
      "125:\tlearn: 0.1335054\ttotal: 12.6s\tremaining: 1m 27s\n",
      "126:\tlearn: 0.1332535\ttotal: 12.7s\tremaining: 1m 27s\n",
      "127:\tlearn: 0.1331183\ttotal: 12.8s\tremaining: 1m 27s\n",
      "128:\tlearn: 0.1328604\ttotal: 12.9s\tremaining: 1m 27s\n",
      "129:\tlearn: 0.1327031\ttotal: 13s\tremaining: 1m 27s\n",
      "130:\tlearn: 0.1325598\ttotal: 13.1s\tremaining: 1m 27s\n",
      "131:\tlearn: 0.1324171\ttotal: 13.2s\tremaining: 1m 26s\n",
      "132:\tlearn: 0.1322828\ttotal: 13.3s\tremaining: 1m 26s\n",
      "133:\tlearn: 0.1321579\ttotal: 13.4s\tremaining: 1m 26s\n",
      "134:\tlearn: 0.1319771\ttotal: 13.5s\tremaining: 1m 26s\n",
      "135:\tlearn: 0.1318266\ttotal: 13.6s\tremaining: 1m 26s\n",
      "136:\tlearn: 0.1316904\ttotal: 13.7s\tremaining: 1m 26s\n",
      "137:\tlearn: 0.1315639\ttotal: 13.8s\tremaining: 1m 26s\n",
      "138:\tlearn: 0.1314143\ttotal: 13.9s\tremaining: 1m 25s\n",
      "139:\tlearn: 0.1311796\ttotal: 14s\tremaining: 1m 25s\n",
      "140:\tlearn: 0.1310619\ttotal: 14.1s\tremaining: 1m 25s\n",
      "141:\tlearn: 0.1308967\ttotal: 14.2s\tremaining: 1m 25s\n",
      "142:\tlearn: 0.1306634\ttotal: 14.3s\tremaining: 1m 25s\n",
      "143:\tlearn: 0.1305444\ttotal: 14.4s\tremaining: 1m 25s\n",
      "144:\tlearn: 0.1303831\ttotal: 14.5s\tremaining: 1m 25s\n",
      "145:\tlearn: 0.1301791\ttotal: 14.6s\tremaining: 1m 25s\n",
      "146:\tlearn: 0.1300869\ttotal: 14.7s\tremaining: 1m 25s\n",
      "147:\tlearn: 0.1298838\ttotal: 14.8s\tremaining: 1m 25s\n",
      "148:\tlearn: 0.1297737\ttotal: 14.9s\tremaining: 1m 24s\n",
      "149:\tlearn: 0.1295595\ttotal: 15s\tremaining: 1m 24s\n",
      "150:\tlearn: 0.1294348\ttotal: 15.1s\tremaining: 1m 24s\n",
      "151:\tlearn: 0.1292791\ttotal: 15.2s\tremaining: 1m 24s\n",
      "152:\tlearn: 0.1291057\ttotal: 15.3s\tremaining: 1m 24s\n",
      "153:\tlearn: 0.1289654\ttotal: 15.4s\tremaining: 1m 24s\n",
      "154:\tlearn: 0.1288361\ttotal: 15.5s\tremaining: 1m 24s\n",
      "155:\tlearn: 0.1286630\ttotal: 15.6s\tremaining: 1m 24s\n",
      "156:\tlearn: 0.1285243\ttotal: 15.7s\tremaining: 1m 24s\n",
      "157:\tlearn: 0.1283835\ttotal: 15.9s\tremaining: 1m 24s\n",
      "158:\tlearn: 0.1283015\ttotal: 16s\tremaining: 1m 24s\n",
      "159:\tlearn: 0.1281388\ttotal: 16.1s\tremaining: 1m 24s\n",
      "160:\tlearn: 0.1280158\ttotal: 16.2s\tremaining: 1m 24s\n",
      "161:\tlearn: 0.1278931\ttotal: 16.3s\tremaining: 1m 24s\n",
      "162:\tlearn: 0.1277901\ttotal: 16.4s\tremaining: 1m 24s\n",
      "163:\tlearn: 0.1276182\ttotal: 16.5s\tremaining: 1m 24s\n",
      "164:\tlearn: 0.1274463\ttotal: 16.6s\tremaining: 1m 24s\n",
      "165:\tlearn: 0.1273051\ttotal: 16.7s\tremaining: 1m 24s\n",
      "166:\tlearn: 0.1271763\ttotal: 16.8s\tremaining: 1m 23s\n",
      "167:\tlearn: 0.1270587\ttotal: 16.9s\tremaining: 1m 23s\n",
      "168:\tlearn: 0.1269730\ttotal: 17s\tremaining: 1m 23s\n",
      "169:\tlearn: 0.1268438\ttotal: 17.1s\tremaining: 1m 23s\n",
      "170:\tlearn: 0.1267019\ttotal: 17.2s\tremaining: 1m 23s\n",
      "171:\tlearn: 0.1265801\ttotal: 17.3s\tremaining: 1m 23s\n",
      "172:\tlearn: 0.1264997\ttotal: 17.4s\tremaining: 1m 23s\n",
      "173:\tlearn: 0.1263765\ttotal: 17.5s\tremaining: 1m 23s\n",
      "174:\tlearn: 0.1261718\ttotal: 17.6s\tremaining: 1m 23s\n",
      "175:\tlearn: 0.1260292\ttotal: 17.7s\tremaining: 1m 23s\n",
      "176:\tlearn: 0.1258658\ttotal: 17.8s\tremaining: 1m 22s\n",
      "177:\tlearn: 0.1257798\ttotal: 17.9s\tremaining: 1m 22s\n",
      "178:\tlearn: 0.1256956\ttotal: 18s\tremaining: 1m 22s\n",
      "179:\tlearn: 0.1255935\ttotal: 18.2s\tremaining: 1m 22s\n",
      "180:\tlearn: 0.1254653\ttotal: 18.3s\tremaining: 1m 22s\n",
      "181:\tlearn: 0.1252999\ttotal: 18.4s\tremaining: 1m 22s\n",
      "182:\tlearn: 0.1251772\ttotal: 18.5s\tremaining: 1m 22s\n",
      "183:\tlearn: 0.1250018\ttotal: 18.6s\tremaining: 1m 22s\n",
      "184:\tlearn: 0.1248675\ttotal: 18.7s\tremaining: 1m 22s\n",
      "185:\tlearn: 0.1247740\ttotal: 18.8s\tremaining: 1m 22s\n",
      "186:\tlearn: 0.1246235\ttotal: 18.9s\tremaining: 1m 22s\n",
      "187:\tlearn: 0.1245189\ttotal: 19s\tremaining: 1m 22s\n",
      "188:\tlearn: 0.1244232\ttotal: 19.1s\tremaining: 1m 21s\n",
      "189:\tlearn: 0.1243023\ttotal: 19.2s\tremaining: 1m 21s\n",
      "190:\tlearn: 0.1241678\ttotal: 19.3s\tremaining: 1m 21s\n",
      "191:\tlearn: 0.1240883\ttotal: 19.4s\tremaining: 1m 21s\n",
      "192:\tlearn: 0.1239963\ttotal: 19.5s\tremaining: 1m 21s\n",
      "193:\tlearn: 0.1238766\ttotal: 19.6s\tremaining: 1m 21s\n",
      "194:\tlearn: 0.1237634\ttotal: 19.7s\tremaining: 1m 21s\n",
      "195:\tlearn: 0.1236590\ttotal: 19.8s\tremaining: 1m 21s\n",
      "196:\tlearn: 0.1235154\ttotal: 19.9s\tremaining: 1m 21s\n",
      "197:\tlearn: 0.1233674\ttotal: 20.2s\tremaining: 1m 21s\n",
      "198:\tlearn: 0.1232850\ttotal: 20.3s\tremaining: 1m 21s\n",
      "199:\tlearn: 0.1232030\ttotal: 20.4s\tremaining: 1m 21s\n",
      "200:\tlearn: 0.1231343\ttotal: 20.5s\tremaining: 1m 21s\n",
      "201:\tlearn: 0.1230357\ttotal: 20.6s\tremaining: 1m 21s\n",
      "202:\tlearn: 0.1229563\ttotal: 20.7s\tremaining: 1m 21s\n",
      "203:\tlearn: 0.1228525\ttotal: 20.8s\tremaining: 1m 21s\n",
      "204:\tlearn: 0.1227472\ttotal: 20.9s\tremaining: 1m 21s\n",
      "205:\tlearn: 0.1226346\ttotal: 21.1s\tremaining: 1m 21s\n",
      "206:\tlearn: 0.1225470\ttotal: 21.2s\tremaining: 1m 21s\n",
      "207:\tlearn: 0.1224360\ttotal: 21.2s\tremaining: 1m 20s\n",
      "208:\tlearn: 0.1223440\ttotal: 21.3s\tremaining: 1m 20s\n",
      "209:\tlearn: 0.1222354\ttotal: 21.4s\tremaining: 1m 20s\n",
      "210:\tlearn: 0.1221762\ttotal: 21.5s\tremaining: 1m 20s\n",
      "211:\tlearn: 0.1220674\ttotal: 21.6s\tremaining: 1m 20s\n",
      "212:\tlearn: 0.1219843\ttotal: 21.7s\tremaining: 1m 20s\n",
      "213:\tlearn: 0.1218660\ttotal: 21.8s\tremaining: 1m 20s\n",
      "214:\tlearn: 0.1217430\ttotal: 21.9s\tremaining: 1m 20s\n",
      "215:\tlearn: 0.1216662\ttotal: 22s\tremaining: 1m 19s\n",
      "216:\tlearn: 0.1215650\ttotal: 22.1s\tremaining: 1m 19s\n",
      "217:\tlearn: 0.1214617\ttotal: 22.2s\tremaining: 1m 19s\n",
      "218:\tlearn: 0.1213566\ttotal: 22.3s\tremaining: 1m 19s\n",
      "219:\tlearn: 0.1212073\ttotal: 22.4s\tremaining: 1m 19s\n",
      "220:\tlearn: 0.1211008\ttotal: 22.5s\tremaining: 1m 19s\n",
      "221:\tlearn: 0.1209761\ttotal: 22.6s\tremaining: 1m 19s\n",
      "222:\tlearn: 0.1208318\ttotal: 22.7s\tremaining: 1m 19s\n",
      "223:\tlearn: 0.1207367\ttotal: 22.8s\tremaining: 1m 19s\n",
      "224:\tlearn: 0.1206508\ttotal: 22.9s\tremaining: 1m 18s\n",
      "225:\tlearn: 0.1204819\ttotal: 23s\tremaining: 1m 18s\n",
      "226:\tlearn: 0.1204202\ttotal: 23.1s\tremaining: 1m 18s\n",
      "227:\tlearn: 0.1203006\ttotal: 23.2s\tremaining: 1m 18s\n",
      "228:\tlearn: 0.1202015\ttotal: 23.3s\tremaining: 1m 18s\n",
      "229:\tlearn: 0.1200898\ttotal: 23.4s\tremaining: 1m 18s\n",
      "230:\tlearn: 0.1199982\ttotal: 23.5s\tremaining: 1m 18s\n",
      "231:\tlearn: 0.1199072\ttotal: 23.6s\tremaining: 1m 18s\n",
      "232:\tlearn: 0.1196949\ttotal: 23.7s\tremaining: 1m 18s\n",
      "233:\tlearn: 0.1196116\ttotal: 23.8s\tremaining: 1m 18s\n",
      "234:\tlearn: 0.1195102\ttotal: 23.9s\tremaining: 1m 17s\n",
      "235:\tlearn: 0.1194351\ttotal: 24s\tremaining: 1m 17s\n",
      "236:\tlearn: 0.1193870\ttotal: 24.1s\tremaining: 1m 17s\n",
      "237:\tlearn: 0.1192903\ttotal: 24.2s\tremaining: 1m 17s\n",
      "238:\tlearn: 0.1191917\ttotal: 24.3s\tremaining: 1m 17s\n",
      "239:\tlearn: 0.1191190\ttotal: 24.4s\tremaining: 1m 17s\n",
      "240:\tlearn: 0.1190360\ttotal: 24.5s\tremaining: 1m 17s\n",
      "241:\tlearn: 0.1189667\ttotal: 24.6s\tremaining: 1m 17s\n",
      "242:\tlearn: 0.1188091\ttotal: 24.7s\tremaining: 1m 17s\n",
      "243:\tlearn: 0.1186939\ttotal: 24.8s\tremaining: 1m 16s\n",
      "244:\tlearn: 0.1186246\ttotal: 24.9s\tremaining: 1m 16s\n",
      "245:\tlearn: 0.1185713\ttotal: 25s\tremaining: 1m 16s\n",
      "246:\tlearn: 0.1184642\ttotal: 25.2s\tremaining: 1m 16s\n",
      "247:\tlearn: 0.1183165\ttotal: 25.3s\tremaining: 1m 16s\n",
      "248:\tlearn: 0.1181996\ttotal: 25.4s\tremaining: 1m 16s\n",
      "249:\tlearn: 0.1181496\ttotal: 25.5s\tremaining: 1m 16s\n",
      "250:\tlearn: 0.1180268\ttotal: 25.6s\tremaining: 1m 16s\n",
      "251:\tlearn: 0.1179813\ttotal: 25.7s\tremaining: 1m 16s\n",
      "252:\tlearn: 0.1179278\ttotal: 25.8s\tremaining: 1m 16s\n",
      "253:\tlearn: 0.1178308\ttotal: 25.9s\tremaining: 1m 16s\n",
      "254:\tlearn: 0.1177507\ttotal: 26s\tremaining: 1m 15s\n",
      "255:\tlearn: 0.1176785\ttotal: 26.1s\tremaining: 1m 15s\n",
      "256:\tlearn: 0.1176023\ttotal: 26.2s\tremaining: 1m 15s\n",
      "257:\tlearn: 0.1175053\ttotal: 26.3s\tremaining: 1m 15s\n",
      "258:\tlearn: 0.1173955\ttotal: 26.4s\tremaining: 1m 15s\n",
      "259:\tlearn: 0.1172984\ttotal: 26.5s\tremaining: 1m 15s\n",
      "260:\tlearn: 0.1172130\ttotal: 26.6s\tremaining: 1m 15s\n",
      "261:\tlearn: 0.1171593\ttotal: 26.7s\tremaining: 1m 15s\n",
      "262:\tlearn: 0.1171059\ttotal: 26.8s\tremaining: 1m 15s\n",
      "263:\tlearn: 0.1169691\ttotal: 26.9s\tremaining: 1m 14s\n",
      "264:\tlearn: 0.1168836\ttotal: 27s\tremaining: 1m 14s\n",
      "265:\tlearn: 0.1168102\ttotal: 27.1s\tremaining: 1m 14s\n",
      "266:\tlearn: 0.1167102\ttotal: 27.2s\tremaining: 1m 14s\n",
      "267:\tlearn: 0.1166016\ttotal: 27.5s\tremaining: 1m 15s\n",
      "268:\tlearn: 0.1165584\ttotal: 27.6s\tremaining: 1m 14s\n",
      "269:\tlearn: 0.1164041\ttotal: 27.7s\tremaining: 1m 14s\n",
      "270:\tlearn: 0.1163614\ttotal: 27.8s\tremaining: 1m 14s\n",
      "271:\tlearn: 0.1163034\ttotal: 27.9s\tremaining: 1m 14s\n",
      "272:\tlearn: 0.1162619\ttotal: 28s\tremaining: 1m 14s\n",
      "273:\tlearn: 0.1162052\ttotal: 28.1s\tremaining: 1m 14s\n",
      "274:\tlearn: 0.1161460\ttotal: 28.2s\tremaining: 1m 14s\n",
      "275:\tlearn: 0.1160980\ttotal: 28.3s\tremaining: 1m 14s\n",
      "276:\tlearn: 0.1159902\ttotal: 28.4s\tremaining: 1m 14s\n",
      "277:\tlearn: 0.1158668\ttotal: 28.6s\tremaining: 1m 14s\n",
      "278:\tlearn: 0.1158112\ttotal: 28.7s\tremaining: 1m 14s\n",
      "279:\tlearn: 0.1156830\ttotal: 28.8s\tremaining: 1m 14s\n",
      "280:\tlearn: 0.1156241\ttotal: 28.9s\tremaining: 1m 13s\n",
      "281:\tlearn: 0.1155715\ttotal: 29s\tremaining: 1m 13s\n",
      "282:\tlearn: 0.1154883\ttotal: 29.1s\tremaining: 1m 13s\n",
      "283:\tlearn: 0.1154318\ttotal: 29.2s\tremaining: 1m 13s\n",
      "284:\tlearn: 0.1153323\ttotal: 29.3s\tremaining: 1m 13s\n",
      "285:\tlearn: 0.1152554\ttotal: 29.5s\tremaining: 1m 13s\n",
      "286:\tlearn: 0.1151541\ttotal: 29.7s\tremaining: 1m 13s\n",
      "287:\tlearn: 0.1150950\ttotal: 29.8s\tremaining: 1m 13s\n",
      "288:\tlearn: 0.1149792\ttotal: 29.9s\tremaining: 1m 13s\n",
      "289:\tlearn: 0.1149214\ttotal: 30s\tremaining: 1m 13s\n",
      "290:\tlearn: 0.1148017\ttotal: 30.2s\tremaining: 1m 13s\n",
      "291:\tlearn: 0.1147290\ttotal: 30.3s\tremaining: 1m 13s\n",
      "292:\tlearn: 0.1146816\ttotal: 30.4s\tremaining: 1m 13s\n",
      "293:\tlearn: 0.1146140\ttotal: 30.5s\tremaining: 1m 13s\n",
      "294:\tlearn: 0.1145324\ttotal: 30.6s\tremaining: 1m 13s\n",
      "295:\tlearn: 0.1144643\ttotal: 30.7s\tremaining: 1m 13s\n",
      "296:\tlearn: 0.1144124\ttotal: 30.8s\tremaining: 1m 12s\n",
      "297:\tlearn: 0.1143456\ttotal: 30.9s\tremaining: 1m 12s\n",
      "298:\tlearn: 0.1142568\ttotal: 31s\tremaining: 1m 12s\n",
      "299:\tlearn: 0.1142061\ttotal: 31.1s\tremaining: 1m 12s\n",
      "300:\tlearn: 0.1141308\ttotal: 31.2s\tremaining: 1m 12s\n",
      "301:\tlearn: 0.1140720\ttotal: 31.3s\tremaining: 1m 12s\n",
      "302:\tlearn: 0.1139677\ttotal: 31.4s\tremaining: 1m 12s\n",
      "303:\tlearn: 0.1138966\ttotal: 31.5s\tremaining: 1m 12s\n",
      "304:\tlearn: 0.1138539\ttotal: 31.6s\tremaining: 1m 11s\n",
      "305:\tlearn: 0.1137663\ttotal: 31.7s\tremaining: 1m 11s\n",
      "306:\tlearn: 0.1136827\ttotal: 31.8s\tremaining: 1m 11s\n",
      "307:\tlearn: 0.1136191\ttotal: 31.9s\tremaining: 1m 11s\n",
      "308:\tlearn: 0.1135369\ttotal: 32s\tremaining: 1m 11s\n",
      "309:\tlearn: 0.1134610\ttotal: 32.1s\tremaining: 1m 11s\n",
      "310:\tlearn: 0.1133372\ttotal: 32.2s\tremaining: 1m 11s\n",
      "311:\tlearn: 0.1132885\ttotal: 32.3s\tremaining: 1m 11s\n",
      "312:\tlearn: 0.1132097\ttotal: 32.4s\tremaining: 1m 11s\n",
      "313:\tlearn: 0.1131379\ttotal: 32.5s\tremaining: 1m 11s\n",
      "314:\tlearn: 0.1130884\ttotal: 32.6s\tremaining: 1m 10s\n",
      "315:\tlearn: 0.1130160\ttotal: 32.7s\tremaining: 1m 10s\n",
      "316:\tlearn: 0.1129398\ttotal: 32.8s\tremaining: 1m 10s\n",
      "317:\tlearn: 0.1128060\ttotal: 32.9s\tremaining: 1m 10s\n",
      "318:\tlearn: 0.1127520\ttotal: 33s\tremaining: 1m 10s\n",
      "319:\tlearn: 0.1126874\ttotal: 33.1s\tremaining: 1m 10s\n",
      "320:\tlearn: 0.1125919\ttotal: 33.2s\tremaining: 1m 10s\n",
      "321:\tlearn: 0.1125258\ttotal: 33.3s\tremaining: 1m 10s\n",
      "322:\tlearn: 0.1123926\ttotal: 33.4s\tremaining: 1m 10s\n",
      "323:\tlearn: 0.1123490\ttotal: 33.5s\tremaining: 1m 9s\n",
      "324:\tlearn: 0.1122619\ttotal: 33.6s\tremaining: 1m 9s\n",
      "325:\tlearn: 0.1121595\ttotal: 33.8s\tremaining: 1m 9s\n",
      "326:\tlearn: 0.1121187\ttotal: 33.8s\tremaining: 1m 9s\n",
      "327:\tlearn: 0.1120610\ttotal: 33.9s\tremaining: 1m 9s\n",
      "328:\tlearn: 0.1120146\ttotal: 34s\tremaining: 1m 9s\n",
      "329:\tlearn: 0.1119559\ttotal: 34.1s\tremaining: 1m 9s\n",
      "330:\tlearn: 0.1119161\ttotal: 34.2s\tremaining: 1m 9s\n",
      "331:\tlearn: 0.1118606\ttotal: 34.3s\tremaining: 1m 9s\n",
      "332:\tlearn: 0.1118120\ttotal: 34.4s\tremaining: 1m 8s\n",
      "333:\tlearn: 0.1116933\ttotal: 34.5s\tremaining: 1m 8s\n",
      "334:\tlearn: 0.1115994\ttotal: 34.6s\tremaining: 1m 8s\n",
      "335:\tlearn: 0.1115390\ttotal: 34.7s\tremaining: 1m 8s\n",
      "336:\tlearn: 0.1114746\ttotal: 34.8s\tremaining: 1m 8s\n",
      "337:\tlearn: 0.1114000\ttotal: 35s\tremaining: 1m 8s\n",
      "338:\tlearn: 0.1113520\ttotal: 35.1s\tremaining: 1m 8s\n",
      "339:\tlearn: 0.1113132\ttotal: 35.1s\tremaining: 1m 8s\n",
      "340:\tlearn: 0.1112665\ttotal: 35.4s\tremaining: 1m 8s\n",
      "341:\tlearn: 0.1111996\ttotal: 35.5s\tremaining: 1m 8s\n",
      "342:\tlearn: 0.1111083\ttotal: 35.6s\tremaining: 1m 8s\n",
      "343:\tlearn: 0.1109961\ttotal: 35.7s\tremaining: 1m 7s\n",
      "344:\tlearn: 0.1109102\ttotal: 35.7s\tremaining: 1m 7s\n",
      "345:\tlearn: 0.1108340\ttotal: 35.8s\tremaining: 1m 7s\n",
      "346:\tlearn: 0.1107933\ttotal: 35.9s\tremaining: 1m 7s\n",
      "347:\tlearn: 0.1107562\ttotal: 36s\tremaining: 1m 7s\n",
      "348:\tlearn: 0.1107027\ttotal: 36.1s\tremaining: 1m 7s\n",
      "349:\tlearn: 0.1106319\ttotal: 36.2s\tremaining: 1m 7s\n",
      "350:\tlearn: 0.1106017\ttotal: 36.3s\tremaining: 1m 7s\n",
      "351:\tlearn: 0.1105413\ttotal: 36.4s\tremaining: 1m 7s\n",
      "352:\tlearn: 0.1104976\ttotal: 36.5s\tremaining: 1m 6s\n",
      "353:\tlearn: 0.1104406\ttotal: 36.6s\tremaining: 1m 6s\n",
      "354:\tlearn: 0.1103206\ttotal: 36.7s\tremaining: 1m 6s\n",
      "355:\tlearn: 0.1102795\ttotal: 36.8s\tremaining: 1m 6s\n",
      "356:\tlearn: 0.1102216\ttotal: 36.9s\tremaining: 1m 6s\n",
      "357:\tlearn: 0.1101729\ttotal: 37s\tremaining: 1m 6s\n",
      "358:\tlearn: 0.1101206\ttotal: 37.1s\tremaining: 1m 6s\n",
      "359:\tlearn: 0.1100653\ttotal: 37.2s\tremaining: 1m 6s\n",
      "360:\tlearn: 0.1099345\ttotal: 37.3s\tremaining: 1m 5s\n",
      "361:\tlearn: 0.1098405\ttotal: 37.4s\tremaining: 1m 5s\n",
      "362:\tlearn: 0.1097425\ttotal: 37.5s\tremaining: 1m 5s\n",
      "363:\tlearn: 0.1096913\ttotal: 37.6s\tremaining: 1m 5s\n",
      "364:\tlearn: 0.1096190\ttotal: 37.7s\tremaining: 1m 5s\n",
      "365:\tlearn: 0.1095766\ttotal: 37.8s\tremaining: 1m 5s\n",
      "366:\tlearn: 0.1095159\ttotal: 37.9s\tremaining: 1m 5s\n",
      "367:\tlearn: 0.1094287\ttotal: 38s\tremaining: 1m 5s\n",
      "368:\tlearn: 0.1093722\ttotal: 38s\tremaining: 1m 5s\n",
      "369:\tlearn: 0.1092738\ttotal: 38.1s\tremaining: 1m 4s\n",
      "370:\tlearn: 0.1092311\ttotal: 38.2s\tremaining: 1m 4s\n",
      "371:\tlearn: 0.1091775\ttotal: 38.3s\tremaining: 1m 4s\n",
      "372:\tlearn: 0.1091402\ttotal: 38.4s\tremaining: 1m 4s\n",
      "373:\tlearn: 0.1090840\ttotal: 38.5s\tremaining: 1m 4s\n",
      "374:\tlearn: 0.1090522\ttotal: 38.6s\tremaining: 1m 4s\n",
      "375:\tlearn: 0.1089735\ttotal: 38.7s\tremaining: 1m 4s\n",
      "376:\tlearn: 0.1088690\ttotal: 38.8s\tremaining: 1m 4s\n",
      "377:\tlearn: 0.1088153\ttotal: 38.9s\tremaining: 1m 4s\n",
      "378:\tlearn: 0.1087550\ttotal: 39.1s\tremaining: 1m 4s\n",
      "379:\tlearn: 0.1086844\ttotal: 39.2s\tremaining: 1m 3s\n",
      "380:\tlearn: 0.1086224\ttotal: 39.3s\tremaining: 1m 3s\n",
      "381:\tlearn: 0.1085290\ttotal: 39.4s\tremaining: 1m 3s\n",
      "382:\tlearn: 0.1084175\ttotal: 39.5s\tremaining: 1m 3s\n",
      "383:\tlearn: 0.1083730\ttotal: 39.6s\tremaining: 1m 3s\n",
      "384:\tlearn: 0.1083370\ttotal: 39.7s\tremaining: 1m 3s\n",
      "385:\tlearn: 0.1082561\ttotal: 39.8s\tremaining: 1m 3s\n",
      "386:\tlearn: 0.1082050\ttotal: 39.9s\tremaining: 1m 3s\n",
      "387:\tlearn: 0.1081321\ttotal: 40s\tremaining: 1m 3s\n",
      "388:\tlearn: 0.1080594\ttotal: 40s\tremaining: 1m 2s\n",
      "389:\tlearn: 0.1080021\ttotal: 40.1s\tremaining: 1m 2s\n",
      "390:\tlearn: 0.1079459\ttotal: 40.2s\tremaining: 1m 2s\n",
      "391:\tlearn: 0.1078878\ttotal: 40.3s\tremaining: 1m 2s\n",
      "392:\tlearn: 0.1078359\ttotal: 40.4s\tremaining: 1m 2s\n",
      "393:\tlearn: 0.1077565\ttotal: 40.5s\tremaining: 1m 2s\n",
      "394:\tlearn: 0.1076956\ttotal: 40.6s\tremaining: 1m 2s\n",
      "395:\tlearn: 0.1076372\ttotal: 40.7s\tremaining: 1m 2s\n",
      "396:\tlearn: 0.1075912\ttotal: 40.8s\tremaining: 1m 2s\n",
      "397:\tlearn: 0.1075188\ttotal: 40.9s\tremaining: 1m 1s\n",
      "398:\tlearn: 0.1074547\ttotal: 41s\tremaining: 1m 1s\n",
      "399:\tlearn: 0.1074055\ttotal: 41.1s\tremaining: 1m 1s\n",
      "400:\tlearn: 0.1073740\ttotal: 41.2s\tremaining: 1m 1s\n",
      "401:\tlearn: 0.1073353\ttotal: 41.3s\tremaining: 1m 1s\n",
      "402:\tlearn: 0.1072981\ttotal: 41.4s\tremaining: 1m 1s\n",
      "403:\tlearn: 0.1072322\ttotal: 41.5s\tremaining: 1m 1s\n",
      "404:\tlearn: 0.1072032\ttotal: 41.6s\tremaining: 1m 1s\n",
      "405:\tlearn: 0.1070845\ttotal: 41.7s\tremaining: 1m\n",
      "406:\tlearn: 0.1069629\ttotal: 41.8s\tremaining: 1m\n",
      "407:\tlearn: 0.1069156\ttotal: 41.9s\tremaining: 1m\n",
      "408:\tlearn: 0.1068596\ttotal: 42s\tremaining: 1m\n",
      "409:\tlearn: 0.1067911\ttotal: 42s\tremaining: 1m\n",
      "410:\tlearn: 0.1067102\ttotal: 42.1s\tremaining: 1m\n",
      "411:\tlearn: 0.1066133\ttotal: 42.2s\tremaining: 1m\n",
      "412:\tlearn: 0.1065311\ttotal: 42.3s\tremaining: 1m\n",
      "413:\tlearn: 0.1064668\ttotal: 42.4s\tremaining: 1m\n",
      "414:\tlearn: 0.1064219\ttotal: 42.5s\tremaining: 60s\n",
      "415:\tlearn: 0.1063630\ttotal: 42.6s\tremaining: 59.9s\n",
      "416:\tlearn: 0.1063236\ttotal: 42.7s\tremaining: 59.7s\n",
      "417:\tlearn: 0.1062441\ttotal: 42.8s\tremaining: 59.6s\n",
      "418:\tlearn: 0.1060964\ttotal: 42.9s\tremaining: 59.5s\n",
      "419:\tlearn: 0.1060085\ttotal: 43s\tremaining: 59.4s\n",
      "420:\tlearn: 0.1059371\ttotal: 43.1s\tremaining: 59.3s\n",
      "421:\tlearn: 0.1058656\ttotal: 43.2s\tremaining: 59.2s\n",
      "422:\tlearn: 0.1058231\ttotal: 43.3s\tremaining: 59.1s\n",
      "423:\tlearn: 0.1057320\ttotal: 43.4s\tremaining: 59s\n",
      "424:\tlearn: 0.1056566\ttotal: 43.6s\tremaining: 58.9s\n",
      "425:\tlearn: 0.1055982\ttotal: 43.7s\tremaining: 58.8s\n",
      "426:\tlearn: 0.1055493\ttotal: 43.8s\tremaining: 58.7s\n",
      "427:\tlearn: 0.1055111\ttotal: 43.8s\tremaining: 58.6s\n",
      "428:\tlearn: 0.1054509\ttotal: 43.9s\tremaining: 58.5s\n",
      "429:\tlearn: 0.1053899\ttotal: 44s\tremaining: 58.4s\n",
      "430:\tlearn: 0.1053627\ttotal: 44.1s\tremaining: 58.3s\n",
      "431:\tlearn: 0.1052984\ttotal: 44.2s\tremaining: 58.2s\n",
      "432:\tlearn: 0.1052237\ttotal: 44.3s\tremaining: 58s\n",
      "433:\tlearn: 0.1051998\ttotal: 44.4s\tremaining: 57.9s\n",
      "434:\tlearn: 0.1051745\ttotal: 44.5s\tremaining: 57.8s\n",
      "435:\tlearn: 0.1050984\ttotal: 44.6s\tremaining: 57.7s\n",
      "436:\tlearn: 0.1050525\ttotal: 44.7s\tremaining: 57.6s\n",
      "437:\tlearn: 0.1050192\ttotal: 44.8s\tremaining: 57.5s\n",
      "438:\tlearn: 0.1049298\ttotal: 44.9s\tremaining: 57.4s\n",
      "439:\tlearn: 0.1048939\ttotal: 45s\tremaining: 57.3s\n",
      "440:\tlearn: 0.1048248\ttotal: 45.1s\tremaining: 57.2s\n",
      "441:\tlearn: 0.1047913\ttotal: 45.2s\tremaining: 57.1s\n",
      "442:\tlearn: 0.1047529\ttotal: 45.3s\tremaining: 57s\n",
      "443:\tlearn: 0.1046771\ttotal: 45.4s\tremaining: 56.9s\n",
      "444:\tlearn: 0.1046207\ttotal: 45.5s\tremaining: 56.8s\n",
      "445:\tlearn: 0.1044995\ttotal: 45.6s\tremaining: 56.7s\n",
      "446:\tlearn: 0.1044582\ttotal: 45.7s\tremaining: 56.5s\n",
      "447:\tlearn: 0.1044027\ttotal: 45.8s\tremaining: 56.4s\n",
      "448:\tlearn: 0.1043562\ttotal: 45.9s\tremaining: 56.4s\n",
      "449:\tlearn: 0.1042889\ttotal: 46.1s\tremaining: 56.4s\n",
      "450:\tlearn: 0.1042313\ttotal: 46.2s\tremaining: 56.3s\n",
      "451:\tlearn: 0.1041694\ttotal: 46.3s\tremaining: 56.1s\n",
      "452:\tlearn: 0.1040984\ttotal: 46.4s\tremaining: 56s\n",
      "453:\tlearn: 0.1040385\ttotal: 46.5s\tremaining: 55.9s\n",
      "454:\tlearn: 0.1039949\ttotal: 46.6s\tremaining: 55.8s\n",
      "455:\tlearn: 0.1038895\ttotal: 46.7s\tremaining: 55.7s\n",
      "456:\tlearn: 0.1038325\ttotal: 46.8s\tremaining: 55.6s\n",
      "457:\tlearn: 0.1037561\ttotal: 46.9s\tremaining: 55.5s\n",
      "458:\tlearn: 0.1037000\ttotal: 47s\tremaining: 55.4s\n",
      "459:\tlearn: 0.1036289\ttotal: 47.1s\tremaining: 55.3s\n",
      "460:\tlearn: 0.1035799\ttotal: 47.2s\tremaining: 55.2s\n",
      "461:\tlearn: 0.1035177\ttotal: 47.3s\tremaining: 55.1s\n",
      "462:\tlearn: 0.1034864\ttotal: 47.4s\tremaining: 55s\n",
      "463:\tlearn: 0.1034590\ttotal: 47.5s\tremaining: 54.9s\n",
      "464:\tlearn: 0.1033898\ttotal: 47.6s\tremaining: 54.7s\n",
      "465:\tlearn: 0.1033042\ttotal: 47.7s\tremaining: 54.6s\n",
      "466:\tlearn: 0.1032103\ttotal: 47.8s\tremaining: 54.5s\n",
      "467:\tlearn: 0.1031351\ttotal: 47.9s\tremaining: 54.4s\n",
      "468:\tlearn: 0.1030860\ttotal: 48s\tremaining: 54.3s\n",
      "469:\tlearn: 0.1030631\ttotal: 48.1s\tremaining: 54.2s\n",
      "470:\tlearn: 0.1030259\ttotal: 48.2s\tremaining: 54.1s\n",
      "471:\tlearn: 0.1029938\ttotal: 48.3s\tremaining: 54s\n",
      "472:\tlearn: 0.1028833\ttotal: 48.4s\tremaining: 53.9s\n",
      "473:\tlearn: 0.1028097\ttotal: 48.5s\tremaining: 53.8s\n",
      "474:\tlearn: 0.1027862\ttotal: 48.6s\tremaining: 53.7s\n",
      "475:\tlearn: 0.1027162\ttotal: 48.7s\tremaining: 53.6s\n",
      "476:\tlearn: 0.1026755\ttotal: 48.8s\tremaining: 53.5s\n",
      "477:\tlearn: 0.1026549\ttotal: 48.9s\tremaining: 53.4s\n",
      "478:\tlearn: 0.1026251\ttotal: 49s\tremaining: 53.3s\n",
      "479:\tlearn: 0.1025878\ttotal: 49.1s\tremaining: 53.2s\n",
      "480:\tlearn: 0.1025361\ttotal: 49.2s\tremaining: 53.1s\n",
      "481:\tlearn: 0.1024861\ttotal: 49.3s\tremaining: 53s\n",
      "482:\tlearn: 0.1024437\ttotal: 49.4s\tremaining: 52.9s\n",
      "483:\tlearn: 0.1024066\ttotal: 49.5s\tremaining: 52.7s\n",
      "484:\tlearn: 0.1023713\ttotal: 49.6s\tremaining: 52.6s\n",
      "485:\tlearn: 0.1023109\ttotal: 49.7s\tremaining: 52.5s\n",
      "486:\tlearn: 0.1022574\ttotal: 49.8s\tremaining: 52.4s\n",
      "487:\tlearn: 0.1022163\ttotal: 49.9s\tremaining: 52.3s\n",
      "488:\tlearn: 0.1021451\ttotal: 50s\tremaining: 52.2s\n",
      "489:\tlearn: 0.1020817\ttotal: 50.1s\tremaining: 52.1s\n",
      "490:\tlearn: 0.1020182\ttotal: 50.2s\tremaining: 52s\n",
      "491:\tlearn: 0.1019708\ttotal: 50.3s\tremaining: 51.9s\n",
      "492:\tlearn: 0.1019123\ttotal: 50.4s\tremaining: 51.8s\n",
      "493:\tlearn: 0.1018660\ttotal: 50.5s\tremaining: 51.7s\n",
      "494:\tlearn: 0.1017651\ttotal: 50.6s\tremaining: 51.6s\n",
      "495:\tlearn: 0.1017090\ttotal: 50.7s\tremaining: 51.5s\n",
      "496:\tlearn: 0.1016331\ttotal: 50.8s\tremaining: 51.4s\n",
      "497:\tlearn: 0.1015746\ttotal: 50.9s\tremaining: 51.3s\n",
      "498:\tlearn: 0.1014801\ttotal: 51s\tremaining: 51.2s\n",
      "499:\tlearn: 0.1014184\ttotal: 51.1s\tremaining: 51.1s\n",
      "500:\tlearn: 0.1013925\ttotal: 51.2s\tremaining: 51s\n",
      "501:\tlearn: 0.1013701\ttotal: 51.3s\tremaining: 50.9s\n",
      "502:\tlearn: 0.1012575\ttotal: 51.4s\tremaining: 50.8s\n",
      "503:\tlearn: 0.1011925\ttotal: 51.5s\tremaining: 50.7s\n",
      "504:\tlearn: 0.1011324\ttotal: 51.6s\tremaining: 50.6s\n",
      "505:\tlearn: 0.1010925\ttotal: 51.7s\tremaining: 50.5s\n",
      "506:\tlearn: 0.1010333\ttotal: 51.8s\tremaining: 50.3s\n",
      "507:\tlearn: 0.1009471\ttotal: 51.9s\tremaining: 50.3s\n",
      "508:\tlearn: 0.1008977\ttotal: 52s\tremaining: 50.2s\n",
      "509:\tlearn: 0.1008496\ttotal: 52.1s\tremaining: 50.1s\n",
      "510:\tlearn: 0.1007901\ttotal: 52.2s\tremaining: 50s\n",
      "511:\tlearn: 0.1007600\ttotal: 52.3s\tremaining: 49.8s\n",
      "512:\tlearn: 0.1007084\ttotal: 52.4s\tremaining: 49.7s\n",
      "513:\tlearn: 0.1006685\ttotal: 52.5s\tremaining: 49.6s\n",
      "514:\tlearn: 0.1006464\ttotal: 52.6s\tremaining: 49.5s\n",
      "515:\tlearn: 0.1005762\ttotal: 52.7s\tremaining: 49.4s\n",
      "516:\tlearn: 0.1005382\ttotal: 52.8s\tremaining: 49.3s\n",
      "517:\tlearn: 0.1004418\ttotal: 53s\tremaining: 49.3s\n",
      "518:\tlearn: 0.1004000\ttotal: 53.1s\tremaining: 49.2s\n",
      "519:\tlearn: 0.1003716\ttotal: 53.2s\tremaining: 49.1s\n",
      "520:\tlearn: 0.1003344\ttotal: 53.3s\tremaining: 49s\n",
      "521:\tlearn: 0.1002352\ttotal: 53.4s\tremaining: 48.9s\n",
      "522:\tlearn: 0.1001807\ttotal: 53.5s\tremaining: 48.8s\n",
      "523:\tlearn: 0.1001111\ttotal: 53.6s\tremaining: 48.7s\n",
      "524:\tlearn: 0.1000516\ttotal: 53.7s\tremaining: 48.6s\n",
      "525:\tlearn: 0.1000196\ttotal: 53.8s\tremaining: 48.5s\n",
      "526:\tlearn: 0.0999298\ttotal: 53.9s\tremaining: 48.3s\n",
      "527:\tlearn: 0.0998698\ttotal: 54s\tremaining: 48.2s\n",
      "528:\tlearn: 0.0998429\ttotal: 54.1s\tremaining: 48.1s\n",
      "529:\tlearn: 0.0998284\ttotal: 54.2s\tremaining: 48s\n",
      "530:\tlearn: 0.0997813\ttotal: 54.3s\tremaining: 47.9s\n",
      "531:\tlearn: 0.0997484\ttotal: 54.3s\tremaining: 47.8s\n",
      "532:\tlearn: 0.0997146\ttotal: 54.4s\tremaining: 47.7s\n",
      "533:\tlearn: 0.0996414\ttotal: 54.5s\tremaining: 47.6s\n",
      "534:\tlearn: 0.0995815\ttotal: 54.6s\tremaining: 47.5s\n",
      "535:\tlearn: 0.0995382\ttotal: 54.7s\tremaining: 47.4s\n",
      "536:\tlearn: 0.0994989\ttotal: 54.8s\tremaining: 47.3s\n",
      "537:\tlearn: 0.0994539\ttotal: 54.9s\tremaining: 47.2s\n",
      "538:\tlearn: 0.0994183\ttotal: 55s\tremaining: 47.1s\n",
      "539:\tlearn: 0.0993571\ttotal: 55.1s\tremaining: 46.9s\n",
      "540:\tlearn: 0.0993016\ttotal: 55.2s\tremaining: 46.8s\n",
      "541:\tlearn: 0.0992686\ttotal: 55.3s\tremaining: 46.7s\n",
      "542:\tlearn: 0.0992401\ttotal: 55.4s\tremaining: 46.6s\n",
      "543:\tlearn: 0.0992287\ttotal: 55.5s\tremaining: 46.5s\n",
      "544:\tlearn: 0.0991902\ttotal: 55.6s\tremaining: 46.4s\n",
      "545:\tlearn: 0.0991833\ttotal: 55.7s\tremaining: 46.3s\n",
      "546:\tlearn: 0.0991040\ttotal: 55.8s\tremaining: 46.2s\n",
      "547:\tlearn: 0.0990838\ttotal: 55.9s\tremaining: 46.1s\n",
      "548:\tlearn: 0.0990497\ttotal: 55.9s\tremaining: 46s\n",
      "549:\tlearn: 0.0989911\ttotal: 56s\tremaining: 45.8s\n",
      "550:\tlearn: 0.0989180\ttotal: 56.1s\tremaining: 45.7s\n",
      "551:\tlearn: 0.0988747\ttotal: 56.2s\tremaining: 45.6s\n",
      "552:\tlearn: 0.0988080\ttotal: 56.3s\tremaining: 45.5s\n",
      "553:\tlearn: 0.0987348\ttotal: 56.4s\tremaining: 45.4s\n",
      "554:\tlearn: 0.0986941\ttotal: 56.5s\tremaining: 45.3s\n",
      "555:\tlearn: 0.0986555\ttotal: 56.6s\tremaining: 45.2s\n",
      "556:\tlearn: 0.0985957\ttotal: 56.7s\tremaining: 45.1s\n",
      "557:\tlearn: 0.0985564\ttotal: 56.8s\tremaining: 45s\n",
      "558:\tlearn: 0.0984865\ttotal: 56.9s\tremaining: 44.9s\n",
      "559:\tlearn: 0.0984425\ttotal: 57s\tremaining: 44.8s\n",
      "560:\tlearn: 0.0983452\ttotal: 57.1s\tremaining: 44.7s\n",
      "561:\tlearn: 0.0983170\ttotal: 57.2s\tremaining: 44.6s\n",
      "562:\tlearn: 0.0982325\ttotal: 57.3s\tremaining: 44.5s\n",
      "563:\tlearn: 0.0981875\ttotal: 57.4s\tremaining: 44.4s\n",
      "564:\tlearn: 0.0981615\ttotal: 57.5s\tremaining: 44.3s\n",
      "565:\tlearn: 0.0981176\ttotal: 57.6s\tremaining: 44.2s\n",
      "566:\tlearn: 0.0980736\ttotal: 57.7s\tremaining: 44.1s\n",
      "567:\tlearn: 0.0980360\ttotal: 57.8s\tremaining: 44s\n",
      "568:\tlearn: 0.0979939\ttotal: 57.9s\tremaining: 43.9s\n",
      "569:\tlearn: 0.0979679\ttotal: 58s\tremaining: 43.8s\n",
      "570:\tlearn: 0.0978865\ttotal: 58.1s\tremaining: 43.7s\n",
      "571:\tlearn: 0.0978285\ttotal: 58.2s\tremaining: 43.6s\n",
      "572:\tlearn: 0.0977676\ttotal: 58.3s\tremaining: 43.5s\n",
      "573:\tlearn: 0.0977309\ttotal: 58.4s\tremaining: 43.4s\n",
      "574:\tlearn: 0.0977017\ttotal: 58.5s\tremaining: 43.3s\n",
      "575:\tlearn: 0.0976539\ttotal: 58.6s\tremaining: 43.1s\n",
      "576:\tlearn: 0.0976037\ttotal: 58.7s\tremaining: 43s\n",
      "577:\tlearn: 0.0975717\ttotal: 58.8s\tremaining: 42.9s\n",
      "578:\tlearn: 0.0975332\ttotal: 58.9s\tremaining: 42.8s\n",
      "579:\tlearn: 0.0974218\ttotal: 59s\tremaining: 42.7s\n",
      "580:\tlearn: 0.0973910\ttotal: 59.1s\tremaining: 42.6s\n",
      "581:\tlearn: 0.0973398\ttotal: 59.2s\tremaining: 42.5s\n",
      "582:\tlearn: 0.0972992\ttotal: 59.3s\tremaining: 42.4s\n",
      "583:\tlearn: 0.0972528\ttotal: 59.4s\tremaining: 42.3s\n",
      "584:\tlearn: 0.0971898\ttotal: 59.5s\tremaining: 42.2s\n",
      "585:\tlearn: 0.0971315\ttotal: 59.6s\tremaining: 42.1s\n",
      "586:\tlearn: 0.0970943\ttotal: 59.7s\tremaining: 42s\n",
      "587:\tlearn: 0.0970660\ttotal: 59.7s\tremaining: 41.9s\n",
      "588:\tlearn: 0.0969891\ttotal: 59.8s\tremaining: 41.8s\n",
      "589:\tlearn: 0.0969551\ttotal: 59.9s\tremaining: 41.7s\n",
      "590:\tlearn: 0.0968932\ttotal: 1m\tremaining: 41.6s\n",
      "591:\tlearn: 0.0968445\ttotal: 1m\tremaining: 41.4s\n",
      "592:\tlearn: 0.0967952\ttotal: 1m\tremaining: 41.3s\n",
      "593:\tlearn: 0.0967010\ttotal: 1m\tremaining: 41.2s\n",
      "594:\tlearn: 0.0966325\ttotal: 1m\tremaining: 41.1s\n",
      "595:\tlearn: 0.0965786\ttotal: 1m\tremaining: 41s\n",
      "596:\tlearn: 0.0965195\ttotal: 1m\tremaining: 40.9s\n",
      "597:\tlearn: 0.0964636\ttotal: 1m\tremaining: 40.8s\n",
      "598:\tlearn: 0.0964237\ttotal: 1m\tremaining: 40.7s\n",
      "599:\tlearn: 0.0963890\ttotal: 1m\tremaining: 40.6s\n",
      "600:\tlearn: 0.0963449\ttotal: 1m\tremaining: 40.5s\n",
      "601:\tlearn: 0.0962687\ttotal: 1m 1s\tremaining: 40.4s\n",
      "602:\tlearn: 0.0962201\ttotal: 1m 1s\tremaining: 40.3s\n",
      "603:\tlearn: 0.0961486\ttotal: 1m 1s\tremaining: 40.2s\n",
      "604:\tlearn: 0.0960690\ttotal: 1m 1s\tremaining: 40.1s\n",
      "605:\tlearn: 0.0960303\ttotal: 1m 1s\tremaining: 40s\n",
      "606:\tlearn: 0.0959689\ttotal: 1m 1s\tremaining: 39.9s\n",
      "607:\tlearn: 0.0959449\ttotal: 1m 1s\tremaining: 39.8s\n",
      "608:\tlearn: 0.0958971\ttotal: 1m 1s\tremaining: 39.7s\n",
      "609:\tlearn: 0.0958782\ttotal: 1m 1s\tremaining: 39.5s\n",
      "610:\tlearn: 0.0958490\ttotal: 1m 1s\tremaining: 39.4s\n",
      "611:\tlearn: 0.0958011\ttotal: 1m 2s\tremaining: 39.3s\n",
      "612:\tlearn: 0.0957782\ttotal: 1m 2s\tremaining: 39.2s\n",
      "613:\tlearn: 0.0957257\ttotal: 1m 2s\tremaining: 39.1s\n",
      "614:\tlearn: 0.0956810\ttotal: 1m 2s\tremaining: 39s\n",
      "615:\tlearn: 0.0956620\ttotal: 1m 2s\tremaining: 38.9s\n",
      "616:\tlearn: 0.0955953\ttotal: 1m 2s\tremaining: 38.8s\n",
      "617:\tlearn: 0.0955726\ttotal: 1m 2s\tremaining: 38.7s\n",
      "618:\tlearn: 0.0955168\ttotal: 1m 2s\tremaining: 38.6s\n",
      "619:\tlearn: 0.0954812\ttotal: 1m 2s\tremaining: 38.5s\n",
      "620:\tlearn: 0.0954486\ttotal: 1m 2s\tremaining: 38.4s\n",
      "621:\tlearn: 0.0954225\ttotal: 1m 3s\tremaining: 38.3s\n",
      "622:\tlearn: 0.0953873\ttotal: 1m 3s\tremaining: 38.2s\n",
      "623:\tlearn: 0.0953148\ttotal: 1m 3s\tremaining: 38.1s\n",
      "624:\tlearn: 0.0952666\ttotal: 1m 3s\tremaining: 38s\n",
      "625:\tlearn: 0.0952421\ttotal: 1m 3s\tremaining: 37.9s\n",
      "626:\tlearn: 0.0952331\ttotal: 1m 3s\tremaining: 37.8s\n",
      "627:\tlearn: 0.0951834\ttotal: 1m 3s\tremaining: 37.7s\n",
      "628:\tlearn: 0.0951540\ttotal: 1m 3s\tremaining: 37.6s\n",
      "629:\tlearn: 0.0951020\ttotal: 1m 3s\tremaining: 37.5s\n",
      "630:\tlearn: 0.0950829\ttotal: 1m 3s\tremaining: 37.4s\n",
      "631:\tlearn: 0.0950474\ttotal: 1m 4s\tremaining: 37.3s\n",
      "632:\tlearn: 0.0949808\ttotal: 1m 4s\tremaining: 37.2s\n",
      "633:\tlearn: 0.0949523\ttotal: 1m 4s\tremaining: 37.1s\n",
      "634:\tlearn: 0.0948882\ttotal: 1m 4s\tremaining: 37s\n",
      "635:\tlearn: 0.0948299\ttotal: 1m 4s\tremaining: 36.9s\n",
      "636:\tlearn: 0.0947895\ttotal: 1m 4s\tremaining: 36.8s\n",
      "637:\tlearn: 0.0947588\ttotal: 1m 4s\tremaining: 36.7s\n",
      "638:\tlearn: 0.0947142\ttotal: 1m 4s\tremaining: 36.6s\n",
      "639:\tlearn: 0.0946905\ttotal: 1m 4s\tremaining: 36.5s\n",
      "640:\tlearn: 0.0946365\ttotal: 1m 4s\tremaining: 36.4s\n",
      "641:\tlearn: 0.0945863\ttotal: 1m 5s\tremaining: 36.3s\n",
      "642:\tlearn: 0.0945598\ttotal: 1m 5s\tremaining: 36.2s\n",
      "643:\tlearn: 0.0945232\ttotal: 1m 5s\tremaining: 36.1s\n",
      "644:\tlearn: 0.0944666\ttotal: 1m 5s\tremaining: 36s\n",
      "645:\tlearn: 0.0944137\ttotal: 1m 5s\tremaining: 35.9s\n",
      "646:\tlearn: 0.0943834\ttotal: 1m 5s\tremaining: 35.8s\n",
      "647:\tlearn: 0.0943440\ttotal: 1m 5s\tremaining: 35.6s\n",
      "648:\tlearn: 0.0943027\ttotal: 1m 5s\tremaining: 35.5s\n",
      "649:\tlearn: 0.0942741\ttotal: 1m 5s\tremaining: 35.4s\n",
      "650:\tlearn: 0.0942298\ttotal: 1m 5s\tremaining: 35.3s\n",
      "651:\tlearn: 0.0941972\ttotal: 1m 6s\tremaining: 35.2s\n",
      "652:\tlearn: 0.0941660\ttotal: 1m 6s\tremaining: 35.1s\n",
      "653:\tlearn: 0.0940963\ttotal: 1m 6s\tremaining: 35s\n",
      "654:\tlearn: 0.0940655\ttotal: 1m 6s\tremaining: 34.9s\n",
      "655:\tlearn: 0.0940313\ttotal: 1m 6s\tremaining: 34.8s\n",
      "656:\tlearn: 0.0939815\ttotal: 1m 6s\tremaining: 34.7s\n",
      "657:\tlearn: 0.0939295\ttotal: 1m 6s\tremaining: 34.6s\n",
      "658:\tlearn: 0.0938751\ttotal: 1m 6s\tremaining: 34.5s\n",
      "659:\tlearn: 0.0938327\ttotal: 1m 6s\tremaining: 34.4s\n",
      "660:\tlearn: 0.0937807\ttotal: 1m 6s\tremaining: 34.3s\n",
      "661:\tlearn: 0.0937565\ttotal: 1m 6s\tremaining: 34.2s\n",
      "662:\tlearn: 0.0936789\ttotal: 1m 7s\tremaining: 34.1s\n",
      "663:\tlearn: 0.0936515\ttotal: 1m 7s\tremaining: 34s\n",
      "664:\tlearn: 0.0936297\ttotal: 1m 7s\tremaining: 33.9s\n",
      "665:\tlearn: 0.0935856\ttotal: 1m 7s\tremaining: 33.8s\n",
      "666:\tlearn: 0.0935616\ttotal: 1m 7s\tremaining: 33.7s\n",
      "667:\tlearn: 0.0934966\ttotal: 1m 7s\tremaining: 33.6s\n",
      "668:\tlearn: 0.0934587\ttotal: 1m 7s\tremaining: 33.5s\n",
      "669:\tlearn: 0.0934319\ttotal: 1m 7s\tremaining: 33.4s\n",
      "670:\tlearn: 0.0933700\ttotal: 1m 7s\tremaining: 33.3s\n",
      "671:\tlearn: 0.0932815\ttotal: 1m 7s\tremaining: 33.2s\n",
      "672:\tlearn: 0.0932319\ttotal: 1m 8s\tremaining: 33s\n",
      "673:\tlearn: 0.0931783\ttotal: 1m 8s\tremaining: 32.9s\n",
      "674:\tlearn: 0.0931365\ttotal: 1m 8s\tremaining: 32.8s\n",
      "675:\tlearn: 0.0930814\ttotal: 1m 8s\tremaining: 32.7s\n",
      "676:\tlearn: 0.0930375\ttotal: 1m 8s\tremaining: 32.6s\n",
      "677:\tlearn: 0.0929837\ttotal: 1m 8s\tremaining: 32.5s\n",
      "678:\tlearn: 0.0929215\ttotal: 1m 8s\tremaining: 32.4s\n",
      "679:\tlearn: 0.0928851\ttotal: 1m 8s\tremaining: 32.3s\n",
      "680:\tlearn: 0.0928553\ttotal: 1m 8s\tremaining: 32.2s\n",
      "681:\tlearn: 0.0927671\ttotal: 1m 8s\tremaining: 32.1s\n",
      "682:\tlearn: 0.0927436\ttotal: 1m 8s\tremaining: 32s\n",
      "683:\tlearn: 0.0927022\ttotal: 1m 9s\tremaining: 31.9s\n",
      "684:\tlearn: 0.0926832\ttotal: 1m 9s\tremaining: 31.8s\n",
      "685:\tlearn: 0.0926534\ttotal: 1m 9s\tremaining: 31.7s\n",
      "686:\tlearn: 0.0926307\ttotal: 1m 9s\tremaining: 31.6s\n",
      "687:\tlearn: 0.0926029\ttotal: 1m 9s\tremaining: 31.5s\n",
      "688:\tlearn: 0.0925776\ttotal: 1m 9s\tremaining: 31.4s\n",
      "689:\tlearn: 0.0925347\ttotal: 1m 9s\tremaining: 31.3s\n",
      "690:\tlearn: 0.0924938\ttotal: 1m 9s\tremaining: 31.2s\n",
      "691:\tlearn: 0.0924626\ttotal: 1m 9s\tremaining: 31.1s\n",
      "692:\tlearn: 0.0924184\ttotal: 1m 9s\tremaining: 31s\n",
      "693:\tlearn: 0.0923711\ttotal: 1m 10s\tremaining: 30.9s\n",
      "694:\tlearn: 0.0923355\ttotal: 1m 10s\tremaining: 30.8s\n",
      "695:\tlearn: 0.0923022\ttotal: 1m 10s\tremaining: 30.7s\n",
      "696:\tlearn: 0.0922252\ttotal: 1m 10s\tremaining: 30.6s\n",
      "697:\tlearn: 0.0921896\ttotal: 1m 10s\tremaining: 30.5s\n",
      "698:\tlearn: 0.0921362\ttotal: 1m 10s\tremaining: 30.4s\n",
      "699:\tlearn: 0.0920932\ttotal: 1m 10s\tremaining: 30.3s\n",
      "700:\tlearn: 0.0920000\ttotal: 1m 10s\tremaining: 30.2s\n",
      "701:\tlearn: 0.0919572\ttotal: 1m 10s\tremaining: 30.1s\n",
      "702:\tlearn: 0.0918968\ttotal: 1m 10s\tremaining: 29.9s\n",
      "703:\tlearn: 0.0918060\ttotal: 1m 11s\tremaining: 29.9s\n",
      "704:\tlearn: 0.0917554\ttotal: 1m 11s\tremaining: 29.8s\n",
      "705:\tlearn: 0.0917313\ttotal: 1m 11s\tremaining: 29.7s\n",
      "706:\tlearn: 0.0917065\ttotal: 1m 11s\tremaining: 29.6s\n",
      "707:\tlearn: 0.0916832\ttotal: 1m 11s\tremaining: 29.4s\n",
      "708:\tlearn: 0.0916284\ttotal: 1m 11s\tremaining: 29.3s\n",
      "709:\tlearn: 0.0915722\ttotal: 1m 11s\tremaining: 29.2s\n",
      "710:\tlearn: 0.0915156\ttotal: 1m 11s\tremaining: 29.1s\n",
      "711:\tlearn: 0.0914619\ttotal: 1m 11s\tremaining: 29s\n",
      "712:\tlearn: 0.0914208\ttotal: 1m 11s\tremaining: 28.9s\n",
      "713:\tlearn: 0.0913642\ttotal: 1m 11s\tremaining: 28.8s\n",
      "714:\tlearn: 0.0913174\ttotal: 1m 12s\tremaining: 28.7s\n",
      "715:\tlearn: 0.0912747\ttotal: 1m 12s\tremaining: 28.6s\n",
      "716:\tlearn: 0.0912602\ttotal: 1m 12s\tremaining: 28.5s\n",
      "717:\tlearn: 0.0911684\ttotal: 1m 12s\tremaining: 28.4s\n",
      "718:\tlearn: 0.0911295\ttotal: 1m 12s\tremaining: 28.3s\n",
      "719:\tlearn: 0.0910830\ttotal: 1m 12s\tremaining: 28.2s\n",
      "720:\tlearn: 0.0910508\ttotal: 1m 12s\tremaining: 28.1s\n",
      "721:\tlearn: 0.0909968\ttotal: 1m 12s\tremaining: 28s\n",
      "722:\tlearn: 0.0909582\ttotal: 1m 12s\tremaining: 27.9s\n",
      "723:\tlearn: 0.0908864\ttotal: 1m 12s\tremaining: 27.8s\n",
      "724:\tlearn: 0.0908421\ttotal: 1m 13s\tremaining: 27.7s\n",
      "725:\tlearn: 0.0908037\ttotal: 1m 13s\tremaining: 27.6s\n",
      "726:\tlearn: 0.0907607\ttotal: 1m 13s\tremaining: 27.5s\n",
      "727:\tlearn: 0.0907371\ttotal: 1m 13s\tremaining: 27.4s\n",
      "728:\tlearn: 0.0907252\ttotal: 1m 13s\tremaining: 27.3s\n",
      "729:\tlearn: 0.0906981\ttotal: 1m 13s\tremaining: 27.2s\n",
      "730:\tlearn: 0.0906652\ttotal: 1m 13s\tremaining: 27.1s\n",
      "731:\tlearn: 0.0906268\ttotal: 1m 13s\tremaining: 27s\n",
      "732:\tlearn: 0.0906216\ttotal: 1m 13s\tremaining: 26.9s\n",
      "733:\tlearn: 0.0905835\ttotal: 1m 13s\tremaining: 26.8s\n",
      "734:\tlearn: 0.0905330\ttotal: 1m 13s\tremaining: 26.7s\n",
      "735:\tlearn: 0.0904948\ttotal: 1m 14s\tremaining: 26.6s\n",
      "736:\tlearn: 0.0904500\ttotal: 1m 14s\tremaining: 26.5s\n",
      "737:\tlearn: 0.0904026\ttotal: 1m 14s\tremaining: 26.4s\n",
      "738:\tlearn: 0.0903596\ttotal: 1m 14s\tremaining: 26.3s\n",
      "739:\tlearn: 0.0902523\ttotal: 1m 14s\tremaining: 26.2s\n",
      "740:\tlearn: 0.0902030\ttotal: 1m 14s\tremaining: 26.1s\n",
      "741:\tlearn: 0.0901312\ttotal: 1m 14s\tremaining: 26s\n",
      "742:\tlearn: 0.0900960\ttotal: 1m 14s\tremaining: 25.9s\n",
      "743:\tlearn: 0.0900860\ttotal: 1m 14s\tremaining: 25.8s\n",
      "744:\tlearn: 0.0900743\ttotal: 1m 15s\tremaining: 25.7s\n",
      "745:\tlearn: 0.0900147\ttotal: 1m 15s\tremaining: 25.6s\n",
      "746:\tlearn: 0.0899773\ttotal: 1m 15s\tremaining: 25.5s\n",
      "747:\tlearn: 0.0899682\ttotal: 1m 15s\tremaining: 25.4s\n",
      "748:\tlearn: 0.0899191\ttotal: 1m 15s\tremaining: 25.3s\n",
      "749:\tlearn: 0.0898746\ttotal: 1m 15s\tremaining: 25.2s\n",
      "750:\tlearn: 0.0898332\ttotal: 1m 15s\tremaining: 25.1s\n",
      "751:\tlearn: 0.0897862\ttotal: 1m 15s\tremaining: 25s\n",
      "752:\tlearn: 0.0897064\ttotal: 1m 15s\tremaining: 24.9s\n",
      "753:\tlearn: 0.0896686\ttotal: 1m 15s\tremaining: 24.8s\n",
      "754:\tlearn: 0.0896608\ttotal: 1m 15s\tremaining: 24.7s\n",
      "755:\tlearn: 0.0896340\ttotal: 1m 16s\tremaining: 24.6s\n",
      "756:\tlearn: 0.0895661\ttotal: 1m 16s\tremaining: 24.5s\n",
      "757:\tlearn: 0.0895314\ttotal: 1m 16s\tremaining: 24.4s\n",
      "758:\tlearn: 0.0895031\ttotal: 1m 16s\tremaining: 24.3s\n",
      "759:\tlearn: 0.0894616\ttotal: 1m 16s\tremaining: 24.2s\n",
      "760:\tlearn: 0.0894335\ttotal: 1m 16s\tremaining: 24.1s\n",
      "761:\tlearn: 0.0893986\ttotal: 1m 16s\tremaining: 24s\n",
      "762:\tlearn: 0.0893753\ttotal: 1m 16s\tremaining: 23.9s\n",
      "763:\tlearn: 0.0893266\ttotal: 1m 16s\tremaining: 23.8s\n",
      "764:\tlearn: 0.0892692\ttotal: 1m 16s\tremaining: 23.6s\n",
      "765:\tlearn: 0.0892262\ttotal: 1m 17s\tremaining: 23.5s\n",
      "766:\tlearn: 0.0891771\ttotal: 1m 17s\tremaining: 23.4s\n",
      "767:\tlearn: 0.0891509\ttotal: 1m 17s\tremaining: 23.3s\n",
      "768:\tlearn: 0.0891070\ttotal: 1m 17s\tremaining: 23.2s\n",
      "769:\tlearn: 0.0890614\ttotal: 1m 17s\tremaining: 23.1s\n",
      "770:\tlearn: 0.0890159\ttotal: 1m 17s\tremaining: 23s\n",
      "771:\tlearn: 0.0889898\ttotal: 1m 17s\tremaining: 22.9s\n",
      "772:\tlearn: 0.0889361\ttotal: 1m 17s\tremaining: 22.8s\n",
      "773:\tlearn: 0.0889054\ttotal: 1m 17s\tremaining: 22.7s\n",
      "774:\tlearn: 0.0888588\ttotal: 1m 17s\tremaining: 22.6s\n",
      "775:\tlearn: 0.0887957\ttotal: 1m 18s\tremaining: 22.5s\n",
      "776:\tlearn: 0.0887673\ttotal: 1m 18s\tremaining: 22.4s\n",
      "777:\tlearn: 0.0886998\ttotal: 1m 18s\tremaining: 22.3s\n",
      "778:\tlearn: 0.0886917\ttotal: 1m 18s\tremaining: 22.2s\n",
      "779:\tlearn: 0.0886541\ttotal: 1m 18s\tremaining: 22.1s\n",
      "780:\tlearn: 0.0886037\ttotal: 1m 18s\tremaining: 22s\n",
      "781:\tlearn: 0.0885834\ttotal: 1m 18s\tremaining: 21.9s\n",
      "782:\tlearn: 0.0884994\ttotal: 1m 18s\tremaining: 21.8s\n",
      "783:\tlearn: 0.0884850\ttotal: 1m 18s\tremaining: 21.7s\n",
      "784:\tlearn: 0.0884275\ttotal: 1m 18s\tremaining: 21.6s\n",
      "785:\tlearn: 0.0883301\ttotal: 1m 19s\tremaining: 21.5s\n",
      "786:\tlearn: 0.0882605\ttotal: 1m 19s\tremaining: 21.4s\n",
      "787:\tlearn: 0.0882126\ttotal: 1m 19s\tremaining: 21.3s\n",
      "788:\tlearn: 0.0881540\ttotal: 1m 19s\tremaining: 21.2s\n",
      "789:\tlearn: 0.0881212\ttotal: 1m 19s\tremaining: 21.1s\n",
      "790:\tlearn: 0.0880761\ttotal: 1m 19s\tremaining: 21s\n",
      "791:\tlearn: 0.0880448\ttotal: 1m 19s\tremaining: 20.9s\n",
      "792:\tlearn: 0.0880144\ttotal: 1m 19s\tremaining: 20.8s\n",
      "793:\tlearn: 0.0879583\ttotal: 1m 19s\tremaining: 20.7s\n",
      "794:\tlearn: 0.0879391\ttotal: 1m 19s\tremaining: 20.6s\n",
      "795:\tlearn: 0.0879141\ttotal: 1m 20s\tremaining: 20.5s\n",
      "796:\tlearn: 0.0879110\ttotal: 1m 20s\tremaining: 20.4s\n",
      "797:\tlearn: 0.0878602\ttotal: 1m 20s\tremaining: 20.3s\n",
      "798:\tlearn: 0.0878374\ttotal: 1m 20s\tremaining: 20.2s\n",
      "799:\tlearn: 0.0878128\ttotal: 1m 20s\tremaining: 20.1s\n",
      "800:\tlearn: 0.0877718\ttotal: 1m 20s\tremaining: 20s\n",
      "801:\tlearn: 0.0877463\ttotal: 1m 20s\tremaining: 19.9s\n",
      "802:\tlearn: 0.0877013\ttotal: 1m 20s\tremaining: 19.8s\n",
      "803:\tlearn: 0.0876619\ttotal: 1m 20s\tremaining: 19.7s\n",
      "804:\tlearn: 0.0876314\ttotal: 1m 20s\tremaining: 19.6s\n",
      "805:\tlearn: 0.0875443\ttotal: 1m 20s\tremaining: 19.5s\n",
      "806:\tlearn: 0.0875146\ttotal: 1m 21s\tremaining: 19.4s\n",
      "807:\tlearn: 0.0874864\ttotal: 1m 21s\tremaining: 19.3s\n",
      "808:\tlearn: 0.0874413\ttotal: 1m 21s\tremaining: 19.2s\n",
      "809:\tlearn: 0.0873945\ttotal: 1m 21s\tremaining: 19.1s\n",
      "810:\tlearn: 0.0873923\ttotal: 1m 21s\tremaining: 19s\n",
      "811:\tlearn: 0.0873713\ttotal: 1m 21s\tremaining: 18.9s\n",
      "812:\tlearn: 0.0873467\ttotal: 1m 21s\tremaining: 18.8s\n",
      "813:\tlearn: 0.0872789\ttotal: 1m 21s\tremaining: 18.7s\n",
      "814:\tlearn: 0.0872787\ttotal: 1m 21s\tremaining: 18.6s\n",
      "815:\tlearn: 0.0872384\ttotal: 1m 21s\tremaining: 18.5s\n",
      "816:\tlearn: 0.0871977\ttotal: 1m 22s\tremaining: 18.4s\n",
      "817:\tlearn: 0.0871808\ttotal: 1m 22s\tremaining: 18.3s\n",
      "818:\tlearn: 0.0871504\ttotal: 1m 22s\tremaining: 18.2s\n",
      "819:\tlearn: 0.0871009\ttotal: 1m 22s\tremaining: 18.1s\n",
      "820:\tlearn: 0.0870712\ttotal: 1m 22s\tremaining: 18s\n",
      "821:\tlearn: 0.0870203\ttotal: 1m 22s\tremaining: 17.9s\n",
      "822:\tlearn: 0.0869722\ttotal: 1m 22s\tremaining: 17.8s\n",
      "823:\tlearn: 0.0869267\ttotal: 1m 22s\tremaining: 17.7s\n",
      "824:\tlearn: 0.0868910\ttotal: 1m 22s\tremaining: 17.6s\n",
      "825:\tlearn: 0.0868703\ttotal: 1m 22s\tremaining: 17.5s\n",
      "826:\tlearn: 0.0868395\ttotal: 1m 23s\tremaining: 17.4s\n",
      "827:\tlearn: 0.0867417\ttotal: 1m 23s\tremaining: 17.3s\n",
      "828:\tlearn: 0.0866837\ttotal: 1m 23s\tremaining: 17.2s\n",
      "829:\tlearn: 0.0866497\ttotal: 1m 23s\tremaining: 17.1s\n",
      "830:\tlearn: 0.0865959\ttotal: 1m 23s\tremaining: 17s\n",
      "831:\tlearn: 0.0865462\ttotal: 1m 23s\tremaining: 16.9s\n",
      "832:\tlearn: 0.0865130\ttotal: 1m 23s\tremaining: 16.8s\n",
      "833:\tlearn: 0.0865052\ttotal: 1m 23s\tremaining: 16.7s\n",
      "834:\tlearn: 0.0864572\ttotal: 1m 23s\tremaining: 16.6s\n",
      "835:\tlearn: 0.0864219\ttotal: 1m 24s\tremaining: 16.5s\n",
      "836:\tlearn: 0.0863532\ttotal: 1m 24s\tremaining: 16.4s\n",
      "837:\tlearn: 0.0863174\ttotal: 1m 24s\tremaining: 16.3s\n",
      "838:\tlearn: 0.0862952\ttotal: 1m 24s\tremaining: 16.2s\n",
      "839:\tlearn: 0.0862500\ttotal: 1m 24s\tremaining: 16.1s\n",
      "840:\tlearn: 0.0862233\ttotal: 1m 24s\tremaining: 16s\n",
      "841:\tlearn: 0.0861940\ttotal: 1m 24s\tremaining: 15.9s\n",
      "842:\tlearn: 0.0861544\ttotal: 1m 24s\tremaining: 15.8s\n",
      "843:\tlearn: 0.0860868\ttotal: 1m 24s\tremaining: 15.7s\n",
      "844:\tlearn: 0.0860611\ttotal: 1m 24s\tremaining: 15.6s\n",
      "845:\tlearn: 0.0860379\ttotal: 1m 24s\tremaining: 15.5s\n",
      "846:\tlearn: 0.0859970\ttotal: 1m 25s\tremaining: 15.4s\n",
      "847:\tlearn: 0.0859537\ttotal: 1m 25s\tremaining: 15.3s\n",
      "848:\tlearn: 0.0859151\ttotal: 1m 25s\tremaining: 15.2s\n",
      "849:\tlearn: 0.0858897\ttotal: 1m 25s\tremaining: 15.1s\n",
      "850:\tlearn: 0.0858701\ttotal: 1m 25s\tremaining: 15s\n",
      "851:\tlearn: 0.0858469\ttotal: 1m 25s\tremaining: 14.9s\n",
      "852:\tlearn: 0.0858056\ttotal: 1m 25s\tremaining: 14.8s\n",
      "853:\tlearn: 0.0857782\ttotal: 1m 25s\tremaining: 14.7s\n",
      "854:\tlearn: 0.0857331\ttotal: 1m 25s\tremaining: 14.6s\n",
      "855:\tlearn: 0.0857009\ttotal: 1m 25s\tremaining: 14.5s\n",
      "856:\tlearn: 0.0856595\ttotal: 1m 26s\tremaining: 14.4s\n",
      "857:\tlearn: 0.0856253\ttotal: 1m 26s\tremaining: 14.3s\n",
      "858:\tlearn: 0.0855916\ttotal: 1m 26s\tremaining: 14.2s\n",
      "859:\tlearn: 0.0855730\ttotal: 1m 26s\tremaining: 14.1s\n",
      "860:\tlearn: 0.0855232\ttotal: 1m 26s\tremaining: 14s\n",
      "861:\tlearn: 0.0854990\ttotal: 1m 26s\tremaining: 13.9s\n",
      "862:\tlearn: 0.0854636\ttotal: 1m 26s\tremaining: 13.8s\n",
      "863:\tlearn: 0.0854052\ttotal: 1m 26s\tremaining: 13.7s\n",
      "864:\tlearn: 0.0853828\ttotal: 1m 26s\tremaining: 13.6s\n",
      "865:\tlearn: 0.0853762\ttotal: 1m 26s\tremaining: 13.4s\n",
      "866:\tlearn: 0.0853537\ttotal: 1m 27s\tremaining: 13.3s\n",
      "867:\tlearn: 0.0853245\ttotal: 1m 27s\tremaining: 13.2s\n",
      "868:\tlearn: 0.0852981\ttotal: 1m 27s\tremaining: 13.1s\n",
      "869:\tlearn: 0.0852670\ttotal: 1m 27s\tremaining: 13s\n",
      "870:\tlearn: 0.0852255\ttotal: 1m 27s\tremaining: 12.9s\n",
      "871:\tlearn: 0.0852240\ttotal: 1m 27s\tremaining: 12.8s\n",
      "872:\tlearn: 0.0851882\ttotal: 1m 27s\tremaining: 12.7s\n",
      "873:\tlearn: 0.0851758\ttotal: 1m 27s\tremaining: 12.6s\n",
      "874:\tlearn: 0.0851356\ttotal: 1m 27s\tremaining: 12.5s\n",
      "875:\tlearn: 0.0851100\ttotal: 1m 27s\tremaining: 12.4s\n",
      "876:\tlearn: 0.0850813\ttotal: 1m 27s\tremaining: 12.3s\n",
      "877:\tlearn: 0.0850431\ttotal: 1m 28s\tremaining: 12.2s\n",
      "878:\tlearn: 0.0850004\ttotal: 1m 28s\tremaining: 12.1s\n",
      "879:\tlearn: 0.0849878\ttotal: 1m 28s\tremaining: 12s\n",
      "880:\tlearn: 0.0849661\ttotal: 1m 28s\tremaining: 11.9s\n",
      "881:\tlearn: 0.0849547\ttotal: 1m 28s\tremaining: 11.8s\n",
      "882:\tlearn: 0.0848921\ttotal: 1m 28s\tremaining: 11.7s\n",
      "883:\tlearn: 0.0848796\ttotal: 1m 28s\tremaining: 11.6s\n",
      "884:\tlearn: 0.0848440\ttotal: 1m 28s\tremaining: 11.5s\n",
      "885:\tlearn: 0.0848326\ttotal: 1m 28s\tremaining: 11.4s\n",
      "886:\tlearn: 0.0847918\ttotal: 1m 28s\tremaining: 11.3s\n",
      "887:\tlearn: 0.0847584\ttotal: 1m 29s\tremaining: 11.2s\n",
      "888:\tlearn: 0.0847281\ttotal: 1m 29s\tremaining: 11.1s\n",
      "889:\tlearn: 0.0847005\ttotal: 1m 29s\tremaining: 11s\n",
      "890:\tlearn: 0.0846560\ttotal: 1m 29s\tremaining: 10.9s\n",
      "891:\tlearn: 0.0846327\ttotal: 1m 29s\tremaining: 10.8s\n",
      "892:\tlearn: 0.0845780\ttotal: 1m 29s\tremaining: 10.7s\n",
      "893:\tlearn: 0.0845601\ttotal: 1m 29s\tremaining: 10.6s\n",
      "894:\tlearn: 0.0845403\ttotal: 1m 29s\tremaining: 10.5s\n",
      "895:\tlearn: 0.0845190\ttotal: 1m 29s\tremaining: 10.4s\n",
      "896:\tlearn: 0.0844936\ttotal: 1m 29s\tremaining: 10.3s\n",
      "897:\tlearn: 0.0844691\ttotal: 1m 29s\tremaining: 10.2s\n",
      "898:\tlearn: 0.0844366\ttotal: 1m 30s\tremaining: 10.1s\n",
      "899:\tlearn: 0.0844279\ttotal: 1m 30s\tremaining: 10s\n",
      "900:\tlearn: 0.0844003\ttotal: 1m 30s\tremaining: 9.92s\n",
      "901:\tlearn: 0.0843755\ttotal: 1m 30s\tremaining: 9.82s\n",
      "902:\tlearn: 0.0843432\ttotal: 1m 30s\tremaining: 9.72s\n",
      "903:\tlearn: 0.0843358\ttotal: 1m 30s\tremaining: 9.62s\n",
      "904:\tlearn: 0.0842879\ttotal: 1m 30s\tremaining: 9.52s\n",
      "905:\tlearn: 0.0842538\ttotal: 1m 30s\tremaining: 9.42s\n",
      "906:\tlearn: 0.0842134\ttotal: 1m 30s\tremaining: 9.32s\n",
      "907:\tlearn: 0.0841971\ttotal: 1m 31s\tremaining: 9.22s\n",
      "908:\tlearn: 0.0841587\ttotal: 1m 31s\tremaining: 9.12s\n",
      "909:\tlearn: 0.0841127\ttotal: 1m 31s\tremaining: 9.02s\n",
      "910:\tlearn: 0.0840864\ttotal: 1m 31s\tremaining: 8.92s\n",
      "911:\tlearn: 0.0840372\ttotal: 1m 31s\tremaining: 8.83s\n",
      "912:\tlearn: 0.0840005\ttotal: 1m 31s\tremaining: 8.73s\n",
      "913:\tlearn: 0.0839884\ttotal: 1m 31s\tremaining: 8.63s\n",
      "914:\tlearn: 0.0839448\ttotal: 1m 31s\tremaining: 8.53s\n",
      "915:\tlearn: 0.0838899\ttotal: 1m 31s\tremaining: 8.43s\n",
      "916:\tlearn: 0.0838297\ttotal: 1m 32s\tremaining: 8.33s\n",
      "917:\tlearn: 0.0838107\ttotal: 1m 32s\tremaining: 8.23s\n",
      "918:\tlearn: 0.0837399\ttotal: 1m 32s\tremaining: 8.14s\n",
      "919:\tlearn: 0.0837332\ttotal: 1m 32s\tremaining: 8.04s\n",
      "920:\tlearn: 0.0836994\ttotal: 1m 32s\tremaining: 7.94s\n",
      "921:\tlearn: 0.0836678\ttotal: 1m 32s\tremaining: 7.84s\n",
      "922:\tlearn: 0.0836367\ttotal: 1m 32s\tremaining: 7.74s\n",
      "923:\tlearn: 0.0835794\ttotal: 1m 32s\tremaining: 7.64s\n",
      "924:\tlearn: 0.0835337\ttotal: 1m 32s\tremaining: 7.54s\n",
      "925:\tlearn: 0.0834712\ttotal: 1m 33s\tremaining: 7.44s\n",
      "926:\tlearn: 0.0834235\ttotal: 1m 33s\tremaining: 7.34s\n",
      "927:\tlearn: 0.0833784\ttotal: 1m 33s\tremaining: 7.24s\n",
      "928:\tlearn: 0.0833642\ttotal: 1m 33s\tremaining: 7.13s\n",
      "929:\tlearn: 0.0833110\ttotal: 1m 33s\tremaining: 7.03s\n",
      "930:\tlearn: 0.0832857\ttotal: 1m 33s\tremaining: 6.93s\n",
      "931:\tlearn: 0.0832443\ttotal: 1m 33s\tremaining: 6.83s\n",
      "932:\tlearn: 0.0832393\ttotal: 1m 33s\tremaining: 6.73s\n",
      "933:\tlearn: 0.0831671\ttotal: 1m 33s\tremaining: 6.63s\n",
      "934:\tlearn: 0.0831579\ttotal: 1m 33s\tremaining: 6.53s\n",
      "935:\tlearn: 0.0831258\ttotal: 1m 34s\tremaining: 6.43s\n",
      "936:\tlearn: 0.0831033\ttotal: 1m 34s\tremaining: 6.33s\n",
      "937:\tlearn: 0.0830741\ttotal: 1m 34s\tremaining: 6.23s\n",
      "938:\tlearn: 0.0830382\ttotal: 1m 34s\tremaining: 6.13s\n",
      "939:\tlearn: 0.0829956\ttotal: 1m 34s\tremaining: 6.03s\n",
      "940:\tlearn: 0.0829601\ttotal: 1m 34s\tremaining: 5.93s\n",
      "941:\tlearn: 0.0829405\ttotal: 1m 34s\tremaining: 5.83s\n",
      "942:\tlearn: 0.0828888\ttotal: 1m 34s\tremaining: 5.73s\n",
      "943:\tlearn: 0.0828719\ttotal: 1m 34s\tremaining: 5.63s\n",
      "944:\tlearn: 0.0828429\ttotal: 1m 34s\tremaining: 5.53s\n",
      "945:\tlearn: 0.0828020\ttotal: 1m 35s\tremaining: 5.43s\n",
      "946:\tlearn: 0.0827779\ttotal: 1m 35s\tremaining: 5.33s\n",
      "947:\tlearn: 0.0827367\ttotal: 1m 35s\tremaining: 5.23s\n",
      "948:\tlearn: 0.0827218\ttotal: 1m 35s\tremaining: 5.13s\n",
      "949:\tlearn: 0.0826648\ttotal: 1m 35s\tremaining: 5.03s\n",
      "950:\tlearn: 0.0825964\ttotal: 1m 35s\tremaining: 4.92s\n",
      "951:\tlearn: 0.0825402\ttotal: 1m 35s\tremaining: 4.83s\n",
      "952:\tlearn: 0.0825062\ttotal: 1m 35s\tremaining: 4.72s\n",
      "953:\tlearn: 0.0824808\ttotal: 1m 35s\tremaining: 4.62s\n",
      "954:\tlearn: 0.0824547\ttotal: 1m 36s\tremaining: 4.52s\n",
      "955:\tlearn: 0.0824123\ttotal: 1m 36s\tremaining: 4.42s\n",
      "956:\tlearn: 0.0823745\ttotal: 1m 36s\tremaining: 4.32s\n",
      "957:\tlearn: 0.0823275\ttotal: 1m 36s\tremaining: 4.22s\n",
      "958:\tlearn: 0.0823097\ttotal: 1m 36s\tremaining: 4.12s\n",
      "959:\tlearn: 0.0822568\ttotal: 1m 36s\tremaining: 4.02s\n",
      "960:\tlearn: 0.0822325\ttotal: 1m 36s\tremaining: 3.92s\n",
      "961:\tlearn: 0.0821962\ttotal: 1m 36s\tremaining: 3.82s\n",
      "962:\tlearn: 0.0821598\ttotal: 1m 36s\tremaining: 3.72s\n",
      "963:\tlearn: 0.0821322\ttotal: 1m 36s\tremaining: 3.62s\n",
      "964:\tlearn: 0.0820874\ttotal: 1m 37s\tremaining: 3.52s\n",
      "965:\tlearn: 0.0820173\ttotal: 1m 37s\tremaining: 3.42s\n",
      "966:\tlearn: 0.0819875\ttotal: 1m 37s\tremaining: 3.32s\n",
      "967:\tlearn: 0.0819329\ttotal: 1m 37s\tremaining: 3.22s\n",
      "968:\tlearn: 0.0818950\ttotal: 1m 37s\tremaining: 3.12s\n",
      "969:\tlearn: 0.0818568\ttotal: 1m 37s\tremaining: 3.02s\n",
      "970:\tlearn: 0.0818498\ttotal: 1m 37s\tremaining: 2.92s\n",
      "971:\tlearn: 0.0817927\ttotal: 1m 37s\tremaining: 2.82s\n",
      "972:\tlearn: 0.0817556\ttotal: 1m 37s\tremaining: 2.71s\n",
      "973:\tlearn: 0.0817349\ttotal: 1m 37s\tremaining: 2.62s\n",
      "974:\tlearn: 0.0817011\ttotal: 1m 38s\tremaining: 2.51s\n",
      "975:\tlearn: 0.0816690\ttotal: 1m 38s\tremaining: 2.41s\n",
      "976:\tlearn: 0.0816346\ttotal: 1m 38s\tremaining: 2.31s\n",
      "977:\tlearn: 0.0816118\ttotal: 1m 38s\tremaining: 2.21s\n",
      "978:\tlearn: 0.0815759\ttotal: 1m 38s\tremaining: 2.11s\n",
      "979:\tlearn: 0.0815432\ttotal: 1m 38s\tremaining: 2.01s\n",
      "980:\tlearn: 0.0815139\ttotal: 1m 38s\tremaining: 1.91s\n",
      "981:\tlearn: 0.0814732\ttotal: 1m 38s\tremaining: 1.81s\n",
      "982:\tlearn: 0.0814522\ttotal: 1m 38s\tremaining: 1.71s\n",
      "983:\tlearn: 0.0814096\ttotal: 1m 38s\tremaining: 1.61s\n",
      "984:\tlearn: 0.0813404\ttotal: 1m 39s\tremaining: 1.51s\n",
      "985:\tlearn: 0.0812773\ttotal: 1m 39s\tremaining: 1.41s\n",
      "986:\tlearn: 0.0812708\ttotal: 1m 39s\tremaining: 1.31s\n",
      "987:\tlearn: 0.0812477\ttotal: 1m 39s\tremaining: 1.21s\n",
      "988:\tlearn: 0.0811997\ttotal: 1m 39s\tremaining: 1.11s\n",
      "989:\tlearn: 0.0811564\ttotal: 1m 39s\tremaining: 1s\n",
      "990:\tlearn: 0.0811173\ttotal: 1m 39s\tremaining: 905ms\n",
      "991:\tlearn: 0.0810857\ttotal: 1m 39s\tremaining: 805ms\n",
      "992:\tlearn: 0.0810359\ttotal: 1m 39s\tremaining: 704ms\n",
      "993:\tlearn: 0.0810180\ttotal: 1m 39s\tremaining: 604ms\n",
      "994:\tlearn: 0.0809777\ttotal: 1m 40s\tremaining: 503ms\n",
      "995:\tlearn: 0.0809706\ttotal: 1m 40s\tremaining: 403ms\n",
      "996:\tlearn: 0.0809552\ttotal: 1m 40s\tremaining: 302ms\n",
      "997:\tlearn: 0.0809192\ttotal: 1m 40s\tremaining: 202ms\n",
      "998:\tlearn: 0.0808906\ttotal: 1m 40s\tremaining: 101ms\n",
      "999:\tlearn: 0.0808143\ttotal: 1m 40s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[116144,  15911],\n",
       "       [ 12994, 506852]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# On crée la matrice de confusion en passant les classes et les prédictions du modèle, puis on l'affiche\n",
    "y_train_pred_cat = cross_val_predict(catboost_pipeline, X_train, y_train)\n",
    "cm = confusion_matrix(y_train, y_train_pred_cat)\n",
    "display(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff197ee99f0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage avec un plot\n",
    "graph_cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Default\", \"Paid\"])\n",
    "graph_cm.plot(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5255308\ttotal: 300ms\tremaining: 4m 59s\n",
      "1:\tlearn: 0.4266843\ttotal: 570ms\tremaining: 4m 44s\n",
      "2:\tlearn: 0.3591964\ttotal: 866ms\tremaining: 4m 47s\n",
      "3:\tlearn: 0.3200147\ttotal: 1.06s\tremaining: 4m 25s\n",
      "4:\tlearn: 0.2879499\ttotal: 1.28s\tremaining: 4m 14s\n",
      "5:\tlearn: 0.2706185\ttotal: 1.46s\tremaining: 4m 2s\n",
      "6:\tlearn: 0.2554469\ttotal: 1.65s\tremaining: 3m 53s\n",
      "7:\tlearn: 0.2452866\ttotal: 1.85s\tremaining: 3m 49s\n",
      "8:\tlearn: 0.2355867\ttotal: 2.04s\tremaining: 3m 45s\n",
      "9:\tlearn: 0.2296170\ttotal: 2.25s\tremaining: 3m 42s\n",
      "10:\tlearn: 0.2202703\ttotal: 2.5s\tremaining: 3m 44s\n",
      "11:\tlearn: 0.2132522\ttotal: 2.79s\tremaining: 3m 49s\n",
      "12:\tlearn: 0.2084120\ttotal: 3.04s\tremaining: 3m 50s\n",
      "13:\tlearn: 0.2050314\ttotal: 3.24s\tremaining: 3m 48s\n",
      "14:\tlearn: 0.2024214\ttotal: 3.44s\tremaining: 3m 46s\n",
      "15:\tlearn: 0.1987115\ttotal: 3.66s\tremaining: 3m 44s\n",
      "16:\tlearn: 0.1969163\ttotal: 3.94s\tremaining: 3m 47s\n",
      "17:\tlearn: 0.1937762\ttotal: 4.14s\tremaining: 3m 45s\n",
      "18:\tlearn: 0.1920942\ttotal: 4.34s\tremaining: 3m 44s\n",
      "19:\tlearn: 0.1891100\ttotal: 4.54s\tremaining: 3m 42s\n",
      "20:\tlearn: 0.1870727\ttotal: 4.73s\tremaining: 3m 40s\n",
      "21:\tlearn: 0.1855121\ttotal: 4.93s\tremaining: 3m 39s\n",
      "22:\tlearn: 0.1840807\ttotal: 5.16s\tremaining: 3m 39s\n",
      "23:\tlearn: 0.1821267\ttotal: 5.49s\tremaining: 3m 43s\n",
      "24:\tlearn: 0.1803673\ttotal: 5.69s\tremaining: 3m 42s\n",
      "25:\tlearn: 0.1789097\ttotal: 5.95s\tremaining: 3m 42s\n",
      "26:\tlearn: 0.1776885\ttotal: 6.16s\tremaining: 3m 41s\n",
      "27:\tlearn: 0.1751001\ttotal: 6.34s\tremaining: 3m 40s\n",
      "28:\tlearn: 0.1739717\ttotal: 6.53s\tremaining: 3m 38s\n",
      "29:\tlearn: 0.1729444\ttotal: 6.78s\tremaining: 3m 39s\n",
      "30:\tlearn: 0.1715579\ttotal: 7s\tremaining: 3m 38s\n",
      "31:\tlearn: 0.1702893\ttotal: 7.21s\tremaining: 3m 38s\n",
      "32:\tlearn: 0.1693290\ttotal: 7.42s\tremaining: 3m 37s\n",
      "33:\tlearn: 0.1681971\ttotal: 7.66s\tremaining: 3m 37s\n",
      "34:\tlearn: 0.1671508\ttotal: 7.88s\tremaining: 3m 37s\n",
      "35:\tlearn: 0.1663694\ttotal: 8.08s\tremaining: 3m 36s\n",
      "36:\tlearn: 0.1657139\ttotal: 8.28s\tremaining: 3m 35s\n",
      "37:\tlearn: 0.1647414\ttotal: 8.47s\tremaining: 3m 34s\n",
      "38:\tlearn: 0.1642004\ttotal: 8.69s\tremaining: 3m 34s\n",
      "39:\tlearn: 0.1635239\ttotal: 8.9s\tremaining: 3m 33s\n",
      "40:\tlearn: 0.1624333\ttotal: 9.09s\tremaining: 3m 32s\n",
      "41:\tlearn: 0.1618368\ttotal: 9.31s\tremaining: 3m 32s\n",
      "42:\tlearn: 0.1613731\ttotal: 9.51s\tremaining: 3m 31s\n",
      "43:\tlearn: 0.1605005\ttotal: 9.71s\tremaining: 3m 30s\n",
      "44:\tlearn: 0.1598489\ttotal: 9.91s\tremaining: 3m 30s\n",
      "45:\tlearn: 0.1591947\ttotal: 10.1s\tremaining: 3m 30s\n",
      "46:\tlearn: 0.1580296\ttotal: 10.4s\tremaining: 3m 30s\n",
      "47:\tlearn: 0.1576193\ttotal: 10.6s\tremaining: 3m 29s\n",
      "48:\tlearn: 0.1570927\ttotal: 10.8s\tremaining: 3m 29s\n",
      "49:\tlearn: 0.1566162\ttotal: 11s\tremaining: 3m 28s\n",
      "50:\tlearn: 0.1560280\ttotal: 11.2s\tremaining: 3m 28s\n",
      "51:\tlearn: 0.1555946\ttotal: 11.4s\tremaining: 3m 28s\n",
      "52:\tlearn: 0.1550531\ttotal: 11.7s\tremaining: 3m 28s\n",
      "53:\tlearn: 0.1547673\ttotal: 12s\tremaining: 3m 29s\n",
      "54:\tlearn: 0.1543669\ttotal: 12.2s\tremaining: 3m 29s\n",
      "55:\tlearn: 0.1538132\ttotal: 12.4s\tremaining: 3m 29s\n",
      "56:\tlearn: 0.1533790\ttotal: 12.7s\tremaining: 3m 29s\n",
      "57:\tlearn: 0.1528569\ttotal: 12.9s\tremaining: 3m 28s\n",
      "58:\tlearn: 0.1522603\ttotal: 13.1s\tremaining: 3m 28s\n",
      "59:\tlearn: 0.1520564\ttotal: 13.3s\tremaining: 3m 27s\n",
      "60:\tlearn: 0.1517411\ttotal: 13.4s\tremaining: 3m 27s\n",
      "61:\tlearn: 0.1513472\ttotal: 13.6s\tremaining: 3m 26s\n",
      "62:\tlearn: 0.1508998\ttotal: 13.9s\tremaining: 3m 26s\n",
      "63:\tlearn: 0.1502666\ttotal: 14.1s\tremaining: 3m 26s\n",
      "64:\tlearn: 0.1499267\ttotal: 14.3s\tremaining: 3m 26s\n",
      "65:\tlearn: 0.1495246\ttotal: 14.6s\tremaining: 3m 26s\n",
      "66:\tlearn: 0.1490905\ttotal: 14.8s\tremaining: 3m 26s\n",
      "67:\tlearn: 0.1488346\ttotal: 15s\tremaining: 3m 26s\n",
      "68:\tlearn: 0.1485455\ttotal: 15.3s\tremaining: 3m 26s\n",
      "69:\tlearn: 0.1483229\ttotal: 15.6s\tremaining: 3m 26s\n",
      "70:\tlearn: 0.1478641\ttotal: 15.8s\tremaining: 3m 27s\n",
      "71:\tlearn: 0.1474471\ttotal: 16.1s\tremaining: 3m 27s\n",
      "72:\tlearn: 0.1470838\ttotal: 16.3s\tremaining: 3m 26s\n",
      "73:\tlearn: 0.1466358\ttotal: 16.5s\tremaining: 3m 26s\n",
      "74:\tlearn: 0.1462041\ttotal: 16.7s\tremaining: 3m 26s\n",
      "75:\tlearn: 0.1459654\ttotal: 16.9s\tremaining: 3m 25s\n",
      "76:\tlearn: 0.1456781\ttotal: 17.1s\tremaining: 3m 25s\n",
      "77:\tlearn: 0.1454639\ttotal: 17.3s\tremaining: 3m 24s\n",
      "78:\tlearn: 0.1452306\ttotal: 17.5s\tremaining: 3m 24s\n",
      "79:\tlearn: 0.1448327\ttotal: 17.8s\tremaining: 3m 24s\n",
      "80:\tlearn: 0.1444152\ttotal: 17.9s\tremaining: 3m 23s\n",
      "81:\tlearn: 0.1441224\ttotal: 18.2s\tremaining: 3m 23s\n",
      "82:\tlearn: 0.1438455\ttotal: 18.4s\tremaining: 3m 22s\n",
      "83:\tlearn: 0.1433623\ttotal: 18.6s\tremaining: 3m 22s\n",
      "84:\tlearn: 0.1429565\ttotal: 18.8s\tremaining: 3m 22s\n",
      "85:\tlearn: 0.1427892\ttotal: 19.1s\tremaining: 3m 22s\n",
      "86:\tlearn: 0.1425296\ttotal: 19.3s\tremaining: 3m 22s\n",
      "87:\tlearn: 0.1422294\ttotal: 19.5s\tremaining: 3m 22s\n",
      "88:\tlearn: 0.1419719\ttotal: 19.7s\tremaining: 3m 21s\n",
      "89:\tlearn: 0.1417244\ttotal: 20s\tremaining: 3m 21s\n",
      "90:\tlearn: 0.1414504\ttotal: 20.1s\tremaining: 3m 21s\n",
      "91:\tlearn: 0.1412334\ttotal: 20.3s\tremaining: 3m 20s\n",
      "92:\tlearn: 0.1410560\ttotal: 20.5s\tremaining: 3m 20s\n",
      "93:\tlearn: 0.1407645\ttotal: 20.7s\tremaining: 3m 19s\n",
      "94:\tlearn: 0.1403873\ttotal: 20.9s\tremaining: 3m 19s\n",
      "95:\tlearn: 0.1398465\ttotal: 21.1s\tremaining: 3m 19s\n",
      "96:\tlearn: 0.1395808\ttotal: 21.4s\tremaining: 3m 18s\n",
      "97:\tlearn: 0.1393791\ttotal: 21.6s\tremaining: 3m 18s\n",
      "98:\tlearn: 0.1392229\ttotal: 21.8s\tremaining: 3m 18s\n",
      "99:\tlearn: 0.1389842\ttotal: 22s\tremaining: 3m 18s\n",
      "100:\tlearn: 0.1386990\ttotal: 22.2s\tremaining: 3m 17s\n",
      "101:\tlearn: 0.1384958\ttotal: 22.5s\tremaining: 3m 17s\n",
      "102:\tlearn: 0.1383692\ttotal: 22.7s\tremaining: 3m 17s\n",
      "103:\tlearn: 0.1381883\ttotal: 22.9s\tremaining: 3m 17s\n",
      "104:\tlearn: 0.1379813\ttotal: 23.1s\tremaining: 3m 17s\n",
      "105:\tlearn: 0.1378096\ttotal: 23.4s\tremaining: 3m 17s\n",
      "106:\tlearn: 0.1374999\ttotal: 23.7s\tremaining: 3m 17s\n",
      "107:\tlearn: 0.1370680\ttotal: 23.8s\tremaining: 3m 16s\n",
      "108:\tlearn: 0.1368735\ttotal: 24.1s\tremaining: 3m 16s\n",
      "109:\tlearn: 0.1366627\ttotal: 24.3s\tremaining: 3m 16s\n",
      "110:\tlearn: 0.1365077\ttotal: 24.5s\tremaining: 3m 16s\n",
      "111:\tlearn: 0.1363243\ttotal: 24.7s\tremaining: 3m 15s\n",
      "112:\tlearn: 0.1362193\ttotal: 24.9s\tremaining: 3m 15s\n",
      "113:\tlearn: 0.1359663\ttotal: 25.1s\tremaining: 3m 14s\n",
      "114:\tlearn: 0.1357844\ttotal: 25.3s\tremaining: 3m 14s\n",
      "115:\tlearn: 0.1355615\ttotal: 25.5s\tremaining: 3m 14s\n",
      "116:\tlearn: 0.1353443\ttotal: 25.7s\tremaining: 3m 13s\n",
      "117:\tlearn: 0.1350614\ttotal: 25.9s\tremaining: 3m 13s\n",
      "118:\tlearn: 0.1348293\ttotal: 26.2s\tremaining: 3m 13s\n",
      "119:\tlearn: 0.1346912\ttotal: 26.4s\tremaining: 3m 13s\n",
      "120:\tlearn: 0.1344290\ttotal: 26.7s\tremaining: 3m 13s\n",
      "121:\tlearn: 0.1341555\ttotal: 26.9s\tremaining: 3m 13s\n",
      "122:\tlearn: 0.1339731\ttotal: 27.2s\tremaining: 3m 13s\n",
      "123:\tlearn: 0.1337764\ttotal: 27.4s\tremaining: 3m 13s\n",
      "124:\tlearn: 0.1336165\ttotal: 27.6s\tremaining: 3m 13s\n",
      "125:\tlearn: 0.1334745\ttotal: 27.8s\tremaining: 3m 12s\n",
      "126:\tlearn: 0.1332594\ttotal: 28.1s\tremaining: 3m 12s\n",
      "127:\tlearn: 0.1330316\ttotal: 28.2s\tremaining: 3m 12s\n",
      "128:\tlearn: 0.1328997\ttotal: 28.4s\tremaining: 3m 12s\n",
      "129:\tlearn: 0.1327200\ttotal: 28.7s\tremaining: 3m 11s\n",
      "130:\tlearn: 0.1325168\ttotal: 28.9s\tremaining: 3m 11s\n",
      "131:\tlearn: 0.1324177\ttotal: 29.1s\tremaining: 3m 11s\n",
      "132:\tlearn: 0.1322172\ttotal: 29.3s\tremaining: 3m 11s\n",
      "133:\tlearn: 0.1320158\ttotal: 29.5s\tremaining: 3m 10s\n",
      "134:\tlearn: 0.1317821\ttotal: 29.7s\tremaining: 3m 10s\n",
      "135:\tlearn: 0.1315842\ttotal: 30s\tremaining: 3m 10s\n",
      "136:\tlearn: 0.1314595\ttotal: 30.2s\tremaining: 3m 10s\n",
      "137:\tlearn: 0.1313028\ttotal: 30.5s\tremaining: 3m 10s\n",
      "138:\tlearn: 0.1311673\ttotal: 30.7s\tremaining: 3m 10s\n",
      "139:\tlearn: 0.1309650\ttotal: 30.9s\tremaining: 3m 10s\n",
      "140:\tlearn: 0.1307964\ttotal: 31.2s\tremaining: 3m 9s\n",
      "141:\tlearn: 0.1305710\ttotal: 31.4s\tremaining: 3m 9s\n",
      "142:\tlearn: 0.1303848\ttotal: 31.7s\tremaining: 3m 9s\n",
      "143:\tlearn: 0.1302175\ttotal: 31.9s\tremaining: 3m 9s\n",
      "144:\tlearn: 0.1301154\ttotal: 32.1s\tremaining: 3m 9s\n",
      "145:\tlearn: 0.1300224\ttotal: 32.3s\tremaining: 3m 8s\n",
      "146:\tlearn: 0.1299007\ttotal: 32.5s\tremaining: 3m 8s\n",
      "147:\tlearn: 0.1298037\ttotal: 32.7s\tremaining: 3m 8s\n",
      "148:\tlearn: 0.1296381\ttotal: 33s\tremaining: 3m 8s\n",
      "149:\tlearn: 0.1294381\ttotal: 33.2s\tremaining: 3m 7s\n",
      "150:\tlearn: 0.1293205\ttotal: 33.4s\tremaining: 3m 7s\n",
      "151:\tlearn: 0.1291421\ttotal: 33.6s\tremaining: 3m 7s\n",
      "152:\tlearn: 0.1289646\ttotal: 33.8s\tremaining: 3m 6s\n",
      "153:\tlearn: 0.1288277\ttotal: 34s\tremaining: 3m 6s\n",
      "154:\tlearn: 0.1286574\ttotal: 34.2s\tremaining: 3m 6s\n",
      "155:\tlearn: 0.1285411\ttotal: 34.4s\tremaining: 3m 5s\n",
      "156:\tlearn: 0.1284081\ttotal: 34.6s\tremaining: 3m 6s\n",
      "157:\tlearn: 0.1282780\ttotal: 34.9s\tremaining: 3m 6s\n",
      "158:\tlearn: 0.1281609\ttotal: 35.2s\tremaining: 3m 6s\n",
      "159:\tlearn: 0.1279973\ttotal: 35.5s\tremaining: 3m 6s\n",
      "160:\tlearn: 0.1277480\ttotal: 35.8s\tremaining: 3m 6s\n",
      "161:\tlearn: 0.1276532\ttotal: 36s\tremaining: 3m 6s\n",
      "162:\tlearn: 0.1275249\ttotal: 36.2s\tremaining: 3m 5s\n",
      "163:\tlearn: 0.1274306\ttotal: 36.4s\tremaining: 3m 5s\n",
      "164:\tlearn: 0.1272968\ttotal: 36.6s\tremaining: 3m 5s\n",
      "165:\tlearn: 0.1271717\ttotal: 36.8s\tremaining: 3m 4s\n",
      "166:\tlearn: 0.1270548\ttotal: 37s\tremaining: 3m 4s\n",
      "167:\tlearn: 0.1268903\ttotal: 37.1s\tremaining: 3m 3s\n",
      "168:\tlearn: 0.1267191\ttotal: 37.3s\tremaining: 3m 3s\n",
      "169:\tlearn: 0.1265886\ttotal: 37.5s\tremaining: 3m 3s\n",
      "170:\tlearn: 0.1264711\ttotal: 37.7s\tremaining: 3m 2s\n",
      "171:\tlearn: 0.1263969\ttotal: 37.9s\tremaining: 3m 2s\n",
      "172:\tlearn: 0.1262491\ttotal: 38.1s\tremaining: 3m 2s\n",
      "173:\tlearn: 0.1261359\ttotal: 38.4s\tremaining: 3m 2s\n",
      "174:\tlearn: 0.1260275\ttotal: 38.6s\tremaining: 3m 2s\n",
      "175:\tlearn: 0.1258801\ttotal: 38.8s\tremaining: 3m 1s\n",
      "176:\tlearn: 0.1257426\ttotal: 39.1s\tremaining: 3m 1s\n",
      "177:\tlearn: 0.1255992\ttotal: 39.3s\tremaining: 3m 1s\n",
      "178:\tlearn: 0.1254848\ttotal: 39.5s\tremaining: 3m 1s\n",
      "179:\tlearn: 0.1252936\ttotal: 39.7s\tremaining: 3m\n",
      "180:\tlearn: 0.1251635\ttotal: 39.9s\tremaining: 3m\n",
      "181:\tlearn: 0.1250411\ttotal: 40.2s\tremaining: 3m\n",
      "182:\tlearn: 0.1249671\ttotal: 40.4s\tremaining: 3m\n",
      "183:\tlearn: 0.1248743\ttotal: 40.6s\tremaining: 2m 59s\n",
      "184:\tlearn: 0.1246733\ttotal: 40.7s\tremaining: 2m 59s\n",
      "185:\tlearn: 0.1245744\ttotal: 40.9s\tremaining: 2m 59s\n",
      "186:\tlearn: 0.1244250\ttotal: 41.1s\tremaining: 2m 58s\n",
      "187:\tlearn: 0.1243543\ttotal: 41.3s\tremaining: 2m 58s\n",
      "188:\tlearn: 0.1242209\ttotal: 41.5s\tremaining: 2m 58s\n",
      "189:\tlearn: 0.1241337\ttotal: 41.7s\tremaining: 2m 57s\n",
      "190:\tlearn: 0.1240492\ttotal: 41.9s\tremaining: 2m 57s\n",
      "191:\tlearn: 0.1239798\ttotal: 42.2s\tremaining: 2m 57s\n",
      "192:\tlearn: 0.1238823\ttotal: 42.4s\tremaining: 2m 57s\n",
      "193:\tlearn: 0.1237694\ttotal: 42.6s\tremaining: 2m 56s\n",
      "194:\tlearn: 0.1236523\ttotal: 42.8s\tremaining: 2m 56s\n",
      "195:\tlearn: 0.1235538\ttotal: 43s\tremaining: 2m 56s\n",
      "196:\tlearn: 0.1234028\ttotal: 43.3s\tremaining: 2m 56s\n",
      "197:\tlearn: 0.1232906\ttotal: 43.5s\tremaining: 2m 56s\n",
      "198:\tlearn: 0.1232120\ttotal: 43.8s\tremaining: 2m 56s\n",
      "199:\tlearn: 0.1231105\ttotal: 44s\tremaining: 2m 55s\n",
      "200:\tlearn: 0.1229849\ttotal: 44.2s\tremaining: 2m 55s\n",
      "201:\tlearn: 0.1229220\ttotal: 44.4s\tremaining: 2m 55s\n",
      "202:\tlearn: 0.1228234\ttotal: 44.5s\tremaining: 2m 54s\n",
      "203:\tlearn: 0.1227124\ttotal: 44.8s\tremaining: 2m 54s\n",
      "204:\tlearn: 0.1226400\ttotal: 44.9s\tremaining: 2m 54s\n",
      "205:\tlearn: 0.1225324\ttotal: 45.2s\tremaining: 2m 54s\n",
      "206:\tlearn: 0.1224537\ttotal: 45.4s\tremaining: 2m 53s\n",
      "207:\tlearn: 0.1223591\ttotal: 45.6s\tremaining: 2m 53s\n",
      "208:\tlearn: 0.1222832\ttotal: 45.8s\tremaining: 2m 53s\n",
      "209:\tlearn: 0.1222291\ttotal: 46.1s\tremaining: 2m 53s\n",
      "210:\tlearn: 0.1221078\ttotal: 46.3s\tremaining: 2m 53s\n",
      "211:\tlearn: 0.1219951\ttotal: 46.6s\tremaining: 2m 53s\n",
      "212:\tlearn: 0.1218245\ttotal: 46.8s\tremaining: 2m 53s\n",
      "213:\tlearn: 0.1217314\ttotal: 47.1s\tremaining: 2m 53s\n",
      "214:\tlearn: 0.1216536\ttotal: 47.3s\tremaining: 2m 52s\n",
      "215:\tlearn: 0.1215888\ttotal: 47.5s\tremaining: 2m 52s\n",
      "216:\tlearn: 0.1214786\ttotal: 47.8s\tremaining: 2m 52s\n",
      "217:\tlearn: 0.1213901\ttotal: 48s\tremaining: 2m 52s\n",
      "218:\tlearn: 0.1212720\ttotal: 48.2s\tremaining: 2m 51s\n",
      "219:\tlearn: 0.1211830\ttotal: 48.4s\tremaining: 2m 51s\n",
      "220:\tlearn: 0.1211086\ttotal: 48.6s\tremaining: 2m 51s\n",
      "221:\tlearn: 0.1210225\ttotal: 48.8s\tremaining: 2m 50s\n",
      "222:\tlearn: 0.1209315\ttotal: 49s\tremaining: 2m 50s\n",
      "223:\tlearn: 0.1208519\ttotal: 49.2s\tremaining: 2m 50s\n",
      "224:\tlearn: 0.1207485\ttotal: 49.4s\tremaining: 2m 50s\n",
      "225:\tlearn: 0.1206290\ttotal: 49.6s\tremaining: 2m 50s\n",
      "226:\tlearn: 0.1205266\ttotal: 49.8s\tremaining: 2m 49s\n",
      "227:\tlearn: 0.1204416\ttotal: 50s\tremaining: 2m 49s\n",
      "228:\tlearn: 0.1203149\ttotal: 50.2s\tremaining: 2m 49s\n",
      "229:\tlearn: 0.1202519\ttotal: 50.5s\tremaining: 2m 49s\n",
      "230:\tlearn: 0.1201116\ttotal: 50.7s\tremaining: 2m 48s\n",
      "231:\tlearn: 0.1200454\ttotal: 50.9s\tremaining: 2m 48s\n",
      "232:\tlearn: 0.1199442\ttotal: 51.2s\tremaining: 2m 48s\n",
      "233:\tlearn: 0.1198294\ttotal: 51.4s\tremaining: 2m 48s\n",
      "234:\tlearn: 0.1196434\ttotal: 51.6s\tremaining: 2m 48s\n",
      "235:\tlearn: 0.1195909\ttotal: 51.8s\tremaining: 2m 47s\n",
      "236:\tlearn: 0.1194410\ttotal: 52.1s\tremaining: 2m 47s\n",
      "237:\tlearn: 0.1193759\ttotal: 52.3s\tremaining: 2m 47s\n",
      "238:\tlearn: 0.1192932\ttotal: 52.4s\tremaining: 2m 47s\n",
      "239:\tlearn: 0.1191965\ttotal: 52.6s\tremaining: 2m 46s\n",
      "240:\tlearn: 0.1191094\ttotal: 52.9s\tremaining: 2m 46s\n",
      "241:\tlearn: 0.1190189\ttotal: 53.1s\tremaining: 2m 46s\n",
      "242:\tlearn: 0.1189308\ttotal: 53.3s\tremaining: 2m 46s\n",
      "243:\tlearn: 0.1188614\ttotal: 53.5s\tremaining: 2m 45s\n",
      "244:\tlearn: 0.1187886\ttotal: 53.7s\tremaining: 2m 45s\n",
      "245:\tlearn: 0.1186995\ttotal: 53.9s\tremaining: 2m 45s\n",
      "246:\tlearn: 0.1185815\ttotal: 54.1s\tremaining: 2m 44s\n",
      "247:\tlearn: 0.1185232\ttotal: 54.3s\tremaining: 2m 44s\n",
      "248:\tlearn: 0.1183903\ttotal: 54.6s\tremaining: 2m 44s\n",
      "249:\tlearn: 0.1183429\ttotal: 54.8s\tremaining: 2m 44s\n",
      "250:\tlearn: 0.1182335\ttotal: 55s\tremaining: 2m 44s\n",
      "251:\tlearn: 0.1181324\ttotal: 55.3s\tremaining: 2m 44s\n",
      "252:\tlearn: 0.1180521\ttotal: 55.7s\tremaining: 2m 44s\n",
      "253:\tlearn: 0.1179884\ttotal: 55.9s\tremaining: 2m 44s\n",
      "254:\tlearn: 0.1178994\ttotal: 56.1s\tremaining: 2m 43s\n",
      "255:\tlearn: 0.1178190\ttotal: 56.4s\tremaining: 2m 43s\n",
      "256:\tlearn: 0.1177509\ttotal: 56.6s\tremaining: 2m 43s\n",
      "257:\tlearn: 0.1177043\ttotal: 56.9s\tremaining: 2m 43s\n",
      "258:\tlearn: 0.1176323\ttotal: 57.1s\tremaining: 2m 43s\n",
      "259:\tlearn: 0.1175464\ttotal: 57.4s\tremaining: 2m 43s\n",
      "260:\tlearn: 0.1174811\ttotal: 57.7s\tremaining: 2m 43s\n",
      "261:\tlearn: 0.1173292\ttotal: 57.9s\tremaining: 2m 43s\n",
      "262:\tlearn: 0.1172630\ttotal: 58.2s\tremaining: 2m 42s\n",
      "263:\tlearn: 0.1171631\ttotal: 58.4s\tremaining: 2m 42s\n",
      "264:\tlearn: 0.1170794\ttotal: 58.6s\tremaining: 2m 42s\n",
      "265:\tlearn: 0.1169735\ttotal: 58.9s\tremaining: 2m 42s\n",
      "266:\tlearn: 0.1168613\ttotal: 59.2s\tremaining: 2m 42s\n",
      "267:\tlearn: 0.1167697\ttotal: 59.4s\tremaining: 2m 42s\n",
      "268:\tlearn: 0.1167221\ttotal: 59.7s\tremaining: 2m 42s\n",
      "269:\tlearn: 0.1166609\ttotal: 59.9s\tremaining: 2m 41s\n",
      "270:\tlearn: 0.1165954\ttotal: 1m\tremaining: 2m 41s\n",
      "271:\tlearn: 0.1165240\ttotal: 1m\tremaining: 2m 41s\n",
      "272:\tlearn: 0.1164386\ttotal: 1m\tremaining: 2m 41s\n",
      "273:\tlearn: 0.1163481\ttotal: 1m\tremaining: 2m 40s\n",
      "274:\tlearn: 0.1162075\ttotal: 1m\tremaining: 2m 40s\n",
      "275:\tlearn: 0.1161354\ttotal: 1m 1s\tremaining: 2m 40s\n",
      "276:\tlearn: 0.1160074\ttotal: 1m 1s\tremaining: 2m 40s\n",
      "277:\tlearn: 0.1159134\ttotal: 1m 1s\tremaining: 2m 40s\n",
      "278:\tlearn: 0.1158136\ttotal: 1m 1s\tremaining: 2m 39s\n",
      "279:\tlearn: 0.1156897\ttotal: 1m 2s\tremaining: 2m 39s\n",
      "280:\tlearn: 0.1156245\ttotal: 1m 2s\tremaining: 2m 39s\n",
      "281:\tlearn: 0.1155582\ttotal: 1m 2s\tremaining: 2m 39s\n",
      "282:\tlearn: 0.1154669\ttotal: 1m 2s\tremaining: 2m 39s\n",
      "283:\tlearn: 0.1153866\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "284:\tlearn: 0.1153227\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "285:\tlearn: 0.1152299\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "286:\tlearn: 0.1151530\ttotal: 1m 4s\tremaining: 2m 39s\n",
      "287:\tlearn: 0.1150557\ttotal: 1m 4s\tremaining: 2m 39s\n",
      "288:\tlearn: 0.1149817\ttotal: 1m 4s\tremaining: 2m 38s\n",
      "289:\tlearn: 0.1149167\ttotal: 1m 4s\tremaining: 2m 38s\n",
      "290:\tlearn: 0.1148671\ttotal: 1m 5s\tremaining: 2m 38s\n",
      "291:\tlearn: 0.1147123\ttotal: 1m 5s\tremaining: 2m 38s\n",
      "292:\tlearn: 0.1146414\ttotal: 1m 5s\tremaining: 2m 38s\n",
      "293:\tlearn: 0.1146022\ttotal: 1m 5s\tremaining: 2m 38s\n",
      "294:\tlearn: 0.1145477\ttotal: 1m 6s\tremaining: 2m 38s\n",
      "295:\tlearn: 0.1144632\ttotal: 1m 6s\tremaining: 2m 38s\n",
      "296:\tlearn: 0.1144034\ttotal: 1m 6s\tremaining: 2m 38s\n",
      "297:\tlearn: 0.1143151\ttotal: 1m 7s\tremaining: 2m 38s\n",
      "298:\tlearn: 0.1142358\ttotal: 1m 7s\tremaining: 2m 37s\n",
      "299:\tlearn: 0.1141885\ttotal: 1m 7s\tremaining: 2m 37s\n",
      "300:\tlearn: 0.1141214\ttotal: 1m 7s\tremaining: 2m 37s\n",
      "301:\tlearn: 0.1140236\ttotal: 1m 8s\tremaining: 2m 37s\n",
      "302:\tlearn: 0.1139415\ttotal: 1m 8s\tremaining: 2m 37s\n",
      "303:\tlearn: 0.1138426\ttotal: 1m 8s\tremaining: 2m 37s\n",
      "304:\tlearn: 0.1137681\ttotal: 1m 8s\tremaining: 2m 36s\n",
      "305:\tlearn: 0.1136945\ttotal: 1m 9s\tremaining: 2m 36s\n",
      "306:\tlearn: 0.1135865\ttotal: 1m 9s\tremaining: 2m 36s\n",
      "307:\tlearn: 0.1135251\ttotal: 1m 9s\tremaining: 2m 36s\n",
      "308:\tlearn: 0.1133833\ttotal: 1m 9s\tremaining: 2m 36s\n",
      "309:\tlearn: 0.1133104\ttotal: 1m 10s\tremaining: 2m 36s\n",
      "310:\tlearn: 0.1132768\ttotal: 1m 10s\tremaining: 2m 36s\n",
      "311:\tlearn: 0.1132080\ttotal: 1m 10s\tremaining: 2m 35s\n",
      "312:\tlearn: 0.1130956\ttotal: 1m 10s\tremaining: 2m 35s\n",
      "313:\tlearn: 0.1129764\ttotal: 1m 11s\tremaining: 2m 35s\n",
      "314:\tlearn: 0.1129095\ttotal: 1m 11s\tremaining: 2m 35s\n",
      "315:\tlearn: 0.1128267\ttotal: 1m 11s\tremaining: 2m 35s\n",
      "316:\tlearn: 0.1127224\ttotal: 1m 12s\tremaining: 2m 35s\n",
      "317:\tlearn: 0.1126764\ttotal: 1m 12s\tremaining: 2m 35s\n",
      "318:\tlearn: 0.1126181\ttotal: 1m 12s\tremaining: 2m 35s\n",
      "319:\tlearn: 0.1125359\ttotal: 1m 12s\tremaining: 2m 35s\n",
      "320:\tlearn: 0.1124711\ttotal: 1m 13s\tremaining: 2m 34s\n",
      "321:\tlearn: 0.1124024\ttotal: 1m 13s\tremaining: 2m 34s\n",
      "322:\tlearn: 0.1123259\ttotal: 1m 13s\tremaining: 2m 34s\n",
      "323:\tlearn: 0.1122252\ttotal: 1m 14s\tremaining: 2m 34s\n",
      "324:\tlearn: 0.1121648\ttotal: 1m 14s\tremaining: 2m 34s\n",
      "325:\tlearn: 0.1120810\ttotal: 1m 14s\tremaining: 2m 34s\n",
      "326:\tlearn: 0.1119967\ttotal: 1m 14s\tremaining: 2m 33s\n",
      "327:\tlearn: 0.1118738\ttotal: 1m 14s\tremaining: 2m 33s\n",
      "328:\tlearn: 0.1118041\ttotal: 1m 15s\tremaining: 2m 33s\n",
      "329:\tlearn: 0.1117604\ttotal: 1m 15s\tremaining: 2m 33s\n",
      "330:\tlearn: 0.1116849\ttotal: 1m 15s\tremaining: 2m 32s\n",
      "331:\tlearn: 0.1116289\ttotal: 1m 15s\tremaining: 2m 32s\n",
      "332:\tlearn: 0.1115532\ttotal: 1m 16s\tremaining: 2m 32s\n",
      "333:\tlearn: 0.1114874\ttotal: 1m 16s\tremaining: 2m 32s\n",
      "334:\tlearn: 0.1114314\ttotal: 1m 16s\tremaining: 2m 31s\n",
      "335:\tlearn: 0.1113869\ttotal: 1m 16s\tremaining: 2m 31s\n",
      "336:\tlearn: 0.1113416\ttotal: 1m 17s\tremaining: 2m 31s\n",
      "337:\tlearn: 0.1112432\ttotal: 1m 17s\tremaining: 2m 31s\n",
      "338:\tlearn: 0.1111890\ttotal: 1m 17s\tremaining: 2m 30s\n",
      "339:\tlearn: 0.1111242\ttotal: 1m 17s\tremaining: 2m 30s\n",
      "340:\tlearn: 0.1111020\ttotal: 1m 17s\tremaining: 2m 30s\n",
      "341:\tlearn: 0.1110408\ttotal: 1m 18s\tremaining: 2m 30s\n",
      "342:\tlearn: 0.1110005\ttotal: 1m 18s\tremaining: 2m 29s\n",
      "343:\tlearn: 0.1109241\ttotal: 1m 18s\tremaining: 2m 29s\n",
      "344:\tlearn: 0.1108479\ttotal: 1m 18s\tremaining: 2m 29s\n",
      "345:\tlearn: 0.1107707\ttotal: 1m 19s\tremaining: 2m 29s\n",
      "346:\tlearn: 0.1107160\ttotal: 1m 19s\tremaining: 2m 29s\n",
      "347:\tlearn: 0.1106612\ttotal: 1m 19s\tremaining: 2m 28s\n",
      "348:\tlearn: 0.1106095\ttotal: 1m 19s\tremaining: 2m 28s\n",
      "349:\tlearn: 0.1105013\ttotal: 1m 20s\tremaining: 2m 28s\n",
      "350:\tlearn: 0.1104252\ttotal: 1m 20s\tremaining: 2m 28s\n",
      "351:\tlearn: 0.1103362\ttotal: 1m 20s\tremaining: 2m 28s\n",
      "352:\tlearn: 0.1102784\ttotal: 1m 20s\tremaining: 2m 27s\n",
      "353:\tlearn: 0.1101721\ttotal: 1m 20s\tremaining: 2m 27s\n",
      "354:\tlearn: 0.1101166\ttotal: 1m 21s\tremaining: 2m 27s\n",
      "355:\tlearn: 0.1100423\ttotal: 1m 21s\tremaining: 2m 27s\n",
      "356:\tlearn: 0.1100082\ttotal: 1m 21s\tremaining: 2m 27s\n",
      "357:\tlearn: 0.1099396\ttotal: 1m 22s\tremaining: 2m 27s\n",
      "358:\tlearn: 0.1098816\ttotal: 1m 22s\tremaining: 2m 27s\n",
      "359:\tlearn: 0.1097791\ttotal: 1m 22s\tremaining: 2m 27s\n",
      "360:\tlearn: 0.1097244\ttotal: 1m 23s\tremaining: 2m 27s\n",
      "361:\tlearn: 0.1096760\ttotal: 1m 23s\tremaining: 2m 27s\n",
      "362:\tlearn: 0.1096350\ttotal: 1m 23s\tremaining: 2m 27s\n",
      "363:\tlearn: 0.1095970\ttotal: 1m 24s\tremaining: 2m 26s\n",
      "364:\tlearn: 0.1095481\ttotal: 1m 24s\tremaining: 2m 26s\n",
      "365:\tlearn: 0.1094471\ttotal: 1m 24s\tremaining: 2m 26s\n",
      "366:\tlearn: 0.1093680\ttotal: 1m 24s\tremaining: 2m 26s\n",
      "367:\tlearn: 0.1093365\ttotal: 1m 25s\tremaining: 2m 26s\n",
      "368:\tlearn: 0.1092769\ttotal: 1m 25s\tremaining: 2m 25s\n",
      "369:\tlearn: 0.1092060\ttotal: 1m 25s\tremaining: 2m 25s\n",
      "370:\tlearn: 0.1091440\ttotal: 1m 25s\tremaining: 2m 25s\n",
      "371:\tlearn: 0.1090660\ttotal: 1m 25s\tremaining: 2m 25s\n",
      "372:\tlearn: 0.1089882\ttotal: 1m 26s\tremaining: 2m 24s\n",
      "373:\tlearn: 0.1089323\ttotal: 1m 26s\tremaining: 2m 24s\n",
      "374:\tlearn: 0.1088858\ttotal: 1m 26s\tremaining: 2m 24s\n",
      "375:\tlearn: 0.1088403\ttotal: 1m 26s\tremaining: 2m 24s\n",
      "376:\tlearn: 0.1088026\ttotal: 1m 27s\tremaining: 2m 23s\n",
      "377:\tlearn: 0.1087703\ttotal: 1m 27s\tremaining: 2m 23s\n",
      "378:\tlearn: 0.1087404\ttotal: 1m 27s\tremaining: 2m 23s\n",
      "379:\tlearn: 0.1086438\ttotal: 1m 27s\tremaining: 2m 23s\n",
      "380:\tlearn: 0.1085983\ttotal: 1m 28s\tremaining: 2m 23s\n",
      "381:\tlearn: 0.1085114\ttotal: 1m 28s\tremaining: 2m 23s\n",
      "382:\tlearn: 0.1084244\ttotal: 1m 28s\tremaining: 2m 22s\n",
      "383:\tlearn: 0.1083682\ttotal: 1m 28s\tremaining: 2m 22s\n",
      "384:\tlearn: 0.1083366\ttotal: 1m 29s\tremaining: 2m 22s\n",
      "385:\tlearn: 0.1082919\ttotal: 1m 29s\tremaining: 2m 22s\n",
      "386:\tlearn: 0.1082268\ttotal: 1m 29s\tremaining: 2m 22s\n",
      "387:\tlearn: 0.1081537\ttotal: 1m 29s\tremaining: 2m 21s\n",
      "388:\tlearn: 0.1080550\ttotal: 1m 30s\tremaining: 2m 21s\n",
      "389:\tlearn: 0.1079731\ttotal: 1m 30s\tremaining: 2m 21s\n",
      "390:\tlearn: 0.1078830\ttotal: 1m 30s\tremaining: 2m 21s\n",
      "391:\tlearn: 0.1078206\ttotal: 1m 30s\tremaining: 2m 20s\n",
      "392:\tlearn: 0.1077699\ttotal: 1m 31s\tremaining: 2m 20s\n",
      "393:\tlearn: 0.1077343\ttotal: 1m 31s\tremaining: 2m 20s\n",
      "394:\tlearn: 0.1076493\ttotal: 1m 31s\tremaining: 2m 20s\n",
      "395:\tlearn: 0.1075909\ttotal: 1m 31s\tremaining: 2m 19s\n",
      "396:\tlearn: 0.1075423\ttotal: 1m 31s\tremaining: 2m 19s\n",
      "397:\tlearn: 0.1074369\ttotal: 1m 32s\tremaining: 2m 19s\n",
      "398:\tlearn: 0.1073740\ttotal: 1m 32s\tremaining: 2m 19s\n",
      "399:\tlearn: 0.1072875\ttotal: 1m 32s\tremaining: 2m 18s\n",
      "400:\tlearn: 0.1071992\ttotal: 1m 32s\tremaining: 2m 18s\n",
      "401:\tlearn: 0.1071347\ttotal: 1m 33s\tremaining: 2m 18s\n",
      "402:\tlearn: 0.1070507\ttotal: 1m 33s\tremaining: 2m 18s\n",
      "403:\tlearn: 0.1069665\ttotal: 1m 33s\tremaining: 2m 17s\n",
      "404:\tlearn: 0.1069057\ttotal: 1m 33s\tremaining: 2m 17s\n",
      "405:\tlearn: 0.1068634\ttotal: 1m 33s\tremaining: 2m 17s\n",
      "406:\tlearn: 0.1068092\ttotal: 1m 34s\tremaining: 2m 17s\n",
      "407:\tlearn: 0.1067085\ttotal: 1m 34s\tremaining: 2m 16s\n",
      "408:\tlearn: 0.1066506\ttotal: 1m 34s\tremaining: 2m 16s\n",
      "409:\tlearn: 0.1066068\ttotal: 1m 34s\tremaining: 2m 16s\n",
      "410:\tlearn: 0.1065016\ttotal: 1m 34s\tremaining: 2m 15s\n",
      "411:\tlearn: 0.1064580\ttotal: 1m 35s\tremaining: 2m 15s\n",
      "412:\tlearn: 0.1064131\ttotal: 1m 35s\tremaining: 2m 15s\n",
      "413:\tlearn: 0.1062865\ttotal: 1m 35s\tremaining: 2m 15s\n",
      "414:\tlearn: 0.1062522\ttotal: 1m 35s\tremaining: 2m 14s\n",
      "415:\tlearn: 0.1061467\ttotal: 1m 35s\tremaining: 2m 14s\n",
      "416:\tlearn: 0.1061023\ttotal: 1m 36s\tremaining: 2m 14s\n",
      "417:\tlearn: 0.1060484\ttotal: 1m 36s\tremaining: 2m 14s\n",
      "418:\tlearn: 0.1059826\ttotal: 1m 36s\tremaining: 2m 13s\n",
      "419:\tlearn: 0.1059230\ttotal: 1m 36s\tremaining: 2m 13s\n",
      "420:\tlearn: 0.1058511\ttotal: 1m 36s\tremaining: 2m 13s\n",
      "421:\tlearn: 0.1058153\ttotal: 1m 37s\tremaining: 2m 12s\n",
      "422:\tlearn: 0.1057508\ttotal: 1m 37s\tremaining: 2m 12s\n",
      "423:\tlearn: 0.1056961\ttotal: 1m 37s\tremaining: 2m 12s\n",
      "424:\tlearn: 0.1056082\ttotal: 1m 37s\tremaining: 2m 12s\n",
      "425:\tlearn: 0.1055704\ttotal: 1m 37s\tremaining: 2m 11s\n",
      "426:\tlearn: 0.1054941\ttotal: 1m 37s\tremaining: 2m 11s\n",
      "427:\tlearn: 0.1054303\ttotal: 1m 38s\tremaining: 2m 11s\n",
      "428:\tlearn: 0.1053756\ttotal: 1m 38s\tremaining: 2m 10s\n",
      "429:\tlearn: 0.1052860\ttotal: 1m 38s\tremaining: 2m 10s\n",
      "430:\tlearn: 0.1052516\ttotal: 1m 38s\tremaining: 2m 10s\n",
      "431:\tlearn: 0.1051898\ttotal: 1m 38s\tremaining: 2m 10s\n",
      "432:\tlearn: 0.1050998\ttotal: 1m 39s\tremaining: 2m 9s\n",
      "433:\tlearn: 0.1050600\ttotal: 1m 39s\tremaining: 2m 9s\n",
      "434:\tlearn: 0.1049861\ttotal: 1m 39s\tremaining: 2m 9s\n",
      "435:\tlearn: 0.1048991\ttotal: 1m 39s\tremaining: 2m 9s\n",
      "436:\tlearn: 0.1048390\ttotal: 1m 39s\tremaining: 2m 8s\n",
      "437:\tlearn: 0.1048151\ttotal: 1m 40s\tremaining: 2m 8s\n",
      "438:\tlearn: 0.1047618\ttotal: 1m 40s\tremaining: 2m 8s\n",
      "439:\tlearn: 0.1047287\ttotal: 1m 40s\tremaining: 2m 7s\n",
      "440:\tlearn: 0.1046825\ttotal: 1m 40s\tremaining: 2m 7s\n",
      "441:\tlearn: 0.1046643\ttotal: 1m 40s\tremaining: 2m 7s\n",
      "442:\tlearn: 0.1046160\ttotal: 1m 41s\tremaining: 2m 7s\n",
      "443:\tlearn: 0.1045506\ttotal: 1m 41s\tremaining: 2m 6s\n",
      "444:\tlearn: 0.1045067\ttotal: 1m 41s\tremaining: 2m 6s\n",
      "445:\tlearn: 0.1044555\ttotal: 1m 41s\tremaining: 2m 6s\n",
      "446:\tlearn: 0.1044085\ttotal: 1m 41s\tremaining: 2m 6s\n",
      "447:\tlearn: 0.1043758\ttotal: 1m 42s\tremaining: 2m 5s\n",
      "448:\tlearn: 0.1043565\ttotal: 1m 42s\tremaining: 2m 5s\n",
      "449:\tlearn: 0.1043115\ttotal: 1m 42s\tremaining: 2m 5s\n",
      "450:\tlearn: 0.1042115\ttotal: 1m 42s\tremaining: 2m 5s\n",
      "451:\tlearn: 0.1041576\ttotal: 1m 43s\tremaining: 2m 4s\n",
      "452:\tlearn: 0.1040758\ttotal: 1m 43s\tremaining: 2m 4s\n",
      "453:\tlearn: 0.1040070\ttotal: 1m 43s\tremaining: 2m 4s\n",
      "454:\tlearn: 0.1039422\ttotal: 1m 43s\tremaining: 2m 4s\n",
      "455:\tlearn: 0.1038935\ttotal: 1m 43s\tremaining: 2m 3s\n",
      "456:\tlearn: 0.1038491\ttotal: 1m 44s\tremaining: 2m 3s\n",
      "457:\tlearn: 0.1037959\ttotal: 1m 44s\tremaining: 2m 3s\n",
      "458:\tlearn: 0.1037489\ttotal: 1m 44s\tremaining: 2m 3s\n",
      "459:\tlearn: 0.1036706\ttotal: 1m 44s\tremaining: 2m 2s\n",
      "460:\tlearn: 0.1036374\ttotal: 1m 44s\tremaining: 2m 2s\n",
      "461:\tlearn: 0.1035928\ttotal: 1m 45s\tremaining: 2m 2s\n",
      "462:\tlearn: 0.1035213\ttotal: 1m 45s\tremaining: 2m 2s\n",
      "463:\tlearn: 0.1034663\ttotal: 1m 45s\tremaining: 2m 1s\n",
      "464:\tlearn: 0.1034092\ttotal: 1m 45s\tremaining: 2m 1s\n",
      "465:\tlearn: 0.1033374\ttotal: 1m 45s\tremaining: 2m 1s\n",
      "466:\tlearn: 0.1032581\ttotal: 1m 45s\tremaining: 2m\n",
      "467:\tlearn: 0.1032262\ttotal: 1m 46s\tremaining: 2m\n",
      "468:\tlearn: 0.1031809\ttotal: 1m 46s\tremaining: 2m\n",
      "469:\tlearn: 0.1031160\ttotal: 1m 46s\tremaining: 2m\n",
      "470:\tlearn: 0.1030886\ttotal: 1m 46s\tremaining: 1m 59s\n",
      "471:\tlearn: 0.1030426\ttotal: 1m 46s\tremaining: 1m 59s\n",
      "472:\tlearn: 0.1030019\ttotal: 1m 47s\tremaining: 1m 59s\n",
      "473:\tlearn: 0.1029448\ttotal: 1m 47s\tremaining: 1m 59s\n",
      "474:\tlearn: 0.1028600\ttotal: 1m 47s\tremaining: 1m 58s\n",
      "475:\tlearn: 0.1027966\ttotal: 1m 47s\tremaining: 1m 58s\n",
      "476:\tlearn: 0.1027598\ttotal: 1m 48s\tremaining: 1m 58s\n",
      "477:\tlearn: 0.1027184\ttotal: 1m 48s\tremaining: 1m 58s\n",
      "478:\tlearn: 0.1026844\ttotal: 1m 48s\tremaining: 1m 58s\n",
      "479:\tlearn: 0.1026395\ttotal: 1m 48s\tremaining: 1m 58s\n",
      "480:\tlearn: 0.1026084\ttotal: 1m 49s\tremaining: 1m 57s\n",
      "481:\tlearn: 0.1025635\ttotal: 1m 49s\tremaining: 1m 57s\n",
      "482:\tlearn: 0.1025134\ttotal: 1m 49s\tremaining: 1m 57s\n",
      "483:\tlearn: 0.1024677\ttotal: 1m 49s\tremaining: 1m 57s\n",
      "484:\tlearn: 0.1024323\ttotal: 1m 50s\tremaining: 1m 56s\n",
      "485:\tlearn: 0.1023847\ttotal: 1m 50s\tremaining: 1m 56s\n",
      "486:\tlearn: 0.1022965\ttotal: 1m 50s\tremaining: 1m 56s\n",
      "487:\tlearn: 0.1022227\ttotal: 1m 50s\tremaining: 1m 56s\n",
      "488:\tlearn: 0.1021275\ttotal: 1m 50s\tremaining: 1m 55s\n",
      "489:\tlearn: 0.1020517\ttotal: 1m 51s\tremaining: 1m 55s\n",
      "490:\tlearn: 0.1020073\ttotal: 1m 51s\tremaining: 1m 55s\n",
      "491:\tlearn: 0.1019487\ttotal: 1m 51s\tremaining: 1m 55s\n",
      "492:\tlearn: 0.1019062\ttotal: 1m 51s\tremaining: 1m 54s\n",
      "493:\tlearn: 0.1018498\ttotal: 1m 51s\tremaining: 1m 54s\n",
      "494:\tlearn: 0.1017845\ttotal: 1m 52s\tremaining: 1m 54s\n",
      "495:\tlearn: 0.1017370\ttotal: 1m 52s\tremaining: 1m 54s\n",
      "496:\tlearn: 0.1016781\ttotal: 1m 52s\tremaining: 1m 54s\n",
      "497:\tlearn: 0.1016511\ttotal: 1m 52s\tremaining: 1m 53s\n",
      "498:\tlearn: 0.1016132\ttotal: 1m 53s\tremaining: 1m 53s\n",
      "499:\tlearn: 0.1015724\ttotal: 1m 53s\tremaining: 1m 53s\n",
      "500:\tlearn: 0.1015168\ttotal: 1m 53s\tremaining: 1m 52s\n",
      "501:\tlearn: 0.1014214\ttotal: 1m 53s\tremaining: 1m 52s\n",
      "502:\tlearn: 0.1013696\ttotal: 1m 53s\tremaining: 1m 52s\n",
      "503:\tlearn: 0.1013259\ttotal: 1m 54s\tremaining: 1m 52s\n",
      "504:\tlearn: 0.1012913\ttotal: 1m 54s\tremaining: 1m 51s\n",
      "505:\tlearn: 0.1012604\ttotal: 1m 54s\tremaining: 1m 51s\n",
      "506:\tlearn: 0.1012238\ttotal: 1m 54s\tremaining: 1m 51s\n",
      "507:\tlearn: 0.1011774\ttotal: 1m 54s\tremaining: 1m 51s\n",
      "508:\tlearn: 0.1011070\ttotal: 1m 55s\tremaining: 1m 51s\n",
      "509:\tlearn: 0.1010618\ttotal: 1m 55s\tremaining: 1m 50s\n",
      "510:\tlearn: 0.1009896\ttotal: 1m 55s\tremaining: 1m 50s\n",
      "511:\tlearn: 0.1009590\ttotal: 1m 55s\tremaining: 1m 50s\n",
      "512:\tlearn: 0.1008753\ttotal: 1m 55s\tremaining: 1m 50s\n",
      "513:\tlearn: 0.1008331\ttotal: 1m 56s\tremaining: 1m 49s\n",
      "514:\tlearn: 0.1007906\ttotal: 1m 56s\tremaining: 1m 49s\n",
      "515:\tlearn: 0.1007573\ttotal: 1m 56s\tremaining: 1m 49s\n",
      "516:\tlearn: 0.1006833\ttotal: 1m 56s\tremaining: 1m 49s\n",
      "517:\tlearn: 0.1006415\ttotal: 1m 57s\tremaining: 1m 48s\n",
      "518:\tlearn: 0.1006175\ttotal: 1m 57s\tremaining: 1m 48s\n",
      "519:\tlearn: 0.1005646\ttotal: 1m 57s\tremaining: 1m 48s\n",
      "520:\tlearn: 0.1005266\ttotal: 1m 57s\tremaining: 1m 48s\n",
      "521:\tlearn: 0.1004747\ttotal: 1m 57s\tremaining: 1m 47s\n",
      "522:\tlearn: 0.1004284\ttotal: 1m 58s\tremaining: 1m 47s\n",
      "523:\tlearn: 0.1003936\ttotal: 1m 58s\tremaining: 1m 47s\n",
      "524:\tlearn: 0.1003636\ttotal: 1m 58s\tremaining: 1m 47s\n",
      "525:\tlearn: 0.1002921\ttotal: 1m 58s\tremaining: 1m 46s\n",
      "526:\tlearn: 0.1002589\ttotal: 1m 58s\tremaining: 1m 46s\n",
      "527:\tlearn: 0.1002276\ttotal: 1m 59s\tremaining: 1m 46s\n",
      "528:\tlearn: 0.1001553\ttotal: 1m 59s\tremaining: 1m 46s\n",
      "529:\tlearn: 0.1001289\ttotal: 1m 59s\tremaining: 1m 46s\n",
      "530:\tlearn: 0.1000641\ttotal: 1m 59s\tremaining: 1m 45s\n",
      "531:\tlearn: 0.0999276\ttotal: 1m 59s\tremaining: 1m 45s\n",
      "532:\tlearn: 0.0998858\ttotal: 2m\tremaining: 1m 45s\n",
      "533:\tlearn: 0.0997886\ttotal: 2m\tremaining: 1m 45s\n",
      "534:\tlearn: 0.0997390\ttotal: 2m\tremaining: 1m 44s\n",
      "535:\tlearn: 0.0996961\ttotal: 2m\tremaining: 1m 44s\n",
      "536:\tlearn: 0.0996582\ttotal: 2m 1s\tremaining: 1m 44s\n",
      "537:\tlearn: 0.0996213\ttotal: 2m 1s\tremaining: 1m 44s\n",
      "538:\tlearn: 0.0995418\ttotal: 2m 1s\tremaining: 1m 43s\n",
      "539:\tlearn: 0.0995139\ttotal: 2m 1s\tremaining: 1m 43s\n",
      "540:\tlearn: 0.0994677\ttotal: 2m 1s\tremaining: 1m 43s\n",
      "541:\tlearn: 0.0994040\ttotal: 2m 2s\tremaining: 1m 43s\n",
      "542:\tlearn: 0.0993690\ttotal: 2m 2s\tremaining: 1m 42s\n",
      "543:\tlearn: 0.0993077\ttotal: 2m 2s\tremaining: 1m 42s\n",
      "544:\tlearn: 0.0992844\ttotal: 2m 2s\tremaining: 1m 42s\n",
      "545:\tlearn: 0.0992353\ttotal: 2m 2s\tremaining: 1m 42s\n",
      "546:\tlearn: 0.0991778\ttotal: 2m 3s\tremaining: 1m 41s\n",
      "547:\tlearn: 0.0991481\ttotal: 2m 3s\tremaining: 1m 41s\n",
      "548:\tlearn: 0.0991105\ttotal: 2m 3s\tremaining: 1m 41s\n",
      "549:\tlearn: 0.0990695\ttotal: 2m 3s\tremaining: 1m 41s\n",
      "550:\tlearn: 0.0990362\ttotal: 2m 3s\tremaining: 1m 40s\n",
      "551:\tlearn: 0.0990136\ttotal: 2m 4s\tremaining: 1m 40s\n",
      "552:\tlearn: 0.0989828\ttotal: 2m 4s\tremaining: 1m 40s\n",
      "553:\tlearn: 0.0989331\ttotal: 2m 4s\tremaining: 1m 40s\n",
      "554:\tlearn: 0.0988791\ttotal: 2m 4s\tremaining: 1m 40s\n",
      "555:\tlearn: 0.0988358\ttotal: 2m 4s\tremaining: 1m 39s\n",
      "556:\tlearn: 0.0987935\ttotal: 2m 5s\tremaining: 1m 39s\n",
      "557:\tlearn: 0.0987068\ttotal: 2m 5s\tremaining: 1m 39s\n",
      "558:\tlearn: 0.0986876\ttotal: 2m 5s\tremaining: 1m 39s\n",
      "559:\tlearn: 0.0986266\ttotal: 2m 5s\tremaining: 1m 38s\n",
      "560:\tlearn: 0.0985775\ttotal: 2m 5s\tremaining: 1m 38s\n",
      "561:\tlearn: 0.0985091\ttotal: 2m 6s\tremaining: 1m 38s\n",
      "562:\tlearn: 0.0984320\ttotal: 2m 6s\tremaining: 1m 38s\n",
      "563:\tlearn: 0.0983913\ttotal: 2m 6s\tremaining: 1m 37s\n",
      "564:\tlearn: 0.0983332\ttotal: 2m 6s\tremaining: 1m 37s\n",
      "565:\tlearn: 0.0983062\ttotal: 2m 6s\tremaining: 1m 37s\n",
      "566:\tlearn: 0.0982582\ttotal: 2m 7s\tremaining: 1m 37s\n",
      "567:\tlearn: 0.0982257\ttotal: 2m 7s\tremaining: 1m 36s\n",
      "568:\tlearn: 0.0982111\ttotal: 2m 7s\tremaining: 1m 36s\n",
      "569:\tlearn: 0.0981633\ttotal: 2m 7s\tremaining: 1m 36s\n",
      "570:\tlearn: 0.0981370\ttotal: 2m 7s\tremaining: 1m 36s\n",
      "571:\tlearn: 0.0980834\ttotal: 2m 8s\tremaining: 1m 35s\n",
      "572:\tlearn: 0.0980560\ttotal: 2m 8s\tremaining: 1m 35s\n",
      "573:\tlearn: 0.0980077\ttotal: 2m 8s\tremaining: 1m 35s\n",
      "574:\tlearn: 0.0979653\ttotal: 2m 8s\tremaining: 1m 35s\n",
      "575:\tlearn: 0.0979514\ttotal: 2m 8s\tremaining: 1m 34s\n",
      "576:\tlearn: 0.0979277\ttotal: 2m 9s\tremaining: 1m 34s\n",
      "577:\tlearn: 0.0978950\ttotal: 2m 9s\tremaining: 1m 34s\n",
      "578:\tlearn: 0.0978303\ttotal: 2m 9s\tremaining: 1m 34s\n",
      "579:\tlearn: 0.0977537\ttotal: 2m 9s\tremaining: 1m 33s\n",
      "580:\tlearn: 0.0976707\ttotal: 2m 9s\tremaining: 1m 33s\n",
      "581:\tlearn: 0.0976588\ttotal: 2m 9s\tremaining: 1m 33s\n",
      "582:\tlearn: 0.0976081\ttotal: 2m 10s\tremaining: 1m 33s\n",
      "583:\tlearn: 0.0975676\ttotal: 2m 10s\tremaining: 1m 32s\n",
      "584:\tlearn: 0.0975429\ttotal: 2m 10s\tremaining: 1m 32s\n",
      "585:\tlearn: 0.0975108\ttotal: 2m 10s\tremaining: 1m 32s\n",
      "586:\tlearn: 0.0974681\ttotal: 2m 11s\tremaining: 1m 32s\n",
      "587:\tlearn: 0.0974022\ttotal: 2m 11s\tremaining: 1m 31s\n",
      "588:\tlearn: 0.0973679\ttotal: 2m 11s\tremaining: 1m 31s\n",
      "589:\tlearn: 0.0972856\ttotal: 2m 11s\tremaining: 1m 31s\n",
      "590:\tlearn: 0.0971816\ttotal: 2m 11s\tremaining: 1m 31s\n",
      "591:\tlearn: 0.0971316\ttotal: 2m 12s\tremaining: 1m 31s\n",
      "592:\tlearn: 0.0970328\ttotal: 2m 12s\tremaining: 1m 30s\n",
      "593:\tlearn: 0.0969838\ttotal: 2m 12s\tremaining: 1m 30s\n",
      "594:\tlearn: 0.0969258\ttotal: 2m 12s\tremaining: 1m 30s\n",
      "595:\tlearn: 0.0968972\ttotal: 2m 12s\tremaining: 1m 30s\n",
      "596:\tlearn: 0.0968476\ttotal: 2m 13s\tremaining: 1m 29s\n",
      "597:\tlearn: 0.0967822\ttotal: 2m 13s\tremaining: 1m 29s\n",
      "598:\tlearn: 0.0967140\ttotal: 2m 13s\tremaining: 1m 29s\n",
      "599:\tlearn: 0.0966947\ttotal: 2m 13s\tremaining: 1m 29s\n",
      "600:\tlearn: 0.0966397\ttotal: 2m 14s\tremaining: 1m 29s\n",
      "601:\tlearn: 0.0966059\ttotal: 2m 14s\tremaining: 1m 28s\n",
      "602:\tlearn: 0.0965625\ttotal: 2m 14s\tremaining: 1m 28s\n",
      "603:\tlearn: 0.0965232\ttotal: 2m 14s\tremaining: 1m 28s\n",
      "604:\tlearn: 0.0964941\ttotal: 2m 14s\tremaining: 1m 28s\n",
      "605:\tlearn: 0.0964145\ttotal: 2m 15s\tremaining: 1m 27s\n",
      "606:\tlearn: 0.0963226\ttotal: 2m 15s\tremaining: 1m 27s\n",
      "607:\tlearn: 0.0962825\ttotal: 2m 15s\tremaining: 1m 27s\n",
      "608:\tlearn: 0.0962429\ttotal: 2m 15s\tremaining: 1m 27s\n",
      "609:\tlearn: 0.0962197\ttotal: 2m 15s\tremaining: 1m 26s\n",
      "610:\tlearn: 0.0961617\ttotal: 2m 16s\tremaining: 1m 26s\n",
      "611:\tlearn: 0.0961314\ttotal: 2m 16s\tremaining: 1m 26s\n",
      "612:\tlearn: 0.0961075\ttotal: 2m 16s\tremaining: 1m 26s\n",
      "613:\tlearn: 0.0960716\ttotal: 2m 16s\tremaining: 1m 25s\n",
      "614:\tlearn: 0.0960104\ttotal: 2m 16s\tremaining: 1m 25s\n",
      "615:\tlearn: 0.0959535\ttotal: 2m 17s\tremaining: 1m 25s\n",
      "616:\tlearn: 0.0959245\ttotal: 2m 17s\tremaining: 1m 25s\n",
      "617:\tlearn: 0.0958975\ttotal: 2m 17s\tremaining: 1m 25s\n",
      "618:\tlearn: 0.0958511\ttotal: 2m 17s\tremaining: 1m 24s\n",
      "619:\tlearn: 0.0958259\ttotal: 2m 17s\tremaining: 1m 24s\n",
      "620:\tlearn: 0.0958057\ttotal: 2m 18s\tremaining: 1m 24s\n",
      "621:\tlearn: 0.0957018\ttotal: 2m 18s\tremaining: 1m 24s\n",
      "622:\tlearn: 0.0956596\ttotal: 2m 18s\tremaining: 1m 23s\n",
      "623:\tlearn: 0.0956241\ttotal: 2m 18s\tremaining: 1m 23s\n",
      "624:\tlearn: 0.0955872\ttotal: 2m 18s\tremaining: 1m 23s\n",
      "625:\tlearn: 0.0955572\ttotal: 2m 19s\tremaining: 1m 23s\n",
      "626:\tlearn: 0.0955351\ttotal: 2m 19s\tremaining: 1m 22s\n",
      "627:\tlearn: 0.0954955\ttotal: 2m 19s\tremaining: 1m 22s\n",
      "628:\tlearn: 0.0954275\ttotal: 2m 19s\tremaining: 1m 22s\n",
      "629:\tlearn: 0.0953735\ttotal: 2m 20s\tremaining: 1m 22s\n",
      "630:\tlearn: 0.0953277\ttotal: 2m 20s\tremaining: 1m 22s\n",
      "631:\tlearn: 0.0952933\ttotal: 2m 20s\tremaining: 1m 21s\n",
      "632:\tlearn: 0.0952304\ttotal: 2m 20s\tremaining: 1m 21s\n",
      "633:\tlearn: 0.0951976\ttotal: 2m 21s\tremaining: 1m 21s\n",
      "634:\tlearn: 0.0951161\ttotal: 2m 21s\tremaining: 1m 21s\n",
      "635:\tlearn: 0.0950928\ttotal: 2m 21s\tremaining: 1m 21s\n",
      "636:\tlearn: 0.0950140\ttotal: 2m 21s\tremaining: 1m 20s\n",
      "637:\tlearn: 0.0949777\ttotal: 2m 22s\tremaining: 1m 20s\n",
      "638:\tlearn: 0.0949492\ttotal: 2m 22s\tremaining: 1m 20s\n",
      "639:\tlearn: 0.0948682\ttotal: 2m 22s\tremaining: 1m 20s\n",
      "640:\tlearn: 0.0948041\ttotal: 2m 22s\tremaining: 1m 20s\n",
      "641:\tlearn: 0.0947812\ttotal: 2m 23s\tremaining: 1m 19s\n",
      "642:\tlearn: 0.0947044\ttotal: 2m 23s\tremaining: 1m 19s\n",
      "643:\tlearn: 0.0946516\ttotal: 2m 23s\tremaining: 1m 19s\n",
      "644:\tlearn: 0.0946222\ttotal: 2m 23s\tremaining: 1m 19s\n",
      "645:\tlearn: 0.0945639\ttotal: 2m 23s\tremaining: 1m 18s\n",
      "646:\tlearn: 0.0945069\ttotal: 2m 24s\tremaining: 1m 18s\n",
      "647:\tlearn: 0.0944674\ttotal: 2m 24s\tremaining: 1m 18s\n",
      "648:\tlearn: 0.0944120\ttotal: 2m 24s\tremaining: 1m 18s\n",
      "649:\tlearn: 0.0943791\ttotal: 2m 24s\tremaining: 1m 17s\n",
      "650:\tlearn: 0.0943535\ttotal: 2m 24s\tremaining: 1m 17s\n",
      "651:\tlearn: 0.0943063\ttotal: 2m 25s\tremaining: 1m 17s\n",
      "652:\tlearn: 0.0942385\ttotal: 2m 25s\tremaining: 1m 17s\n",
      "653:\tlearn: 0.0942114\ttotal: 2m 25s\tremaining: 1m 17s\n",
      "654:\tlearn: 0.0941747\ttotal: 2m 25s\tremaining: 1m 16s\n",
      "655:\tlearn: 0.0941127\ttotal: 2m 25s\tremaining: 1m 16s\n",
      "656:\tlearn: 0.0940375\ttotal: 2m 26s\tremaining: 1m 16s\n",
      "657:\tlearn: 0.0940273\ttotal: 2m 26s\tremaining: 1m 16s\n",
      "658:\tlearn: 0.0939991\ttotal: 2m 26s\tremaining: 1m 15s\n",
      "659:\tlearn: 0.0939225\ttotal: 2m 26s\tremaining: 1m 15s\n",
      "660:\tlearn: 0.0938777\ttotal: 2m 26s\tremaining: 1m 15s\n",
      "661:\tlearn: 0.0938710\ttotal: 2m 27s\tremaining: 1m 15s\n",
      "662:\tlearn: 0.0938353\ttotal: 2m 27s\tremaining: 1m 14s\n",
      "663:\tlearn: 0.0937790\ttotal: 2m 27s\tremaining: 1m 14s\n",
      "664:\tlearn: 0.0937402\ttotal: 2m 27s\tremaining: 1m 14s\n",
      "665:\tlearn: 0.0936873\ttotal: 2m 27s\tremaining: 1m 14s\n",
      "666:\tlearn: 0.0936142\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "667:\tlearn: 0.0935196\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "668:\tlearn: 0.0934809\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "669:\tlearn: 0.0934278\ttotal: 2m 28s\tremaining: 1m 13s\n",
      "670:\tlearn: 0.0933740\ttotal: 2m 29s\tremaining: 1m 13s\n",
      "671:\tlearn: 0.0933481\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "672:\tlearn: 0.0933111\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "673:\tlearn: 0.0932768\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "674:\tlearn: 0.0932309\ttotal: 2m 29s\tremaining: 1m 12s\n",
      "675:\tlearn: 0.0931697\ttotal: 2m 30s\tremaining: 1m 11s\n",
      "676:\tlearn: 0.0931246\ttotal: 2m 30s\tremaining: 1m 11s\n",
      "677:\tlearn: 0.0930703\ttotal: 2m 30s\tremaining: 1m 11s\n",
      "678:\tlearn: 0.0930005\ttotal: 2m 30s\tremaining: 1m 11s\n",
      "679:\tlearn: 0.0929717\ttotal: 2m 31s\tremaining: 1m 11s\n",
      "680:\tlearn: 0.0929357\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "681:\tlearn: 0.0928789\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "682:\tlearn: 0.0928506\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "683:\tlearn: 0.0927678\ttotal: 2m 31s\tremaining: 1m 10s\n",
      "684:\tlearn: 0.0927167\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "685:\tlearn: 0.0926784\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "686:\tlearn: 0.0926203\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "687:\tlearn: 0.0925962\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "688:\tlearn: 0.0925569\ttotal: 2m 32s\tremaining: 1m 9s\n",
      "689:\tlearn: 0.0925078\ttotal: 2m 33s\tremaining: 1m 8s\n",
      "690:\tlearn: 0.0924936\ttotal: 2m 33s\tremaining: 1m 8s\n",
      "691:\tlearn: 0.0923860\ttotal: 2m 33s\tremaining: 1m 8s\n",
      "692:\tlearn: 0.0923015\ttotal: 2m 33s\tremaining: 1m 8s\n",
      "693:\tlearn: 0.0922824\ttotal: 2m 34s\tremaining: 1m 7s\n",
      "694:\tlearn: 0.0922338\ttotal: 2m 34s\tremaining: 1m 7s\n",
      "695:\tlearn: 0.0921863\ttotal: 2m 34s\tremaining: 1m 7s\n",
      "696:\tlearn: 0.0921389\ttotal: 2m 34s\tremaining: 1m 7s\n",
      "697:\tlearn: 0.0920995\ttotal: 2m 34s\tremaining: 1m 6s\n",
      "698:\tlearn: 0.0920650\ttotal: 2m 35s\tremaining: 1m 6s\n",
      "699:\tlearn: 0.0920217\ttotal: 2m 35s\tremaining: 1m 6s\n",
      "700:\tlearn: 0.0919924\ttotal: 2m 35s\tremaining: 1m 6s\n",
      "701:\tlearn: 0.0919147\ttotal: 2m 35s\tremaining: 1m 6s\n",
      "702:\tlearn: 0.0918857\ttotal: 2m 35s\tremaining: 1m 5s\n",
      "703:\tlearn: 0.0918567\ttotal: 2m 36s\tremaining: 1m 5s\n",
      "704:\tlearn: 0.0918203\ttotal: 2m 36s\tremaining: 1m 5s\n",
      "705:\tlearn: 0.0917721\ttotal: 2m 36s\tremaining: 1m 5s\n",
      "706:\tlearn: 0.0917063\ttotal: 2m 36s\tremaining: 1m 4s\n",
      "707:\tlearn: 0.0916622\ttotal: 2m 36s\tremaining: 1m 4s\n",
      "708:\tlearn: 0.0915981\ttotal: 2m 37s\tremaining: 1m 4s\n",
      "709:\tlearn: 0.0915826\ttotal: 2m 37s\tremaining: 1m 4s\n",
      "710:\tlearn: 0.0915310\ttotal: 2m 37s\tremaining: 1m 4s\n",
      "711:\tlearn: 0.0914788\ttotal: 2m 37s\tremaining: 1m 3s\n",
      "712:\tlearn: 0.0914422\ttotal: 2m 37s\tremaining: 1m 3s\n",
      "713:\tlearn: 0.0913835\ttotal: 2m 38s\tremaining: 1m 3s\n",
      "714:\tlearn: 0.0913781\ttotal: 2m 38s\tremaining: 1m 3s\n",
      "715:\tlearn: 0.0913539\ttotal: 2m 38s\tremaining: 1m 2s\n",
      "716:\tlearn: 0.0913074\ttotal: 2m 38s\tremaining: 1m 2s\n",
      "717:\tlearn: 0.0912849\ttotal: 2m 38s\tremaining: 1m 2s\n",
      "718:\tlearn: 0.0912395\ttotal: 2m 39s\tremaining: 1m 2s\n",
      "719:\tlearn: 0.0912063\ttotal: 2m 39s\tremaining: 1m 1s\n",
      "720:\tlearn: 0.0911531\ttotal: 2m 39s\tremaining: 1m 1s\n",
      "721:\tlearn: 0.0911259\ttotal: 2m 39s\tremaining: 1m 1s\n",
      "722:\tlearn: 0.0910785\ttotal: 2m 39s\tremaining: 1m 1s\n",
      "723:\tlearn: 0.0910487\ttotal: 2m 40s\tremaining: 1m 1s\n",
      "724:\tlearn: 0.0910185\ttotal: 2m 40s\tremaining: 1m\n",
      "725:\tlearn: 0.0909557\ttotal: 2m 40s\tremaining: 1m\n",
      "726:\tlearn: 0.0909367\ttotal: 2m 40s\tremaining: 1m\n",
      "727:\tlearn: 0.0909054\ttotal: 2m 40s\tremaining: 1m\n",
      "728:\tlearn: 0.0908826\ttotal: 2m 41s\tremaining: 59.9s\n",
      "729:\tlearn: 0.0908226\ttotal: 2m 41s\tremaining: 59.6s\n",
      "730:\tlearn: 0.0908003\ttotal: 2m 41s\tremaining: 59.4s\n",
      "731:\tlearn: 0.0907530\ttotal: 2m 41s\tremaining: 59.2s\n",
      "732:\tlearn: 0.0907211\ttotal: 2m 41s\tremaining: 58.9s\n",
      "733:\tlearn: 0.0907016\ttotal: 2m 41s\tremaining: 58.7s\n",
      "734:\tlearn: 0.0906625\ttotal: 2m 42s\tremaining: 58.5s\n",
      "735:\tlearn: 0.0906301\ttotal: 2m 42s\tremaining: 58.2s\n",
      "736:\tlearn: 0.0905633\ttotal: 2m 42s\tremaining: 58s\n",
      "737:\tlearn: 0.0905204\ttotal: 2m 42s\tremaining: 57.8s\n",
      "738:\tlearn: 0.0904988\ttotal: 2m 42s\tremaining: 57.5s\n",
      "739:\tlearn: 0.0904594\ttotal: 2m 43s\tremaining: 57.3s\n",
      "740:\tlearn: 0.0904160\ttotal: 2m 43s\tremaining: 57.1s\n",
      "741:\tlearn: 0.0903778\ttotal: 2m 43s\tremaining: 56.9s\n",
      "742:\tlearn: 0.0903432\ttotal: 2m 43s\tremaining: 56.6s\n",
      "743:\tlearn: 0.0903047\ttotal: 2m 44s\tremaining: 56.4s\n",
      "744:\tlearn: 0.0902361\ttotal: 2m 44s\tremaining: 56.2s\n",
      "745:\tlearn: 0.0901819\ttotal: 2m 44s\tremaining: 56s\n",
      "746:\tlearn: 0.0901462\ttotal: 2m 44s\tremaining: 55.7s\n",
      "747:\tlearn: 0.0901299\ttotal: 2m 44s\tremaining: 55.5s\n",
      "748:\tlearn: 0.0900964\ttotal: 2m 44s\tremaining: 55.3s\n",
      "749:\tlearn: 0.0900729\ttotal: 2m 45s\tremaining: 55s\n",
      "750:\tlearn: 0.0900552\ttotal: 2m 45s\tremaining: 54.8s\n",
      "751:\tlearn: 0.0900144\ttotal: 2m 45s\tremaining: 54.6s\n",
      "752:\tlearn: 0.0899217\ttotal: 2m 45s\tremaining: 54.4s\n",
      "753:\tlearn: 0.0898736\ttotal: 2m 45s\tremaining: 54.1s\n",
      "754:\tlearn: 0.0898229\ttotal: 2m 46s\tremaining: 53.9s\n",
      "755:\tlearn: 0.0897805\ttotal: 2m 46s\tremaining: 53.7s\n",
      "756:\tlearn: 0.0897554\ttotal: 2m 46s\tremaining: 53.5s\n",
      "757:\tlearn: 0.0897106\ttotal: 2m 46s\tremaining: 53.3s\n",
      "758:\tlearn: 0.0896911\ttotal: 2m 47s\tremaining: 53.1s\n",
      "759:\tlearn: 0.0896307\ttotal: 2m 47s\tremaining: 52.8s\n",
      "760:\tlearn: 0.0896233\ttotal: 2m 47s\tremaining: 52.6s\n",
      "761:\tlearn: 0.0895693\ttotal: 2m 47s\tremaining: 52.4s\n",
      "762:\tlearn: 0.0895323\ttotal: 2m 47s\tremaining: 52.2s\n",
      "763:\tlearn: 0.0895039\ttotal: 2m 48s\tremaining: 51.9s\n",
      "764:\tlearn: 0.0894307\ttotal: 2m 48s\tremaining: 51.7s\n",
      "765:\tlearn: 0.0893869\ttotal: 2m 48s\tremaining: 51.5s\n",
      "766:\tlearn: 0.0893191\ttotal: 2m 48s\tremaining: 51.3s\n",
      "767:\tlearn: 0.0892937\ttotal: 2m 48s\tremaining: 51s\n",
      "768:\tlearn: 0.0892412\ttotal: 2m 49s\tremaining: 50.8s\n",
      "769:\tlearn: 0.0892168\ttotal: 2m 49s\tremaining: 50.6s\n",
      "770:\tlearn: 0.0891961\ttotal: 2m 49s\tremaining: 50.3s\n",
      "771:\tlearn: 0.0891376\ttotal: 2m 49s\tremaining: 50.1s\n",
      "772:\tlearn: 0.0891131\ttotal: 2m 49s\tremaining: 49.9s\n",
      "773:\tlearn: 0.0890567\ttotal: 2m 50s\tremaining: 49.7s\n",
      "774:\tlearn: 0.0889969\ttotal: 2m 50s\tremaining: 49.4s\n",
      "775:\tlearn: 0.0889485\ttotal: 2m 50s\tremaining: 49.2s\n",
      "776:\tlearn: 0.0889196\ttotal: 2m 50s\tremaining: 49s\n",
      "777:\tlearn: 0.0889057\ttotal: 2m 50s\tremaining: 48.8s\n",
      "778:\tlearn: 0.0888645\ttotal: 2m 51s\tremaining: 48.5s\n",
      "779:\tlearn: 0.0888407\ttotal: 2m 51s\tremaining: 48.3s\n",
      "780:\tlearn: 0.0888116\ttotal: 2m 51s\tremaining: 48.1s\n",
      "781:\tlearn: 0.0887757\ttotal: 2m 51s\tremaining: 47.9s\n",
      "782:\tlearn: 0.0887536\ttotal: 2m 51s\tremaining: 47.6s\n",
      "783:\tlearn: 0.0886968\ttotal: 2m 52s\tremaining: 47.4s\n",
      "784:\tlearn: 0.0886826\ttotal: 2m 52s\tremaining: 47.2s\n",
      "785:\tlearn: 0.0886396\ttotal: 2m 52s\tremaining: 47s\n",
      "786:\tlearn: 0.0886050\ttotal: 2m 52s\tremaining: 46.7s\n",
      "787:\tlearn: 0.0885460\ttotal: 2m 52s\tremaining: 46.5s\n",
      "788:\tlearn: 0.0884951\ttotal: 2m 53s\tremaining: 46.3s\n",
      "789:\tlearn: 0.0884345\ttotal: 2m 53s\tremaining: 46.1s\n",
      "790:\tlearn: 0.0884095\ttotal: 2m 53s\tremaining: 45.9s\n",
      "791:\tlearn: 0.0883626\ttotal: 2m 53s\tremaining: 45.6s\n",
      "792:\tlearn: 0.0883107\ttotal: 2m 53s\tremaining: 45.4s\n",
      "793:\tlearn: 0.0882640\ttotal: 2m 54s\tremaining: 45.2s\n",
      "794:\tlearn: 0.0882117\ttotal: 2m 54s\tremaining: 44.9s\n",
      "795:\tlearn: 0.0881050\ttotal: 2m 54s\tremaining: 44.7s\n",
      "796:\tlearn: 0.0880455\ttotal: 2m 54s\tremaining: 44.5s\n",
      "797:\tlearn: 0.0879860\ttotal: 2m 54s\tremaining: 44.3s\n",
      "798:\tlearn: 0.0879373\ttotal: 2m 55s\tremaining: 44s\n",
      "799:\tlearn: 0.0878837\ttotal: 2m 55s\tremaining: 43.8s\n",
      "800:\tlearn: 0.0878485\ttotal: 2m 55s\tremaining: 43.6s\n",
      "801:\tlearn: 0.0878010\ttotal: 2m 55s\tremaining: 43.4s\n",
      "802:\tlearn: 0.0877459\ttotal: 2m 55s\tremaining: 43.2s\n",
      "803:\tlearn: 0.0877097\ttotal: 2m 56s\tremaining: 42.9s\n",
      "804:\tlearn: 0.0876765\ttotal: 2m 56s\tremaining: 42.7s\n",
      "805:\tlearn: 0.0876380\ttotal: 2m 56s\tremaining: 42.5s\n",
      "806:\tlearn: 0.0876087\ttotal: 2m 56s\tremaining: 42.3s\n",
      "807:\tlearn: 0.0875834\ttotal: 2m 57s\tremaining: 42.1s\n",
      "808:\tlearn: 0.0875225\ttotal: 2m 57s\tremaining: 41.8s\n",
      "809:\tlearn: 0.0874879\ttotal: 2m 57s\tremaining: 41.6s\n",
      "810:\tlearn: 0.0874419\ttotal: 2m 57s\tremaining: 41.4s\n",
      "811:\tlearn: 0.0874220\ttotal: 2m 57s\tremaining: 41.2s\n",
      "812:\tlearn: 0.0873710\ttotal: 2m 58s\tremaining: 41s\n",
      "813:\tlearn: 0.0873232\ttotal: 2m 58s\tremaining: 40.7s\n",
      "814:\tlearn: 0.0872879\ttotal: 2m 58s\tremaining: 40.5s\n",
      "815:\tlearn: 0.0872524\ttotal: 2m 58s\tremaining: 40.3s\n",
      "816:\tlearn: 0.0872038\ttotal: 2m 58s\tremaining: 40.1s\n",
      "817:\tlearn: 0.0871285\ttotal: 2m 59s\tremaining: 39.8s\n",
      "818:\tlearn: 0.0870944\ttotal: 2m 59s\tremaining: 39.6s\n",
      "819:\tlearn: 0.0870452\ttotal: 2m 59s\tremaining: 39.4s\n",
      "820:\tlearn: 0.0870117\ttotal: 2m 59s\tremaining: 39.2s\n",
      "821:\tlearn: 0.0869624\ttotal: 2m 59s\tremaining: 38.9s\n",
      "822:\tlearn: 0.0869128\ttotal: 3m\tremaining: 38.7s\n",
      "823:\tlearn: 0.0868751\ttotal: 3m\tremaining: 38.5s\n",
      "824:\tlearn: 0.0868454\ttotal: 3m\tremaining: 38.3s\n",
      "825:\tlearn: 0.0868298\ttotal: 3m\tremaining: 38s\n",
      "826:\tlearn: 0.0868223\ttotal: 3m\tremaining: 37.8s\n",
      "827:\tlearn: 0.0867897\ttotal: 3m\tremaining: 37.6s\n",
      "828:\tlearn: 0.0867808\ttotal: 3m 1s\tremaining: 37.4s\n",
      "829:\tlearn: 0.0867603\ttotal: 3m 1s\tremaining: 37.1s\n",
      "830:\tlearn: 0.0867297\ttotal: 3m 1s\tremaining: 36.9s\n",
      "831:\tlearn: 0.0867046\ttotal: 3m 1s\tremaining: 36.7s\n",
      "832:\tlearn: 0.0866697\ttotal: 3m 1s\tremaining: 36.5s\n",
      "833:\tlearn: 0.0866315\ttotal: 3m 2s\tremaining: 36.3s\n",
      "834:\tlearn: 0.0865912\ttotal: 3m 2s\tremaining: 36s\n",
      "835:\tlearn: 0.0865761\ttotal: 3m 2s\tremaining: 35.8s\n",
      "836:\tlearn: 0.0865539\ttotal: 3m 2s\tremaining: 35.6s\n",
      "837:\tlearn: 0.0865178\ttotal: 3m 3s\tremaining: 35.4s\n",
      "838:\tlearn: 0.0864804\ttotal: 3m 3s\tremaining: 35.2s\n",
      "839:\tlearn: 0.0864374\ttotal: 3m 3s\tremaining: 34.9s\n",
      "840:\tlearn: 0.0863932\ttotal: 3m 3s\tremaining: 34.7s\n",
      "841:\tlearn: 0.0863517\ttotal: 3m 3s\tremaining: 34.5s\n",
      "842:\tlearn: 0.0862858\ttotal: 3m 4s\tremaining: 34.3s\n",
      "843:\tlearn: 0.0862218\ttotal: 3m 4s\tremaining: 34.1s\n",
      "844:\tlearn: 0.0862005\ttotal: 3m 4s\tremaining: 33.9s\n",
      "845:\tlearn: 0.0861364\ttotal: 3m 4s\tremaining: 33.7s\n",
      "846:\tlearn: 0.0860874\ttotal: 3m 5s\tremaining: 33.4s\n",
      "847:\tlearn: 0.0860584\ttotal: 3m 5s\tremaining: 33.2s\n",
      "848:\tlearn: 0.0860285\ttotal: 3m 5s\tremaining: 33s\n",
      "849:\tlearn: 0.0859915\ttotal: 3m 5s\tremaining: 32.8s\n",
      "850:\tlearn: 0.0859512\ttotal: 3m 6s\tremaining: 32.6s\n",
      "851:\tlearn: 0.0858969\ttotal: 3m 6s\tremaining: 32.4s\n",
      "852:\tlearn: 0.0858549\ttotal: 3m 6s\tremaining: 32.2s\n",
      "853:\tlearn: 0.0858397\ttotal: 3m 6s\tremaining: 32s\n",
      "854:\tlearn: 0.0858062\ttotal: 3m 7s\tremaining: 31.7s\n",
      "855:\tlearn: 0.0857730\ttotal: 3m 7s\tremaining: 31.5s\n",
      "856:\tlearn: 0.0857515\ttotal: 3m 7s\tremaining: 31.3s\n",
      "857:\tlearn: 0.0857151\ttotal: 3m 7s\tremaining: 31.1s\n",
      "858:\tlearn: 0.0856986\ttotal: 3m 8s\tremaining: 30.9s\n",
      "859:\tlearn: 0.0856324\ttotal: 3m 8s\tremaining: 30.6s\n",
      "860:\tlearn: 0.0855780\ttotal: 3m 8s\tremaining: 30.4s\n",
      "861:\tlearn: 0.0855515\ttotal: 3m 8s\tremaining: 30.2s\n",
      "862:\tlearn: 0.0855253\ttotal: 3m 8s\tremaining: 30s\n",
      "863:\tlearn: 0.0855022\ttotal: 3m 9s\tremaining: 29.8s\n",
      "864:\tlearn: 0.0854825\ttotal: 3m 9s\tremaining: 29.5s\n",
      "865:\tlearn: 0.0854379\ttotal: 3m 9s\tremaining: 29.3s\n",
      "866:\tlearn: 0.0854164\ttotal: 3m 9s\tremaining: 29.1s\n",
      "867:\tlearn: 0.0853783\ttotal: 3m 9s\tremaining: 28.9s\n",
      "868:\tlearn: 0.0853491\ttotal: 3m 10s\tremaining: 28.7s\n",
      "869:\tlearn: 0.0853310\ttotal: 3m 10s\tremaining: 28.4s\n",
      "870:\tlearn: 0.0853157\ttotal: 3m 10s\tremaining: 28.2s\n",
      "871:\tlearn: 0.0852754\ttotal: 3m 10s\tremaining: 28s\n",
      "872:\tlearn: 0.0852555\ttotal: 3m 11s\tremaining: 27.8s\n",
      "873:\tlearn: 0.0852112\ttotal: 3m 11s\tremaining: 27.6s\n",
      "874:\tlearn: 0.0851700\ttotal: 3m 11s\tremaining: 27.4s\n",
      "875:\tlearn: 0.0851654\ttotal: 3m 11s\tremaining: 27.1s\n",
      "876:\tlearn: 0.0851226\ttotal: 3m 11s\tremaining: 26.9s\n",
      "877:\tlearn: 0.0850877\ttotal: 3m 12s\tremaining: 26.7s\n",
      "878:\tlearn: 0.0850463\ttotal: 3m 12s\tremaining: 26.5s\n",
      "879:\tlearn: 0.0850151\ttotal: 3m 12s\tremaining: 26.2s\n",
      "880:\tlearn: 0.0849817\ttotal: 3m 12s\tremaining: 26s\n",
      "881:\tlearn: 0.0849274\ttotal: 3m 12s\tremaining: 25.8s\n",
      "882:\tlearn: 0.0848867\ttotal: 3m 13s\tremaining: 25.6s\n",
      "883:\tlearn: 0.0848688\ttotal: 3m 13s\tremaining: 25.4s\n",
      "884:\tlearn: 0.0848318\ttotal: 3m 13s\tremaining: 25.1s\n",
      "885:\tlearn: 0.0847865\ttotal: 3m 13s\tremaining: 24.9s\n",
      "886:\tlearn: 0.0847535\ttotal: 3m 13s\tremaining: 24.7s\n",
      "887:\tlearn: 0.0846970\ttotal: 3m 14s\tremaining: 24.5s\n",
      "888:\tlearn: 0.0846818\ttotal: 3m 14s\tremaining: 24.3s\n",
      "889:\tlearn: 0.0846308\ttotal: 3m 14s\tremaining: 24s\n",
      "890:\tlearn: 0.0846157\ttotal: 3m 14s\tremaining: 23.8s\n",
      "891:\tlearn: 0.0845862\ttotal: 3m 14s\tremaining: 23.6s\n",
      "892:\tlearn: 0.0845368\ttotal: 3m 15s\tremaining: 23.4s\n",
      "893:\tlearn: 0.0844933\ttotal: 3m 15s\tremaining: 23.2s\n",
      "894:\tlearn: 0.0844505\ttotal: 3m 15s\tremaining: 23s\n",
      "895:\tlearn: 0.0843833\ttotal: 3m 15s\tremaining: 22.7s\n",
      "896:\tlearn: 0.0843415\ttotal: 3m 16s\tremaining: 22.5s\n",
      "897:\tlearn: 0.0842955\ttotal: 3m 16s\tremaining: 22.3s\n",
      "898:\tlearn: 0.0842424\ttotal: 3m 16s\tremaining: 22.1s\n",
      "899:\tlearn: 0.0842207\ttotal: 3m 16s\tremaining: 21.8s\n",
      "900:\tlearn: 0.0841781\ttotal: 3m 16s\tremaining: 21.6s\n",
      "901:\tlearn: 0.0841522\ttotal: 3m 17s\tremaining: 21.4s\n",
      "902:\tlearn: 0.0841340\ttotal: 3m 17s\tremaining: 21.2s\n",
      "903:\tlearn: 0.0840658\ttotal: 3m 17s\tremaining: 21s\n",
      "904:\tlearn: 0.0840293\ttotal: 3m 17s\tremaining: 20.8s\n",
      "905:\tlearn: 0.0839984\ttotal: 3m 17s\tremaining: 20.5s\n",
      "906:\tlearn: 0.0839582\ttotal: 3m 18s\tremaining: 20.3s\n",
      "907:\tlearn: 0.0839474\ttotal: 3m 18s\tremaining: 20.1s\n",
      "908:\tlearn: 0.0839327\ttotal: 3m 18s\tremaining: 19.9s\n",
      "909:\tlearn: 0.0838760\ttotal: 3m 18s\tremaining: 19.7s\n",
      "910:\tlearn: 0.0838486\ttotal: 3m 19s\tremaining: 19.4s\n",
      "911:\tlearn: 0.0838212\ttotal: 3m 19s\tremaining: 19.2s\n",
      "912:\tlearn: 0.0837484\ttotal: 3m 19s\tremaining: 19s\n",
      "913:\tlearn: 0.0837133\ttotal: 3m 19s\tremaining: 18.8s\n",
      "914:\tlearn: 0.0836918\ttotal: 3m 20s\tremaining: 18.6s\n",
      "915:\tlearn: 0.0836648\ttotal: 3m 20s\tremaining: 18.4s\n",
      "916:\tlearn: 0.0836401\ttotal: 3m 20s\tremaining: 18.1s\n",
      "917:\tlearn: 0.0835852\ttotal: 3m 20s\tremaining: 17.9s\n",
      "918:\tlearn: 0.0835526\ttotal: 3m 20s\tremaining: 17.7s\n",
      "919:\tlearn: 0.0834838\ttotal: 3m 21s\tremaining: 17.5s\n",
      "920:\tlearn: 0.0834613\ttotal: 3m 21s\tremaining: 17.3s\n",
      "921:\tlearn: 0.0834298\ttotal: 3m 21s\tremaining: 17.1s\n",
      "922:\tlearn: 0.0833928\ttotal: 3m 21s\tremaining: 16.8s\n",
      "923:\tlearn: 0.0833594\ttotal: 3m 21s\tremaining: 16.6s\n",
      "924:\tlearn: 0.0832939\ttotal: 3m 22s\tremaining: 16.4s\n",
      "925:\tlearn: 0.0832728\ttotal: 3m 22s\tremaining: 16.2s\n",
      "926:\tlearn: 0.0832353\ttotal: 3m 22s\tremaining: 15.9s\n",
      "927:\tlearn: 0.0832121\ttotal: 3m 22s\tremaining: 15.7s\n",
      "928:\tlearn: 0.0831841\ttotal: 3m 22s\tremaining: 15.5s\n",
      "929:\tlearn: 0.0831542\ttotal: 3m 23s\tremaining: 15.3s\n",
      "930:\tlearn: 0.0831227\ttotal: 3m 23s\tremaining: 15.1s\n",
      "931:\tlearn: 0.0830987\ttotal: 3m 23s\tremaining: 14.9s\n",
      "932:\tlearn: 0.0830657\ttotal: 3m 23s\tremaining: 14.6s\n",
      "933:\tlearn: 0.0830293\ttotal: 3m 24s\tremaining: 14.4s\n",
      "934:\tlearn: 0.0829380\ttotal: 3m 24s\tremaining: 14.2s\n",
      "935:\tlearn: 0.0828924\ttotal: 3m 24s\tremaining: 14s\n",
      "936:\tlearn: 0.0828637\ttotal: 3m 24s\tremaining: 13.8s\n",
      "937:\tlearn: 0.0828392\ttotal: 3m 24s\tremaining: 13.5s\n",
      "938:\tlearn: 0.0828084\ttotal: 3m 25s\tremaining: 13.3s\n",
      "939:\tlearn: 0.0827826\ttotal: 3m 25s\tremaining: 13.1s\n",
      "940:\tlearn: 0.0827791\ttotal: 3m 25s\tremaining: 12.9s\n",
      "941:\tlearn: 0.0827450\ttotal: 3m 25s\tremaining: 12.7s\n",
      "942:\tlearn: 0.0826899\ttotal: 3m 25s\tremaining: 12.4s\n",
      "943:\tlearn: 0.0826237\ttotal: 3m 26s\tremaining: 12.2s\n",
      "944:\tlearn: 0.0825989\ttotal: 3m 26s\tremaining: 12s\n",
      "945:\tlearn: 0.0825496\ttotal: 3m 26s\tremaining: 11.8s\n",
      "946:\tlearn: 0.0825154\ttotal: 3m 26s\tremaining: 11.6s\n",
      "947:\tlearn: 0.0824798\ttotal: 3m 26s\tremaining: 11.3s\n",
      "948:\tlearn: 0.0824506\ttotal: 3m 27s\tremaining: 11.1s\n",
      "949:\tlearn: 0.0824223\ttotal: 3m 27s\tremaining: 10.9s\n",
      "950:\tlearn: 0.0823994\ttotal: 3m 27s\tremaining: 10.7s\n",
      "951:\tlearn: 0.0823722\ttotal: 3m 27s\tremaining: 10.5s\n",
      "952:\tlearn: 0.0823396\ttotal: 3m 28s\tremaining: 10.3s\n",
      "953:\tlearn: 0.0823265\ttotal: 3m 28s\tremaining: 10s\n",
      "954:\tlearn: 0.0822770\ttotal: 3m 28s\tremaining: 9.82s\n",
      "955:\tlearn: 0.0822595\ttotal: 3m 28s\tremaining: 9.6s\n",
      "956:\tlearn: 0.0822200\ttotal: 3m 28s\tremaining: 9.38s\n",
      "957:\tlearn: 0.0821897\ttotal: 3m 29s\tremaining: 9.16s\n",
      "958:\tlearn: 0.0821718\ttotal: 3m 29s\tremaining: 8.95s\n",
      "959:\tlearn: 0.0821349\ttotal: 3m 29s\tremaining: 8.73s\n",
      "960:\tlearn: 0.0821116\ttotal: 3m 29s\tremaining: 8.51s\n",
      "961:\tlearn: 0.0821095\ttotal: 3m 29s\tremaining: 8.29s\n",
      "962:\tlearn: 0.0820960\ttotal: 3m 30s\tremaining: 8.07s\n",
      "963:\tlearn: 0.0820562\ttotal: 3m 30s\tremaining: 7.86s\n",
      "964:\tlearn: 0.0820414\ttotal: 3m 30s\tremaining: 7.64s\n",
      "965:\tlearn: 0.0819948\ttotal: 3m 30s\tremaining: 7.42s\n",
      "966:\tlearn: 0.0819611\ttotal: 3m 30s\tremaining: 7.2s\n",
      "967:\tlearn: 0.0819215\ttotal: 3m 31s\tremaining: 6.98s\n",
      "968:\tlearn: 0.0818954\ttotal: 3m 31s\tremaining: 6.76s\n",
      "969:\tlearn: 0.0818745\ttotal: 3m 31s\tremaining: 6.55s\n",
      "970:\tlearn: 0.0818240\ttotal: 3m 31s\tremaining: 6.33s\n",
      "971:\tlearn: 0.0817879\ttotal: 3m 32s\tremaining: 6.11s\n",
      "972:\tlearn: 0.0817535\ttotal: 3m 32s\tremaining: 5.89s\n",
      "973:\tlearn: 0.0817049\ttotal: 3m 32s\tremaining: 5.68s\n",
      "974:\tlearn: 0.0816644\ttotal: 3m 32s\tremaining: 5.46s\n",
      "975:\tlearn: 0.0816416\ttotal: 3m 33s\tremaining: 5.24s\n",
      "976:\tlearn: 0.0816255\ttotal: 3m 33s\tremaining: 5.02s\n",
      "977:\tlearn: 0.0816035\ttotal: 3m 33s\tremaining: 4.8s\n",
      "978:\tlearn: 0.0815717\ttotal: 3m 33s\tremaining: 4.58s\n",
      "979:\tlearn: 0.0815427\ttotal: 3m 33s\tremaining: 4.36s\n",
      "980:\tlearn: 0.0815326\ttotal: 3m 34s\tremaining: 4.14s\n",
      "981:\tlearn: 0.0815017\ttotal: 3m 34s\tremaining: 3.93s\n",
      "982:\tlearn: 0.0814583\ttotal: 3m 34s\tremaining: 3.71s\n",
      "983:\tlearn: 0.0814556\ttotal: 3m 34s\tremaining: 3.49s\n",
      "984:\tlearn: 0.0814223\ttotal: 3m 34s\tremaining: 3.27s\n",
      "985:\tlearn: 0.0813882\ttotal: 3m 35s\tremaining: 3.05s\n",
      "986:\tlearn: 0.0813352\ttotal: 3m 35s\tremaining: 2.84s\n",
      "987:\tlearn: 0.0812957\ttotal: 3m 35s\tremaining: 2.62s\n",
      "988:\tlearn: 0.0812649\ttotal: 3m 35s\tremaining: 2.4s\n",
      "989:\tlearn: 0.0812289\ttotal: 3m 36s\tremaining: 2.18s\n",
      "990:\tlearn: 0.0812253\ttotal: 3m 36s\tremaining: 1.96s\n",
      "991:\tlearn: 0.0811748\ttotal: 3m 36s\tremaining: 1.75s\n",
      "992:\tlearn: 0.0811394\ttotal: 3m 36s\tremaining: 1.53s\n",
      "993:\tlearn: 0.0810907\ttotal: 3m 36s\tremaining: 1.31s\n",
      "994:\tlearn: 0.0810456\ttotal: 3m 37s\tremaining: 1.09s\n",
      "995:\tlearn: 0.0809829\ttotal: 3m 37s\tremaining: 873ms\n",
      "996:\tlearn: 0.0809718\ttotal: 3m 37s\tremaining: 655ms\n",
      "997:\tlearn: 0.0809228\ttotal: 3m 37s\tremaining: 436ms\n",
      "998:\tlearn: 0.0808925\ttotal: 3m 37s\tremaining: 218ms\n",
      "999:\tlearn: 0.0808792\ttotal: 3m 38s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5211691\ttotal: 208ms\tremaining: 3m 27s\n",
      "1:\tlearn: 0.4200526\ttotal: 406ms\tremaining: 3m 22s\n",
      "2:\tlearn: 0.3597106\ttotal: 659ms\tremaining: 3m 39s\n",
      "3:\tlearn: 0.3197255\ttotal: 856ms\tremaining: 3m 33s\n",
      "4:\tlearn: 0.2908641\ttotal: 1.05s\tremaining: 3m 30s\n",
      "5:\tlearn: 0.2733459\ttotal: 1.27s\tremaining: 3m 30s\n",
      "6:\tlearn: 0.2546270\ttotal: 1.5s\tremaining: 3m 32s\n",
      "7:\tlearn: 0.2435633\ttotal: 1.73s\tremaining: 3m 34s\n",
      "8:\tlearn: 0.2360412\ttotal: 1.92s\tremaining: 3m 31s\n",
      "9:\tlearn: 0.2292038\ttotal: 2.15s\tremaining: 3m 32s\n",
      "10:\tlearn: 0.2211590\ttotal: 2.37s\tremaining: 3m 33s\n",
      "11:\tlearn: 0.2140363\ttotal: 2.58s\tremaining: 3m 32s\n",
      "12:\tlearn: 0.2098879\ttotal: 2.79s\tremaining: 3m 31s\n",
      "13:\tlearn: 0.2064556\ttotal: 2.99s\tremaining: 3m 30s\n",
      "14:\tlearn: 0.2035417\ttotal: 3.18s\tremaining: 3m 28s\n",
      "15:\tlearn: 0.2010872\ttotal: 3.44s\tremaining: 3m 31s\n",
      "16:\tlearn: 0.1975529\ttotal: 3.64s\tremaining: 3m 30s\n",
      "17:\tlearn: 0.1940443\ttotal: 3.85s\tremaining: 3m 29s\n",
      "18:\tlearn: 0.1915215\ttotal: 4.03s\tremaining: 3m 28s\n",
      "19:\tlearn: 0.1899049\ttotal: 4.2s\tremaining: 3m 25s\n",
      "20:\tlearn: 0.1866946\ttotal: 4.4s\tremaining: 3m 25s\n",
      "21:\tlearn: 0.1850914\ttotal: 4.67s\tremaining: 3m 27s\n",
      "22:\tlearn: 0.1836718\ttotal: 4.95s\tremaining: 3m 30s\n",
      "23:\tlearn: 0.1815527\ttotal: 5.22s\tremaining: 3m 32s\n",
      "24:\tlearn: 0.1802116\ttotal: 5.46s\tremaining: 3m 32s\n",
      "25:\tlearn: 0.1788067\ttotal: 5.75s\tremaining: 3m 35s\n",
      "26:\tlearn: 0.1775007\ttotal: 6.01s\tremaining: 3m 36s\n",
      "27:\tlearn: 0.1763929\ttotal: 6.26s\tremaining: 3m 37s\n",
      "28:\tlearn: 0.1753212\ttotal: 6.46s\tremaining: 3m 36s\n",
      "29:\tlearn: 0.1740792\ttotal: 6.72s\tremaining: 3m 37s\n",
      "30:\tlearn: 0.1726293\ttotal: 6.94s\tremaining: 3m 36s\n",
      "31:\tlearn: 0.1715839\ttotal: 7.14s\tremaining: 3m 35s\n",
      "32:\tlearn: 0.1703617\ttotal: 7.37s\tremaining: 3m 35s\n",
      "33:\tlearn: 0.1691504\ttotal: 7.57s\tremaining: 3m 35s\n",
      "34:\tlearn: 0.1683277\ttotal: 7.8s\tremaining: 3m 35s\n",
      "35:\tlearn: 0.1674780\ttotal: 8.01s\tremaining: 3m 34s\n",
      "36:\tlearn: 0.1666048\ttotal: 8.24s\tremaining: 3m 34s\n",
      "37:\tlearn: 0.1657180\ttotal: 8.41s\tremaining: 3m 32s\n",
      "38:\tlearn: 0.1642392\ttotal: 8.64s\tremaining: 3m 33s\n",
      "39:\tlearn: 0.1635020\ttotal: 8.85s\tremaining: 3m 32s\n",
      "40:\tlearn: 0.1626315\ttotal: 9.07s\tremaining: 3m 32s\n",
      "41:\tlearn: 0.1621333\ttotal: 9.27s\tremaining: 3m 31s\n",
      "42:\tlearn: 0.1614210\ttotal: 9.49s\tremaining: 3m 31s\n",
      "43:\tlearn: 0.1608758\ttotal: 9.7s\tremaining: 3m 30s\n",
      "44:\tlearn: 0.1602065\ttotal: 9.91s\tremaining: 3m 30s\n",
      "45:\tlearn: 0.1595714\ttotal: 10.1s\tremaining: 3m 30s\n",
      "46:\tlearn: 0.1587836\ttotal: 10.4s\tremaining: 3m 30s\n",
      "47:\tlearn: 0.1580839\ttotal: 10.6s\tremaining: 3m 30s\n",
      "48:\tlearn: 0.1575400\ttotal: 10.8s\tremaining: 3m 30s\n",
      "49:\tlearn: 0.1564996\ttotal: 11s\tremaining: 3m 29s\n",
      "50:\tlearn: 0.1561541\ttotal: 11.2s\tremaining: 3m 28s\n",
      "51:\tlearn: 0.1558827\ttotal: 11.4s\tremaining: 3m 28s\n",
      "52:\tlearn: 0.1552686\ttotal: 11.6s\tremaining: 3m 27s\n",
      "53:\tlearn: 0.1546514\ttotal: 11.8s\tremaining: 3m 27s\n",
      "54:\tlearn: 0.1542434\ttotal: 12.1s\tremaining: 3m 27s\n",
      "55:\tlearn: 0.1537890\ttotal: 12.3s\tremaining: 3m 27s\n",
      "56:\tlearn: 0.1532031\ttotal: 12.6s\tremaining: 3m 28s\n",
      "57:\tlearn: 0.1527206\ttotal: 12.8s\tremaining: 3m 27s\n",
      "58:\tlearn: 0.1523588\ttotal: 12.9s\tremaining: 3m 26s\n",
      "59:\tlearn: 0.1518992\ttotal: 13.2s\tremaining: 3m 26s\n",
      "60:\tlearn: 0.1515321\ttotal: 13.4s\tremaining: 3m 25s\n",
      "61:\tlearn: 0.1510704\ttotal: 13.6s\tremaining: 3m 25s\n",
      "62:\tlearn: 0.1507053\ttotal: 13.8s\tremaining: 3m 24s\n",
      "63:\tlearn: 0.1502566\ttotal: 14s\tremaining: 3m 25s\n",
      "64:\tlearn: 0.1499391\ttotal: 14.2s\tremaining: 3m 24s\n",
      "65:\tlearn: 0.1493668\ttotal: 14.5s\tremaining: 3m 24s\n",
      "66:\tlearn: 0.1490278\ttotal: 14.8s\tremaining: 3m 25s\n",
      "67:\tlearn: 0.1485931\ttotal: 15s\tremaining: 3m 26s\n",
      "68:\tlearn: 0.1482406\ttotal: 15.3s\tremaining: 3m 26s\n",
      "69:\tlearn: 0.1479181\ttotal: 15.5s\tremaining: 3m 25s\n",
      "70:\tlearn: 0.1476299\ttotal: 15.7s\tremaining: 3m 25s\n",
      "71:\tlearn: 0.1473918\ttotal: 15.9s\tremaining: 3m 25s\n",
      "72:\tlearn: 0.1469702\ttotal: 16.2s\tremaining: 3m 25s\n",
      "73:\tlearn: 0.1466777\ttotal: 16.4s\tremaining: 3m 25s\n",
      "74:\tlearn: 0.1462928\ttotal: 16.6s\tremaining: 3m 25s\n",
      "75:\tlearn: 0.1459086\ttotal: 16.8s\tremaining: 3m 24s\n",
      "76:\tlearn: 0.1455842\ttotal: 17s\tremaining: 3m 23s\n",
      "77:\tlearn: 0.1453525\ttotal: 17.2s\tremaining: 3m 23s\n",
      "78:\tlearn: 0.1450910\ttotal: 17.4s\tremaining: 3m 23s\n",
      "79:\tlearn: 0.1447951\ttotal: 17.6s\tremaining: 3m 22s\n",
      "80:\tlearn: 0.1445356\ttotal: 17.9s\tremaining: 3m 23s\n",
      "81:\tlearn: 0.1442225\ttotal: 18.1s\tremaining: 3m 22s\n",
      "82:\tlearn: 0.1438244\ttotal: 18.3s\tremaining: 3m 22s\n",
      "83:\tlearn: 0.1435428\ttotal: 18.5s\tremaining: 3m 21s\n",
      "84:\tlearn: 0.1431024\ttotal: 18.7s\tremaining: 3m 20s\n",
      "85:\tlearn: 0.1428425\ttotal: 18.9s\tremaining: 3m 20s\n",
      "86:\tlearn: 0.1425543\ttotal: 19.1s\tremaining: 3m 20s\n",
      "87:\tlearn: 0.1423498\ttotal: 19.3s\tremaining: 3m 20s\n",
      "88:\tlearn: 0.1419717\ttotal: 19.5s\tremaining: 3m 20s\n",
      "89:\tlearn: 0.1417724\ttotal: 19.7s\tremaining: 3m 19s\n",
      "90:\tlearn: 0.1415495\ttotal: 20s\tremaining: 3m 19s\n",
      "91:\tlearn: 0.1412851\ttotal: 20.2s\tremaining: 3m 19s\n",
      "92:\tlearn: 0.1409616\ttotal: 20.4s\tremaining: 3m 19s\n",
      "93:\tlearn: 0.1407322\ttotal: 20.6s\tremaining: 3m 18s\n",
      "94:\tlearn: 0.1405906\ttotal: 20.9s\tremaining: 3m 19s\n",
      "95:\tlearn: 0.1403786\ttotal: 21.1s\tremaining: 3m 18s\n",
      "96:\tlearn: 0.1401055\ttotal: 21.3s\tremaining: 3m 18s\n",
      "97:\tlearn: 0.1396606\ttotal: 21.5s\tremaining: 3m 18s\n",
      "98:\tlearn: 0.1392229\ttotal: 21.7s\tremaining: 3m 17s\n",
      "99:\tlearn: 0.1390202\ttotal: 21.9s\tremaining: 3m 17s\n",
      "100:\tlearn: 0.1387813\ttotal: 22.1s\tremaining: 3m 16s\n",
      "101:\tlearn: 0.1385332\ttotal: 22.3s\tremaining: 3m 16s\n",
      "102:\tlearn: 0.1382904\ttotal: 22.5s\tremaining: 3m 15s\n",
      "103:\tlearn: 0.1380119\ttotal: 22.7s\tremaining: 3m 15s\n",
      "104:\tlearn: 0.1377709\ttotal: 22.9s\tremaining: 3m 15s\n",
      "105:\tlearn: 0.1374697\ttotal: 23.1s\tremaining: 3m 14s\n",
      "106:\tlearn: 0.1372835\ttotal: 23.3s\tremaining: 3m 14s\n",
      "107:\tlearn: 0.1369995\ttotal: 23.5s\tremaining: 3m 13s\n",
      "108:\tlearn: 0.1367952\ttotal: 23.7s\tremaining: 3m 13s\n",
      "109:\tlearn: 0.1365647\ttotal: 23.9s\tremaining: 3m 13s\n",
      "110:\tlearn: 0.1363058\ttotal: 24.2s\tremaining: 3m 13s\n",
      "111:\tlearn: 0.1361617\ttotal: 24.4s\tremaining: 3m 13s\n",
      "112:\tlearn: 0.1359816\ttotal: 24.6s\tremaining: 3m 13s\n",
      "113:\tlearn: 0.1357031\ttotal: 24.9s\tremaining: 3m 13s\n",
      "114:\tlearn: 0.1354538\ttotal: 25.1s\tremaining: 3m 13s\n",
      "115:\tlearn: 0.1352984\ttotal: 25.3s\tremaining: 3m 13s\n",
      "116:\tlearn: 0.1350833\ttotal: 25.6s\tremaining: 3m 12s\n",
      "117:\tlearn: 0.1348919\ttotal: 25.8s\tremaining: 3m 12s\n",
      "118:\tlearn: 0.1347003\ttotal: 26s\tremaining: 3m 12s\n",
      "119:\tlearn: 0.1345250\ttotal: 26.2s\tremaining: 3m 12s\n",
      "120:\tlearn: 0.1343722\ttotal: 26.4s\tremaining: 3m 11s\n",
      "121:\tlearn: 0.1342245\ttotal: 26.6s\tremaining: 3m 11s\n",
      "122:\tlearn: 0.1339723\ttotal: 26.9s\tremaining: 3m 11s\n",
      "123:\tlearn: 0.1337706\ttotal: 27.1s\tremaining: 3m 11s\n",
      "124:\tlearn: 0.1335742\ttotal: 27.4s\tremaining: 3m 11s\n",
      "125:\tlearn: 0.1333631\ttotal: 27.6s\tremaining: 3m 11s\n",
      "126:\tlearn: 0.1330814\ttotal: 27.8s\tremaining: 3m 11s\n",
      "127:\tlearn: 0.1329411\ttotal: 28s\tremaining: 3m 10s\n",
      "128:\tlearn: 0.1327874\ttotal: 28.2s\tremaining: 3m 10s\n",
      "129:\tlearn: 0.1325468\ttotal: 28.4s\tremaining: 3m 9s\n",
      "130:\tlearn: 0.1323757\ttotal: 28.6s\tremaining: 3m 9s\n",
      "131:\tlearn: 0.1322368\ttotal: 28.9s\tremaining: 3m 9s\n",
      "132:\tlearn: 0.1320376\ttotal: 29.1s\tremaining: 3m 9s\n",
      "133:\tlearn: 0.1318728\ttotal: 29.4s\tremaining: 3m 9s\n",
      "134:\tlearn: 0.1317129\ttotal: 29.6s\tremaining: 3m 9s\n",
      "135:\tlearn: 0.1315188\ttotal: 30s\tremaining: 3m 10s\n",
      "136:\tlearn: 0.1313719\ttotal: 30.3s\tremaining: 3m 10s\n",
      "137:\tlearn: 0.1312147\ttotal: 30.5s\tremaining: 3m 10s\n",
      "138:\tlearn: 0.1310679\ttotal: 30.9s\tremaining: 3m 11s\n",
      "139:\tlearn: 0.1309452\ttotal: 31.1s\tremaining: 3m 10s\n",
      "140:\tlearn: 0.1308129\ttotal: 31.4s\tremaining: 3m 11s\n",
      "141:\tlearn: 0.1306802\ttotal: 31.7s\tremaining: 3m 11s\n",
      "142:\tlearn: 0.1305384\ttotal: 31.9s\tremaining: 3m 11s\n",
      "143:\tlearn: 0.1303838\ttotal: 32.2s\tremaining: 3m 11s\n",
      "144:\tlearn: 0.1302657\ttotal: 32.4s\tremaining: 3m 10s\n",
      "145:\tlearn: 0.1301232\ttotal: 32.6s\tremaining: 3m 10s\n",
      "146:\tlearn: 0.1299330\ttotal: 32.9s\tremaining: 3m 10s\n",
      "147:\tlearn: 0.1297663\ttotal: 33.1s\tremaining: 3m 10s\n",
      "148:\tlearn: 0.1296512\ttotal: 33.3s\tremaining: 3m 10s\n",
      "149:\tlearn: 0.1295232\ttotal: 33.5s\tremaining: 3m 9s\n",
      "150:\tlearn: 0.1293786\ttotal: 33.7s\tremaining: 3m 9s\n",
      "151:\tlearn: 0.1292787\ttotal: 33.9s\tremaining: 3m 9s\n",
      "152:\tlearn: 0.1291185\ttotal: 34.2s\tremaining: 3m 9s\n",
      "153:\tlearn: 0.1289880\ttotal: 34.4s\tremaining: 3m 8s\n",
      "154:\tlearn: 0.1288838\ttotal: 34.6s\tremaining: 3m 8s\n",
      "155:\tlearn: 0.1287291\ttotal: 34.8s\tremaining: 3m 8s\n",
      "156:\tlearn: 0.1286445\ttotal: 35s\tremaining: 3m 8s\n",
      "157:\tlearn: 0.1284750\ttotal: 35.2s\tremaining: 3m 7s\n",
      "158:\tlearn: 0.1283408\ttotal: 35.4s\tremaining: 3m 7s\n",
      "159:\tlearn: 0.1282338\ttotal: 35.6s\tremaining: 3m 7s\n",
      "160:\tlearn: 0.1280699\ttotal: 35.9s\tremaining: 3m 7s\n",
      "161:\tlearn: 0.1277990\ttotal: 36.1s\tremaining: 3m 6s\n",
      "162:\tlearn: 0.1276700\ttotal: 36.3s\tremaining: 3m 6s\n",
      "163:\tlearn: 0.1275194\ttotal: 36.5s\tremaining: 3m 6s\n",
      "164:\tlearn: 0.1274022\ttotal: 36.7s\tremaining: 3m 5s\n",
      "165:\tlearn: 0.1272719\ttotal: 36.9s\tremaining: 3m 5s\n",
      "166:\tlearn: 0.1271644\ttotal: 37.2s\tremaining: 3m 5s\n",
      "167:\tlearn: 0.1270595\ttotal: 37.3s\tremaining: 3m 4s\n",
      "168:\tlearn: 0.1269421\ttotal: 37.5s\tremaining: 3m 4s\n",
      "169:\tlearn: 0.1268745\ttotal: 37.7s\tremaining: 3m 4s\n",
      "170:\tlearn: 0.1267115\ttotal: 38s\tremaining: 3m 4s\n",
      "171:\tlearn: 0.1266242\ttotal: 38.2s\tremaining: 3m 3s\n",
      "172:\tlearn: 0.1265126\ttotal: 38.4s\tremaining: 3m 3s\n",
      "173:\tlearn: 0.1264237\ttotal: 38.6s\tremaining: 3m 3s\n",
      "174:\tlearn: 0.1263438\ttotal: 38.9s\tremaining: 3m 3s\n",
      "175:\tlearn: 0.1261748\ttotal: 39s\tremaining: 3m 2s\n",
      "176:\tlearn: 0.1260576\ttotal: 39.2s\tremaining: 3m 2s\n",
      "177:\tlearn: 0.1259747\ttotal: 39.4s\tremaining: 3m 2s\n",
      "178:\tlearn: 0.1258657\ttotal: 39.6s\tremaining: 3m 1s\n",
      "179:\tlearn: 0.1257771\ttotal: 39.8s\tremaining: 3m 1s\n",
      "180:\tlearn: 0.1256882\ttotal: 40.1s\tremaining: 3m 1s\n",
      "181:\tlearn: 0.1256271\ttotal: 40.3s\tremaining: 3m\n",
      "182:\tlearn: 0.1254047\ttotal: 40.5s\tremaining: 3m\n",
      "183:\tlearn: 0.1252981\ttotal: 40.7s\tremaining: 3m\n",
      "184:\tlearn: 0.1252007\ttotal: 40.9s\tremaining: 3m\n",
      "185:\tlearn: 0.1251093\ttotal: 41.1s\tremaining: 2m 59s\n",
      "186:\tlearn: 0.1249519\ttotal: 41.3s\tremaining: 2m 59s\n",
      "187:\tlearn: 0.1248610\ttotal: 41.5s\tremaining: 2m 59s\n",
      "188:\tlearn: 0.1247212\ttotal: 41.8s\tremaining: 2m 59s\n",
      "189:\tlearn: 0.1245976\ttotal: 42s\tremaining: 2m 59s\n",
      "190:\tlearn: 0.1245156\ttotal: 42.2s\tremaining: 2m 58s\n",
      "191:\tlearn: 0.1244149\ttotal: 42.4s\tremaining: 2m 58s\n",
      "192:\tlearn: 0.1242626\ttotal: 42.7s\tremaining: 2m 58s\n",
      "193:\tlearn: 0.1241766\ttotal: 42.9s\tremaining: 2m 58s\n",
      "194:\tlearn: 0.1240493\ttotal: 43.1s\tremaining: 2m 58s\n",
      "195:\tlearn: 0.1239210\ttotal: 43.4s\tremaining: 2m 57s\n",
      "196:\tlearn: 0.1237872\ttotal: 43.6s\tremaining: 2m 57s\n",
      "197:\tlearn: 0.1236971\ttotal: 43.8s\tremaining: 2m 57s\n",
      "198:\tlearn: 0.1235721\ttotal: 44s\tremaining: 2m 57s\n",
      "199:\tlearn: 0.1234619\ttotal: 44.2s\tremaining: 2m 56s\n",
      "200:\tlearn: 0.1233668\ttotal: 44.5s\tremaining: 2m 56s\n",
      "201:\tlearn: 0.1232981\ttotal: 44.7s\tremaining: 2m 56s\n",
      "202:\tlearn: 0.1232180\ttotal: 45s\tremaining: 2m 56s\n",
      "203:\tlearn: 0.1231169\ttotal: 45.2s\tremaining: 2m 56s\n",
      "204:\tlearn: 0.1230028\ttotal: 45.4s\tremaining: 2m 56s\n",
      "205:\tlearn: 0.1228465\ttotal: 45.6s\tremaining: 2m 55s\n",
      "206:\tlearn: 0.1227728\ttotal: 45.9s\tremaining: 2m 55s\n",
      "207:\tlearn: 0.1226520\ttotal: 46.1s\tremaining: 2m 55s\n",
      "208:\tlearn: 0.1225820\ttotal: 46.3s\tremaining: 2m 55s\n",
      "209:\tlearn: 0.1224994\ttotal: 46.5s\tremaining: 2m 54s\n",
      "210:\tlearn: 0.1224371\ttotal: 46.8s\tremaining: 2m 54s\n",
      "211:\tlearn: 0.1223299\ttotal: 47s\tremaining: 2m 54s\n",
      "212:\tlearn: 0.1222701\ttotal: 47.2s\tremaining: 2m 54s\n",
      "213:\tlearn: 0.1221571\ttotal: 47.4s\tremaining: 2m 54s\n",
      "214:\tlearn: 0.1220317\ttotal: 47.6s\tremaining: 2m 53s\n",
      "215:\tlearn: 0.1219346\ttotal: 47.9s\tremaining: 2m 53s\n",
      "216:\tlearn: 0.1218410\ttotal: 48.1s\tremaining: 2m 53s\n",
      "217:\tlearn: 0.1216555\ttotal: 48.4s\tremaining: 2m 53s\n",
      "218:\tlearn: 0.1215481\ttotal: 48.9s\tremaining: 2m 54s\n",
      "219:\tlearn: 0.1214421\ttotal: 49.1s\tremaining: 2m 54s\n",
      "220:\tlearn: 0.1213389\ttotal: 49.4s\tremaining: 2m 54s\n",
      "221:\tlearn: 0.1211909\ttotal: 49.7s\tremaining: 2m 54s\n",
      "222:\tlearn: 0.1211384\ttotal: 50s\tremaining: 2m 54s\n",
      "223:\tlearn: 0.1210266\ttotal: 50.2s\tremaining: 2m 53s\n",
      "224:\tlearn: 0.1209255\ttotal: 50.3s\tremaining: 2m 53s\n",
      "225:\tlearn: 0.1208530\ttotal: 50.5s\tremaining: 2m 53s\n",
      "226:\tlearn: 0.1207430\ttotal: 50.7s\tremaining: 2m 52s\n",
      "227:\tlearn: 0.1206337\ttotal: 50.9s\tremaining: 2m 52s\n",
      "228:\tlearn: 0.1205085\ttotal: 51.1s\tremaining: 2m 52s\n",
      "229:\tlearn: 0.1203936\ttotal: 51.3s\tremaining: 2m 51s\n",
      "230:\tlearn: 0.1203194\ttotal: 51.6s\tremaining: 2m 51s\n",
      "231:\tlearn: 0.1202189\ttotal: 51.8s\tremaining: 2m 51s\n",
      "232:\tlearn: 0.1201592\ttotal: 52s\tremaining: 2m 51s\n",
      "233:\tlearn: 0.1200745\ttotal: 52.2s\tremaining: 2m 51s\n",
      "234:\tlearn: 0.1199980\ttotal: 52.5s\tremaining: 2m 50s\n",
      "235:\tlearn: 0.1198603\ttotal: 52.7s\tremaining: 2m 50s\n",
      "236:\tlearn: 0.1197057\ttotal: 53s\tremaining: 2m 50s\n",
      "237:\tlearn: 0.1195730\ttotal: 53.2s\tremaining: 2m 50s\n",
      "238:\tlearn: 0.1194951\ttotal: 53.4s\tremaining: 2m 49s\n",
      "239:\tlearn: 0.1194085\ttotal: 53.6s\tremaining: 2m 49s\n",
      "240:\tlearn: 0.1193150\ttotal: 53.8s\tremaining: 2m 49s\n",
      "241:\tlearn: 0.1192504\ttotal: 54s\tremaining: 2m 49s\n",
      "242:\tlearn: 0.1191666\ttotal: 54.3s\tremaining: 2m 49s\n",
      "243:\tlearn: 0.1190818\ttotal: 54.5s\tremaining: 2m 48s\n",
      "244:\tlearn: 0.1189929\ttotal: 54.7s\tremaining: 2m 48s\n",
      "245:\tlearn: 0.1188788\ttotal: 54.9s\tremaining: 2m 48s\n",
      "246:\tlearn: 0.1188006\ttotal: 55.2s\tremaining: 2m 48s\n",
      "247:\tlearn: 0.1187351\ttotal: 55.4s\tremaining: 2m 48s\n",
      "248:\tlearn: 0.1186851\ttotal: 55.7s\tremaining: 2m 47s\n",
      "249:\tlearn: 0.1185885\ttotal: 55.9s\tremaining: 2m 47s\n",
      "250:\tlearn: 0.1185028\ttotal: 56.1s\tremaining: 2m 47s\n",
      "251:\tlearn: 0.1184307\ttotal: 56.3s\tremaining: 2m 47s\n",
      "252:\tlearn: 0.1183744\ttotal: 56.5s\tremaining: 2m 46s\n",
      "253:\tlearn: 0.1182677\ttotal: 56.8s\tremaining: 2m 46s\n",
      "254:\tlearn: 0.1181590\ttotal: 57s\tremaining: 2m 46s\n",
      "255:\tlearn: 0.1181050\ttotal: 57.2s\tremaining: 2m 46s\n",
      "256:\tlearn: 0.1180213\ttotal: 57.4s\tremaining: 2m 45s\n",
      "257:\tlearn: 0.1179566\ttotal: 57.7s\tremaining: 2m 45s\n",
      "258:\tlearn: 0.1178667\ttotal: 57.9s\tremaining: 2m 45s\n",
      "259:\tlearn: 0.1176746\ttotal: 58.1s\tremaining: 2m 45s\n",
      "260:\tlearn: 0.1175080\ttotal: 58.3s\tremaining: 2m 45s\n",
      "261:\tlearn: 0.1173948\ttotal: 58.5s\tremaining: 2m 44s\n",
      "262:\tlearn: 0.1173381\ttotal: 58.7s\tremaining: 2m 44s\n",
      "263:\tlearn: 0.1172383\ttotal: 59s\tremaining: 2m 44s\n",
      "264:\tlearn: 0.1171795\ttotal: 59.1s\tremaining: 2m 44s\n",
      "265:\tlearn: 0.1170854\ttotal: 59.4s\tremaining: 2m 43s\n",
      "266:\tlearn: 0.1169713\ttotal: 59.6s\tremaining: 2m 43s\n",
      "267:\tlearn: 0.1169031\ttotal: 59.9s\tremaining: 2m 43s\n",
      "268:\tlearn: 0.1168208\ttotal: 1m\tremaining: 2m 43s\n",
      "269:\tlearn: 0.1166971\ttotal: 1m\tremaining: 2m 42s\n",
      "270:\tlearn: 0.1166080\ttotal: 1m\tremaining: 2m 42s\n",
      "271:\tlearn: 0.1165493\ttotal: 1m\tremaining: 2m 42s\n",
      "272:\tlearn: 0.1164760\ttotal: 1m\tremaining: 2m 42s\n",
      "273:\tlearn: 0.1163557\ttotal: 1m 1s\tremaining: 2m 41s\n",
      "274:\tlearn: 0.1162587\ttotal: 1m 1s\tremaining: 2m 41s\n",
      "275:\tlearn: 0.1161997\ttotal: 1m 1s\tremaining: 2m 41s\n",
      "276:\tlearn: 0.1160707\ttotal: 1m 1s\tremaining: 2m 41s\n",
      "277:\tlearn: 0.1160190\ttotal: 1m 1s\tremaining: 2m 40s\n",
      "278:\tlearn: 0.1158847\ttotal: 1m 2s\tremaining: 2m 40s\n",
      "279:\tlearn: 0.1157714\ttotal: 1m 2s\tremaining: 2m 40s\n",
      "280:\tlearn: 0.1157136\ttotal: 1m 2s\tremaining: 2m 40s\n",
      "281:\tlearn: 0.1156254\ttotal: 1m 2s\tremaining: 2m 40s\n",
      "282:\tlearn: 0.1155445\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "283:\tlearn: 0.1154512\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "284:\tlearn: 0.1153590\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "285:\tlearn: 0.1152958\ttotal: 1m 3s\tremaining: 2m 39s\n",
      "286:\tlearn: 0.1152351\ttotal: 1m 4s\tremaining: 2m 39s\n",
      "287:\tlearn: 0.1151378\ttotal: 1m 4s\tremaining: 2m 38s\n",
      "288:\tlearn: 0.1150619\ttotal: 1m 4s\tremaining: 2m 38s\n",
      "289:\tlearn: 0.1149687\ttotal: 1m 4s\tremaining: 2m 38s\n",
      "290:\tlearn: 0.1148940\ttotal: 1m 4s\tremaining: 2m 38s\n",
      "291:\tlearn: 0.1148107\ttotal: 1m 5s\tremaining: 2m 37s\n",
      "292:\tlearn: 0.1147076\ttotal: 1m 5s\tremaining: 2m 37s\n",
      "293:\tlearn: 0.1146393\ttotal: 1m 5s\tremaining: 2m 37s\n",
      "294:\tlearn: 0.1145847\ttotal: 1m 5s\tremaining: 2m 37s\n",
      "295:\tlearn: 0.1145272\ttotal: 1m 5s\tremaining: 2m 36s\n",
      "296:\tlearn: 0.1144240\ttotal: 1m 6s\tremaining: 2m 36s\n",
      "297:\tlearn: 0.1143243\ttotal: 1m 6s\tremaining: 2m 36s\n",
      "298:\tlearn: 0.1142680\ttotal: 1m 6s\tremaining: 2m 35s\n",
      "299:\tlearn: 0.1141849\ttotal: 1m 6s\tremaining: 2m 35s\n",
      "300:\tlearn: 0.1141195\ttotal: 1m 6s\tremaining: 2m 35s\n",
      "301:\tlearn: 0.1140221\ttotal: 1m 7s\tremaining: 2m 35s\n",
      "302:\tlearn: 0.1139311\ttotal: 1m 7s\tremaining: 2m 34s\n",
      "303:\tlearn: 0.1138797\ttotal: 1m 7s\tremaining: 2m 34s\n",
      "304:\tlearn: 0.1138100\ttotal: 1m 7s\tremaining: 2m 34s\n",
      "305:\tlearn: 0.1137274\ttotal: 1m 8s\tremaining: 2m 34s\n",
      "306:\tlearn: 0.1136137\ttotal: 1m 8s\tremaining: 2m 33s\n",
      "307:\tlearn: 0.1135502\ttotal: 1m 8s\tremaining: 2m 33s\n",
      "308:\tlearn: 0.1134754\ttotal: 1m 8s\tremaining: 2m 33s\n",
      "309:\tlearn: 0.1133985\ttotal: 1m 8s\tremaining: 2m 33s\n",
      "310:\tlearn: 0.1132854\ttotal: 1m 9s\tremaining: 2m 33s\n",
      "311:\tlearn: 0.1131786\ttotal: 1m 9s\tremaining: 2m 32s\n",
      "312:\tlearn: 0.1131296\ttotal: 1m 9s\tremaining: 2m 32s\n",
      "313:\tlearn: 0.1130143\ttotal: 1m 9s\tremaining: 2m 32s\n",
      "314:\tlearn: 0.1129274\ttotal: 1m 10s\tremaining: 2m 32s\n",
      "315:\tlearn: 0.1128332\ttotal: 1m 10s\tremaining: 2m 32s\n",
      "316:\tlearn: 0.1127688\ttotal: 1m 10s\tremaining: 2m 31s\n",
      "317:\tlearn: 0.1126650\ttotal: 1m 10s\tremaining: 2m 31s\n",
      "318:\tlearn: 0.1125846\ttotal: 1m 10s\tremaining: 2m 31s\n",
      "319:\tlearn: 0.1125414\ttotal: 1m 11s\tremaining: 2m 31s\n",
      "320:\tlearn: 0.1124967\ttotal: 1m 11s\tremaining: 2m 30s\n",
      "321:\tlearn: 0.1124355\ttotal: 1m 11s\tremaining: 2m 30s\n",
      "322:\tlearn: 0.1123492\ttotal: 1m 11s\tremaining: 2m 30s\n",
      "323:\tlearn: 0.1122641\ttotal: 1m 12s\tremaining: 2m 30s\n",
      "324:\tlearn: 0.1122002\ttotal: 1m 12s\tremaining: 2m 30s\n",
      "325:\tlearn: 0.1121585\ttotal: 1m 12s\tremaining: 2m 29s\n",
      "326:\tlearn: 0.1120853\ttotal: 1m 12s\tremaining: 2m 29s\n",
      "327:\tlearn: 0.1120040\ttotal: 1m 12s\tremaining: 2m 29s\n",
      "328:\tlearn: 0.1119500\ttotal: 1m 13s\tremaining: 2m 29s\n",
      "329:\tlearn: 0.1118905\ttotal: 1m 13s\tremaining: 2m 29s\n",
      "330:\tlearn: 0.1118503\ttotal: 1m 13s\tremaining: 2m 28s\n",
      "331:\tlearn: 0.1117870\ttotal: 1m 13s\tremaining: 2m 28s\n",
      "332:\tlearn: 0.1117286\ttotal: 1m 14s\tremaining: 2m 28s\n",
      "333:\tlearn: 0.1116416\ttotal: 1m 14s\tremaining: 2m 28s\n",
      "334:\tlearn: 0.1115962\ttotal: 1m 14s\tremaining: 2m 27s\n",
      "335:\tlearn: 0.1115446\ttotal: 1m 14s\tremaining: 2m 27s\n",
      "336:\tlearn: 0.1114624\ttotal: 1m 14s\tremaining: 2m 27s\n",
      "337:\tlearn: 0.1113442\ttotal: 1m 15s\tremaining: 2m 27s\n",
      "338:\tlearn: 0.1112703\ttotal: 1m 15s\tremaining: 2m 27s\n",
      "339:\tlearn: 0.1112362\ttotal: 1m 15s\tremaining: 2m 26s\n",
      "340:\tlearn: 0.1111952\ttotal: 1m 15s\tremaining: 2m 26s\n",
      "341:\tlearn: 0.1111281\ttotal: 1m 16s\tremaining: 2m 26s\n",
      "342:\tlearn: 0.1110621\ttotal: 1m 16s\tremaining: 2m 26s\n",
      "343:\tlearn: 0.1110129\ttotal: 1m 16s\tremaining: 2m 25s\n",
      "344:\tlearn: 0.1109706\ttotal: 1m 16s\tremaining: 2m 25s\n",
      "345:\tlearn: 0.1108865\ttotal: 1m 16s\tremaining: 2m 25s\n",
      "346:\tlearn: 0.1108014\ttotal: 1m 17s\tremaining: 2m 25s\n",
      "347:\tlearn: 0.1107430\ttotal: 1m 17s\tremaining: 2m 24s\n",
      "348:\tlearn: 0.1106814\ttotal: 1m 17s\tremaining: 2m 24s\n",
      "349:\tlearn: 0.1106362\ttotal: 1m 17s\tremaining: 2m 24s\n",
      "350:\tlearn: 0.1105800\ttotal: 1m 17s\tremaining: 2m 24s\n",
      "351:\tlearn: 0.1105706\ttotal: 1m 18s\tremaining: 2m 23s\n",
      "352:\tlearn: 0.1105024\ttotal: 1m 18s\tremaining: 2m 23s\n",
      "353:\tlearn: 0.1104476\ttotal: 1m 18s\tremaining: 2m 23s\n",
      "354:\tlearn: 0.1103577\ttotal: 1m 18s\tremaining: 2m 23s\n",
      "355:\tlearn: 0.1102972\ttotal: 1m 19s\tremaining: 2m 23s\n",
      "356:\tlearn: 0.1102495\ttotal: 1m 19s\tremaining: 2m 22s\n",
      "357:\tlearn: 0.1101842\ttotal: 1m 19s\tremaining: 2m 22s\n",
      "358:\tlearn: 0.1101092\ttotal: 1m 19s\tremaining: 2m 22s\n",
      "359:\tlearn: 0.1100483\ttotal: 1m 20s\tremaining: 2m 22s\n",
      "360:\tlearn: 0.1099706\ttotal: 1m 20s\tremaining: 2m 21s\n",
      "361:\tlearn: 0.1098669\ttotal: 1m 20s\tremaining: 2m 21s\n",
      "362:\tlearn: 0.1097967\ttotal: 1m 20s\tremaining: 2m 21s\n",
      "363:\tlearn: 0.1097086\ttotal: 1m 20s\tremaining: 2m 21s\n",
      "364:\tlearn: 0.1096393\ttotal: 1m 21s\tremaining: 2m 20s\n",
      "365:\tlearn: 0.1095888\ttotal: 1m 21s\tremaining: 2m 20s\n",
      "366:\tlearn: 0.1095496\ttotal: 1m 21s\tremaining: 2m 20s\n",
      "367:\tlearn: 0.1094561\ttotal: 1m 21s\tremaining: 2m 20s\n",
      "368:\tlearn: 0.1093941\ttotal: 1m 21s\tremaining: 2m 19s\n",
      "369:\tlearn: 0.1093466\ttotal: 1m 22s\tremaining: 2m 19s\n",
      "370:\tlearn: 0.1092910\ttotal: 1m 22s\tremaining: 2m 19s\n",
      "371:\tlearn: 0.1092230\ttotal: 1m 22s\tremaining: 2m 19s\n",
      "372:\tlearn: 0.1091265\ttotal: 1m 22s\tremaining: 2m 19s\n",
      "373:\tlearn: 0.1090895\ttotal: 1m 22s\tremaining: 2m 18s\n",
      "374:\tlearn: 0.1090281\ttotal: 1m 23s\tremaining: 2m 18s\n",
      "375:\tlearn: 0.1089319\ttotal: 1m 23s\tremaining: 2m 18s\n",
      "376:\tlearn: 0.1088987\ttotal: 1m 23s\tremaining: 2m 18s\n",
      "377:\tlearn: 0.1088387\ttotal: 1m 23s\tremaining: 2m 18s\n",
      "378:\tlearn: 0.1087800\ttotal: 1m 24s\tremaining: 2m 17s\n",
      "379:\tlearn: 0.1087434\ttotal: 1m 24s\tremaining: 2m 17s\n",
      "380:\tlearn: 0.1086824\ttotal: 1m 24s\tremaining: 2m 17s\n",
      "381:\tlearn: 0.1086495\ttotal: 1m 24s\tremaining: 2m 17s\n",
      "382:\tlearn: 0.1086182\ttotal: 1m 25s\tremaining: 2m 16s\n",
      "383:\tlearn: 0.1085745\ttotal: 1m 25s\tremaining: 2m 16s\n",
      "384:\tlearn: 0.1084697\ttotal: 1m 25s\tremaining: 2m 16s\n",
      "385:\tlearn: 0.1083928\ttotal: 1m 25s\tremaining: 2m 16s\n",
      "386:\tlearn: 0.1083556\ttotal: 1m 25s\tremaining: 2m 15s\n",
      "387:\tlearn: 0.1083086\ttotal: 1m 26s\tremaining: 2m 15s\n",
      "388:\tlearn: 0.1082540\ttotal: 1m 26s\tremaining: 2m 15s\n",
      "389:\tlearn: 0.1081667\ttotal: 1m 26s\tremaining: 2m 15s\n",
      "390:\tlearn: 0.1081289\ttotal: 1m 26s\tremaining: 2m 14s\n",
      "391:\tlearn: 0.1080201\ttotal: 1m 26s\tremaining: 2m 14s\n",
      "392:\tlearn: 0.1079532\ttotal: 1m 27s\tremaining: 2m 14s\n",
      "393:\tlearn: 0.1078902\ttotal: 1m 27s\tremaining: 2m 14s\n",
      "394:\tlearn: 0.1078220\ttotal: 1m 27s\tremaining: 2m 13s\n",
      "395:\tlearn: 0.1077704\ttotal: 1m 27s\tremaining: 2m 13s\n",
      "396:\tlearn: 0.1077158\ttotal: 1m 27s\tremaining: 2m 13s\n",
      "397:\tlearn: 0.1076701\ttotal: 1m 28s\tremaining: 2m 13s\n",
      "398:\tlearn: 0.1076415\ttotal: 1m 28s\tremaining: 2m 13s\n",
      "399:\tlearn: 0.1075722\ttotal: 1m 28s\tremaining: 2m 12s\n",
      "400:\tlearn: 0.1075367\ttotal: 1m 28s\tremaining: 2m 12s\n",
      "401:\tlearn: 0.1074949\ttotal: 1m 28s\tremaining: 2m 12s\n",
      "402:\tlearn: 0.1074399\ttotal: 1m 29s\tremaining: 2m 12s\n",
      "403:\tlearn: 0.1073927\ttotal: 1m 29s\tremaining: 2m 11s\n",
      "404:\tlearn: 0.1073107\ttotal: 1m 29s\tremaining: 2m 11s\n",
      "405:\tlearn: 0.1072737\ttotal: 1m 29s\tremaining: 2m 11s\n",
      "406:\tlearn: 0.1072016\ttotal: 1m 29s\tremaining: 2m 11s\n",
      "407:\tlearn: 0.1071573\ttotal: 1m 30s\tremaining: 2m 10s\n",
      "408:\tlearn: 0.1071032\ttotal: 1m 30s\tremaining: 2m 10s\n",
      "409:\tlearn: 0.1070629\ttotal: 1m 30s\tremaining: 2m 10s\n",
      "410:\tlearn: 0.1070096\ttotal: 1m 30s\tremaining: 2m 10s\n",
      "411:\tlearn: 0.1069065\ttotal: 1m 30s\tremaining: 2m 9s\n",
      "412:\tlearn: 0.1068305\ttotal: 1m 31s\tremaining: 2m 9s\n",
      "413:\tlearn: 0.1067398\ttotal: 1m 31s\tremaining: 2m 9s\n",
      "414:\tlearn: 0.1066835\ttotal: 1m 31s\tremaining: 2m 9s\n",
      "415:\tlearn: 0.1066321\ttotal: 1m 31s\tremaining: 2m 8s\n",
      "416:\tlearn: 0.1065449\ttotal: 1m 32s\tremaining: 2m 8s\n",
      "417:\tlearn: 0.1064867\ttotal: 1m 32s\tremaining: 2m 8s\n",
      "418:\tlearn: 0.1064187\ttotal: 1m 32s\tremaining: 2m 8s\n",
      "419:\tlearn: 0.1063868\ttotal: 1m 32s\tremaining: 2m 7s\n",
      "420:\tlearn: 0.1063338\ttotal: 1m 32s\tremaining: 2m 7s\n",
      "421:\tlearn: 0.1062896\ttotal: 1m 33s\tremaining: 2m 7s\n",
      "422:\tlearn: 0.1062186\ttotal: 1m 33s\tremaining: 2m 7s\n",
      "423:\tlearn: 0.1061680\ttotal: 1m 33s\tremaining: 2m 6s\n",
      "424:\tlearn: 0.1061301\ttotal: 1m 33s\tremaining: 2m 6s\n",
      "425:\tlearn: 0.1060750\ttotal: 1m 33s\tremaining: 2m 6s\n",
      "426:\tlearn: 0.1060239\ttotal: 1m 34s\tremaining: 2m 6s\n",
      "427:\tlearn: 0.1059806\ttotal: 1m 34s\tremaining: 2m 6s\n",
      "428:\tlearn: 0.1059270\ttotal: 1m 34s\tremaining: 2m 5s\n",
      "429:\tlearn: 0.1058472\ttotal: 1m 34s\tremaining: 2m 5s\n",
      "430:\tlearn: 0.1057429\ttotal: 1m 34s\tremaining: 2m 5s\n",
      "431:\tlearn: 0.1056912\ttotal: 1m 35s\tremaining: 2m 5s\n",
      "432:\tlearn: 0.1056377\ttotal: 1m 35s\tremaining: 2m 4s\n",
      "433:\tlearn: 0.1055845\ttotal: 1m 35s\tremaining: 2m 4s\n",
      "434:\tlearn: 0.1055123\ttotal: 1m 35s\tremaining: 2m 4s\n",
      "435:\tlearn: 0.1054672\ttotal: 1m 36s\tremaining: 2m 4s\n",
      "436:\tlearn: 0.1054018\ttotal: 1m 36s\tremaining: 2m 4s\n",
      "437:\tlearn: 0.1052862\ttotal: 1m 36s\tremaining: 2m 3s\n",
      "438:\tlearn: 0.1052567\ttotal: 1m 36s\tremaining: 2m 3s\n",
      "439:\tlearn: 0.1051886\ttotal: 1m 36s\tremaining: 2m 3s\n",
      "440:\tlearn: 0.1051227\ttotal: 1m 37s\tremaining: 2m 3s\n",
      "441:\tlearn: 0.1050796\ttotal: 1m 37s\tremaining: 2m 3s\n",
      "442:\tlearn: 0.1050291\ttotal: 1m 37s\tremaining: 2m 2s\n",
      "443:\tlearn: 0.1049738\ttotal: 1m 37s\tremaining: 2m 2s\n",
      "444:\tlearn: 0.1049357\ttotal: 1m 38s\tremaining: 2m 2s\n",
      "445:\tlearn: 0.1048985\ttotal: 1m 38s\tremaining: 2m 2s\n",
      "446:\tlearn: 0.1048359\ttotal: 1m 38s\tremaining: 2m 1s\n",
      "447:\tlearn: 0.1047805\ttotal: 1m 38s\tremaining: 2m 1s\n",
      "448:\tlearn: 0.1047211\ttotal: 1m 38s\tremaining: 2m 1s\n",
      "449:\tlearn: 0.1046742\ttotal: 1m 39s\tremaining: 2m 1s\n",
      "450:\tlearn: 0.1046423\ttotal: 1m 39s\tremaining: 2m\n",
      "451:\tlearn: 0.1045621\ttotal: 1m 39s\tremaining: 2m\n",
      "452:\tlearn: 0.1044719\ttotal: 1m 39s\tremaining: 2m\n",
      "453:\tlearn: 0.1043749\ttotal: 1m 40s\tremaining: 2m\n",
      "454:\tlearn: 0.1043288\ttotal: 1m 40s\tremaining: 2m\n",
      "455:\tlearn: 0.1042854\ttotal: 1m 40s\tremaining: 1m 59s\n",
      "456:\tlearn: 0.1041718\ttotal: 1m 40s\tremaining: 1m 59s\n",
      "457:\tlearn: 0.1040948\ttotal: 1m 40s\tremaining: 1m 59s\n",
      "458:\tlearn: 0.1040489\ttotal: 1m 41s\tremaining: 1m 59s\n",
      "459:\tlearn: 0.1039894\ttotal: 1m 41s\tremaining: 1m 58s\n",
      "460:\tlearn: 0.1039739\ttotal: 1m 41s\tremaining: 1m 58s\n",
      "461:\tlearn: 0.1039405\ttotal: 1m 41s\tremaining: 1m 58s\n",
      "462:\tlearn: 0.1038767\ttotal: 1m 41s\tremaining: 1m 58s\n",
      "463:\tlearn: 0.1038085\ttotal: 1m 42s\tremaining: 1m 58s\n",
      "464:\tlearn: 0.1037622\ttotal: 1m 42s\tremaining: 1m 57s\n",
      "465:\tlearn: 0.1037120\ttotal: 1m 42s\tremaining: 1m 57s\n",
      "466:\tlearn: 0.1036539\ttotal: 1m 42s\tremaining: 1m 57s\n",
      "467:\tlearn: 0.1036175\ttotal: 1m 43s\tremaining: 1m 57s\n",
      "468:\tlearn: 0.1035825\ttotal: 1m 43s\tremaining: 1m 56s\n",
      "469:\tlearn: 0.1035189\ttotal: 1m 43s\tremaining: 1m 56s\n",
      "470:\tlearn: 0.1034425\ttotal: 1m 43s\tremaining: 1m 56s\n",
      "471:\tlearn: 0.1033964\ttotal: 1m 43s\tremaining: 1m 56s\n",
      "472:\tlearn: 0.1033251\ttotal: 1m 44s\tremaining: 1m 55s\n",
      "473:\tlearn: 0.1032887\ttotal: 1m 44s\tremaining: 1m 55s\n",
      "474:\tlearn: 0.1032589\ttotal: 1m 44s\tremaining: 1m 55s\n",
      "475:\tlearn: 0.1032294\ttotal: 1m 44s\tremaining: 1m 55s\n",
      "476:\tlearn: 0.1031774\ttotal: 1m 44s\tremaining: 1m 55s\n",
      "477:\tlearn: 0.1031341\ttotal: 1m 45s\tremaining: 1m 54s\n",
      "478:\tlearn: 0.1030696\ttotal: 1m 45s\tremaining: 1m 54s\n",
      "479:\tlearn: 0.1030449\ttotal: 1m 45s\tremaining: 1m 54s\n",
      "480:\tlearn: 0.1029362\ttotal: 1m 45s\tremaining: 1m 54s\n",
      "481:\tlearn: 0.1028846\ttotal: 1m 46s\tremaining: 1m 53s\n",
      "482:\tlearn: 0.1028151\ttotal: 1m 46s\tremaining: 1m 53s\n",
      "483:\tlearn: 0.1027623\ttotal: 1m 46s\tremaining: 1m 53s\n",
      "484:\tlearn: 0.1027005\ttotal: 1m 46s\tremaining: 1m 53s\n",
      "485:\tlearn: 0.1026214\ttotal: 1m 46s\tremaining: 1m 53s\n",
      "486:\tlearn: 0.1025398\ttotal: 1m 47s\tremaining: 1m 52s\n",
      "487:\tlearn: 0.1024778\ttotal: 1m 47s\tremaining: 1m 52s\n",
      "488:\tlearn: 0.1023926\ttotal: 1m 47s\tremaining: 1m 52s\n",
      "489:\tlearn: 0.1023540\ttotal: 1m 47s\tremaining: 1m 52s\n",
      "490:\tlearn: 0.1023019\ttotal: 1m 47s\tremaining: 1m 51s\n",
      "491:\tlearn: 0.1022900\ttotal: 1m 48s\tremaining: 1m 51s\n",
      "492:\tlearn: 0.1022455\ttotal: 1m 48s\tremaining: 1m 51s\n",
      "493:\tlearn: 0.1021764\ttotal: 1m 48s\tremaining: 1m 51s\n",
      "494:\tlearn: 0.1021147\ttotal: 1m 48s\tremaining: 1m 51s\n",
      "495:\tlearn: 0.1020688\ttotal: 1m 49s\tremaining: 1m 50s\n",
      "496:\tlearn: 0.1019749\ttotal: 1m 49s\tremaining: 1m 50s\n",
      "497:\tlearn: 0.1019287\ttotal: 1m 49s\tremaining: 1m 50s\n",
      "498:\tlearn: 0.1018849\ttotal: 1m 49s\tremaining: 1m 50s\n",
      "499:\tlearn: 0.1018536\ttotal: 1m 49s\tremaining: 1m 49s\n",
      "500:\tlearn: 0.1017914\ttotal: 1m 50s\tremaining: 1m 49s\n",
      "501:\tlearn: 0.1017423\ttotal: 1m 50s\tremaining: 1m 49s\n",
      "502:\tlearn: 0.1016775\ttotal: 1m 50s\tremaining: 1m 49s\n",
      "503:\tlearn: 0.1016303\ttotal: 1m 50s\tremaining: 1m 48s\n",
      "504:\tlearn: 0.1015800\ttotal: 1m 50s\tremaining: 1m 48s\n",
      "505:\tlearn: 0.1015437\ttotal: 1m 51s\tremaining: 1m 48s\n",
      "506:\tlearn: 0.1014758\ttotal: 1m 51s\tremaining: 1m 48s\n",
      "507:\tlearn: 0.1014117\ttotal: 1m 51s\tremaining: 1m 48s\n",
      "508:\tlearn: 0.1013429\ttotal: 1m 51s\tremaining: 1m 47s\n",
      "509:\tlearn: 0.1012989\ttotal: 1m 52s\tremaining: 1m 47s\n",
      "510:\tlearn: 0.1012368\ttotal: 1m 52s\tremaining: 1m 47s\n",
      "511:\tlearn: 0.1012078\ttotal: 1m 52s\tremaining: 1m 47s\n",
      "512:\tlearn: 0.1011579\ttotal: 1m 52s\tremaining: 1m 47s\n",
      "513:\tlearn: 0.1011204\ttotal: 1m 53s\tremaining: 1m 46s\n",
      "514:\tlearn: 0.1010868\ttotal: 1m 53s\tremaining: 1m 46s\n",
      "515:\tlearn: 0.1010314\ttotal: 1m 53s\tremaining: 1m 46s\n",
      "516:\tlearn: 0.1009990\ttotal: 1m 53s\tremaining: 1m 46s\n",
      "517:\tlearn: 0.1009596\ttotal: 1m 53s\tremaining: 1m 46s\n",
      "518:\tlearn: 0.1008981\ttotal: 1m 54s\tremaining: 1m 45s\n",
      "519:\tlearn: 0.1008521\ttotal: 1m 54s\tremaining: 1m 45s\n",
      "520:\tlearn: 0.1008171\ttotal: 1m 54s\tremaining: 1m 45s\n",
      "521:\tlearn: 0.1007661\ttotal: 1m 54s\tremaining: 1m 45s\n",
      "522:\tlearn: 0.1007054\ttotal: 1m 54s\tremaining: 1m 44s\n",
      "523:\tlearn: 0.1006349\ttotal: 1m 55s\tremaining: 1m 44s\n",
      "524:\tlearn: 0.1006140\ttotal: 1m 55s\tremaining: 1m 44s\n",
      "525:\tlearn: 0.1005503\ttotal: 1m 55s\tremaining: 1m 44s\n",
      "526:\tlearn: 0.1005195\ttotal: 1m 55s\tremaining: 1m 43s\n",
      "527:\tlearn: 0.1004262\ttotal: 1m 55s\tremaining: 1m 43s\n",
      "528:\tlearn: 0.1003929\ttotal: 1m 56s\tremaining: 1m 43s\n",
      "529:\tlearn: 0.1003348\ttotal: 1m 56s\tremaining: 1m 43s\n",
      "530:\tlearn: 0.1003025\ttotal: 1m 56s\tremaining: 1m 42s\n",
      "531:\tlearn: 0.1002468\ttotal: 1m 56s\tremaining: 1m 42s\n",
      "532:\tlearn: 0.1002046\ttotal: 1m 57s\tremaining: 1m 42s\n",
      "533:\tlearn: 0.1001418\ttotal: 1m 57s\tremaining: 1m 42s\n",
      "534:\tlearn: 0.1000844\ttotal: 1m 57s\tremaining: 1m 42s\n",
      "535:\tlearn: 0.1000391\ttotal: 1m 57s\tremaining: 1m 41s\n",
      "536:\tlearn: 0.1000123\ttotal: 1m 57s\tremaining: 1m 41s\n",
      "537:\tlearn: 0.0999504\ttotal: 1m 58s\tremaining: 1m 41s\n",
      "538:\tlearn: 0.0998788\ttotal: 1m 58s\tremaining: 1m 41s\n",
      "539:\tlearn: 0.0998563\ttotal: 1m 58s\tremaining: 1m 40s\n",
      "540:\tlearn: 0.0998053\ttotal: 1m 58s\tremaining: 1m 40s\n",
      "541:\tlearn: 0.0997537\ttotal: 1m 58s\tremaining: 1m 40s\n",
      "542:\tlearn: 0.0997132\ttotal: 1m 59s\tremaining: 1m 40s\n",
      "543:\tlearn: 0.0996574\ttotal: 1m 59s\tremaining: 1m 40s\n",
      "544:\tlearn: 0.0996246\ttotal: 1m 59s\tremaining: 1m 39s\n",
      "545:\tlearn: 0.0995954\ttotal: 1m 59s\tremaining: 1m 39s\n",
      "546:\tlearn: 0.0995399\ttotal: 2m\tremaining: 1m 39s\n",
      "547:\tlearn: 0.0995068\ttotal: 2m\tremaining: 1m 39s\n",
      "548:\tlearn: 0.0994277\ttotal: 2m\tremaining: 1m 39s\n",
      "549:\tlearn: 0.0993849\ttotal: 2m\tremaining: 1m 38s\n",
      "550:\tlearn: 0.0993279\ttotal: 2m 1s\tremaining: 1m 38s\n",
      "551:\tlearn: 0.0992743\ttotal: 2m 1s\tremaining: 1m 38s\n",
      "552:\tlearn: 0.0992141\ttotal: 2m 1s\tremaining: 1m 38s\n",
      "553:\tlearn: 0.0991762\ttotal: 2m 1s\tremaining: 1m 37s\n",
      "554:\tlearn: 0.0991094\ttotal: 2m 1s\tremaining: 1m 37s\n",
      "555:\tlearn: 0.0990492\ttotal: 2m 2s\tremaining: 1m 37s\n",
      "556:\tlearn: 0.0990046\ttotal: 2m 2s\tremaining: 1m 37s\n",
      "557:\tlearn: 0.0989280\ttotal: 2m 2s\tremaining: 1m 37s\n",
      "558:\tlearn: 0.0989000\ttotal: 2m 2s\tremaining: 1m 36s\n",
      "559:\tlearn: 0.0988953\ttotal: 2m 2s\tremaining: 1m 36s\n",
      "560:\tlearn: 0.0988565\ttotal: 2m 3s\tremaining: 1m 36s\n",
      "561:\tlearn: 0.0988315\ttotal: 2m 3s\tremaining: 1m 36s\n",
      "562:\tlearn: 0.0987933\ttotal: 2m 3s\tremaining: 1m 35s\n",
      "563:\tlearn: 0.0987562\ttotal: 2m 3s\tremaining: 1m 35s\n",
      "564:\tlearn: 0.0987175\ttotal: 2m 4s\tremaining: 1m 35s\n",
      "565:\tlearn: 0.0986766\ttotal: 2m 4s\tremaining: 1m 35s\n",
      "566:\tlearn: 0.0986303\ttotal: 2m 4s\tremaining: 1m 35s\n",
      "567:\tlearn: 0.0986147\ttotal: 2m 4s\tremaining: 1m 34s\n",
      "568:\tlearn: 0.0985611\ttotal: 2m 4s\tremaining: 1m 34s\n",
      "569:\tlearn: 0.0985068\ttotal: 2m 5s\tremaining: 1m 34s\n",
      "570:\tlearn: 0.0984330\ttotal: 2m 5s\tremaining: 1m 34s\n",
      "571:\tlearn: 0.0984242\ttotal: 2m 5s\tremaining: 1m 33s\n",
      "572:\tlearn: 0.0983795\ttotal: 2m 5s\tremaining: 1m 33s\n",
      "573:\tlearn: 0.0983191\ttotal: 2m 5s\tremaining: 1m 33s\n",
      "574:\tlearn: 0.0982516\ttotal: 2m 6s\tremaining: 1m 33s\n",
      "575:\tlearn: 0.0982220\ttotal: 2m 6s\tremaining: 1m 33s\n",
      "576:\tlearn: 0.0981799\ttotal: 2m 6s\tremaining: 1m 32s\n",
      "577:\tlearn: 0.0981359\ttotal: 2m 6s\tremaining: 1m 32s\n",
      "578:\tlearn: 0.0980818\ttotal: 2m 7s\tremaining: 1m 32s\n",
      "579:\tlearn: 0.0980575\ttotal: 2m 7s\tremaining: 1m 32s\n",
      "580:\tlearn: 0.0980448\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "581:\tlearn: 0.0979924\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "582:\tlearn: 0.0979277\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "583:\tlearn: 0.0978838\ttotal: 2m 8s\tremaining: 1m 31s\n",
      "584:\tlearn: 0.0978730\ttotal: 2m 8s\tremaining: 1m 31s\n",
      "585:\tlearn: 0.0978349\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "586:\tlearn: 0.0977914\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "587:\tlearn: 0.0977592\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "588:\tlearn: 0.0976884\ttotal: 2m 9s\tremaining: 1m 30s\n",
      "589:\tlearn: 0.0976624\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "590:\tlearn: 0.0975929\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "591:\tlearn: 0.0975693\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "592:\tlearn: 0.0975205\ttotal: 2m 10s\tremaining: 1m 29s\n",
      "593:\tlearn: 0.0974530\ttotal: 2m 10s\tremaining: 1m 29s\n",
      "594:\tlearn: 0.0974110\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "595:\tlearn: 0.0973676\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "596:\tlearn: 0.0973317\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "597:\tlearn: 0.0972952\ttotal: 2m 11s\tremaining: 1m 28s\n",
      "598:\tlearn: 0.0972590\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "599:\tlearn: 0.0972285\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "600:\tlearn: 0.0971894\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "601:\tlearn: 0.0971724\ttotal: 2m 12s\tremaining: 1m 27s\n",
      "602:\tlearn: 0.0971174\ttotal: 2m 12s\tremaining: 1m 27s\n",
      "603:\tlearn: 0.0970931\ttotal: 2m 12s\tremaining: 1m 26s\n",
      "604:\tlearn: 0.0970495\ttotal: 2m 12s\tremaining: 1m 26s\n",
      "605:\tlearn: 0.0970075\ttotal: 2m 12s\tremaining: 1m 26s\n",
      "606:\tlearn: 0.0968905\ttotal: 2m 13s\tremaining: 1m 26s\n",
      "607:\tlearn: 0.0968387\ttotal: 2m 13s\tremaining: 1m 25s\n",
      "608:\tlearn: 0.0968158\ttotal: 2m 13s\tremaining: 1m 25s\n",
      "609:\tlearn: 0.0967594\ttotal: 2m 13s\tremaining: 1m 25s\n",
      "610:\tlearn: 0.0967027\ttotal: 2m 13s\tremaining: 1m 25s\n",
      "611:\tlearn: 0.0966041\ttotal: 2m 14s\tremaining: 1m 25s\n",
      "612:\tlearn: 0.0965647\ttotal: 2m 14s\tremaining: 1m 24s\n",
      "613:\tlearn: 0.0965016\ttotal: 2m 14s\tremaining: 1m 24s\n",
      "614:\tlearn: 0.0964605\ttotal: 2m 14s\tremaining: 1m 24s\n",
      "615:\tlearn: 0.0964458\ttotal: 2m 15s\tremaining: 1m 24s\n",
      "616:\tlearn: 0.0964201\ttotal: 2m 15s\tremaining: 1m 24s\n",
      "617:\tlearn: 0.0963939\ttotal: 2m 15s\tremaining: 1m 23s\n",
      "618:\tlearn: 0.0963508\ttotal: 2m 15s\tremaining: 1m 23s\n",
      "619:\tlearn: 0.0962796\ttotal: 2m 16s\tremaining: 1m 23s\n",
      "620:\tlearn: 0.0962180\ttotal: 2m 16s\tremaining: 1m 23s\n",
      "621:\tlearn: 0.0961604\ttotal: 2m 16s\tremaining: 1m 22s\n",
      "622:\tlearn: 0.0961045\ttotal: 2m 16s\tremaining: 1m 22s\n",
      "623:\tlearn: 0.0960844\ttotal: 2m 16s\tremaining: 1m 22s\n",
      "624:\tlearn: 0.0960393\ttotal: 2m 17s\tremaining: 1m 22s\n",
      "625:\tlearn: 0.0960005\ttotal: 2m 17s\tremaining: 1m 22s\n",
      "626:\tlearn: 0.0959484\ttotal: 2m 17s\tremaining: 1m 21s\n",
      "627:\tlearn: 0.0959140\ttotal: 2m 17s\tremaining: 1m 21s\n",
      "628:\tlearn: 0.0957903\ttotal: 2m 17s\tremaining: 1m 21s\n",
      "629:\tlearn: 0.0957651\ttotal: 2m 18s\tremaining: 1m 21s\n",
      "630:\tlearn: 0.0957190\ttotal: 2m 18s\tremaining: 1m 20s\n",
      "631:\tlearn: 0.0956714\ttotal: 2m 18s\tremaining: 1m 20s\n",
      "632:\tlearn: 0.0956132\ttotal: 2m 18s\tremaining: 1m 20s\n",
      "633:\tlearn: 0.0955901\ttotal: 2m 19s\tremaining: 1m 20s\n",
      "634:\tlearn: 0.0955367\ttotal: 2m 19s\tremaining: 1m 20s\n",
      "635:\tlearn: 0.0955072\ttotal: 2m 19s\tremaining: 1m 19s\n",
      "636:\tlearn: 0.0954433\ttotal: 2m 19s\tremaining: 1m 19s\n",
      "637:\tlearn: 0.0954116\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "638:\tlearn: 0.0953613\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "639:\tlearn: 0.0953139\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "640:\tlearn: 0.0952949\ttotal: 2m 20s\tremaining: 1m 18s\n",
      "641:\tlearn: 0.0952395\ttotal: 2m 20s\tremaining: 1m 18s\n",
      "642:\tlearn: 0.0951744\ttotal: 2m 21s\tremaining: 1m 18s\n",
      "643:\tlearn: 0.0951135\ttotal: 2m 21s\tremaining: 1m 18s\n",
      "644:\tlearn: 0.0950593\ttotal: 2m 21s\tremaining: 1m 17s\n",
      "645:\tlearn: 0.0950293\ttotal: 2m 21s\tremaining: 1m 17s\n",
      "646:\tlearn: 0.0950078\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "647:\tlearn: 0.0949383\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "648:\tlearn: 0.0949255\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "649:\tlearn: 0.0948513\ttotal: 2m 22s\tremaining: 1m 16s\n",
      "650:\tlearn: 0.0948066\ttotal: 2m 22s\tremaining: 1m 16s\n",
      "651:\tlearn: 0.0947596\ttotal: 2m 23s\tremaining: 1m 16s\n",
      "652:\tlearn: 0.0946656\ttotal: 2m 23s\tremaining: 1m 16s\n",
      "653:\tlearn: 0.0946031\ttotal: 2m 23s\tremaining: 1m 15s\n",
      "654:\tlearn: 0.0945637\ttotal: 2m 23s\tremaining: 1m 15s\n",
      "655:\tlearn: 0.0945392\ttotal: 2m 23s\tremaining: 1m 15s\n",
      "656:\tlearn: 0.0944812\ttotal: 2m 24s\tremaining: 1m 15s\n",
      "657:\tlearn: 0.0944267\ttotal: 2m 24s\tremaining: 1m 15s\n",
      "658:\tlearn: 0.0943925\ttotal: 2m 24s\tremaining: 1m 14s\n",
      "659:\tlearn: 0.0943453\ttotal: 2m 24s\tremaining: 1m 14s\n",
      "660:\tlearn: 0.0942815\ttotal: 2m 24s\tremaining: 1m 14s\n",
      "661:\tlearn: 0.0942562\ttotal: 2m 25s\tremaining: 1m 14s\n",
      "662:\tlearn: 0.0942197\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "663:\tlearn: 0.0941446\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "664:\tlearn: 0.0941045\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "665:\tlearn: 0.0940724\ttotal: 2m 26s\tremaining: 1m 13s\n",
      "666:\tlearn: 0.0940367\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "667:\tlearn: 0.0940215\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "668:\tlearn: 0.0939855\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "669:\tlearn: 0.0939344\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "670:\tlearn: 0.0938990\ttotal: 2m 27s\tremaining: 1m 12s\n",
      "671:\tlearn: 0.0938552\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "672:\tlearn: 0.0938324\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "673:\tlearn: 0.0937458\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "674:\tlearn: 0.0936745\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "675:\tlearn: 0.0936227\ttotal: 2m 28s\tremaining: 1m 11s\n",
      "676:\tlearn: 0.0935879\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "677:\tlearn: 0.0935404\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "678:\tlearn: 0.0934867\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "679:\tlearn: 0.0934389\ttotal: 2m 29s\tremaining: 1m 10s\n",
      "680:\tlearn: 0.0934206\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "681:\tlearn: 0.0933802\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "682:\tlearn: 0.0932969\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "683:\tlearn: 0.0932459\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "684:\tlearn: 0.0931971\ttotal: 2m 30s\tremaining: 1m 9s\n",
      "685:\tlearn: 0.0931788\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "686:\tlearn: 0.0931649\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "687:\tlearn: 0.0930870\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "688:\tlearn: 0.0930457\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "689:\tlearn: 0.0929875\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "690:\tlearn: 0.0929407\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "691:\tlearn: 0.0928887\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "692:\tlearn: 0.0928474\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "693:\tlearn: 0.0928041\ttotal: 2m 32s\tremaining: 1m 7s\n",
      "694:\tlearn: 0.0927467\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "695:\tlearn: 0.0926975\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "696:\tlearn: 0.0926393\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "697:\tlearn: 0.0925640\ttotal: 2m 33s\tremaining: 1m 6s\n",
      "698:\tlearn: 0.0924920\ttotal: 2m 33s\tremaining: 1m 6s\n",
      "699:\tlearn: 0.0924408\ttotal: 2m 33s\tremaining: 1m 5s\n",
      "700:\tlearn: 0.0924117\ttotal: 2m 33s\tremaining: 1m 5s\n",
      "701:\tlearn: 0.0923821\ttotal: 2m 33s\tremaining: 1m 5s\n",
      "702:\tlearn: 0.0923350\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "703:\tlearn: 0.0923085\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "704:\tlearn: 0.0922786\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "705:\tlearn: 0.0922410\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "706:\tlearn: 0.0922214\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "707:\tlearn: 0.0921793\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "708:\tlearn: 0.0921343\ttotal: 2m 35s\tremaining: 1m 3s\n",
      "709:\tlearn: 0.0921110\ttotal: 2m 35s\tremaining: 1m 3s\n",
      "710:\tlearn: 0.0920577\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "711:\tlearn: 0.0919904\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "712:\tlearn: 0.0919598\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "713:\tlearn: 0.0918855\ttotal: 2m 36s\tremaining: 1m 2s\n",
      "714:\tlearn: 0.0918558\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "715:\tlearn: 0.0917977\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "716:\tlearn: 0.0917598\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "717:\tlearn: 0.0917020\ttotal: 2m 37s\tremaining: 1m 1s\n",
      "718:\tlearn: 0.0916274\ttotal: 2m 37s\tremaining: 1m 1s\n",
      "719:\tlearn: 0.0915941\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "720:\tlearn: 0.0915600\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "721:\tlearn: 0.0915261\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "722:\tlearn: 0.0914849\ttotal: 2m 38s\tremaining: 1m\n",
      "723:\tlearn: 0.0914305\ttotal: 2m 39s\tremaining: 1m\n",
      "724:\tlearn: 0.0913829\ttotal: 2m 39s\tremaining: 1m\n",
      "725:\tlearn: 0.0913508\ttotal: 2m 39s\tremaining: 1m\n",
      "726:\tlearn: 0.0912912\ttotal: 2m 39s\tremaining: 1m\n",
      "727:\tlearn: 0.0912330\ttotal: 2m 40s\tremaining: 59.9s\n",
      "728:\tlearn: 0.0911378\ttotal: 2m 40s\tremaining: 59.6s\n",
      "729:\tlearn: 0.0911085\ttotal: 2m 40s\tremaining: 59.4s\n",
      "730:\tlearn: 0.0910831\ttotal: 2m 40s\tremaining: 59.2s\n",
      "731:\tlearn: 0.0910615\ttotal: 2m 41s\tremaining: 59s\n",
      "732:\tlearn: 0.0909916\ttotal: 2m 41s\tremaining: 58.7s\n",
      "733:\tlearn: 0.0909512\ttotal: 2m 41s\tremaining: 58.5s\n",
      "734:\tlearn: 0.0909029\ttotal: 2m 41s\tremaining: 58.3s\n",
      "735:\tlearn: 0.0908420\ttotal: 2m 41s\tremaining: 58.1s\n",
      "736:\tlearn: 0.0908111\ttotal: 2m 42s\tremaining: 57.8s\n",
      "737:\tlearn: 0.0907322\ttotal: 2m 42s\tremaining: 57.6s\n",
      "738:\tlearn: 0.0907135\ttotal: 2m 42s\tremaining: 57.4s\n",
      "739:\tlearn: 0.0906888\ttotal: 2m 42s\tremaining: 57.2s\n",
      "740:\tlearn: 0.0906630\ttotal: 2m 42s\tremaining: 56.9s\n",
      "741:\tlearn: 0.0906245\ttotal: 2m 43s\tremaining: 56.7s\n",
      "742:\tlearn: 0.0906002\ttotal: 2m 43s\tremaining: 56.5s\n",
      "743:\tlearn: 0.0905616\ttotal: 2m 43s\tremaining: 56.3s\n",
      "744:\tlearn: 0.0905102\ttotal: 2m 43s\tremaining: 56.1s\n",
      "745:\tlearn: 0.0904411\ttotal: 2m 44s\tremaining: 55.9s\n",
      "746:\tlearn: 0.0904074\ttotal: 2m 44s\tremaining: 55.6s\n",
      "747:\tlearn: 0.0903208\ttotal: 2m 44s\tremaining: 55.4s\n",
      "748:\tlearn: 0.0902880\ttotal: 2m 44s\tremaining: 55.2s\n",
      "749:\tlearn: 0.0902180\ttotal: 2m 44s\tremaining: 55s\n",
      "750:\tlearn: 0.0901869\ttotal: 2m 45s\tremaining: 54.8s\n",
      "751:\tlearn: 0.0901519\ttotal: 2m 45s\tremaining: 54.5s\n",
      "752:\tlearn: 0.0901099\ttotal: 2m 45s\tremaining: 54.3s\n",
      "753:\tlearn: 0.0900846\ttotal: 2m 45s\tremaining: 54.1s\n",
      "754:\tlearn: 0.0900437\ttotal: 2m 45s\tremaining: 53.9s\n",
      "755:\tlearn: 0.0900000\ttotal: 2m 46s\tremaining: 53.6s\n",
      "756:\tlearn: 0.0899780\ttotal: 2m 46s\tremaining: 53.4s\n",
      "757:\tlearn: 0.0899295\ttotal: 2m 46s\tremaining: 53.2s\n",
      "758:\tlearn: 0.0898500\ttotal: 2m 46s\tremaining: 53s\n",
      "759:\tlearn: 0.0898173\ttotal: 2m 47s\tremaining: 52.8s\n",
      "760:\tlearn: 0.0897779\ttotal: 2m 47s\tremaining: 52.6s\n",
      "761:\tlearn: 0.0897502\ttotal: 2m 47s\tremaining: 52.3s\n",
      "762:\tlearn: 0.0896862\ttotal: 2m 47s\tremaining: 52.1s\n",
      "763:\tlearn: 0.0895989\ttotal: 2m 48s\tremaining: 51.9s\n",
      "764:\tlearn: 0.0895644\ttotal: 2m 48s\tremaining: 51.7s\n",
      "765:\tlearn: 0.0895113\ttotal: 2m 48s\tremaining: 51.5s\n",
      "766:\tlearn: 0.0894439\ttotal: 2m 48s\tremaining: 51.3s\n",
      "767:\tlearn: 0.0893826\ttotal: 2m 48s\tremaining: 51s\n",
      "768:\tlearn: 0.0893171\ttotal: 2m 49s\tremaining: 50.8s\n",
      "769:\tlearn: 0.0892772\ttotal: 2m 49s\tremaining: 50.6s\n",
      "770:\tlearn: 0.0892521\ttotal: 2m 49s\tremaining: 50.4s\n",
      "771:\tlearn: 0.0892132\ttotal: 2m 49s\tremaining: 50.2s\n",
      "772:\tlearn: 0.0891587\ttotal: 2m 50s\tremaining: 50s\n",
      "773:\tlearn: 0.0890828\ttotal: 2m 50s\tremaining: 49.8s\n",
      "774:\tlearn: 0.0890163\ttotal: 2m 50s\tremaining: 49.6s\n",
      "775:\tlearn: 0.0889595\ttotal: 2m 51s\tremaining: 49.4s\n",
      "776:\tlearn: 0.0889171\ttotal: 2m 51s\tremaining: 49.2s\n",
      "777:\tlearn: 0.0888949\ttotal: 2m 51s\tremaining: 49s\n",
      "778:\tlearn: 0.0888674\ttotal: 2m 51s\tremaining: 48.8s\n",
      "779:\tlearn: 0.0888474\ttotal: 2m 52s\tremaining: 48.6s\n",
      "780:\tlearn: 0.0888075\ttotal: 2m 52s\tremaining: 48.3s\n",
      "781:\tlearn: 0.0887981\ttotal: 2m 52s\tremaining: 48.1s\n",
      "782:\tlearn: 0.0887482\ttotal: 2m 52s\tremaining: 47.9s\n",
      "783:\tlearn: 0.0887116\ttotal: 2m 53s\tremaining: 47.7s\n",
      "784:\tlearn: 0.0886591\ttotal: 2m 53s\tremaining: 47.5s\n",
      "785:\tlearn: 0.0886409\ttotal: 2m 53s\tremaining: 47.3s\n",
      "786:\tlearn: 0.0886378\ttotal: 2m 53s\tremaining: 47s\n",
      "787:\tlearn: 0.0885755\ttotal: 2m 54s\tremaining: 46.8s\n",
      "788:\tlearn: 0.0885366\ttotal: 2m 54s\tremaining: 46.6s\n",
      "789:\tlearn: 0.0885007\ttotal: 2m 54s\tremaining: 46.4s\n",
      "790:\tlearn: 0.0884419\ttotal: 2m 54s\tremaining: 46.2s\n",
      "791:\tlearn: 0.0883980\ttotal: 2m 54s\tremaining: 45.9s\n",
      "792:\tlearn: 0.0883476\ttotal: 2m 55s\tremaining: 45.7s\n",
      "793:\tlearn: 0.0882993\ttotal: 2m 55s\tremaining: 45.5s\n",
      "794:\tlearn: 0.0882529\ttotal: 2m 55s\tremaining: 45.3s\n",
      "795:\tlearn: 0.0881966\ttotal: 2m 55s\tremaining: 45s\n",
      "796:\tlearn: 0.0881631\ttotal: 2m 55s\tremaining: 44.8s\n",
      "797:\tlearn: 0.0881390\ttotal: 2m 56s\tremaining: 44.6s\n",
      "798:\tlearn: 0.0881050\ttotal: 2m 56s\tremaining: 44.4s\n",
      "799:\tlearn: 0.0880786\ttotal: 2m 56s\tremaining: 44.1s\n",
      "800:\tlearn: 0.0880459\ttotal: 2m 56s\tremaining: 43.9s\n",
      "801:\tlearn: 0.0880128\ttotal: 2m 56s\tremaining: 43.7s\n",
      "802:\tlearn: 0.0879626\ttotal: 2m 57s\tremaining: 43.5s\n",
      "803:\tlearn: 0.0879082\ttotal: 2m 57s\tremaining: 43.2s\n",
      "804:\tlearn: 0.0878659\ttotal: 2m 57s\tremaining: 43s\n",
      "805:\tlearn: 0.0878203\ttotal: 2m 57s\tremaining: 42.8s\n",
      "806:\tlearn: 0.0877568\ttotal: 2m 57s\tremaining: 42.6s\n",
      "807:\tlearn: 0.0877319\ttotal: 2m 58s\tremaining: 42.4s\n",
      "808:\tlearn: 0.0876753\ttotal: 2m 58s\tremaining: 42.1s\n",
      "809:\tlearn: 0.0876529\ttotal: 2m 58s\tremaining: 41.9s\n",
      "810:\tlearn: 0.0876181\ttotal: 2m 58s\tremaining: 41.7s\n",
      "811:\tlearn: 0.0875930\ttotal: 2m 59s\tremaining: 41.5s\n",
      "812:\tlearn: 0.0875346\ttotal: 2m 59s\tremaining: 41.3s\n",
      "813:\tlearn: 0.0874960\ttotal: 2m 59s\tremaining: 41s\n",
      "814:\tlearn: 0.0874611\ttotal: 2m 59s\tremaining: 40.8s\n",
      "815:\tlearn: 0.0874325\ttotal: 3m\tremaining: 40.6s\n",
      "816:\tlearn: 0.0874027\ttotal: 3m\tremaining: 40.4s\n",
      "817:\tlearn: 0.0873291\ttotal: 3m\tremaining: 40.2s\n",
      "818:\tlearn: 0.0872924\ttotal: 3m\tremaining: 39.9s\n",
      "819:\tlearn: 0.0872704\ttotal: 3m\tremaining: 39.7s\n",
      "820:\tlearn: 0.0872562\ttotal: 3m 1s\tremaining: 39.5s\n",
      "821:\tlearn: 0.0872010\ttotal: 3m 1s\tremaining: 39.3s\n",
      "822:\tlearn: 0.0871471\ttotal: 3m 1s\tremaining: 39s\n",
      "823:\tlearn: 0.0871287\ttotal: 3m 1s\tremaining: 38.8s\n",
      "824:\tlearn: 0.0870971\ttotal: 3m 2s\tremaining: 38.6s\n",
      "825:\tlearn: 0.0870420\ttotal: 3m 2s\tremaining: 38.4s\n",
      "826:\tlearn: 0.0870307\ttotal: 3m 2s\tremaining: 38.2s\n",
      "827:\tlearn: 0.0869644\ttotal: 3m 2s\tremaining: 38s\n",
      "828:\tlearn: 0.0869322\ttotal: 3m 3s\tremaining: 37.8s\n",
      "829:\tlearn: 0.0868976\ttotal: 3m 3s\tremaining: 37.5s\n",
      "830:\tlearn: 0.0868590\ttotal: 3m 3s\tremaining: 37.3s\n",
      "831:\tlearn: 0.0868366\ttotal: 3m 3s\tremaining: 37.1s\n",
      "832:\tlearn: 0.0868131\ttotal: 3m 3s\tremaining: 36.9s\n",
      "833:\tlearn: 0.0867653\ttotal: 3m 4s\tremaining: 36.6s\n",
      "834:\tlearn: 0.0867432\ttotal: 3m 4s\tremaining: 36.4s\n",
      "835:\tlearn: 0.0867070\ttotal: 3m 4s\tremaining: 36.2s\n",
      "836:\tlearn: 0.0866844\ttotal: 3m 4s\tremaining: 36s\n",
      "837:\tlearn: 0.0866775\ttotal: 3m 4s\tremaining: 35.7s\n",
      "838:\tlearn: 0.0866725\ttotal: 3m 5s\tremaining: 35.5s\n",
      "839:\tlearn: 0.0866161\ttotal: 3m 5s\tremaining: 35.3s\n",
      "840:\tlearn: 0.0865859\ttotal: 3m 5s\tremaining: 35.1s\n",
      "841:\tlearn: 0.0865510\ttotal: 3m 5s\tremaining: 34.8s\n",
      "842:\tlearn: 0.0865035\ttotal: 3m 5s\tremaining: 34.6s\n",
      "843:\tlearn: 0.0864608\ttotal: 3m 6s\tremaining: 34.4s\n",
      "844:\tlearn: 0.0864461\ttotal: 3m 6s\tremaining: 34.2s\n",
      "845:\tlearn: 0.0863920\ttotal: 3m 6s\tremaining: 34s\n",
      "846:\tlearn: 0.0863384\ttotal: 3m 6s\tremaining: 33.8s\n",
      "847:\tlearn: 0.0863126\ttotal: 3m 7s\tremaining: 33.5s\n",
      "848:\tlearn: 0.0862925\ttotal: 3m 7s\tremaining: 33.3s\n",
      "849:\tlearn: 0.0862523\ttotal: 3m 7s\tremaining: 33.1s\n",
      "850:\tlearn: 0.0861975\ttotal: 3m 7s\tremaining: 32.9s\n",
      "851:\tlearn: 0.0861311\ttotal: 3m 8s\tremaining: 32.7s\n",
      "852:\tlearn: 0.0861094\ttotal: 3m 8s\tremaining: 32.5s\n",
      "853:\tlearn: 0.0860775\ttotal: 3m 8s\tremaining: 32.2s\n",
      "854:\tlearn: 0.0860392\ttotal: 3m 8s\tremaining: 32s\n",
      "855:\tlearn: 0.0859835\ttotal: 3m 9s\tremaining: 31.8s\n",
      "856:\tlearn: 0.0859414\ttotal: 3m 9s\tremaining: 31.6s\n",
      "857:\tlearn: 0.0859261\ttotal: 3m 9s\tremaining: 31.4s\n",
      "858:\tlearn: 0.0858892\ttotal: 3m 9s\tremaining: 31.1s\n",
      "859:\tlearn: 0.0858548\ttotal: 3m 9s\tremaining: 30.9s\n",
      "860:\tlearn: 0.0858428\ttotal: 3m 10s\tremaining: 30.7s\n",
      "861:\tlearn: 0.0858223\ttotal: 3m 10s\tremaining: 30.5s\n",
      "862:\tlearn: 0.0857671\ttotal: 3m 10s\tremaining: 30.3s\n",
      "863:\tlearn: 0.0857205\ttotal: 3m 10s\tremaining: 30s\n",
      "864:\tlearn: 0.0856942\ttotal: 3m 11s\tremaining: 29.8s\n",
      "865:\tlearn: 0.0856749\ttotal: 3m 11s\tremaining: 29.6s\n",
      "866:\tlearn: 0.0856401\ttotal: 3m 11s\tremaining: 29.4s\n",
      "867:\tlearn: 0.0855827\ttotal: 3m 11s\tremaining: 29.1s\n",
      "868:\tlearn: 0.0855535\ttotal: 3m 11s\tremaining: 28.9s\n",
      "869:\tlearn: 0.0855203\ttotal: 3m 12s\tremaining: 28.7s\n",
      "870:\tlearn: 0.0855055\ttotal: 3m 12s\tremaining: 28.5s\n",
      "871:\tlearn: 0.0854846\ttotal: 3m 12s\tremaining: 28.3s\n",
      "872:\tlearn: 0.0854291\ttotal: 3m 12s\tremaining: 28.1s\n",
      "873:\tlearn: 0.0854042\ttotal: 3m 13s\tremaining: 27.8s\n",
      "874:\tlearn: 0.0853747\ttotal: 3m 13s\tremaining: 27.6s\n",
      "875:\tlearn: 0.0853427\ttotal: 3m 13s\tremaining: 27.4s\n",
      "876:\tlearn: 0.0853042\ttotal: 3m 13s\tremaining: 27.2s\n",
      "877:\tlearn: 0.0852812\ttotal: 3m 13s\tremaining: 26.9s\n",
      "878:\tlearn: 0.0852528\ttotal: 3m 14s\tremaining: 26.7s\n",
      "879:\tlearn: 0.0852093\ttotal: 3m 14s\tremaining: 26.5s\n",
      "880:\tlearn: 0.0851546\ttotal: 3m 14s\tremaining: 26.3s\n",
      "881:\tlearn: 0.0850998\ttotal: 3m 14s\tremaining: 26.1s\n",
      "882:\tlearn: 0.0850127\ttotal: 3m 15s\tremaining: 25.8s\n",
      "883:\tlearn: 0.0849701\ttotal: 3m 15s\tremaining: 25.6s\n",
      "884:\tlearn: 0.0849439\ttotal: 3m 15s\tremaining: 25.4s\n",
      "885:\tlearn: 0.0849162\ttotal: 3m 15s\tremaining: 25.2s\n",
      "886:\tlearn: 0.0848864\ttotal: 3m 15s\tremaining: 25s\n",
      "887:\tlearn: 0.0848443\ttotal: 3m 16s\tremaining: 24.7s\n",
      "888:\tlearn: 0.0848142\ttotal: 3m 16s\tremaining: 24.5s\n",
      "889:\tlearn: 0.0847868\ttotal: 3m 16s\tremaining: 24.3s\n",
      "890:\tlearn: 0.0847532\ttotal: 3m 16s\tremaining: 24.1s\n",
      "891:\tlearn: 0.0847269\ttotal: 3m 17s\tremaining: 23.9s\n",
      "892:\tlearn: 0.0846837\ttotal: 3m 17s\tremaining: 23.6s\n",
      "893:\tlearn: 0.0846595\ttotal: 3m 17s\tremaining: 23.4s\n",
      "894:\tlearn: 0.0846275\ttotal: 3m 17s\tremaining: 23.2s\n",
      "895:\tlearn: 0.0845881\ttotal: 3m 17s\tremaining: 23s\n",
      "896:\tlearn: 0.0845594\ttotal: 3m 18s\tremaining: 22.7s\n",
      "897:\tlearn: 0.0845054\ttotal: 3m 18s\tremaining: 22.5s\n",
      "898:\tlearn: 0.0844831\ttotal: 3m 18s\tremaining: 22.3s\n",
      "899:\tlearn: 0.0844798\ttotal: 3m 18s\tremaining: 22.1s\n",
      "900:\tlearn: 0.0844410\ttotal: 3m 18s\tremaining: 21.9s\n",
      "901:\tlearn: 0.0844111\ttotal: 3m 19s\tremaining: 21.6s\n",
      "902:\tlearn: 0.0843339\ttotal: 3m 19s\tremaining: 21.4s\n",
      "903:\tlearn: 0.0843011\ttotal: 3m 19s\tremaining: 21.2s\n",
      "904:\tlearn: 0.0842677\ttotal: 3m 19s\tremaining: 21s\n",
      "905:\tlearn: 0.0842463\ttotal: 3m 20s\tremaining: 20.8s\n",
      "906:\tlearn: 0.0842225\ttotal: 3m 20s\tremaining: 20.5s\n",
      "907:\tlearn: 0.0841969\ttotal: 3m 20s\tremaining: 20.3s\n",
      "908:\tlearn: 0.0841806\ttotal: 3m 20s\tremaining: 20.1s\n",
      "909:\tlearn: 0.0841147\ttotal: 3m 21s\tremaining: 19.9s\n",
      "910:\tlearn: 0.0840492\ttotal: 3m 21s\tremaining: 19.7s\n",
      "911:\tlearn: 0.0839854\ttotal: 3m 21s\tremaining: 19.4s\n",
      "912:\tlearn: 0.0839552\ttotal: 3m 21s\tremaining: 19.2s\n",
      "913:\tlearn: 0.0839365\ttotal: 3m 21s\tremaining: 19s\n",
      "914:\tlearn: 0.0838931\ttotal: 3m 22s\tremaining: 18.8s\n",
      "915:\tlearn: 0.0838562\ttotal: 3m 22s\tremaining: 18.6s\n",
      "916:\tlearn: 0.0838025\ttotal: 3m 22s\tremaining: 18.3s\n",
      "917:\tlearn: 0.0837723\ttotal: 3m 22s\tremaining: 18.1s\n",
      "918:\tlearn: 0.0837465\ttotal: 3m 23s\tremaining: 17.9s\n",
      "919:\tlearn: 0.0837278\ttotal: 3m 23s\tremaining: 17.7s\n",
      "920:\tlearn: 0.0836881\ttotal: 3m 23s\tremaining: 17.5s\n",
      "921:\tlearn: 0.0836682\ttotal: 3m 23s\tremaining: 17.2s\n",
      "922:\tlearn: 0.0836067\ttotal: 3m 24s\tremaining: 17s\n",
      "923:\tlearn: 0.0835603\ttotal: 3m 24s\tremaining: 16.8s\n",
      "924:\tlearn: 0.0835008\ttotal: 3m 24s\tremaining: 16.6s\n",
      "925:\tlearn: 0.0834723\ttotal: 3m 24s\tremaining: 16.4s\n",
      "926:\tlearn: 0.0834191\ttotal: 3m 24s\tremaining: 16.1s\n",
      "927:\tlearn: 0.0834103\ttotal: 3m 25s\tremaining: 15.9s\n",
      "928:\tlearn: 0.0833856\ttotal: 3m 25s\tremaining: 15.7s\n",
      "929:\tlearn: 0.0833668\ttotal: 3m 25s\tremaining: 15.5s\n",
      "930:\tlearn: 0.0833417\ttotal: 3m 25s\tremaining: 15.3s\n",
      "931:\tlearn: 0.0832930\ttotal: 3m 26s\tremaining: 15s\n",
      "932:\tlearn: 0.0832515\ttotal: 3m 26s\tremaining: 14.8s\n",
      "933:\tlearn: 0.0832390\ttotal: 3m 26s\tremaining: 14.6s\n",
      "934:\tlearn: 0.0832205\ttotal: 3m 26s\tremaining: 14.4s\n",
      "935:\tlearn: 0.0831516\ttotal: 3m 27s\tremaining: 14.2s\n",
      "936:\tlearn: 0.0830738\ttotal: 3m 27s\tremaining: 13.9s\n",
      "937:\tlearn: 0.0830320\ttotal: 3m 27s\tremaining: 13.7s\n",
      "938:\tlearn: 0.0829835\ttotal: 3m 27s\tremaining: 13.5s\n",
      "939:\tlearn: 0.0829682\ttotal: 3m 28s\tremaining: 13.3s\n",
      "940:\tlearn: 0.0829291\ttotal: 3m 28s\tremaining: 13.1s\n",
      "941:\tlearn: 0.0828771\ttotal: 3m 28s\tremaining: 12.8s\n",
      "942:\tlearn: 0.0828277\ttotal: 3m 28s\tremaining: 12.6s\n",
      "943:\tlearn: 0.0828008\ttotal: 3m 28s\tremaining: 12.4s\n",
      "944:\tlearn: 0.0827823\ttotal: 3m 28s\tremaining: 12.2s\n",
      "945:\tlearn: 0.0827299\ttotal: 3m 29s\tremaining: 11.9s\n",
      "946:\tlearn: 0.0826716\ttotal: 3m 29s\tremaining: 11.7s\n",
      "947:\tlearn: 0.0826537\ttotal: 3m 29s\tremaining: 11.5s\n",
      "948:\tlearn: 0.0826121\ttotal: 3m 29s\tremaining: 11.3s\n",
      "949:\tlearn: 0.0825572\ttotal: 3m 30s\tremaining: 11.1s\n",
      "950:\tlearn: 0.0825175\ttotal: 3m 30s\tremaining: 10.8s\n",
      "951:\tlearn: 0.0825094\ttotal: 3m 30s\tremaining: 10.6s\n",
      "952:\tlearn: 0.0824405\ttotal: 3m 30s\tremaining: 10.4s\n",
      "953:\tlearn: 0.0823989\ttotal: 3m 31s\tremaining: 10.2s\n",
      "954:\tlearn: 0.0823621\ttotal: 3m 31s\tremaining: 9.96s\n",
      "955:\tlearn: 0.0823152\ttotal: 3m 31s\tremaining: 9.73s\n",
      "956:\tlearn: 0.0822900\ttotal: 3m 31s\tremaining: 9.51s\n",
      "957:\tlearn: 0.0822459\ttotal: 3m 31s\tremaining: 9.29s\n",
      "958:\tlearn: 0.0822018\ttotal: 3m 32s\tremaining: 9.07s\n",
      "959:\tlearn: 0.0821660\ttotal: 3m 32s\tremaining: 8.85s\n",
      "960:\tlearn: 0.0821277\ttotal: 3m 32s\tremaining: 8.63s\n",
      "961:\tlearn: 0.0820876\ttotal: 3m 32s\tremaining: 8.41s\n",
      "962:\tlearn: 0.0820606\ttotal: 3m 33s\tremaining: 8.19s\n",
      "963:\tlearn: 0.0820286\ttotal: 3m 33s\tremaining: 7.96s\n",
      "964:\tlearn: 0.0819883\ttotal: 3m 33s\tremaining: 7.74s\n",
      "965:\tlearn: 0.0819762\ttotal: 3m 33s\tremaining: 7.52s\n",
      "966:\tlearn: 0.0819395\ttotal: 3m 33s\tremaining: 7.3s\n",
      "967:\tlearn: 0.0819176\ttotal: 3m 34s\tremaining: 7.08s\n",
      "968:\tlearn: 0.0818374\ttotal: 3m 34s\tremaining: 6.86s\n",
      "969:\tlearn: 0.0817800\ttotal: 3m 34s\tremaining: 6.63s\n",
      "970:\tlearn: 0.0817252\ttotal: 3m 34s\tremaining: 6.41s\n",
      "971:\tlearn: 0.0816987\ttotal: 3m 34s\tremaining: 6.19s\n",
      "972:\tlearn: 0.0816660\ttotal: 3m 35s\tremaining: 5.97s\n",
      "973:\tlearn: 0.0816310\ttotal: 3m 35s\tremaining: 5.75s\n",
      "974:\tlearn: 0.0816118\ttotal: 3m 35s\tremaining: 5.53s\n",
      "975:\tlearn: 0.0815673\ttotal: 3m 35s\tremaining: 5.3s\n",
      "976:\tlearn: 0.0815196\ttotal: 3m 35s\tremaining: 5.08s\n",
      "977:\tlearn: 0.0815076\ttotal: 3m 36s\tremaining: 4.86s\n",
      "978:\tlearn: 0.0814812\ttotal: 3m 36s\tremaining: 4.64s\n",
      "979:\tlearn: 0.0814609\ttotal: 3m 36s\tremaining: 4.42s\n",
      "980:\tlearn: 0.0814199\ttotal: 3m 36s\tremaining: 4.2s\n",
      "981:\tlearn: 0.0813941\ttotal: 3m 37s\tremaining: 3.98s\n",
      "982:\tlearn: 0.0813611\ttotal: 3m 37s\tremaining: 3.76s\n",
      "983:\tlearn: 0.0813564\ttotal: 3m 37s\tremaining: 3.54s\n",
      "984:\tlearn: 0.0813133\ttotal: 3m 37s\tremaining: 3.31s\n",
      "985:\tlearn: 0.0813029\ttotal: 3m 37s\tremaining: 3.09s\n",
      "986:\tlearn: 0.0812701\ttotal: 3m 38s\tremaining: 2.87s\n",
      "987:\tlearn: 0.0812621\ttotal: 3m 38s\tremaining: 2.65s\n",
      "988:\tlearn: 0.0812197\ttotal: 3m 38s\tremaining: 2.43s\n",
      "989:\tlearn: 0.0812066\ttotal: 3m 38s\tremaining: 2.21s\n",
      "990:\tlearn: 0.0811583\ttotal: 3m 38s\tremaining: 1.99s\n",
      "991:\tlearn: 0.0811099\ttotal: 3m 39s\tremaining: 1.77s\n",
      "992:\tlearn: 0.0810867\ttotal: 3m 39s\tremaining: 1.55s\n",
      "993:\tlearn: 0.0810643\ttotal: 3m 39s\tremaining: 1.32s\n",
      "994:\tlearn: 0.0810392\ttotal: 3m 39s\tremaining: 1.1s\n",
      "995:\tlearn: 0.0810146\ttotal: 3m 40s\tremaining: 884ms\n",
      "996:\tlearn: 0.0809997\ttotal: 3m 40s\tremaining: 663ms\n",
      "997:\tlearn: 0.0809420\ttotal: 3m 40s\tremaining: 442ms\n",
      "998:\tlearn: 0.0809115\ttotal: 3m 40s\tremaining: 221ms\n",
      "999:\tlearn: 0.0808875\ttotal: 3m 40s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5211385\ttotal: 217ms\tremaining: 3m 37s\n",
      "1:\tlearn: 0.4199298\ttotal: 444ms\tremaining: 3m 41s\n",
      "2:\tlearn: 0.3597274\ttotal: 681ms\tremaining: 3m 46s\n",
      "3:\tlearn: 0.3244429\ttotal: 930ms\tremaining: 3m 51s\n",
      "4:\tlearn: 0.2954856\ttotal: 1.17s\tremaining: 3m 52s\n",
      "5:\tlearn: 0.2754128\ttotal: 1.39s\tremaining: 3m 49s\n",
      "6:\tlearn: 0.2560135\ttotal: 1.63s\tremaining: 3m 50s\n",
      "7:\tlearn: 0.2465401\ttotal: 1.91s\tremaining: 3m 57s\n",
      "8:\tlearn: 0.2386340\ttotal: 2.13s\tremaining: 3m 54s\n",
      "9:\tlearn: 0.2296214\ttotal: 2.34s\tremaining: 3m 52s\n",
      "10:\tlearn: 0.2209595\ttotal: 2.54s\tremaining: 3m 48s\n",
      "11:\tlearn: 0.2165613\ttotal: 2.73s\tremaining: 3m 45s\n",
      "12:\tlearn: 0.2121491\ttotal: 2.94s\tremaining: 3m 43s\n",
      "13:\tlearn: 0.2086831\ttotal: 3.15s\tremaining: 3m 41s\n",
      "14:\tlearn: 0.2032733\ttotal: 3.37s\tremaining: 3m 41s\n",
      "15:\tlearn: 0.2005342\ttotal: 3.6s\tremaining: 3m 41s\n",
      "16:\tlearn: 0.1974480\ttotal: 3.81s\tremaining: 3m 40s\n",
      "17:\tlearn: 0.1939229\ttotal: 4s\tremaining: 3m 38s\n",
      "18:\tlearn: 0.1915960\ttotal: 4.25s\tremaining: 3m 39s\n",
      "19:\tlearn: 0.1900127\ttotal: 4.42s\tremaining: 3m 36s\n",
      "20:\tlearn: 0.1874462\ttotal: 4.61s\tremaining: 3m 34s\n",
      "21:\tlearn: 0.1860939\ttotal: 4.82s\tremaining: 3m 34s\n",
      "22:\tlearn: 0.1840861\ttotal: 5.01s\tremaining: 3m 32s\n",
      "23:\tlearn: 0.1826245\ttotal: 5.22s\tremaining: 3m 32s\n",
      "24:\tlearn: 0.1813497\ttotal: 5.4s\tremaining: 3m 30s\n",
      "25:\tlearn: 0.1797918\ttotal: 5.64s\tremaining: 3m 31s\n",
      "26:\tlearn: 0.1775047\ttotal: 5.84s\tremaining: 3m 30s\n",
      "27:\tlearn: 0.1761492\ttotal: 6.06s\tremaining: 3m 30s\n",
      "28:\tlearn: 0.1745850\ttotal: 6.29s\tremaining: 3m 30s\n",
      "29:\tlearn: 0.1733966\ttotal: 6.48s\tremaining: 3m 29s\n",
      "30:\tlearn: 0.1721382\ttotal: 6.69s\tremaining: 3m 29s\n",
      "31:\tlearn: 0.1711056\ttotal: 6.89s\tremaining: 3m 28s\n",
      "32:\tlearn: 0.1702234\ttotal: 7.12s\tremaining: 3m 28s\n",
      "33:\tlearn: 0.1690966\ttotal: 7.37s\tremaining: 3m 29s\n",
      "34:\tlearn: 0.1682667\ttotal: 7.59s\tremaining: 3m 29s\n",
      "35:\tlearn: 0.1675365\ttotal: 7.82s\tremaining: 3m 29s\n",
      "36:\tlearn: 0.1668711\ttotal: 8.08s\tremaining: 3m 30s\n",
      "37:\tlearn: 0.1658979\ttotal: 8.31s\tremaining: 3m 30s\n",
      "38:\tlearn: 0.1649908\ttotal: 8.58s\tremaining: 3m 31s\n",
      "39:\tlearn: 0.1641555\ttotal: 8.79s\tremaining: 3m 30s\n",
      "40:\tlearn: 0.1635327\ttotal: 9.02s\tremaining: 3m 30s\n",
      "41:\tlearn: 0.1624868\ttotal: 9.25s\tremaining: 3m 30s\n",
      "42:\tlearn: 0.1618071\ttotal: 9.52s\tremaining: 3m 31s\n",
      "43:\tlearn: 0.1610306\ttotal: 9.75s\tremaining: 3m 31s\n",
      "44:\tlearn: 0.1604645\ttotal: 10s\tremaining: 3m 32s\n",
      "45:\tlearn: 0.1597777\ttotal: 10.2s\tremaining: 3m 32s\n",
      "46:\tlearn: 0.1590894\ttotal: 10.4s\tremaining: 3m 31s\n",
      "47:\tlearn: 0.1582579\ttotal: 10.6s\tremaining: 3m 30s\n",
      "48:\tlearn: 0.1576012\ttotal: 10.8s\tremaining: 3m 30s\n",
      "49:\tlearn: 0.1569271\ttotal: 11s\tremaining: 3m 29s\n",
      "50:\tlearn: 0.1562749\ttotal: 11.2s\tremaining: 3m 28s\n",
      "51:\tlearn: 0.1557155\ttotal: 11.4s\tremaining: 3m 27s\n",
      "52:\tlearn: 0.1552185\ttotal: 11.6s\tremaining: 3m 28s\n",
      "53:\tlearn: 0.1547572\ttotal: 11.8s\tremaining: 3m 27s\n",
      "54:\tlearn: 0.1542758\ttotal: 12.1s\tremaining: 3m 27s\n",
      "55:\tlearn: 0.1537883\ttotal: 12.3s\tremaining: 3m 26s\n",
      "56:\tlearn: 0.1534047\ttotal: 12.4s\tremaining: 3m 25s\n",
      "57:\tlearn: 0.1524953\ttotal: 12.7s\tremaining: 3m 26s\n",
      "58:\tlearn: 0.1522040\ttotal: 12.9s\tremaining: 3m 25s\n",
      "59:\tlearn: 0.1516043\ttotal: 13.1s\tremaining: 3m 25s\n",
      "60:\tlearn: 0.1512170\ttotal: 13.3s\tremaining: 3m 24s\n",
      "61:\tlearn: 0.1508418\ttotal: 13.5s\tremaining: 3m 24s\n",
      "62:\tlearn: 0.1505192\ttotal: 13.7s\tremaining: 3m 24s\n",
      "63:\tlearn: 0.1501648\ttotal: 13.9s\tremaining: 3m 23s\n",
      "64:\tlearn: 0.1498235\ttotal: 14.2s\tremaining: 3m 23s\n",
      "65:\tlearn: 0.1495087\ttotal: 14.4s\tremaining: 3m 24s\n",
      "66:\tlearn: 0.1491097\ttotal: 14.6s\tremaining: 3m 23s\n",
      "67:\tlearn: 0.1487191\ttotal: 14.8s\tremaining: 3m 23s\n",
      "68:\tlearn: 0.1484094\ttotal: 15.1s\tremaining: 3m 23s\n",
      "69:\tlearn: 0.1480374\ttotal: 15.3s\tremaining: 3m 22s\n",
      "70:\tlearn: 0.1476267\ttotal: 15.5s\tremaining: 3m 22s\n",
      "71:\tlearn: 0.1473713\ttotal: 15.7s\tremaining: 3m 22s\n",
      "72:\tlearn: 0.1468446\ttotal: 15.9s\tremaining: 3m 21s\n",
      "73:\tlearn: 0.1465592\ttotal: 16.1s\tremaining: 3m 21s\n",
      "74:\tlearn: 0.1460392\ttotal: 16.3s\tremaining: 3m 21s\n",
      "75:\tlearn: 0.1457462\ttotal: 16.6s\tremaining: 3m 21s\n",
      "76:\tlearn: 0.1455076\ttotal: 16.7s\tremaining: 3m 20s\n",
      "77:\tlearn: 0.1452907\ttotal: 17s\tremaining: 3m 21s\n",
      "78:\tlearn: 0.1450498\ttotal: 17.3s\tremaining: 3m 21s\n",
      "79:\tlearn: 0.1448201\ttotal: 17.5s\tremaining: 3m 20s\n",
      "80:\tlearn: 0.1444677\ttotal: 17.7s\tremaining: 3m 20s\n",
      "81:\tlearn: 0.1441380\ttotal: 17.9s\tremaining: 3m 20s\n",
      "82:\tlearn: 0.1439098\ttotal: 18.1s\tremaining: 3m 20s\n",
      "83:\tlearn: 0.1436315\ttotal: 18.4s\tremaining: 3m 20s\n",
      "84:\tlearn: 0.1434159\ttotal: 18.6s\tremaining: 3m 20s\n",
      "85:\tlearn: 0.1431344\ttotal: 18.9s\tremaining: 3m 20s\n",
      "86:\tlearn: 0.1428791\ttotal: 19.1s\tremaining: 3m 20s\n",
      "87:\tlearn: 0.1426154\ttotal: 19.4s\tremaining: 3m 21s\n",
      "88:\tlearn: 0.1422403\ttotal: 19.6s\tremaining: 3m 20s\n",
      "89:\tlearn: 0.1419942\ttotal: 19.8s\tremaining: 3m 20s\n",
      "90:\tlearn: 0.1417708\ttotal: 20s\tremaining: 3m 20s\n",
      "91:\tlearn: 0.1415536\ttotal: 20.2s\tremaining: 3m 19s\n",
      "92:\tlearn: 0.1411721\ttotal: 20.5s\tremaining: 3m 19s\n",
      "93:\tlearn: 0.1408755\ttotal: 20.6s\tremaining: 3m 18s\n",
      "94:\tlearn: 0.1405716\ttotal: 20.9s\tremaining: 3m 18s\n",
      "95:\tlearn: 0.1402364\ttotal: 21.1s\tremaining: 3m 19s\n",
      "96:\tlearn: 0.1399993\ttotal: 21.4s\tremaining: 3m 18s\n",
      "97:\tlearn: 0.1396635\ttotal: 21.6s\tremaining: 3m 19s\n",
      "98:\tlearn: 0.1393149\ttotal: 21.9s\tremaining: 3m 18s\n",
      "99:\tlearn: 0.1390610\ttotal: 22.1s\tremaining: 3m 18s\n",
      "100:\tlearn: 0.1387945\ttotal: 22.3s\tremaining: 3m 18s\n",
      "101:\tlearn: 0.1385693\ttotal: 22.5s\tremaining: 3m 18s\n",
      "102:\tlearn: 0.1381882\ttotal: 22.8s\tremaining: 3m 18s\n",
      "103:\tlearn: 0.1379874\ttotal: 23s\tremaining: 3m 17s\n",
      "104:\tlearn: 0.1378020\ttotal: 23.2s\tremaining: 3m 17s\n",
      "105:\tlearn: 0.1375790\ttotal: 23.4s\tremaining: 3m 17s\n",
      "106:\tlearn: 0.1372919\ttotal: 23.6s\tremaining: 3m 17s\n",
      "107:\tlearn: 0.1371129\ttotal: 23.9s\tremaining: 3m 17s\n",
      "108:\tlearn: 0.1369480\ttotal: 24.1s\tremaining: 3m 16s\n",
      "109:\tlearn: 0.1366961\ttotal: 24.3s\tremaining: 3m 16s\n",
      "110:\tlearn: 0.1365360\ttotal: 24.6s\tremaining: 3m 16s\n",
      "111:\tlearn: 0.1364032\ttotal: 24.8s\tremaining: 3m 16s\n",
      "112:\tlearn: 0.1362079\ttotal: 25s\tremaining: 3m 16s\n",
      "113:\tlearn: 0.1360724\ttotal: 25.3s\tremaining: 3m 16s\n",
      "114:\tlearn: 0.1358805\ttotal: 25.5s\tremaining: 3m 16s\n",
      "115:\tlearn: 0.1356826\ttotal: 25.7s\tremaining: 3m 15s\n",
      "116:\tlearn: 0.1354588\ttotal: 25.9s\tremaining: 3m 15s\n",
      "117:\tlearn: 0.1353083\ttotal: 26.1s\tremaining: 3m 15s\n",
      "118:\tlearn: 0.1349330\ttotal: 26.3s\tremaining: 3m 14s\n",
      "119:\tlearn: 0.1347723\ttotal: 26.5s\tremaining: 3m 14s\n",
      "120:\tlearn: 0.1345770\ttotal: 26.7s\tremaining: 3m 14s\n",
      "121:\tlearn: 0.1343750\ttotal: 27s\tremaining: 3m 14s\n",
      "122:\tlearn: 0.1342176\ttotal: 27.2s\tremaining: 3m 14s\n",
      "123:\tlearn: 0.1340237\ttotal: 27.4s\tremaining: 3m 13s\n",
      "124:\tlearn: 0.1338666\ttotal: 27.6s\tremaining: 3m 13s\n",
      "125:\tlearn: 0.1337350\ttotal: 27.9s\tremaining: 3m 13s\n",
      "126:\tlearn: 0.1335451\ttotal: 28.1s\tremaining: 3m 13s\n",
      "127:\tlearn: 0.1332221\ttotal: 28.3s\tremaining: 3m 12s\n",
      "128:\tlearn: 0.1330862\ttotal: 28.5s\tremaining: 3m 12s\n",
      "129:\tlearn: 0.1328685\ttotal: 28.7s\tremaining: 3m 11s\n",
      "130:\tlearn: 0.1326139\ttotal: 29s\tremaining: 3m 12s\n",
      "131:\tlearn: 0.1324016\ttotal: 29.2s\tremaining: 3m 12s\n",
      "132:\tlearn: 0.1322537\ttotal: 29.5s\tremaining: 3m 12s\n",
      "133:\tlearn: 0.1321398\ttotal: 29.7s\tremaining: 3m 11s\n",
      "134:\tlearn: 0.1319547\ttotal: 29.9s\tremaining: 3m 11s\n",
      "135:\tlearn: 0.1317674\ttotal: 30.1s\tremaining: 3m 10s\n",
      "136:\tlearn: 0.1316360\ttotal: 30.3s\tremaining: 3m 10s\n",
      "137:\tlearn: 0.1314730\ttotal: 30.5s\tremaining: 3m 10s\n",
      "138:\tlearn: 0.1312698\ttotal: 30.8s\tremaining: 3m 10s\n",
      "139:\tlearn: 0.1311308\ttotal: 31s\tremaining: 3m 10s\n",
      "140:\tlearn: 0.1309739\ttotal: 31.2s\tremaining: 3m 10s\n",
      "141:\tlearn: 0.1308736\ttotal: 31.4s\tremaining: 3m 9s\n",
      "142:\tlearn: 0.1306843\ttotal: 31.7s\tremaining: 3m 9s\n",
      "143:\tlearn: 0.1304552\ttotal: 31.9s\tremaining: 3m 9s\n",
      "144:\tlearn: 0.1302769\ttotal: 32.1s\tremaining: 3m 9s\n",
      "145:\tlearn: 0.1301389\ttotal: 32.3s\tremaining: 3m 9s\n",
      "146:\tlearn: 0.1300020\ttotal: 32.6s\tremaining: 3m 9s\n",
      "147:\tlearn: 0.1298374\ttotal: 32.8s\tremaining: 3m 9s\n",
      "148:\tlearn: 0.1297152\ttotal: 33.1s\tremaining: 3m 8s\n",
      "149:\tlearn: 0.1295500\ttotal: 33.3s\tremaining: 3m 8s\n",
      "150:\tlearn: 0.1294668\ttotal: 33.5s\tremaining: 3m 8s\n",
      "151:\tlearn: 0.1292859\ttotal: 33.8s\tremaining: 3m 8s\n",
      "152:\tlearn: 0.1290822\ttotal: 34s\tremaining: 3m 8s\n",
      "153:\tlearn: 0.1287806\ttotal: 34.2s\tremaining: 3m 8s\n",
      "154:\tlearn: 0.1286692\ttotal: 34.4s\tremaining: 3m 7s\n",
      "155:\tlearn: 0.1284897\ttotal: 34.6s\tremaining: 3m 7s\n",
      "156:\tlearn: 0.1283175\ttotal: 34.8s\tremaining: 3m 6s\n",
      "157:\tlearn: 0.1281503\ttotal: 35s\tremaining: 3m 6s\n",
      "158:\tlearn: 0.1280345\ttotal: 35.2s\tremaining: 3m 6s\n",
      "159:\tlearn: 0.1278315\ttotal: 35.4s\tremaining: 3m 5s\n",
      "160:\tlearn: 0.1277111\ttotal: 35.6s\tremaining: 3m 5s\n",
      "161:\tlearn: 0.1275777\ttotal: 35.8s\tremaining: 3m 5s\n",
      "162:\tlearn: 0.1273881\ttotal: 36s\tremaining: 3m 5s\n",
      "163:\tlearn: 0.1272598\ttotal: 36.2s\tremaining: 3m 4s\n",
      "164:\tlearn: 0.1271399\ttotal: 36.5s\tremaining: 3m 4s\n",
      "165:\tlearn: 0.1269952\ttotal: 36.7s\tremaining: 3m 4s\n",
      "166:\tlearn: 0.1268341\ttotal: 36.9s\tremaining: 3m 4s\n",
      "167:\tlearn: 0.1267173\ttotal: 37.1s\tremaining: 3m 3s\n",
      "168:\tlearn: 0.1266155\ttotal: 37.3s\tremaining: 3m 3s\n",
      "169:\tlearn: 0.1265475\ttotal: 37.6s\tremaining: 3m 3s\n",
      "170:\tlearn: 0.1263895\ttotal: 37.8s\tremaining: 3m 3s\n",
      "171:\tlearn: 0.1262735\ttotal: 38s\tremaining: 3m 2s\n",
      "172:\tlearn: 0.1261161\ttotal: 38.2s\tremaining: 3m 2s\n",
      "173:\tlearn: 0.1259303\ttotal: 38.5s\tremaining: 3m 2s\n",
      "174:\tlearn: 0.1257729\ttotal: 38.7s\tremaining: 3m 2s\n",
      "175:\tlearn: 0.1256055\ttotal: 38.9s\tremaining: 3m 2s\n",
      "176:\tlearn: 0.1254295\ttotal: 39.1s\tremaining: 3m 2s\n",
      "177:\tlearn: 0.1253550\ttotal: 39.3s\tremaining: 3m 1s\n",
      "178:\tlearn: 0.1252533\ttotal: 39.6s\tremaining: 3m 1s\n",
      "179:\tlearn: 0.1251056\ttotal: 39.8s\tremaining: 3m 1s\n",
      "180:\tlearn: 0.1250114\ttotal: 40s\tremaining: 3m 1s\n",
      "181:\tlearn: 0.1248858\ttotal: 40.2s\tremaining: 3m\n",
      "182:\tlearn: 0.1247641\ttotal: 40.5s\tremaining: 3m\n",
      "183:\tlearn: 0.1246582\ttotal: 40.7s\tremaining: 3m\n",
      "184:\tlearn: 0.1245118\ttotal: 40.9s\tremaining: 3m\n",
      "185:\tlearn: 0.1244145\ttotal: 41.1s\tremaining: 2m 59s\n",
      "186:\tlearn: 0.1243286\ttotal: 41.3s\tremaining: 2m 59s\n",
      "187:\tlearn: 0.1241410\ttotal: 41.6s\tremaining: 2m 59s\n",
      "188:\tlearn: 0.1240058\ttotal: 41.9s\tremaining: 2m 59s\n",
      "189:\tlearn: 0.1238628\ttotal: 42.1s\tremaining: 2m 59s\n",
      "190:\tlearn: 0.1237664\ttotal: 42.4s\tremaining: 2m 59s\n",
      "191:\tlearn: 0.1236861\ttotal: 42.6s\tremaining: 2m 59s\n",
      "192:\tlearn: 0.1235424\ttotal: 42.8s\tremaining: 2m 58s\n",
      "193:\tlearn: 0.1233948\ttotal: 43s\tremaining: 2m 58s\n",
      "194:\tlearn: 0.1232537\ttotal: 43.3s\tremaining: 2m 58s\n",
      "195:\tlearn: 0.1231434\ttotal: 43.5s\tremaining: 2m 58s\n",
      "196:\tlearn: 0.1230036\ttotal: 43.7s\tremaining: 2m 58s\n",
      "197:\tlearn: 0.1229000\ttotal: 43.9s\tremaining: 2m 57s\n",
      "198:\tlearn: 0.1228220\ttotal: 44.1s\tremaining: 2m 57s\n",
      "199:\tlearn: 0.1227417\ttotal: 44.4s\tremaining: 2m 57s\n",
      "200:\tlearn: 0.1226562\ttotal: 44.6s\tremaining: 2m 57s\n",
      "201:\tlearn: 0.1225732\ttotal: 44.8s\tremaining: 2m 57s\n",
      "202:\tlearn: 0.1224885\ttotal: 45s\tremaining: 2m 56s\n",
      "203:\tlearn: 0.1224140\ttotal: 45.2s\tremaining: 2m 56s\n",
      "204:\tlearn: 0.1223161\ttotal: 45.5s\tremaining: 2m 56s\n",
      "205:\tlearn: 0.1222252\ttotal: 45.7s\tremaining: 2m 56s\n",
      "206:\tlearn: 0.1221466\ttotal: 45.9s\tremaining: 2m 55s\n",
      "207:\tlearn: 0.1220127\ttotal: 46.2s\tremaining: 2m 55s\n",
      "208:\tlearn: 0.1219251\ttotal: 46.4s\tremaining: 2m 55s\n",
      "209:\tlearn: 0.1217874\ttotal: 46.6s\tremaining: 2m 55s\n",
      "210:\tlearn: 0.1217371\ttotal: 46.9s\tremaining: 2m 55s\n",
      "211:\tlearn: 0.1216125\ttotal: 47.1s\tremaining: 2m 54s\n",
      "212:\tlearn: 0.1214910\ttotal: 47.3s\tremaining: 2m 54s\n",
      "213:\tlearn: 0.1213714\ttotal: 47.5s\tremaining: 2m 54s\n",
      "214:\tlearn: 0.1212833\ttotal: 47.7s\tremaining: 2m 54s\n",
      "215:\tlearn: 0.1211879\ttotal: 47.9s\tremaining: 2m 53s\n",
      "216:\tlearn: 0.1210464\ttotal: 48.2s\tremaining: 2m 53s\n",
      "217:\tlearn: 0.1209488\ttotal: 48.4s\tremaining: 2m 53s\n",
      "218:\tlearn: 0.1208374\ttotal: 48.6s\tremaining: 2m 53s\n",
      "219:\tlearn: 0.1206980\ttotal: 48.8s\tremaining: 2m 52s\n",
      "220:\tlearn: 0.1206262\ttotal: 48.9s\tremaining: 2m 52s\n",
      "221:\tlearn: 0.1205263\ttotal: 49.1s\tremaining: 2m 52s\n",
      "222:\tlearn: 0.1204106\ttotal: 49.4s\tremaining: 2m 51s\n",
      "223:\tlearn: 0.1203250\ttotal: 49.6s\tremaining: 2m 51s\n",
      "224:\tlearn: 0.1201527\ttotal: 49.8s\tremaining: 2m 51s\n",
      "225:\tlearn: 0.1200390\ttotal: 50s\tremaining: 2m 51s\n",
      "226:\tlearn: 0.1199527\ttotal: 50.3s\tremaining: 2m 51s\n",
      "227:\tlearn: 0.1198930\ttotal: 50.5s\tremaining: 2m 50s\n",
      "228:\tlearn: 0.1198202\ttotal: 50.7s\tremaining: 2m 50s\n",
      "229:\tlearn: 0.1196663\ttotal: 51s\tremaining: 2m 50s\n",
      "230:\tlearn: 0.1195892\ttotal: 51.2s\tremaining: 2m 50s\n",
      "231:\tlearn: 0.1194965\ttotal: 51.4s\tremaining: 2m 50s\n",
      "232:\tlearn: 0.1193629\ttotal: 51.7s\tremaining: 2m 50s\n",
      "233:\tlearn: 0.1192701\ttotal: 51.9s\tremaining: 2m 49s\n",
      "234:\tlearn: 0.1191459\ttotal: 52.1s\tremaining: 2m 49s\n",
      "235:\tlearn: 0.1190591\ttotal: 52.3s\tremaining: 2m 49s\n",
      "236:\tlearn: 0.1189754\ttotal: 52.5s\tremaining: 2m 49s\n",
      "237:\tlearn: 0.1188514\ttotal: 52.8s\tremaining: 2m 48s\n",
      "238:\tlearn: 0.1188067\ttotal: 53s\tremaining: 2m 48s\n",
      "239:\tlearn: 0.1187164\ttotal: 53.2s\tremaining: 2m 48s\n",
      "240:\tlearn: 0.1186335\ttotal: 53.5s\tremaining: 2m 48s\n",
      "241:\tlearn: 0.1185298\ttotal: 53.8s\tremaining: 2m 48s\n",
      "242:\tlearn: 0.1184320\ttotal: 54s\tremaining: 2m 48s\n",
      "243:\tlearn: 0.1183572\ttotal: 54.2s\tremaining: 2m 47s\n",
      "244:\tlearn: 0.1182745\ttotal: 54.4s\tremaining: 2m 47s\n",
      "245:\tlearn: 0.1181782\ttotal: 54.6s\tremaining: 2m 47s\n",
      "246:\tlearn: 0.1181131\ttotal: 54.8s\tremaining: 2m 47s\n",
      "247:\tlearn: 0.1180337\ttotal: 55s\tremaining: 2m 46s\n",
      "248:\tlearn: 0.1179580\ttotal: 55.2s\tremaining: 2m 46s\n",
      "249:\tlearn: 0.1178831\ttotal: 55.4s\tremaining: 2m 46s\n",
      "250:\tlearn: 0.1177850\ttotal: 55.6s\tremaining: 2m 45s\n",
      "251:\tlearn: 0.1177055\ttotal: 55.8s\tremaining: 2m 45s\n",
      "252:\tlearn: 0.1176518\ttotal: 56.1s\tremaining: 2m 45s\n",
      "253:\tlearn: 0.1175801\ttotal: 56.3s\tremaining: 2m 45s\n",
      "254:\tlearn: 0.1175080\ttotal: 56.6s\tremaining: 2m 45s\n",
      "255:\tlearn: 0.1174389\ttotal: 56.7s\tremaining: 2m 44s\n",
      "256:\tlearn: 0.1173486\ttotal: 56.9s\tremaining: 2m 44s\n",
      "257:\tlearn: 0.1172792\ttotal: 57.1s\tremaining: 2m 44s\n",
      "258:\tlearn: 0.1172139\ttotal: 57.3s\tremaining: 2m 44s\n",
      "259:\tlearn: 0.1170821\ttotal: 57.5s\tremaining: 2m 43s\n",
      "260:\tlearn: 0.1170171\ttotal: 57.7s\tremaining: 2m 43s\n",
      "261:\tlearn: 0.1169639\ttotal: 58s\tremaining: 2m 43s\n",
      "262:\tlearn: 0.1168728\ttotal: 58.2s\tremaining: 2m 43s\n",
      "263:\tlearn: 0.1167893\ttotal: 58.4s\tremaining: 2m 42s\n",
      "264:\tlearn: 0.1167145\ttotal: 58.6s\tremaining: 2m 42s\n",
      "265:\tlearn: 0.1166463\ttotal: 58.8s\tremaining: 2m 42s\n",
      "266:\tlearn: 0.1165685\ttotal: 59.1s\tremaining: 2m 42s\n",
      "267:\tlearn: 0.1164235\ttotal: 59.3s\tremaining: 2m 42s\n",
      "268:\tlearn: 0.1163703\ttotal: 59.5s\tremaining: 2m 41s\n",
      "269:\tlearn: 0.1163148\ttotal: 59.7s\tremaining: 2m 41s\n",
      "270:\tlearn: 0.1162012\ttotal: 59.9s\tremaining: 2m 41s\n",
      "271:\tlearn: 0.1161507\ttotal: 1m\tremaining: 2m 40s\n",
      "272:\tlearn: 0.1160341\ttotal: 1m\tremaining: 2m 40s\n",
      "273:\tlearn: 0.1159642\ttotal: 1m\tremaining: 2m 40s\n",
      "274:\tlearn: 0.1159071\ttotal: 1m\tremaining: 2m 40s\n",
      "275:\tlearn: 0.1158084\ttotal: 1m\tremaining: 2m 39s\n",
      "276:\tlearn: 0.1157480\ttotal: 1m 1s\tremaining: 2m 39s\n",
      "277:\tlearn: 0.1156226\ttotal: 1m 1s\tremaining: 2m 39s\n",
      "278:\tlearn: 0.1154796\ttotal: 1m 1s\tremaining: 2m 39s\n",
      "279:\tlearn: 0.1154090\ttotal: 1m 1s\tremaining: 2m 38s\n",
      "280:\tlearn: 0.1153240\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "281:\tlearn: 0.1152188\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "282:\tlearn: 0.1151459\ttotal: 1m 2s\tremaining: 2m 38s\n",
      "283:\tlearn: 0.1150646\ttotal: 1m 2s\tremaining: 2m 37s\n",
      "284:\tlearn: 0.1149848\ttotal: 1m 2s\tremaining: 2m 37s\n",
      "285:\tlearn: 0.1149299\ttotal: 1m 3s\tremaining: 2m 37s\n",
      "286:\tlearn: 0.1148613\ttotal: 1m 3s\tremaining: 2m 37s\n",
      "287:\tlearn: 0.1148103\ttotal: 1m 3s\tremaining: 2m 36s\n",
      "288:\tlearn: 0.1146989\ttotal: 1m 3s\tremaining: 2m 36s\n",
      "289:\tlearn: 0.1146205\ttotal: 1m 3s\tremaining: 2m 36s\n",
      "290:\tlearn: 0.1145571\ttotal: 1m 4s\tremaining: 2m 36s\n",
      "291:\tlearn: 0.1144743\ttotal: 1m 4s\tremaining: 2m 36s\n",
      "292:\tlearn: 0.1144075\ttotal: 1m 4s\tremaining: 2m 35s\n",
      "293:\tlearn: 0.1143291\ttotal: 1m 4s\tremaining: 2m 35s\n",
      "294:\tlearn: 0.1142677\ttotal: 1m 5s\tremaining: 2m 35s\n",
      "295:\tlearn: 0.1142227\ttotal: 1m 5s\tremaining: 2m 35s\n",
      "296:\tlearn: 0.1141546\ttotal: 1m 5s\tremaining: 2m 34s\n",
      "297:\tlearn: 0.1140996\ttotal: 1m 5s\tremaining: 2m 34s\n",
      "298:\tlearn: 0.1140430\ttotal: 1m 5s\tremaining: 2m 34s\n",
      "299:\tlearn: 0.1139790\ttotal: 1m 6s\tremaining: 2m 34s\n",
      "300:\tlearn: 0.1138925\ttotal: 1m 6s\tremaining: 2m 33s\n",
      "301:\tlearn: 0.1138212\ttotal: 1m 6s\tremaining: 2m 33s\n",
      "302:\tlearn: 0.1137721\ttotal: 1m 6s\tremaining: 2m 33s\n",
      "303:\tlearn: 0.1137113\ttotal: 1m 6s\tremaining: 2m 33s\n",
      "304:\tlearn: 0.1136544\ttotal: 1m 7s\tremaining: 2m 32s\n",
      "305:\tlearn: 0.1135740\ttotal: 1m 7s\tremaining: 2m 32s\n",
      "306:\tlearn: 0.1135049\ttotal: 1m 7s\tremaining: 2m 32s\n",
      "307:\tlearn: 0.1134695\ttotal: 1m 7s\tremaining: 2m 32s\n",
      "308:\tlearn: 0.1134073\ttotal: 1m 7s\tremaining: 2m 32s\n",
      "309:\tlearn: 0.1133346\ttotal: 1m 8s\tremaining: 2m 31s\n",
      "310:\tlearn: 0.1132575\ttotal: 1m 8s\tremaining: 2m 31s\n",
      "311:\tlearn: 0.1131812\ttotal: 1m 8s\tremaining: 2m 31s\n",
      "312:\tlearn: 0.1130959\ttotal: 1m 8s\tremaining: 2m 31s\n",
      "313:\tlearn: 0.1129989\ttotal: 1m 9s\tremaining: 2m 30s\n",
      "314:\tlearn: 0.1129113\ttotal: 1m 9s\tremaining: 2m 30s\n",
      "315:\tlearn: 0.1128478\ttotal: 1m 9s\tremaining: 2m 30s\n",
      "316:\tlearn: 0.1128046\ttotal: 1m 9s\tremaining: 2m 30s\n",
      "317:\tlearn: 0.1127584\ttotal: 1m 9s\tremaining: 2m 30s\n",
      "318:\tlearn: 0.1127002\ttotal: 1m 10s\tremaining: 2m 29s\n",
      "319:\tlearn: 0.1126547\ttotal: 1m 10s\tremaining: 2m 29s\n",
      "320:\tlearn: 0.1125930\ttotal: 1m 10s\tremaining: 2m 29s\n",
      "321:\tlearn: 0.1125212\ttotal: 1m 10s\tremaining: 2m 29s\n",
      "322:\tlearn: 0.1124563\ttotal: 1m 11s\tremaining: 2m 28s\n",
      "323:\tlearn: 0.1123752\ttotal: 1m 11s\tremaining: 2m 28s\n",
      "324:\tlearn: 0.1123199\ttotal: 1m 11s\tremaining: 2m 28s\n",
      "325:\tlearn: 0.1122807\ttotal: 1m 11s\tremaining: 2m 28s\n",
      "326:\tlearn: 0.1122085\ttotal: 1m 11s\tremaining: 2m 28s\n",
      "327:\tlearn: 0.1121622\ttotal: 1m 12s\tremaining: 2m 27s\n",
      "328:\tlearn: 0.1120856\ttotal: 1m 12s\tremaining: 2m 27s\n",
      "329:\tlearn: 0.1119921\ttotal: 1m 12s\tremaining: 2m 27s\n",
      "330:\tlearn: 0.1119080\ttotal: 1m 12s\tremaining: 2m 27s\n",
      "331:\tlearn: 0.1118517\ttotal: 1m 13s\tremaining: 2m 27s\n",
      "332:\tlearn: 0.1117916\ttotal: 1m 13s\tremaining: 2m 26s\n",
      "333:\tlearn: 0.1116998\ttotal: 1m 13s\tremaining: 2m 26s\n",
      "334:\tlearn: 0.1116033\ttotal: 1m 13s\tremaining: 2m 26s\n",
      "335:\tlearn: 0.1115232\ttotal: 1m 14s\tremaining: 2m 26s\n",
      "336:\tlearn: 0.1114358\ttotal: 1m 14s\tremaining: 2m 26s\n",
      "337:\tlearn: 0.1113826\ttotal: 1m 14s\tremaining: 2m 26s\n",
      "338:\tlearn: 0.1113362\ttotal: 1m 14s\tremaining: 2m 25s\n",
      "339:\tlearn: 0.1112602\ttotal: 1m 14s\tremaining: 2m 25s\n",
      "340:\tlearn: 0.1112201\ttotal: 1m 15s\tremaining: 2m 25s\n",
      "341:\tlearn: 0.1111325\ttotal: 1m 15s\tremaining: 2m 25s\n",
      "342:\tlearn: 0.1110716\ttotal: 1m 15s\tremaining: 2m 24s\n",
      "343:\tlearn: 0.1109310\ttotal: 1m 15s\tremaining: 2m 24s\n",
      "344:\tlearn: 0.1108836\ttotal: 1m 16s\tremaining: 2m 24s\n",
      "345:\tlearn: 0.1108087\ttotal: 1m 16s\tremaining: 2m 24s\n",
      "346:\tlearn: 0.1106967\ttotal: 1m 16s\tremaining: 2m 23s\n",
      "347:\tlearn: 0.1106413\ttotal: 1m 16s\tremaining: 2m 23s\n",
      "348:\tlearn: 0.1105596\ttotal: 1m 16s\tremaining: 2m 23s\n",
      "349:\tlearn: 0.1105047\ttotal: 1m 17s\tremaining: 2m 23s\n",
      "350:\tlearn: 0.1104612\ttotal: 1m 17s\tremaining: 2m 23s\n",
      "351:\tlearn: 0.1103902\ttotal: 1m 17s\tremaining: 2m 22s\n",
      "352:\tlearn: 0.1103429\ttotal: 1m 17s\tremaining: 2m 22s\n",
      "353:\tlearn: 0.1102617\ttotal: 1m 18s\tremaining: 2m 22s\n",
      "354:\tlearn: 0.1101830\ttotal: 1m 18s\tremaining: 2m 22s\n",
      "355:\tlearn: 0.1101188\ttotal: 1m 18s\tremaining: 2m 22s\n",
      "356:\tlearn: 0.1100463\ttotal: 1m 18s\tremaining: 2m 21s\n",
      "357:\tlearn: 0.1099693\ttotal: 1m 18s\tremaining: 2m 21s\n",
      "358:\tlearn: 0.1099112\ttotal: 1m 19s\tremaining: 2m 21s\n",
      "359:\tlearn: 0.1098030\ttotal: 1m 19s\tremaining: 2m 21s\n",
      "360:\tlearn: 0.1097346\ttotal: 1m 19s\tremaining: 2m 20s\n",
      "361:\tlearn: 0.1096992\ttotal: 1m 19s\tremaining: 2m 20s\n",
      "362:\tlearn: 0.1096303\ttotal: 1m 19s\tremaining: 2m 20s\n",
      "363:\tlearn: 0.1095656\ttotal: 1m 20s\tremaining: 2m 20s\n",
      "364:\tlearn: 0.1095022\ttotal: 1m 20s\tremaining: 2m 19s\n",
      "365:\tlearn: 0.1094689\ttotal: 1m 20s\tremaining: 2m 19s\n",
      "366:\tlearn: 0.1094131\ttotal: 1m 20s\tremaining: 2m 19s\n",
      "367:\tlearn: 0.1093319\ttotal: 1m 20s\tremaining: 2m 19s\n",
      "368:\tlearn: 0.1092869\ttotal: 1m 21s\tremaining: 2m 18s\n",
      "369:\tlearn: 0.1092309\ttotal: 1m 21s\tremaining: 2m 18s\n",
      "370:\tlearn: 0.1091657\ttotal: 1m 21s\tremaining: 2m 18s\n",
      "371:\tlearn: 0.1091204\ttotal: 1m 21s\tremaining: 2m 18s\n",
      "372:\tlearn: 0.1090259\ttotal: 1m 22s\tremaining: 2m 18s\n",
      "373:\tlearn: 0.1089723\ttotal: 1m 22s\tremaining: 2m 17s\n",
      "374:\tlearn: 0.1089378\ttotal: 1m 22s\tremaining: 2m 17s\n",
      "375:\tlearn: 0.1088629\ttotal: 1m 22s\tremaining: 2m 17s\n",
      "376:\tlearn: 0.1087733\ttotal: 1m 23s\tremaining: 2m 17s\n",
      "377:\tlearn: 0.1086758\ttotal: 1m 23s\tremaining: 2m 17s\n",
      "378:\tlearn: 0.1086507\ttotal: 1m 23s\tremaining: 2m 16s\n",
      "379:\tlearn: 0.1085697\ttotal: 1m 23s\tremaining: 2m 16s\n",
      "380:\tlearn: 0.1085198\ttotal: 1m 23s\tremaining: 2m 16s\n",
      "381:\tlearn: 0.1084408\ttotal: 1m 24s\tremaining: 2m 16s\n",
      "382:\tlearn: 0.1083960\ttotal: 1m 24s\tremaining: 2m 15s\n",
      "383:\tlearn: 0.1083311\ttotal: 1m 24s\tremaining: 2m 15s\n",
      "384:\tlearn: 0.1082414\ttotal: 1m 24s\tremaining: 2m 15s\n",
      "385:\tlearn: 0.1081919\ttotal: 1m 24s\tremaining: 2m 15s\n",
      "386:\tlearn: 0.1081282\ttotal: 1m 25s\tremaining: 2m 14s\n",
      "387:\tlearn: 0.1080827\ttotal: 1m 25s\tremaining: 2m 14s\n",
      "388:\tlearn: 0.1080083\ttotal: 1m 25s\tremaining: 2m 14s\n",
      "389:\tlearn: 0.1079665\ttotal: 1m 25s\tremaining: 2m 14s\n",
      "390:\tlearn: 0.1079020\ttotal: 1m 25s\tremaining: 2m 13s\n",
      "391:\tlearn: 0.1078728\ttotal: 1m 26s\tremaining: 2m 13s\n",
      "392:\tlearn: 0.1078056\ttotal: 1m 26s\tremaining: 2m 13s\n",
      "393:\tlearn: 0.1077053\ttotal: 1m 26s\tremaining: 2m 13s\n",
      "394:\tlearn: 0.1076442\ttotal: 1m 26s\tremaining: 2m 12s\n",
      "395:\tlearn: 0.1075871\ttotal: 1m 27s\tremaining: 2m 12s\n",
      "396:\tlearn: 0.1075583\ttotal: 1m 27s\tremaining: 2m 12s\n",
      "397:\tlearn: 0.1074900\ttotal: 1m 27s\tremaining: 2m 12s\n",
      "398:\tlearn: 0.1074121\ttotal: 1m 27s\tremaining: 2m 11s\n",
      "399:\tlearn: 0.1073288\ttotal: 1m 27s\tremaining: 2m 11s\n",
      "400:\tlearn: 0.1072570\ttotal: 1m 28s\tremaining: 2m 11s\n",
      "401:\tlearn: 0.1072188\ttotal: 1m 28s\tremaining: 2m 11s\n",
      "402:\tlearn: 0.1071275\ttotal: 1m 28s\tremaining: 2m 11s\n",
      "403:\tlearn: 0.1070398\ttotal: 1m 28s\tremaining: 2m 11s\n",
      "404:\tlearn: 0.1069876\ttotal: 1m 29s\tremaining: 2m 10s\n",
      "405:\tlearn: 0.1069289\ttotal: 1m 29s\tremaining: 2m 10s\n",
      "406:\tlearn: 0.1068483\ttotal: 1m 29s\tremaining: 2m 10s\n",
      "407:\tlearn: 0.1067906\ttotal: 1m 29s\tremaining: 2m 10s\n",
      "408:\tlearn: 0.1067273\ttotal: 1m 29s\tremaining: 2m 10s\n",
      "409:\tlearn: 0.1066546\ttotal: 1m 30s\tremaining: 2m 9s\n",
      "410:\tlearn: 0.1066038\ttotal: 1m 30s\tremaining: 2m 9s\n",
      "411:\tlearn: 0.1065594\ttotal: 1m 30s\tremaining: 2m 9s\n",
      "412:\tlearn: 0.1065067\ttotal: 1m 30s\tremaining: 2m 9s\n",
      "413:\tlearn: 0.1064391\ttotal: 1m 31s\tremaining: 2m 8s\n",
      "414:\tlearn: 0.1063954\ttotal: 1m 31s\tremaining: 2m 8s\n",
      "415:\tlearn: 0.1063550\ttotal: 1m 31s\tremaining: 2m 8s\n",
      "416:\tlearn: 0.1062884\ttotal: 1m 31s\tremaining: 2m 8s\n",
      "417:\tlearn: 0.1062150\ttotal: 1m 31s\tremaining: 2m 7s\n",
      "418:\tlearn: 0.1061539\ttotal: 1m 32s\tremaining: 2m 7s\n",
      "419:\tlearn: 0.1060955\ttotal: 1m 32s\tremaining: 2m 7s\n",
      "420:\tlearn: 0.1060478\ttotal: 1m 32s\tremaining: 2m 7s\n",
      "421:\tlearn: 0.1059977\ttotal: 1m 32s\tremaining: 2m 6s\n",
      "422:\tlearn: 0.1059412\ttotal: 1m 32s\tremaining: 2m 6s\n",
      "423:\tlearn: 0.1058481\ttotal: 1m 33s\tremaining: 2m 6s\n",
      "424:\tlearn: 0.1057303\ttotal: 1m 33s\tremaining: 2m 6s\n",
      "425:\tlearn: 0.1056581\ttotal: 1m 33s\tremaining: 2m 6s\n",
      "426:\tlearn: 0.1056248\ttotal: 1m 33s\tremaining: 2m 5s\n",
      "427:\tlearn: 0.1055908\ttotal: 1m 34s\tremaining: 2m 5s\n",
      "428:\tlearn: 0.1055225\ttotal: 1m 34s\tremaining: 2m 5s\n",
      "429:\tlearn: 0.1054695\ttotal: 1m 34s\tremaining: 2m 5s\n",
      "430:\tlearn: 0.1054146\ttotal: 1m 34s\tremaining: 2m 4s\n",
      "431:\tlearn: 0.1053703\ttotal: 1m 34s\tremaining: 2m 4s\n",
      "432:\tlearn: 0.1053455\ttotal: 1m 35s\tremaining: 2m 4s\n",
      "433:\tlearn: 0.1052853\ttotal: 1m 35s\tremaining: 2m 4s\n",
      "434:\tlearn: 0.1052230\ttotal: 1m 35s\tremaining: 2m 4s\n",
      "435:\tlearn: 0.1051220\ttotal: 1m 35s\tremaining: 2m 3s\n",
      "436:\tlearn: 0.1050658\ttotal: 1m 35s\tremaining: 2m 3s\n",
      "437:\tlearn: 0.1050162\ttotal: 1m 36s\tremaining: 2m 3s\n",
      "438:\tlearn: 0.1049586\ttotal: 1m 36s\tremaining: 2m 3s\n",
      "439:\tlearn: 0.1049037\ttotal: 1m 36s\tremaining: 2m 2s\n",
      "440:\tlearn: 0.1048202\ttotal: 1m 36s\tremaining: 2m 2s\n",
      "441:\tlearn: 0.1047579\ttotal: 1m 37s\tremaining: 2m 2s\n",
      "442:\tlearn: 0.1046858\ttotal: 1m 37s\tremaining: 2m 2s\n",
      "443:\tlearn: 0.1046476\ttotal: 1m 37s\tremaining: 2m 1s\n",
      "444:\tlearn: 0.1045779\ttotal: 1m 37s\tremaining: 2m 1s\n",
      "445:\tlearn: 0.1045495\ttotal: 1m 37s\tremaining: 2m 1s\n",
      "446:\tlearn: 0.1044747\ttotal: 1m 38s\tremaining: 2m 1s\n",
      "447:\tlearn: 0.1043953\ttotal: 1m 38s\tremaining: 2m 1s\n",
      "448:\tlearn: 0.1043512\ttotal: 1m 38s\tremaining: 2m\n",
      "449:\tlearn: 0.1042863\ttotal: 1m 38s\tremaining: 2m\n",
      "450:\tlearn: 0.1042382\ttotal: 1m 38s\tremaining: 2m\n",
      "451:\tlearn: 0.1041784\ttotal: 1m 38s\tremaining: 2m\n",
      "452:\tlearn: 0.1041341\ttotal: 1m 39s\tremaining: 1m 59s\n",
      "453:\tlearn: 0.1040824\ttotal: 1m 39s\tremaining: 1m 59s\n",
      "454:\tlearn: 0.1039992\ttotal: 1m 39s\tremaining: 1m 59s\n",
      "455:\tlearn: 0.1039568\ttotal: 1m 39s\tremaining: 1m 59s\n",
      "456:\tlearn: 0.1039077\ttotal: 1m 39s\tremaining: 1m 58s\n",
      "457:\tlearn: 0.1038634\ttotal: 1m 40s\tremaining: 1m 58s\n",
      "458:\tlearn: 0.1038165\ttotal: 1m 40s\tremaining: 1m 58s\n",
      "459:\tlearn: 0.1037817\ttotal: 1m 40s\tremaining: 1m 58s\n",
      "460:\tlearn: 0.1037370\ttotal: 1m 40s\tremaining: 1m 57s\n",
      "461:\tlearn: 0.1036848\ttotal: 1m 41s\tremaining: 1m 57s\n",
      "462:\tlearn: 0.1036328\ttotal: 1m 41s\tremaining: 1m 57s\n",
      "463:\tlearn: 0.1035921\ttotal: 1m 41s\tremaining: 1m 57s\n",
      "464:\tlearn: 0.1035140\ttotal: 1m 41s\tremaining: 1m 57s\n",
      "465:\tlearn: 0.1034524\ttotal: 1m 42s\tremaining: 1m 56s\n",
      "466:\tlearn: 0.1033764\ttotal: 1m 42s\tremaining: 1m 56s\n",
      "467:\tlearn: 0.1033420\ttotal: 1m 42s\tremaining: 1m 56s\n",
      "468:\tlearn: 0.1033117\ttotal: 1m 42s\tremaining: 1m 56s\n",
      "469:\tlearn: 0.1032509\ttotal: 1m 42s\tremaining: 1m 55s\n",
      "470:\tlearn: 0.1032023\ttotal: 1m 43s\tremaining: 1m 55s\n",
      "471:\tlearn: 0.1031474\ttotal: 1m 43s\tremaining: 1m 55s\n",
      "472:\tlearn: 0.1030982\ttotal: 1m 43s\tremaining: 1m 55s\n",
      "473:\tlearn: 0.1030433\ttotal: 1m 43s\tremaining: 1m 55s\n",
      "474:\tlearn: 0.1030206\ttotal: 1m 43s\tremaining: 1m 54s\n",
      "475:\tlearn: 0.1029323\ttotal: 1m 44s\tremaining: 1m 54s\n",
      "476:\tlearn: 0.1029168\ttotal: 1m 44s\tremaining: 1m 54s\n",
      "477:\tlearn: 0.1028384\ttotal: 1m 44s\tremaining: 1m 54s\n",
      "478:\tlearn: 0.1027775\ttotal: 1m 44s\tremaining: 1m 54s\n",
      "479:\tlearn: 0.1026874\ttotal: 1m 45s\tremaining: 1m 53s\n",
      "480:\tlearn: 0.1026418\ttotal: 1m 45s\tremaining: 1m 53s\n",
      "481:\tlearn: 0.1025994\ttotal: 1m 45s\tremaining: 1m 53s\n",
      "482:\tlearn: 0.1025518\ttotal: 1m 45s\tremaining: 1m 53s\n",
      "483:\tlearn: 0.1024847\ttotal: 1m 46s\tremaining: 1m 53s\n",
      "484:\tlearn: 0.1024625\ttotal: 1m 46s\tremaining: 1m 52s\n",
      "485:\tlearn: 0.1024056\ttotal: 1m 46s\tremaining: 1m 52s\n",
      "486:\tlearn: 0.1023861\ttotal: 1m 46s\tremaining: 1m 52s\n",
      "487:\tlearn: 0.1023252\ttotal: 1m 46s\tremaining: 1m 52s\n",
      "488:\tlearn: 0.1022750\ttotal: 1m 47s\tremaining: 1m 51s\n",
      "489:\tlearn: 0.1022326\ttotal: 1m 47s\tremaining: 1m 51s\n",
      "490:\tlearn: 0.1021923\ttotal: 1m 47s\tremaining: 1m 51s\n",
      "491:\tlearn: 0.1021650\ttotal: 1m 47s\tremaining: 1m 51s\n",
      "492:\tlearn: 0.1020674\ttotal: 1m 47s\tremaining: 1m 50s\n",
      "493:\tlearn: 0.1020512\ttotal: 1m 48s\tremaining: 1m 50s\n",
      "494:\tlearn: 0.1019746\ttotal: 1m 48s\tremaining: 1m 50s\n",
      "495:\tlearn: 0.1019330\ttotal: 1m 48s\tremaining: 1m 50s\n",
      "496:\tlearn: 0.1019006\ttotal: 1m 48s\tremaining: 1m 49s\n",
      "497:\tlearn: 0.1018625\ttotal: 1m 48s\tremaining: 1m 49s\n",
      "498:\tlearn: 0.1017913\ttotal: 1m 49s\tremaining: 1m 49s\n",
      "499:\tlearn: 0.1017415\ttotal: 1m 49s\tremaining: 1m 49s\n",
      "500:\tlearn: 0.1016751\ttotal: 1m 49s\tremaining: 1m 49s\n",
      "501:\tlearn: 0.1016183\ttotal: 1m 49s\tremaining: 1m 48s\n",
      "502:\tlearn: 0.1015624\ttotal: 1m 49s\tremaining: 1m 48s\n",
      "503:\tlearn: 0.1015268\ttotal: 1m 50s\tremaining: 1m 48s\n",
      "504:\tlearn: 0.1014874\ttotal: 1m 50s\tremaining: 1m 48s\n",
      "505:\tlearn: 0.1014345\ttotal: 1m 50s\tremaining: 1m 47s\n",
      "506:\tlearn: 0.1013886\ttotal: 1m 50s\tremaining: 1m 47s\n",
      "507:\tlearn: 0.1013469\ttotal: 1m 50s\tremaining: 1m 47s\n",
      "508:\tlearn: 0.1013103\ttotal: 1m 51s\tremaining: 1m 47s\n",
      "509:\tlearn: 0.1012613\ttotal: 1m 51s\tremaining: 1m 46s\n",
      "510:\tlearn: 0.1012104\ttotal: 1m 51s\tremaining: 1m 46s\n",
      "511:\tlearn: 0.1011367\ttotal: 1m 51s\tremaining: 1m 46s\n",
      "512:\tlearn: 0.1011077\ttotal: 1m 51s\tremaining: 1m 46s\n",
      "513:\tlearn: 0.1010514\ttotal: 1m 52s\tremaining: 1m 46s\n",
      "514:\tlearn: 0.1009632\ttotal: 1m 52s\tremaining: 1m 45s\n",
      "515:\tlearn: 0.1009139\ttotal: 1m 52s\tremaining: 1m 45s\n",
      "516:\tlearn: 0.1008718\ttotal: 1m 52s\tremaining: 1m 45s\n",
      "517:\tlearn: 0.1008205\ttotal: 1m 53s\tremaining: 1m 45s\n",
      "518:\tlearn: 0.1007562\ttotal: 1m 53s\tremaining: 1m 44s\n",
      "519:\tlearn: 0.1006898\ttotal: 1m 53s\tremaining: 1m 44s\n",
      "520:\tlearn: 0.1006087\ttotal: 1m 53s\tremaining: 1m 44s\n",
      "521:\tlearn: 0.1005231\ttotal: 1m 53s\tremaining: 1m 44s\n",
      "522:\tlearn: 0.1004919\ttotal: 1m 54s\tremaining: 1m 44s\n",
      "523:\tlearn: 0.1004559\ttotal: 1m 54s\tremaining: 1m 43s\n",
      "524:\tlearn: 0.1003870\ttotal: 1m 54s\tremaining: 1m 43s\n",
      "525:\tlearn: 0.1003661\ttotal: 1m 54s\tremaining: 1m 43s\n",
      "526:\tlearn: 0.1002724\ttotal: 1m 55s\tremaining: 1m 43s\n",
      "527:\tlearn: 0.1002385\ttotal: 1m 55s\tremaining: 1m 42s\n",
      "528:\tlearn: 0.1001705\ttotal: 1m 55s\tremaining: 1m 42s\n",
      "529:\tlearn: 0.1001355\ttotal: 1m 55s\tremaining: 1m 42s\n",
      "530:\tlearn: 0.1000918\ttotal: 1m 55s\tremaining: 1m 42s\n",
      "531:\tlearn: 0.1000478\ttotal: 1m 56s\tremaining: 1m 42s\n",
      "532:\tlearn: 0.0999447\ttotal: 1m 56s\tremaining: 1m 41s\n",
      "533:\tlearn: 0.0998725\ttotal: 1m 56s\tremaining: 1m 41s\n",
      "534:\tlearn: 0.0998280\ttotal: 1m 56s\tremaining: 1m 41s\n",
      "535:\tlearn: 0.0997628\ttotal: 1m 56s\tremaining: 1m 41s\n",
      "536:\tlearn: 0.0997143\ttotal: 1m 57s\tremaining: 1m 41s\n",
      "537:\tlearn: 0.0996597\ttotal: 1m 57s\tremaining: 1m 40s\n",
      "538:\tlearn: 0.0995946\ttotal: 1m 57s\tremaining: 1m 40s\n",
      "539:\tlearn: 0.0995612\ttotal: 1m 57s\tremaining: 1m 40s\n",
      "540:\tlearn: 0.0995495\ttotal: 1m 58s\tremaining: 1m 40s\n",
      "541:\tlearn: 0.0994632\ttotal: 1m 58s\tremaining: 1m 39s\n",
      "542:\tlearn: 0.0993473\ttotal: 1m 58s\tremaining: 1m 39s\n",
      "543:\tlearn: 0.0992979\ttotal: 1m 58s\tremaining: 1m 39s\n",
      "544:\tlearn: 0.0992413\ttotal: 1m 58s\tremaining: 1m 39s\n",
      "545:\tlearn: 0.0991980\ttotal: 1m 59s\tremaining: 1m 39s\n",
      "546:\tlearn: 0.0991309\ttotal: 1m 59s\tremaining: 1m 38s\n",
      "547:\tlearn: 0.0990756\ttotal: 1m 59s\tremaining: 1m 38s\n",
      "548:\tlearn: 0.0990159\ttotal: 1m 59s\tremaining: 1m 38s\n",
      "549:\tlearn: 0.0989537\ttotal: 2m\tremaining: 1m 38s\n",
      "550:\tlearn: 0.0989303\ttotal: 2m\tremaining: 1m 38s\n",
      "551:\tlearn: 0.0988790\ttotal: 2m\tremaining: 1m 37s\n",
      "552:\tlearn: 0.0988536\ttotal: 2m\tremaining: 1m 37s\n",
      "553:\tlearn: 0.0988144\ttotal: 2m 1s\tremaining: 1m 37s\n",
      "554:\tlearn: 0.0987887\ttotal: 2m 1s\tremaining: 1m 37s\n",
      "555:\tlearn: 0.0987559\ttotal: 2m 1s\tremaining: 1m 37s\n",
      "556:\tlearn: 0.0986912\ttotal: 2m 1s\tremaining: 1m 36s\n",
      "557:\tlearn: 0.0986322\ttotal: 2m 1s\tremaining: 1m 36s\n",
      "558:\tlearn: 0.0985796\ttotal: 2m 2s\tremaining: 1m 36s\n",
      "559:\tlearn: 0.0985318\ttotal: 2m 2s\tremaining: 1m 36s\n",
      "560:\tlearn: 0.0984966\ttotal: 2m 2s\tremaining: 1m 35s\n",
      "561:\tlearn: 0.0984506\ttotal: 2m 2s\tremaining: 1m 35s\n",
      "562:\tlearn: 0.0984250\ttotal: 2m 3s\tremaining: 1m 35s\n",
      "563:\tlearn: 0.0984075\ttotal: 2m 3s\tremaining: 1m 35s\n",
      "564:\tlearn: 0.0983776\ttotal: 2m 3s\tremaining: 1m 35s\n",
      "565:\tlearn: 0.0983218\ttotal: 2m 3s\tremaining: 1m 34s\n",
      "566:\tlearn: 0.0982984\ttotal: 2m 3s\tremaining: 1m 34s\n",
      "567:\tlearn: 0.0982492\ttotal: 2m 4s\tremaining: 1m 34s\n",
      "568:\tlearn: 0.0982041\ttotal: 2m 4s\tremaining: 1m 34s\n",
      "569:\tlearn: 0.0981126\ttotal: 2m 4s\tremaining: 1m 34s\n",
      "570:\tlearn: 0.0980219\ttotal: 2m 4s\tremaining: 1m 33s\n",
      "571:\tlearn: 0.0979527\ttotal: 2m 5s\tremaining: 1m 33s\n",
      "572:\tlearn: 0.0979151\ttotal: 2m 5s\tremaining: 1m 33s\n",
      "573:\tlearn: 0.0978584\ttotal: 2m 5s\tremaining: 1m 33s\n",
      "574:\tlearn: 0.0978256\ttotal: 2m 5s\tremaining: 1m 32s\n",
      "575:\tlearn: 0.0977827\ttotal: 2m 5s\tremaining: 1m 32s\n",
      "576:\tlearn: 0.0977385\ttotal: 2m 6s\tremaining: 1m 32s\n",
      "577:\tlearn: 0.0976886\ttotal: 2m 6s\tremaining: 1m 32s\n",
      "578:\tlearn: 0.0976511\ttotal: 2m 6s\tremaining: 1m 32s\n",
      "579:\tlearn: 0.0975702\ttotal: 2m 6s\tremaining: 1m 31s\n",
      "580:\tlearn: 0.0975247\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "581:\tlearn: 0.0974919\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "582:\tlearn: 0.0974504\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "583:\tlearn: 0.0974280\ttotal: 2m 7s\tremaining: 1m 31s\n",
      "584:\tlearn: 0.0973517\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "585:\tlearn: 0.0973324\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "586:\tlearn: 0.0972901\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "587:\tlearn: 0.0972608\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "588:\tlearn: 0.0972253\ttotal: 2m 8s\tremaining: 1m 30s\n",
      "589:\tlearn: 0.0971786\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "590:\tlearn: 0.0971491\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "591:\tlearn: 0.0970648\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "592:\tlearn: 0.0970025\ttotal: 2m 9s\tremaining: 1m 29s\n",
      "593:\tlearn: 0.0969692\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "594:\tlearn: 0.0969240\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "595:\tlearn: 0.0968893\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "596:\tlearn: 0.0968443\ttotal: 2m 10s\tremaining: 1m 28s\n",
      "597:\tlearn: 0.0968117\ttotal: 2m 11s\tremaining: 1m 28s\n",
      "598:\tlearn: 0.0967651\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "599:\tlearn: 0.0967157\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "600:\tlearn: 0.0966760\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "601:\tlearn: 0.0966385\ttotal: 2m 11s\tremaining: 1m 27s\n",
      "602:\tlearn: 0.0965857\ttotal: 2m 12s\tremaining: 1m 27s\n",
      "603:\tlearn: 0.0965266\ttotal: 2m 12s\tremaining: 1m 26s\n",
      "604:\tlearn: 0.0964627\ttotal: 2m 12s\tremaining: 1m 26s\n",
      "605:\tlearn: 0.0964139\ttotal: 2m 12s\tremaining: 1m 26s\n",
      "606:\tlearn: 0.0963688\ttotal: 2m 13s\tremaining: 1m 26s\n",
      "607:\tlearn: 0.0963214\ttotal: 2m 13s\tremaining: 1m 26s\n",
      "608:\tlearn: 0.0962651\ttotal: 2m 13s\tremaining: 1m 25s\n",
      "609:\tlearn: 0.0962018\ttotal: 2m 13s\tremaining: 1m 25s\n",
      "610:\tlearn: 0.0961493\ttotal: 2m 14s\tremaining: 1m 25s\n",
      "611:\tlearn: 0.0960461\ttotal: 2m 14s\tremaining: 1m 25s\n",
      "612:\tlearn: 0.0959858\ttotal: 2m 14s\tremaining: 1m 24s\n",
      "613:\tlearn: 0.0959548\ttotal: 2m 14s\tremaining: 1m 24s\n",
      "614:\tlearn: 0.0958817\ttotal: 2m 15s\tremaining: 1m 24s\n",
      "615:\tlearn: 0.0958471\ttotal: 2m 15s\tremaining: 1m 24s\n",
      "616:\tlearn: 0.0958108\ttotal: 2m 15s\tremaining: 1m 24s\n",
      "617:\tlearn: 0.0957435\ttotal: 2m 15s\tremaining: 1m 23s\n",
      "618:\tlearn: 0.0956822\ttotal: 2m 15s\tremaining: 1m 23s\n",
      "619:\tlearn: 0.0956442\ttotal: 2m 16s\tremaining: 1m 23s\n",
      "620:\tlearn: 0.0956049\ttotal: 2m 16s\tremaining: 1m 23s\n",
      "621:\tlearn: 0.0955642\ttotal: 2m 16s\tremaining: 1m 22s\n",
      "622:\tlearn: 0.0955249\ttotal: 2m 16s\tremaining: 1m 22s\n",
      "623:\tlearn: 0.0954525\ttotal: 2m 16s\tremaining: 1m 22s\n",
      "624:\tlearn: 0.0954097\ttotal: 2m 17s\tremaining: 1m 22s\n",
      "625:\tlearn: 0.0953840\ttotal: 2m 17s\tremaining: 1m 22s\n",
      "626:\tlearn: 0.0953475\ttotal: 2m 17s\tremaining: 1m 21s\n",
      "627:\tlearn: 0.0953066\ttotal: 2m 17s\tremaining: 1m 21s\n",
      "628:\tlearn: 0.0952539\ttotal: 2m 18s\tremaining: 1m 21s\n",
      "629:\tlearn: 0.0951878\ttotal: 2m 18s\tremaining: 1m 21s\n",
      "630:\tlearn: 0.0951309\ttotal: 2m 18s\tremaining: 1m 21s\n",
      "631:\tlearn: 0.0950807\ttotal: 2m 18s\tremaining: 1m 20s\n",
      "632:\tlearn: 0.0950756\ttotal: 2m 18s\tremaining: 1m 20s\n",
      "633:\tlearn: 0.0949994\ttotal: 2m 19s\tremaining: 1m 20s\n",
      "634:\tlearn: 0.0949223\ttotal: 2m 19s\tremaining: 1m 20s\n",
      "635:\tlearn: 0.0948819\ttotal: 2m 19s\tremaining: 1m 19s\n",
      "636:\tlearn: 0.0948540\ttotal: 2m 19s\tremaining: 1m 19s\n",
      "637:\tlearn: 0.0948104\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "638:\tlearn: 0.0947818\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "639:\tlearn: 0.0947307\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "640:\tlearn: 0.0946773\ttotal: 2m 20s\tremaining: 1m 18s\n",
      "641:\tlearn: 0.0946293\ttotal: 2m 20s\tremaining: 1m 18s\n",
      "642:\tlearn: 0.0945975\ttotal: 2m 21s\tremaining: 1m 18s\n",
      "643:\tlearn: 0.0945727\ttotal: 2m 21s\tremaining: 1m 18s\n",
      "644:\tlearn: 0.0945225\ttotal: 2m 21s\tremaining: 1m 17s\n",
      "645:\tlearn: 0.0944866\ttotal: 2m 21s\tremaining: 1m 17s\n",
      "646:\tlearn: 0.0943776\ttotal: 2m 21s\tremaining: 1m 17s\n",
      "647:\tlearn: 0.0943070\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "648:\tlearn: 0.0942486\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "649:\tlearn: 0.0942284\ttotal: 2m 22s\tremaining: 1m 16s\n",
      "650:\tlearn: 0.0941975\ttotal: 2m 22s\tremaining: 1m 16s\n",
      "651:\tlearn: 0.0941542\ttotal: 2m 23s\tremaining: 1m 16s\n",
      "652:\tlearn: 0.0941064\ttotal: 2m 23s\tremaining: 1m 16s\n",
      "653:\tlearn: 0.0940614\ttotal: 2m 23s\tremaining: 1m 15s\n",
      "654:\tlearn: 0.0940168\ttotal: 2m 23s\tremaining: 1m 15s\n",
      "655:\tlearn: 0.0940054\ttotal: 2m 23s\tremaining: 1m 15s\n",
      "656:\tlearn: 0.0939574\ttotal: 2m 24s\tremaining: 1m 15s\n",
      "657:\tlearn: 0.0939349\ttotal: 2m 24s\tremaining: 1m 15s\n",
      "658:\tlearn: 0.0939082\ttotal: 2m 24s\tremaining: 1m 14s\n",
      "659:\tlearn: 0.0938723\ttotal: 2m 24s\tremaining: 1m 14s\n",
      "660:\tlearn: 0.0937874\ttotal: 2m 24s\tremaining: 1m 14s\n",
      "661:\tlearn: 0.0937563\ttotal: 2m 25s\tremaining: 1m 14s\n",
      "662:\tlearn: 0.0936855\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "663:\tlearn: 0.0936607\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "664:\tlearn: 0.0936137\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "665:\tlearn: 0.0935580\ttotal: 2m 25s\tremaining: 1m 13s\n",
      "666:\tlearn: 0.0935082\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "667:\tlearn: 0.0934761\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "668:\tlearn: 0.0934584\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "669:\tlearn: 0.0933905\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "670:\tlearn: 0.0933170\ttotal: 2m 26s\tremaining: 1m 12s\n",
      "671:\tlearn: 0.0933066\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "672:\tlearn: 0.0932581\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "673:\tlearn: 0.0932296\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "674:\tlearn: 0.0931749\ttotal: 2m 27s\tremaining: 1m 11s\n",
      "675:\tlearn: 0.0931440\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "676:\tlearn: 0.0931091\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "677:\tlearn: 0.0930874\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "678:\tlearn: 0.0930178\ttotal: 2m 28s\tremaining: 1m 10s\n",
      "679:\tlearn: 0.0929678\ttotal: 2m 29s\tremaining: 1m 10s\n",
      "680:\tlearn: 0.0929154\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "681:\tlearn: 0.0928491\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "682:\tlearn: 0.0927948\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "683:\tlearn: 0.0927503\ttotal: 2m 29s\tremaining: 1m 9s\n",
      "684:\tlearn: 0.0926740\ttotal: 2m 30s\tremaining: 1m 9s\n",
      "685:\tlearn: 0.0926498\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "686:\tlearn: 0.0926135\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "687:\tlearn: 0.0926031\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "688:\tlearn: 0.0925435\ttotal: 2m 30s\tremaining: 1m 8s\n",
      "689:\tlearn: 0.0924545\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "690:\tlearn: 0.0923913\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "691:\tlearn: 0.0923313\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "692:\tlearn: 0.0923037\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "693:\tlearn: 0.0922241\ttotal: 2m 31s\tremaining: 1m 7s\n",
      "694:\tlearn: 0.0921737\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "695:\tlearn: 0.0921313\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "696:\tlearn: 0.0920913\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "697:\tlearn: 0.0920002\ttotal: 2m 32s\tremaining: 1m 6s\n",
      "698:\tlearn: 0.0919696\ttotal: 2m 32s\tremaining: 1m 5s\n",
      "699:\tlearn: 0.0919220\ttotal: 2m 33s\tremaining: 1m 5s\n",
      "700:\tlearn: 0.0918898\ttotal: 2m 33s\tremaining: 1m 5s\n",
      "701:\tlearn: 0.0918412\ttotal: 2m 33s\tremaining: 1m 5s\n",
      "702:\tlearn: 0.0918135\ttotal: 2m 33s\tremaining: 1m 4s\n",
      "703:\tlearn: 0.0917849\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "704:\tlearn: 0.0917332\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "705:\tlearn: 0.0916763\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "706:\tlearn: 0.0916397\ttotal: 2m 34s\tremaining: 1m 4s\n",
      "707:\tlearn: 0.0915723\ttotal: 2m 35s\tremaining: 1m 3s\n",
      "708:\tlearn: 0.0915204\ttotal: 2m 35s\tremaining: 1m 3s\n",
      "709:\tlearn: 0.0914543\ttotal: 2m 35s\tremaining: 1m 3s\n",
      "710:\tlearn: 0.0914050\ttotal: 2m 35s\tremaining: 1m 3s\n",
      "711:\tlearn: 0.0913579\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "712:\tlearn: 0.0913434\ttotal: 2m 36s\tremaining: 1m 2s\n",
      "713:\tlearn: 0.0913209\ttotal: 2m 36s\tremaining: 1m 2s\n",
      "714:\tlearn: 0.0912721\ttotal: 2m 36s\tremaining: 1m 2s\n",
      "715:\tlearn: 0.0912417\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "716:\tlearn: 0.0912333\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "717:\tlearn: 0.0912033\ttotal: 2m 37s\tremaining: 1m 1s\n",
      "718:\tlearn: 0.0911777\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "719:\tlearn: 0.0910976\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "720:\tlearn: 0.0910683\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "721:\tlearn: 0.0910613\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "722:\tlearn: 0.0910580\ttotal: 2m 39s\tremaining: 1m\n",
      "723:\tlearn: 0.0910056\ttotal: 2m 39s\tremaining: 1m\n",
      "724:\tlearn: 0.0909437\ttotal: 2m 39s\tremaining: 1m\n",
      "725:\tlearn: 0.0908885\ttotal: 2m 39s\tremaining: 1m\n",
      "726:\tlearn: 0.0908320\ttotal: 2m 39s\tremaining: 1m\n",
      "727:\tlearn: 0.0907801\ttotal: 2m 40s\tremaining: 59.8s\n",
      "728:\tlearn: 0.0907446\ttotal: 2m 40s\tremaining: 59.6s\n",
      "729:\tlearn: 0.0907113\ttotal: 2m 40s\tremaining: 59.4s\n",
      "730:\tlearn: 0.0907004\ttotal: 2m 40s\tremaining: 59.2s\n",
      "731:\tlearn: 0.0906534\ttotal: 2m 40s\tremaining: 58.9s\n",
      "732:\tlearn: 0.0906386\ttotal: 2m 41s\tremaining: 58.7s\n",
      "733:\tlearn: 0.0906057\ttotal: 2m 41s\tremaining: 58.5s\n",
      "734:\tlearn: 0.0905461\ttotal: 2m 41s\tremaining: 58.3s\n",
      "735:\tlearn: 0.0904925\ttotal: 2m 41s\tremaining: 58.1s\n",
      "736:\tlearn: 0.0904564\ttotal: 2m 42s\tremaining: 57.9s\n",
      "737:\tlearn: 0.0904093\ttotal: 2m 42s\tremaining: 57.7s\n",
      "738:\tlearn: 0.0903736\ttotal: 2m 42s\tremaining: 57.5s\n",
      "739:\tlearn: 0.0903399\ttotal: 2m 43s\tremaining: 57.3s\n",
      "740:\tlearn: 0.0902816\ttotal: 2m 43s\tremaining: 57.1s\n",
      "741:\tlearn: 0.0902389\ttotal: 2m 43s\tremaining: 56.9s\n",
      "742:\tlearn: 0.0901857\ttotal: 2m 43s\tremaining: 56.7s\n",
      "743:\tlearn: 0.0901400\ttotal: 2m 43s\tremaining: 56.4s\n",
      "744:\tlearn: 0.0900839\ttotal: 2m 44s\tremaining: 56.2s\n",
      "745:\tlearn: 0.0900594\ttotal: 2m 44s\tremaining: 56s\n",
      "746:\tlearn: 0.0900343\ttotal: 2m 44s\tremaining: 55.8s\n",
      "747:\tlearn: 0.0899951\ttotal: 2m 45s\tremaining: 55.6s\n",
      "748:\tlearn: 0.0899356\ttotal: 2m 45s\tremaining: 55.4s\n",
      "749:\tlearn: 0.0898694\ttotal: 2m 45s\tremaining: 55.2s\n",
      "750:\tlearn: 0.0898313\ttotal: 2m 45s\tremaining: 55s\n",
      "751:\tlearn: 0.0897634\ttotal: 2m 46s\tremaining: 54.8s\n",
      "752:\tlearn: 0.0897437\ttotal: 2m 46s\tremaining: 54.6s\n",
      "753:\tlearn: 0.0897164\ttotal: 2m 46s\tremaining: 54.4s\n",
      "754:\tlearn: 0.0896841\ttotal: 2m 46s\tremaining: 54.1s\n",
      "755:\tlearn: 0.0896625\ttotal: 2m 47s\tremaining: 53.9s\n",
      "756:\tlearn: 0.0896144\ttotal: 2m 47s\tremaining: 53.7s\n",
      "757:\tlearn: 0.0895638\ttotal: 2m 47s\tremaining: 53.4s\n",
      "758:\tlearn: 0.0894872\ttotal: 2m 47s\tremaining: 53.2s\n",
      "759:\tlearn: 0.0894312\ttotal: 2m 47s\tremaining: 53s\n",
      "760:\tlearn: 0.0893939\ttotal: 2m 48s\tremaining: 52.8s\n",
      "761:\tlearn: 0.0893534\ttotal: 2m 48s\tremaining: 52.6s\n",
      "762:\tlearn: 0.0893232\ttotal: 2m 48s\tremaining: 52.4s\n",
      "763:\tlearn: 0.0892903\ttotal: 2m 48s\tremaining: 52.1s\n",
      "764:\tlearn: 0.0892379\ttotal: 2m 49s\tremaining: 51.9s\n",
      "765:\tlearn: 0.0892109\ttotal: 2m 49s\tremaining: 51.7s\n",
      "766:\tlearn: 0.0891376\ttotal: 2m 49s\tremaining: 51.5s\n",
      "767:\tlearn: 0.0890740\ttotal: 2m 49s\tremaining: 51.3s\n",
      "768:\tlearn: 0.0890542\ttotal: 2m 49s\tremaining: 51.1s\n",
      "769:\tlearn: 0.0890221\ttotal: 2m 50s\tremaining: 50.8s\n",
      "770:\tlearn: 0.0889753\ttotal: 2m 50s\tremaining: 50.6s\n",
      "771:\tlearn: 0.0889388\ttotal: 2m 50s\tremaining: 50.4s\n",
      "772:\tlearn: 0.0889161\ttotal: 2m 50s\tremaining: 50.2s\n",
      "773:\tlearn: 0.0888991\ttotal: 2m 50s\tremaining: 49.9s\n",
      "774:\tlearn: 0.0888610\ttotal: 2m 51s\tremaining: 49.7s\n",
      "775:\tlearn: 0.0888141\ttotal: 2m 51s\tremaining: 49.5s\n",
      "776:\tlearn: 0.0887563\ttotal: 2m 51s\tremaining: 49.3s\n",
      "777:\tlearn: 0.0887093\ttotal: 2m 51s\tremaining: 49s\n",
      "778:\tlearn: 0.0886822\ttotal: 2m 52s\tremaining: 48.8s\n",
      "779:\tlearn: 0.0886523\ttotal: 2m 52s\tremaining: 48.6s\n",
      "780:\tlearn: 0.0886350\ttotal: 2m 52s\tremaining: 48.4s\n",
      "781:\tlearn: 0.0885895\ttotal: 2m 52s\tremaining: 48.1s\n",
      "782:\tlearn: 0.0885271\ttotal: 2m 52s\tremaining: 47.9s\n",
      "783:\tlearn: 0.0885209\ttotal: 2m 53s\tremaining: 47.7s\n",
      "784:\tlearn: 0.0884930\ttotal: 2m 53s\tremaining: 47.5s\n",
      "785:\tlearn: 0.0884661\ttotal: 2m 53s\tremaining: 47.3s\n",
      "786:\tlearn: 0.0884088\ttotal: 2m 53s\tremaining: 47.1s\n",
      "787:\tlearn: 0.0883601\ttotal: 2m 54s\tremaining: 46.8s\n",
      "788:\tlearn: 0.0883340\ttotal: 2m 54s\tremaining: 46.6s\n",
      "789:\tlearn: 0.0883041\ttotal: 2m 54s\tremaining: 46.4s\n",
      "790:\tlearn: 0.0882821\ttotal: 2m 54s\tremaining: 46.2s\n",
      "791:\tlearn: 0.0882297\ttotal: 2m 54s\tremaining: 45.9s\n",
      "792:\tlearn: 0.0881843\ttotal: 2m 55s\tremaining: 45.7s\n",
      "793:\tlearn: 0.0881483\ttotal: 2m 55s\tremaining: 45.5s\n",
      "794:\tlearn: 0.0881190\ttotal: 2m 55s\tremaining: 45.3s\n",
      "795:\tlearn: 0.0880615\ttotal: 2m 55s\tremaining: 45.1s\n",
      "796:\tlearn: 0.0880357\ttotal: 2m 56s\tremaining: 44.8s\n",
      "797:\tlearn: 0.0879877\ttotal: 2m 56s\tremaining: 44.6s\n",
      "798:\tlearn: 0.0879389\ttotal: 2m 56s\tremaining: 44.4s\n",
      "799:\tlearn: 0.0879126\ttotal: 2m 56s\tremaining: 44.2s\n",
      "800:\tlearn: 0.0878743\ttotal: 2m 56s\tremaining: 44s\n",
      "801:\tlearn: 0.0878421\ttotal: 2m 57s\tremaining: 43.7s\n",
      "802:\tlearn: 0.0877998\ttotal: 2m 57s\tremaining: 43.5s\n",
      "803:\tlearn: 0.0877607\ttotal: 2m 57s\tremaining: 43.3s\n",
      "804:\tlearn: 0.0877194\ttotal: 2m 57s\tremaining: 43.1s\n",
      "805:\tlearn: 0.0876987\ttotal: 2m 58s\tremaining: 42.9s\n",
      "806:\tlearn: 0.0876496\ttotal: 2m 58s\tremaining: 42.7s\n",
      "807:\tlearn: 0.0875961\ttotal: 2m 58s\tremaining: 42.4s\n",
      "808:\tlearn: 0.0875846\ttotal: 2m 58s\tremaining: 42.2s\n",
      "809:\tlearn: 0.0875410\ttotal: 2m 59s\tremaining: 42s\n",
      "810:\tlearn: 0.0875086\ttotal: 2m 59s\tremaining: 41.8s\n",
      "811:\tlearn: 0.0874859\ttotal: 2m 59s\tremaining: 41.6s\n",
      "812:\tlearn: 0.0874776\ttotal: 2m 59s\tremaining: 41.4s\n",
      "813:\tlearn: 0.0874328\ttotal: 3m\tremaining: 41.1s\n",
      "814:\tlearn: 0.0874221\ttotal: 3m\tremaining: 40.9s\n",
      "815:\tlearn: 0.0873882\ttotal: 3m\tremaining: 40.7s\n",
      "816:\tlearn: 0.0873755\ttotal: 3m\tremaining: 40.5s\n",
      "817:\tlearn: 0.0873271\ttotal: 3m\tremaining: 40.2s\n",
      "818:\tlearn: 0.0872716\ttotal: 3m 1s\tremaining: 40s\n",
      "819:\tlearn: 0.0872262\ttotal: 3m 1s\tremaining: 39.8s\n",
      "820:\tlearn: 0.0871416\ttotal: 3m 1s\tremaining: 39.6s\n",
      "821:\tlearn: 0.0871157\ttotal: 3m 1s\tremaining: 39.4s\n",
      "822:\tlearn: 0.0870892\ttotal: 3m 2s\tremaining: 39.1s\n",
      "823:\tlearn: 0.0870714\ttotal: 3m 2s\tremaining: 38.9s\n",
      "824:\tlearn: 0.0870167\ttotal: 3m 2s\tremaining: 38.7s\n",
      "825:\tlearn: 0.0869802\ttotal: 3m 2s\tremaining: 38.5s\n",
      "826:\tlearn: 0.0869322\ttotal: 3m 2s\tremaining: 38.2s\n",
      "827:\tlearn: 0.0868995\ttotal: 3m 3s\tremaining: 38s\n",
      "828:\tlearn: 0.0868647\ttotal: 3m 3s\tremaining: 37.8s\n",
      "829:\tlearn: 0.0868017\ttotal: 3m 3s\tremaining: 37.6s\n",
      "830:\tlearn: 0.0867076\ttotal: 3m 3s\tremaining: 37.4s\n",
      "831:\tlearn: 0.0866575\ttotal: 3m 3s\tremaining: 37.1s\n",
      "832:\tlearn: 0.0866202\ttotal: 3m 4s\tremaining: 36.9s\n",
      "833:\tlearn: 0.0865742\ttotal: 3m 4s\tremaining: 36.7s\n",
      "834:\tlearn: 0.0865154\ttotal: 3m 4s\tremaining: 36.5s\n",
      "835:\tlearn: 0.0864818\ttotal: 3m 4s\tremaining: 36.3s\n",
      "836:\tlearn: 0.0864578\ttotal: 3m 5s\tremaining: 36s\n",
      "837:\tlearn: 0.0864088\ttotal: 3m 5s\tremaining: 35.8s\n",
      "838:\tlearn: 0.0863691\ttotal: 3m 5s\tremaining: 35.6s\n",
      "839:\tlearn: 0.0863193\ttotal: 3m 5s\tremaining: 35.4s\n",
      "840:\tlearn: 0.0862699\ttotal: 3m 5s\tremaining: 35.2s\n",
      "841:\tlearn: 0.0862616\ttotal: 3m 6s\tremaining: 34.9s\n",
      "842:\tlearn: 0.0862586\ttotal: 3m 6s\tremaining: 34.7s\n",
      "843:\tlearn: 0.0862084\ttotal: 3m 6s\tremaining: 34.5s\n",
      "844:\tlearn: 0.0861771\ttotal: 3m 6s\tremaining: 34.3s\n",
      "845:\tlearn: 0.0861400\ttotal: 3m 7s\tremaining: 34.1s\n",
      "846:\tlearn: 0.0860826\ttotal: 3m 7s\tremaining: 33.8s\n",
      "847:\tlearn: 0.0860240\ttotal: 3m 7s\tremaining: 33.6s\n",
      "848:\tlearn: 0.0860063\ttotal: 3m 7s\tremaining: 33.4s\n",
      "849:\tlearn: 0.0859726\ttotal: 3m 7s\tremaining: 33.2s\n",
      "850:\tlearn: 0.0859493\ttotal: 3m 8s\tremaining: 33s\n",
      "851:\tlearn: 0.0859395\ttotal: 3m 8s\tremaining: 32.7s\n",
      "852:\tlearn: 0.0858956\ttotal: 3m 8s\tremaining: 32.5s\n",
      "853:\tlearn: 0.0858772\ttotal: 3m 8s\tremaining: 32.3s\n",
      "854:\tlearn: 0.0858396\ttotal: 3m 9s\tremaining: 32.1s\n",
      "855:\tlearn: 0.0857966\ttotal: 3m 9s\tremaining: 31.9s\n",
      "856:\tlearn: 0.0857535\ttotal: 3m 9s\tremaining: 31.6s\n",
      "857:\tlearn: 0.0857333\ttotal: 3m 9s\tremaining: 31.4s\n",
      "858:\tlearn: 0.0857050\ttotal: 3m 10s\tremaining: 31.2s\n",
      "859:\tlearn: 0.0856777\ttotal: 3m 10s\tremaining: 31s\n",
      "860:\tlearn: 0.0856285\ttotal: 3m 10s\tremaining: 30.7s\n",
      "861:\tlearn: 0.0855752\ttotal: 3m 10s\tremaining: 30.5s\n",
      "862:\tlearn: 0.0855644\ttotal: 3m 10s\tremaining: 30.3s\n",
      "863:\tlearn: 0.0855201\ttotal: 3m 11s\tremaining: 30.1s\n",
      "864:\tlearn: 0.0854923\ttotal: 3m 11s\tremaining: 29.9s\n",
      "865:\tlearn: 0.0854490\ttotal: 3m 11s\tremaining: 29.6s\n",
      "866:\tlearn: 0.0853964\ttotal: 3m 11s\tremaining: 29.4s\n",
      "867:\tlearn: 0.0853584\ttotal: 3m 12s\tremaining: 29.2s\n",
      "868:\tlearn: 0.0853258\ttotal: 3m 12s\tremaining: 29s\n",
      "869:\tlearn: 0.0852871\ttotal: 3m 12s\tremaining: 28.8s\n",
      "870:\tlearn: 0.0852647\ttotal: 3m 12s\tremaining: 28.5s\n",
      "871:\tlearn: 0.0852396\ttotal: 3m 12s\tremaining: 28.3s\n",
      "872:\tlearn: 0.0851893\ttotal: 3m 13s\tremaining: 28.1s\n",
      "873:\tlearn: 0.0851474\ttotal: 3m 13s\tremaining: 27.9s\n",
      "874:\tlearn: 0.0850820\ttotal: 3m 13s\tremaining: 27.7s\n",
      "875:\tlearn: 0.0850554\ttotal: 3m 13s\tremaining: 27.5s\n",
      "876:\tlearn: 0.0850242\ttotal: 3m 14s\tremaining: 27.2s\n",
      "877:\tlearn: 0.0850063\ttotal: 3m 14s\tremaining: 27s\n",
      "878:\tlearn: 0.0849879\ttotal: 3m 14s\tremaining: 26.8s\n",
      "879:\tlearn: 0.0849744\ttotal: 3m 15s\tremaining: 26.6s\n",
      "880:\tlearn: 0.0849531\ttotal: 3m 15s\tremaining: 26.4s\n",
      "881:\tlearn: 0.0849037\ttotal: 3m 15s\tremaining: 26.2s\n",
      "882:\tlearn: 0.0848883\ttotal: 3m 15s\tremaining: 25.9s\n",
      "883:\tlearn: 0.0848512\ttotal: 3m 15s\tremaining: 25.7s\n",
      "884:\tlearn: 0.0848306\ttotal: 3m 16s\tremaining: 25.5s\n",
      "885:\tlearn: 0.0848088\ttotal: 3m 16s\tremaining: 25.3s\n",
      "886:\tlearn: 0.0847623\ttotal: 3m 16s\tremaining: 25s\n",
      "887:\tlearn: 0.0847373\ttotal: 3m 16s\tremaining: 24.8s\n",
      "888:\tlearn: 0.0847166\ttotal: 3m 17s\tremaining: 24.6s\n",
      "889:\tlearn: 0.0846666\ttotal: 3m 17s\tremaining: 24.4s\n",
      "890:\tlearn: 0.0846298\ttotal: 3m 17s\tremaining: 24.2s\n",
      "891:\tlearn: 0.0846034\ttotal: 3m 17s\tremaining: 23.9s\n",
      "892:\tlearn: 0.0845906\ttotal: 3m 17s\tremaining: 23.7s\n",
      "893:\tlearn: 0.0845630\ttotal: 3m 18s\tremaining: 23.5s\n",
      "894:\tlearn: 0.0845213\ttotal: 3m 18s\tremaining: 23.3s\n",
      "895:\tlearn: 0.0844956\ttotal: 3m 18s\tremaining: 23s\n",
      "896:\tlearn: 0.0844494\ttotal: 3m 18s\tremaining: 22.8s\n",
      "897:\tlearn: 0.0843964\ttotal: 3m 19s\tremaining: 22.6s\n",
      "898:\tlearn: 0.0842826\ttotal: 3m 19s\tremaining: 22.4s\n",
      "899:\tlearn: 0.0842346\ttotal: 3m 19s\tremaining: 22.2s\n",
      "900:\tlearn: 0.0842010\ttotal: 3m 19s\tremaining: 21.9s\n",
      "901:\tlearn: 0.0841797\ttotal: 3m 19s\tremaining: 21.7s\n",
      "902:\tlearn: 0.0841345\ttotal: 3m 20s\tremaining: 21.5s\n",
      "903:\tlearn: 0.0840689\ttotal: 3m 20s\tremaining: 21.3s\n",
      "904:\tlearn: 0.0840287\ttotal: 3m 20s\tremaining: 21s\n",
      "905:\tlearn: 0.0839992\ttotal: 3m 20s\tremaining: 20.8s\n",
      "906:\tlearn: 0.0839761\ttotal: 3m 20s\tremaining: 20.6s\n",
      "907:\tlearn: 0.0838896\ttotal: 3m 21s\tremaining: 20.4s\n",
      "908:\tlearn: 0.0838601\ttotal: 3m 21s\tremaining: 20.2s\n",
      "909:\tlearn: 0.0838289\ttotal: 3m 21s\tremaining: 19.9s\n",
      "910:\tlearn: 0.0837759\ttotal: 3m 21s\tremaining: 19.7s\n",
      "911:\tlearn: 0.0837326\ttotal: 3m 22s\tremaining: 19.5s\n",
      "912:\tlearn: 0.0837027\ttotal: 3m 22s\tremaining: 19.3s\n",
      "913:\tlearn: 0.0836539\ttotal: 3m 22s\tremaining: 19.1s\n",
      "914:\tlearn: 0.0836382\ttotal: 3m 22s\tremaining: 18.8s\n",
      "915:\tlearn: 0.0836196\ttotal: 3m 22s\tremaining: 18.6s\n",
      "916:\tlearn: 0.0835856\ttotal: 3m 23s\tremaining: 18.4s\n",
      "917:\tlearn: 0.0835179\ttotal: 3m 23s\tremaining: 18.2s\n",
      "918:\tlearn: 0.0834838\ttotal: 3m 23s\tremaining: 17.9s\n",
      "919:\tlearn: 0.0834461\ttotal: 3m 23s\tremaining: 17.7s\n",
      "920:\tlearn: 0.0834097\ttotal: 3m 24s\tremaining: 17.5s\n",
      "921:\tlearn: 0.0833811\ttotal: 3m 24s\tremaining: 17.3s\n",
      "922:\tlearn: 0.0833327\ttotal: 3m 24s\tremaining: 17.1s\n",
      "923:\tlearn: 0.0832785\ttotal: 3m 24s\tremaining: 16.8s\n",
      "924:\tlearn: 0.0832444\ttotal: 3m 24s\tremaining: 16.6s\n",
      "925:\tlearn: 0.0832241\ttotal: 3m 25s\tremaining: 16.4s\n",
      "926:\tlearn: 0.0831903\ttotal: 3m 25s\tremaining: 16.2s\n",
      "927:\tlearn: 0.0831405\ttotal: 3m 25s\tremaining: 15.9s\n",
      "928:\tlearn: 0.0831096\ttotal: 3m 25s\tremaining: 15.7s\n",
      "929:\tlearn: 0.0830633\ttotal: 3m 25s\tremaining: 15.5s\n",
      "930:\tlearn: 0.0830501\ttotal: 3m 26s\tremaining: 15.3s\n",
      "931:\tlearn: 0.0829842\ttotal: 3m 26s\tremaining: 15.1s\n",
      "932:\tlearn: 0.0829581\ttotal: 3m 26s\tremaining: 14.8s\n",
      "933:\tlearn: 0.0829297\ttotal: 3m 26s\tremaining: 14.6s\n",
      "934:\tlearn: 0.0828909\ttotal: 3m 27s\tremaining: 14.4s\n",
      "935:\tlearn: 0.0828723\ttotal: 3m 27s\tremaining: 14.2s\n",
      "936:\tlearn: 0.0828456\ttotal: 3m 27s\tremaining: 14s\n",
      "937:\tlearn: 0.0828258\ttotal: 3m 27s\tremaining: 13.7s\n",
      "938:\tlearn: 0.0827779\ttotal: 3m 28s\tremaining: 13.5s\n",
      "939:\tlearn: 0.0827542\ttotal: 3m 28s\tremaining: 13.3s\n",
      "940:\tlearn: 0.0827296\ttotal: 3m 28s\tremaining: 13.1s\n",
      "941:\tlearn: 0.0826805\ttotal: 3m 28s\tremaining: 12.9s\n",
      "942:\tlearn: 0.0826458\ttotal: 3m 28s\tremaining: 12.6s\n",
      "943:\tlearn: 0.0826143\ttotal: 3m 29s\tremaining: 12.4s\n",
      "944:\tlearn: 0.0825783\ttotal: 3m 29s\tremaining: 12.2s\n",
      "945:\tlearn: 0.0825603\ttotal: 3m 29s\tremaining: 12s\n",
      "946:\tlearn: 0.0825022\ttotal: 3m 29s\tremaining: 11.7s\n",
      "947:\tlearn: 0.0824593\ttotal: 3m 30s\tremaining: 11.5s\n",
      "948:\tlearn: 0.0824313\ttotal: 3m 30s\tremaining: 11.3s\n",
      "949:\tlearn: 0.0824188\ttotal: 3m 30s\tremaining: 11.1s\n",
      "950:\tlearn: 0.0823850\ttotal: 3m 30s\tremaining: 10.9s\n",
      "951:\tlearn: 0.0823665\ttotal: 3m 30s\tremaining: 10.6s\n",
      "952:\tlearn: 0.0823222\ttotal: 3m 31s\tremaining: 10.4s\n",
      "953:\tlearn: 0.0822944\ttotal: 3m 31s\tremaining: 10.2s\n",
      "954:\tlearn: 0.0822668\ttotal: 3m 31s\tremaining: 9.97s\n",
      "955:\tlearn: 0.0822585\ttotal: 3m 31s\tremaining: 9.75s\n",
      "956:\tlearn: 0.0822294\ttotal: 3m 32s\tremaining: 9.53s\n",
      "957:\tlearn: 0.0821618\ttotal: 3m 32s\tremaining: 9.31s\n",
      "958:\tlearn: 0.0820893\ttotal: 3m 32s\tremaining: 9.09s\n",
      "959:\tlearn: 0.0820588\ttotal: 3m 32s\tremaining: 8.86s\n",
      "960:\tlearn: 0.0820177\ttotal: 3m 32s\tremaining: 8.64s\n",
      "961:\tlearn: 0.0819605\ttotal: 3m 33s\tremaining: 8.42s\n",
      "962:\tlearn: 0.0819116\ttotal: 3m 33s\tremaining: 8.2s\n",
      "963:\tlearn: 0.0818883\ttotal: 3m 33s\tremaining: 7.98s\n",
      "964:\tlearn: 0.0818723\ttotal: 3m 33s\tremaining: 7.76s\n",
      "965:\tlearn: 0.0818367\ttotal: 3m 34s\tremaining: 7.53s\n",
      "966:\tlearn: 0.0818017\ttotal: 3m 34s\tremaining: 7.31s\n",
      "967:\tlearn: 0.0817427\ttotal: 3m 34s\tremaining: 7.09s\n",
      "968:\tlearn: 0.0816828\ttotal: 3m 34s\tremaining: 6.87s\n",
      "969:\tlearn: 0.0816391\ttotal: 3m 34s\tremaining: 6.65s\n",
      "970:\tlearn: 0.0815966\ttotal: 3m 35s\tremaining: 6.43s\n",
      "971:\tlearn: 0.0815650\ttotal: 3m 35s\tremaining: 6.2s\n",
      "972:\tlearn: 0.0815382\ttotal: 3m 35s\tremaining: 5.98s\n",
      "973:\tlearn: 0.0815174\ttotal: 3m 35s\tremaining: 5.76s\n",
      "974:\tlearn: 0.0815014\ttotal: 3m 36s\tremaining: 5.54s\n",
      "975:\tlearn: 0.0814670\ttotal: 3m 36s\tremaining: 5.32s\n",
      "976:\tlearn: 0.0814382\ttotal: 3m 36s\tremaining: 5.1s\n",
      "977:\tlearn: 0.0814254\ttotal: 3m 36s\tremaining: 4.88s\n",
      "978:\tlearn: 0.0813786\ttotal: 3m 36s\tremaining: 4.65s\n",
      "979:\tlearn: 0.0813265\ttotal: 3m 37s\tremaining: 4.43s\n",
      "980:\tlearn: 0.0813181\ttotal: 3m 37s\tremaining: 4.21s\n",
      "981:\tlearn: 0.0812652\ttotal: 3m 37s\tremaining: 3.99s\n",
      "982:\tlearn: 0.0812404\ttotal: 3m 37s\tremaining: 3.77s\n",
      "983:\tlearn: 0.0811827\ttotal: 3m 37s\tremaining: 3.54s\n",
      "984:\tlearn: 0.0811692\ttotal: 3m 38s\tremaining: 3.32s\n",
      "985:\tlearn: 0.0811328\ttotal: 3m 38s\tremaining: 3.1s\n",
      "986:\tlearn: 0.0811072\ttotal: 3m 38s\tremaining: 2.88s\n",
      "987:\tlearn: 0.0810901\ttotal: 3m 38s\tremaining: 2.66s\n",
      "988:\tlearn: 0.0810618\ttotal: 3m 39s\tremaining: 2.44s\n",
      "989:\tlearn: 0.0810492\ttotal: 3m 39s\tremaining: 2.21s\n",
      "990:\tlearn: 0.0810135\ttotal: 3m 39s\tremaining: 1.99s\n",
      "991:\tlearn: 0.0809598\ttotal: 3m 39s\tremaining: 1.77s\n",
      "992:\tlearn: 0.0809148\ttotal: 3m 39s\tremaining: 1.55s\n",
      "993:\tlearn: 0.0808622\ttotal: 3m 40s\tremaining: 1.33s\n",
      "994:\tlearn: 0.0808157\ttotal: 3m 40s\tremaining: 1.11s\n",
      "995:\tlearn: 0.0808001\ttotal: 3m 40s\tremaining: 886ms\n",
      "996:\tlearn: 0.0807514\ttotal: 3m 40s\tremaining: 664ms\n",
      "997:\tlearn: 0.0807074\ttotal: 3m 40s\tremaining: 443ms\n",
      "998:\tlearn: 0.0806753\ttotal: 3m 41s\tremaining: 221ms\n",
      "999:\tlearn: 0.0806480\ttotal: 3m 41s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melody/Documents/Projects/Classification-brief/brief_classif_sba/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:241: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5226098\ttotal: 215ms\tremaining: 3m 34s\n",
      "1:\tlearn: 0.4253277\ttotal: 438ms\tremaining: 3m 38s\n",
      "2:\tlearn: 0.3583489\ttotal: 709ms\tremaining: 3m 55s\n",
      "3:\tlearn: 0.3133580\ttotal: 923ms\tremaining: 3m 49s\n",
      "4:\tlearn: 0.2917418\ttotal: 1.16s\tremaining: 3m 50s\n",
      "5:\tlearn: 0.2724626\ttotal: 1.37s\tremaining: 3m 47s\n",
      "6:\tlearn: 0.2524285\ttotal: 1.6s\tremaining: 3m 47s\n",
      "7:\tlearn: 0.2419028\ttotal: 1.8s\tremaining: 3m 43s\n",
      "8:\tlearn: 0.2313854\ttotal: 2.13s\tremaining: 3m 54s\n",
      "9:\tlearn: 0.2243781\ttotal: 2.37s\tremaining: 3m 54s\n",
      "10:\tlearn: 0.2186558\ttotal: 2.66s\tremaining: 3m 59s\n",
      "11:\tlearn: 0.2141134\ttotal: 2.89s\tremaining: 3m 58s\n",
      "12:\tlearn: 0.2100277\ttotal: 3.13s\tremaining: 3m 57s\n",
      "13:\tlearn: 0.2072236\ttotal: 3.35s\tremaining: 3m 55s\n",
      "14:\tlearn: 0.2031802\ttotal: 3.6s\tremaining: 3m 56s\n",
      "15:\tlearn: 0.2000097\ttotal: 3.81s\tremaining: 3m 54s\n",
      "16:\tlearn: 0.1962160\ttotal: 4.02s\tremaining: 3m 52s\n",
      "17:\tlearn: 0.1927768\ttotal: 4.24s\tremaining: 3m 51s\n",
      "18:\tlearn: 0.1908606\ttotal: 4.49s\tremaining: 3m 51s\n",
      "19:\tlearn: 0.1879953\ttotal: 4.68s\tremaining: 3m 49s\n",
      "20:\tlearn: 0.1865822\ttotal: 4.9s\tremaining: 3m 48s\n",
      "21:\tlearn: 0.1850314\ttotal: 5.13s\tremaining: 3m 47s\n",
      "22:\tlearn: 0.1830203\ttotal: 5.32s\tremaining: 3m 46s\n",
      "23:\tlearn: 0.1812158\ttotal: 5.61s\tremaining: 3m 47s\n",
      "24:\tlearn: 0.1797985\ttotal: 5.8s\tremaining: 3m 46s\n",
      "25:\tlearn: 0.1785482\ttotal: 6.08s\tremaining: 3m 47s\n",
      "26:\tlearn: 0.1774890\ttotal: 6.29s\tremaining: 3m 46s\n",
      "27:\tlearn: 0.1763351\ttotal: 6.5s\tremaining: 3m 45s\n",
      "28:\tlearn: 0.1749954\ttotal: 6.7s\tremaining: 3m 44s\n",
      "29:\tlearn: 0.1740243\ttotal: 6.93s\tremaining: 3m 44s\n",
      "30:\tlearn: 0.1729000\ttotal: 7.12s\tremaining: 3m 42s\n",
      "31:\tlearn: 0.1722079\ttotal: 7.34s\tremaining: 3m 42s\n",
      "32:\tlearn: 0.1709661\ttotal: 7.55s\tremaining: 3m 41s\n",
      "33:\tlearn: 0.1699579\ttotal: 7.76s\tremaining: 3m 40s\n",
      "34:\tlearn: 0.1688358\ttotal: 7.96s\tremaining: 3m 39s\n",
      "35:\tlearn: 0.1678981\ttotal: 8.18s\tremaining: 3m 39s\n",
      "36:\tlearn: 0.1671522\ttotal: 8.39s\tremaining: 3m 38s\n",
      "37:\tlearn: 0.1664513\ttotal: 8.62s\tremaining: 3m 38s\n",
      "38:\tlearn: 0.1654453\ttotal: 8.82s\tremaining: 3m 37s\n",
      "39:\tlearn: 0.1642708\ttotal: 9.03s\tremaining: 3m 36s\n",
      "40:\tlearn: 0.1636098\ttotal: 9.25s\tremaining: 3m 36s\n",
      "41:\tlearn: 0.1630349\ttotal: 9.44s\tremaining: 3m 35s\n",
      "42:\tlearn: 0.1620005\ttotal: 9.64s\tremaining: 3m 34s\n",
      "43:\tlearn: 0.1613802\ttotal: 9.84s\tremaining: 3m 33s\n",
      "44:\tlearn: 0.1604958\ttotal: 10s\tremaining: 3m 32s\n",
      "45:\tlearn: 0.1597766\ttotal: 10.2s\tremaining: 3m 32s\n",
      "46:\tlearn: 0.1589653\ttotal: 10.4s\tremaining: 3m 31s\n",
      "47:\tlearn: 0.1585454\ttotal: 10.6s\tremaining: 3m 31s\n",
      "48:\tlearn: 0.1577380\ttotal: 10.9s\tremaining: 3m 30s\n",
      "49:\tlearn: 0.1569789\ttotal: 11.1s\tremaining: 3m 30s\n",
      "50:\tlearn: 0.1564169\ttotal: 11.3s\tremaining: 3m 30s\n",
      "51:\tlearn: 0.1556109\ttotal: 11.5s\tremaining: 3m 30s\n",
      "52:\tlearn: 0.1550455\ttotal: 11.7s\tremaining: 3m 29s\n",
      "53:\tlearn: 0.1544320\ttotal: 11.9s\tremaining: 3m 28s\n",
      "54:\tlearn: 0.1539810\ttotal: 12.2s\tremaining: 3m 29s\n",
      "55:\tlearn: 0.1535510\ttotal: 12.4s\tremaining: 3m 28s\n",
      "56:\tlearn: 0.1531045\ttotal: 12.6s\tremaining: 3m 28s\n",
      "57:\tlearn: 0.1525568\ttotal: 12.8s\tremaining: 3m 28s\n",
      "58:\tlearn: 0.1520556\ttotal: 13.1s\tremaining: 3m 28s\n",
      "59:\tlearn: 0.1516181\ttotal: 13.3s\tremaining: 3m 27s\n",
      "60:\tlearn: 0.1511240\ttotal: 13.5s\tremaining: 3m 27s\n",
      "61:\tlearn: 0.1505758\ttotal: 13.7s\tremaining: 3m 27s\n",
      "62:\tlearn: 0.1502246\ttotal: 13.9s\tremaining: 3m 26s\n",
      "63:\tlearn: 0.1498453\ttotal: 14.1s\tremaining: 3m 26s\n",
      "64:\tlearn: 0.1494186\ttotal: 14.4s\tremaining: 3m 26s\n",
      "65:\tlearn: 0.1491197\ttotal: 14.6s\tremaining: 3m 27s\n",
      "66:\tlearn: 0.1488131\ttotal: 14.9s\tremaining: 3m 27s\n",
      "67:\tlearn: 0.1485626\ttotal: 15.2s\tremaining: 3m 27s\n",
      "68:\tlearn: 0.1483501\ttotal: 15.4s\tremaining: 3m 27s\n",
      "69:\tlearn: 0.1480707\ttotal: 15.6s\tremaining: 3m 26s\n",
      "70:\tlearn: 0.1473409\ttotal: 15.8s\tremaining: 3m 26s\n",
      "71:\tlearn: 0.1471120\ttotal: 16s\tremaining: 3m 25s\n",
      "72:\tlearn: 0.1467651\ttotal: 16.2s\tremaining: 3m 25s\n",
      "73:\tlearn: 0.1463757\ttotal: 16.4s\tremaining: 3m 25s\n",
      "74:\tlearn: 0.1461392\ttotal: 16.6s\tremaining: 3m 25s\n",
      "75:\tlearn: 0.1459080\ttotal: 16.9s\tremaining: 3m 25s\n",
      "76:\tlearn: 0.1456025\ttotal: 17.1s\tremaining: 3m 24s\n",
      "77:\tlearn: 0.1453197\ttotal: 17.3s\tremaining: 3m 24s\n",
      "78:\tlearn: 0.1449499\ttotal: 17.5s\tremaining: 3m 24s\n",
      "79:\tlearn: 0.1447049\ttotal: 17.7s\tremaining: 3m 23s\n",
      "80:\tlearn: 0.1444410\ttotal: 17.9s\tremaining: 3m 23s\n",
      "81:\tlearn: 0.1441346\ttotal: 18.1s\tremaining: 3m 23s\n",
      "82:\tlearn: 0.1438986\ttotal: 18.4s\tremaining: 3m 23s\n",
      "83:\tlearn: 0.1435264\ttotal: 18.6s\tremaining: 3m 22s\n",
      "84:\tlearn: 0.1432802\ttotal: 18.8s\tremaining: 3m 22s\n",
      "85:\tlearn: 0.1430939\ttotal: 19s\tremaining: 3m 22s\n",
      "86:\tlearn: 0.1428999\ttotal: 19.2s\tremaining: 3m 21s\n",
      "87:\tlearn: 0.1425997\ttotal: 19.5s\tremaining: 3m 21s\n",
      "88:\tlearn: 0.1421820\ttotal: 19.7s\tremaining: 3m 21s\n",
      "89:\tlearn: 0.1419380\ttotal: 19.9s\tremaining: 3m 20s\n",
      "90:\tlearn: 0.1416133\ttotal: 20.1s\tremaining: 3m 20s\n",
      "91:\tlearn: 0.1414335\ttotal: 20.3s\tremaining: 3m 20s\n",
      "92:\tlearn: 0.1411020\ttotal: 20.5s\tremaining: 3m 19s\n",
      "93:\tlearn: 0.1408034\ttotal: 20.7s\tremaining: 3m 19s\n",
      "94:\tlearn: 0.1405226\ttotal: 20.9s\tremaining: 3m 19s\n",
      "95:\tlearn: 0.1403093\ttotal: 21.1s\tremaining: 3m 18s\n",
      "96:\tlearn: 0.1400531\ttotal: 21.3s\tremaining: 3m 18s\n",
      "97:\tlearn: 0.1397435\ttotal: 21.6s\tremaining: 3m 18s\n",
      "98:\tlearn: 0.1395413\ttotal: 21.8s\tremaining: 3m 18s\n",
      "99:\tlearn: 0.1392428\ttotal: 22s\tremaining: 3m 17s\n",
      "100:\tlearn: 0.1390310\ttotal: 22.2s\tremaining: 3m 17s\n",
      "101:\tlearn: 0.1387215\ttotal: 22.4s\tremaining: 3m 17s\n",
      "102:\tlearn: 0.1385690\ttotal: 22.6s\tremaining: 3m 16s\n",
      "103:\tlearn: 0.1383509\ttotal: 22.8s\tremaining: 3m 16s\n",
      "104:\tlearn: 0.1381220\ttotal: 23s\tremaining: 3m 16s\n",
      "105:\tlearn: 0.1377571\ttotal: 23.2s\tremaining: 3m 15s\n",
      "106:\tlearn: 0.1375305\ttotal: 23.4s\tremaining: 3m 15s\n",
      "107:\tlearn: 0.1372515\ttotal: 23.7s\tremaining: 3m 15s\n",
      "108:\tlearn: 0.1370199\ttotal: 23.9s\tremaining: 3m 14s\n",
      "109:\tlearn: 0.1368467\ttotal: 24s\tremaining: 3m 14s\n",
      "110:\tlearn: 0.1366277\ttotal: 24.3s\tremaining: 3m 14s\n",
      "111:\tlearn: 0.1364285\ttotal: 24.5s\tremaining: 3m 13s\n",
      "112:\tlearn: 0.1362459\ttotal: 24.6s\tremaining: 3m 13s\n",
      "113:\tlearn: 0.1360312\ttotal: 24.9s\tremaining: 3m 13s\n",
      "114:\tlearn: 0.1356838\ttotal: 25.1s\tremaining: 3m 13s\n",
      "115:\tlearn: 0.1355410\ttotal: 25.3s\tremaining: 3m 13s\n",
      "116:\tlearn: 0.1353478\ttotal: 25.5s\tremaining: 3m 12s\n",
      "117:\tlearn: 0.1351049\ttotal: 25.7s\tremaining: 3m 12s\n",
      "118:\tlearn: 0.1349639\ttotal: 25.9s\tremaining: 3m 12s\n",
      "119:\tlearn: 0.1348444\ttotal: 26.1s\tremaining: 3m 11s\n",
      "120:\tlearn: 0.1347010\ttotal: 26.4s\tremaining: 3m 11s\n",
      "121:\tlearn: 0.1345623\ttotal: 26.6s\tremaining: 3m 11s\n",
      "122:\tlearn: 0.1344176\ttotal: 26.8s\tremaining: 3m 10s\n",
      "123:\tlearn: 0.1342958\ttotal: 27s\tremaining: 3m 10s\n",
      "124:\tlearn: 0.1341355\ttotal: 27.2s\tremaining: 3m 10s\n",
      "125:\tlearn: 0.1338755\ttotal: 27.4s\tremaining: 3m 10s\n",
      "126:\tlearn: 0.1337610\ttotal: 27.7s\tremaining: 3m 10s\n",
      "127:\tlearn: 0.1336122\ttotal: 27.9s\tremaining: 3m 9s\n",
      "128:\tlearn: 0.1334221\ttotal: 28.1s\tremaining: 3m 9s\n",
      "129:\tlearn: 0.1332161\ttotal: 28.3s\tremaining: 3m 9s\n",
      "130:\tlearn: 0.1330739\ttotal: 28.5s\tremaining: 3m 9s\n",
      "131:\tlearn: 0.1328295\ttotal: 28.7s\tremaining: 3m 9s\n",
      "132:\tlearn: 0.1325688\ttotal: 28.9s\tremaining: 3m 8s\n",
      "133:\tlearn: 0.1323627\ttotal: 29.2s\tremaining: 3m 8s\n",
      "134:\tlearn: 0.1321827\ttotal: 29.4s\tremaining: 3m 8s\n",
      "135:\tlearn: 0.1319671\ttotal: 29.6s\tremaining: 3m 8s\n",
      "136:\tlearn: 0.1317383\ttotal: 29.8s\tremaining: 3m 7s\n",
      "137:\tlearn: 0.1316295\ttotal: 30s\tremaining: 3m 7s\n",
      "138:\tlearn: 0.1314839\ttotal: 30.2s\tremaining: 3m 7s\n",
      "139:\tlearn: 0.1313767\ttotal: 30.4s\tremaining: 3m 7s\n",
      "140:\tlearn: 0.1312978\ttotal: 30.6s\tremaining: 3m 6s\n",
      "141:\tlearn: 0.1310936\ttotal: 30.8s\tremaining: 3m 6s\n",
      "142:\tlearn: 0.1309267\ttotal: 31s\tremaining: 3m 6s\n",
      "143:\tlearn: 0.1306369\ttotal: 31.3s\tremaining: 3m 5s\n",
      "144:\tlearn: 0.1304757\ttotal: 31.5s\tremaining: 3m 5s\n",
      "145:\tlearn: 0.1302874\ttotal: 31.7s\tremaining: 3m 5s\n",
      "146:\tlearn: 0.1300990\ttotal: 32s\tremaining: 3m 5s\n",
      "147:\tlearn: 0.1299107\ttotal: 32.2s\tremaining: 3m 5s\n",
      "148:\tlearn: 0.1297368\ttotal: 32.5s\tremaining: 3m 5s\n",
      "149:\tlearn: 0.1295719\ttotal: 32.7s\tremaining: 3m 5s\n",
      "150:\tlearn: 0.1294171\ttotal: 32.9s\tremaining: 3m 4s\n",
      "151:\tlearn: 0.1292694\ttotal: 33.1s\tremaining: 3m 4s\n",
      "152:\tlearn: 0.1291386\ttotal: 33.3s\tremaining: 3m 4s\n",
      "153:\tlearn: 0.1290485\ttotal: 33.5s\tremaining: 3m 4s\n",
      "154:\tlearn: 0.1288488\ttotal: 33.7s\tremaining: 3m 3s\n",
      "155:\tlearn: 0.1286993\ttotal: 34s\tremaining: 3m 3s\n",
      "156:\tlearn: 0.1285669\ttotal: 34.1s\tremaining: 3m 3s\n",
      "157:\tlearn: 0.1284651\ttotal: 34.4s\tremaining: 3m 3s\n",
      "158:\tlearn: 0.1282797\ttotal: 34.6s\tremaining: 3m 2s\n",
      "159:\tlearn: 0.1280886\ttotal: 34.8s\tremaining: 3m 2s\n",
      "160:\tlearn: 0.1279476\ttotal: 35s\tremaining: 3m 2s\n",
      "161:\tlearn: 0.1278218\ttotal: 35.2s\tremaining: 3m 2s\n",
      "162:\tlearn: 0.1277039\ttotal: 35.4s\tremaining: 3m 1s\n",
      "163:\tlearn: 0.1276128\ttotal: 35.6s\tremaining: 3m 1s\n",
      "164:\tlearn: 0.1274183\ttotal: 35.8s\tremaining: 3m 1s\n",
      "165:\tlearn: 0.1273009\ttotal: 36s\tremaining: 3m\n",
      "166:\tlearn: 0.1271568\ttotal: 36.2s\tremaining: 3m\n",
      "167:\tlearn: 0.1269142\ttotal: 36.5s\tremaining: 3m\n",
      "168:\tlearn: 0.1268052\ttotal: 36.7s\tremaining: 3m\n",
      "169:\tlearn: 0.1266526\ttotal: 36.9s\tremaining: 3m\n",
      "170:\tlearn: 0.1264942\ttotal: 37.1s\tremaining: 2m 59s\n",
      "171:\tlearn: 0.1263110\ttotal: 37.3s\tremaining: 2m 59s\n",
      "172:\tlearn: 0.1262251\ttotal: 37.5s\tremaining: 2m 59s\n",
      "173:\tlearn: 0.1261010\ttotal: 37.7s\tremaining: 2m 58s\n",
      "174:\tlearn: 0.1260150\ttotal: 37.9s\tremaining: 2m 58s\n",
      "175:\tlearn: 0.1259118\ttotal: 38.1s\tremaining: 2m 58s\n",
      "176:\tlearn: 0.1257941\ttotal: 38.3s\tremaining: 2m 58s\n",
      "177:\tlearn: 0.1256602\ttotal: 38.6s\tremaining: 2m 58s\n",
      "178:\tlearn: 0.1255196\ttotal: 38.8s\tremaining: 2m 57s\n",
      "179:\tlearn: 0.1253764\ttotal: 39s\tremaining: 2m 57s\n",
      "180:\tlearn: 0.1252251\ttotal: 39.2s\tremaining: 2m 57s\n",
      "181:\tlearn: 0.1251303\ttotal: 39.4s\tremaining: 2m 57s\n",
      "182:\tlearn: 0.1250280\ttotal: 39.6s\tremaining: 2m 57s\n",
      "183:\tlearn: 0.1249371\ttotal: 39.9s\tremaining: 2m 56s\n",
      "184:\tlearn: 0.1248163\ttotal: 40.1s\tremaining: 2m 56s\n",
      "185:\tlearn: 0.1247013\ttotal: 40.3s\tremaining: 2m 56s\n",
      "186:\tlearn: 0.1245584\ttotal: 40.5s\tremaining: 2m 55s\n",
      "187:\tlearn: 0.1244239\ttotal: 40.7s\tremaining: 2m 55s\n",
      "188:\tlearn: 0.1242949\ttotal: 40.9s\tremaining: 2m 55s\n",
      "189:\tlearn: 0.1242247\ttotal: 41.1s\tremaining: 2m 55s\n",
      "190:\tlearn: 0.1241037\ttotal: 41.3s\tremaining: 2m 54s\n",
      "191:\tlearn: 0.1239856\ttotal: 41.5s\tremaining: 2m 54s\n",
      "192:\tlearn: 0.1239246\ttotal: 41.7s\tremaining: 2m 54s\n",
      "193:\tlearn: 0.1238226\ttotal: 41.9s\tremaining: 2m 53s\n",
      "194:\tlearn: 0.1236525\ttotal: 42.1s\tremaining: 2m 53s\n",
      "195:\tlearn: 0.1235680\ttotal: 42.3s\tremaining: 2m 53s\n",
      "196:\tlearn: 0.1234206\ttotal: 42.5s\tremaining: 2m 53s\n",
      "197:\tlearn: 0.1232968\ttotal: 42.7s\tremaining: 2m 53s\n",
      "198:\tlearn: 0.1231425\ttotal: 42.9s\tremaining: 2m 52s\n",
      "199:\tlearn: 0.1230324\ttotal: 43.1s\tremaining: 2m 52s\n",
      "200:\tlearn: 0.1228945\ttotal: 43.3s\tremaining: 2m 52s\n",
      "201:\tlearn: 0.1228036\ttotal: 43.6s\tremaining: 2m 52s\n",
      "202:\tlearn: 0.1227207\ttotal: 43.8s\tremaining: 2m 51s\n",
      "203:\tlearn: 0.1226268\ttotal: 44s\tremaining: 2m 51s\n",
      "204:\tlearn: 0.1225169\ttotal: 44.2s\tremaining: 2m 51s\n",
      "205:\tlearn: 0.1223645\ttotal: 44.4s\tremaining: 2m 51s\n",
      "206:\tlearn: 0.1222658\ttotal: 44.6s\tremaining: 2m 50s\n",
      "207:\tlearn: 0.1221249\ttotal: 44.8s\tremaining: 2m 50s\n",
      "208:\tlearn: 0.1220318\ttotal: 45s\tremaining: 2m 50s\n",
      "209:\tlearn: 0.1219404\ttotal: 45.2s\tremaining: 2m 49s\n",
      "210:\tlearn: 0.1217964\ttotal: 45.4s\tremaining: 2m 49s\n",
      "211:\tlearn: 0.1216626\ttotal: 45.6s\tremaining: 2m 49s\n",
      "212:\tlearn: 0.1215707\ttotal: 45.8s\tremaining: 2m 49s\n",
      "213:\tlearn: 0.1214335\ttotal: 46s\tremaining: 2m 49s\n",
      "214:\tlearn: 0.1212939\ttotal: 46.2s\tremaining: 2m 48s\n",
      "215:\tlearn: 0.1211956\ttotal: 46.5s\tremaining: 2m 48s\n",
      "216:\tlearn: 0.1210716\ttotal: 46.7s\tremaining: 2m 48s\n",
      "217:\tlearn: 0.1209572\ttotal: 46.9s\tremaining: 2m 48s\n",
      "218:\tlearn: 0.1208755\ttotal: 47s\tremaining: 2m 47s\n",
      "219:\tlearn: 0.1207442\ttotal: 47.3s\tremaining: 2m 47s\n",
      "220:\tlearn: 0.1206694\ttotal: 47.4s\tremaining: 2m 47s\n",
      "221:\tlearn: 0.1205589\ttotal: 47.6s\tremaining: 2m 46s\n",
      "222:\tlearn: 0.1204810\ttotal: 47.8s\tremaining: 2m 46s\n",
      "223:\tlearn: 0.1204054\ttotal: 48s\tremaining: 2m 46s\n",
      "224:\tlearn: 0.1203081\ttotal: 48.3s\tremaining: 2m 46s\n",
      "225:\tlearn: 0.1202477\ttotal: 48.6s\tremaining: 2m 46s\n",
      "226:\tlearn: 0.1201442\ttotal: 48.8s\tremaining: 2m 46s\n",
      "227:\tlearn: 0.1200368\ttotal: 49s\tremaining: 2m 45s\n",
      "228:\tlearn: 0.1199494\ttotal: 49.2s\tremaining: 2m 45s\n",
      "229:\tlearn: 0.1198661\ttotal: 49.4s\tremaining: 2m 45s\n",
      "230:\tlearn: 0.1198062\ttotal: 49.6s\tremaining: 2m 45s\n",
      "231:\tlearn: 0.1197002\ttotal: 49.8s\tremaining: 2m 44s\n",
      "232:\tlearn: 0.1196017\ttotal: 50s\tremaining: 2m 44s\n",
      "233:\tlearn: 0.1195257\ttotal: 50.2s\tremaining: 2m 44s\n",
      "234:\tlearn: 0.1194230\ttotal: 50.5s\tremaining: 2m 44s\n",
      "235:\tlearn: 0.1193339\ttotal: 50.7s\tremaining: 2m 44s\n",
      "236:\tlearn: 0.1192492\ttotal: 50.9s\tremaining: 2m 44s\n",
      "237:\tlearn: 0.1191170\ttotal: 51.1s\tremaining: 2m 43s\n",
      "238:\tlearn: 0.1190213\ttotal: 51.3s\tremaining: 2m 43s\n",
      "239:\tlearn: 0.1189347\ttotal: 51.5s\tremaining: 2m 43s\n",
      "240:\tlearn: 0.1188193\ttotal: 51.7s\tremaining: 2m 42s\n",
      "241:\tlearn: 0.1187406\ttotal: 51.9s\tremaining: 2m 42s\n",
      "242:\tlearn: 0.1186623\ttotal: 52.1s\tremaining: 2m 42s\n",
      "243:\tlearn: 0.1185862\ttotal: 52.3s\tremaining: 2m 42s\n",
      "244:\tlearn: 0.1185089\ttotal: 52.5s\tremaining: 2m 41s\n",
      "245:\tlearn: 0.1184412\ttotal: 52.7s\tremaining: 2m 41s\n",
      "246:\tlearn: 0.1183003\ttotal: 52.9s\tremaining: 2m 41s\n",
      "247:\tlearn: 0.1182236\ttotal: 53.2s\tremaining: 2m 41s\n",
      "248:\tlearn: 0.1181455\ttotal: 53.4s\tremaining: 2m 40s\n",
      "249:\tlearn: 0.1180760\ttotal: 53.6s\tremaining: 2m 40s\n",
      "250:\tlearn: 0.1180150\ttotal: 53.8s\tremaining: 2m 40s\n",
      "251:\tlearn: 0.1178970\ttotal: 54s\tremaining: 2m 40s\n",
      "252:\tlearn: 0.1177752\ttotal: 54.2s\tremaining: 2m 39s\n",
      "253:\tlearn: 0.1176965\ttotal: 54.3s\tremaining: 2m 39s\n",
      "254:\tlearn: 0.1176117\ttotal: 54.5s\tremaining: 2m 39s\n",
      "255:\tlearn: 0.1175480\ttotal: 54.7s\tremaining: 2m 39s\n",
      "256:\tlearn: 0.1174691\ttotal: 54.9s\tremaining: 2m 38s\n",
      "257:\tlearn: 0.1173812\ttotal: 55.1s\tremaining: 2m 38s\n",
      "258:\tlearn: 0.1172741\ttotal: 55.4s\tremaining: 2m 38s\n",
      "259:\tlearn: 0.1171801\ttotal: 55.6s\tremaining: 2m 38s\n",
      "260:\tlearn: 0.1170995\ttotal: 55.8s\tremaining: 2m 37s\n",
      "261:\tlearn: 0.1170295\ttotal: 56s\tremaining: 2m 37s\n",
      "262:\tlearn: 0.1169585\ttotal: 56.2s\tremaining: 2m 37s\n",
      "263:\tlearn: 0.1168456\ttotal: 56.4s\tremaining: 2m 37s\n",
      "264:\tlearn: 0.1167909\ttotal: 56.6s\tremaining: 2m 37s\n",
      "265:\tlearn: 0.1166968\ttotal: 56.8s\tremaining: 2m 36s\n",
      "266:\tlearn: 0.1166261\ttotal: 57s\tremaining: 2m 36s\n",
      "267:\tlearn: 0.1165372\ttotal: 57.2s\tremaining: 2m 36s\n",
      "268:\tlearn: 0.1164640\ttotal: 57.4s\tremaining: 2m 35s\n",
      "269:\tlearn: 0.1163855\ttotal: 57.6s\tremaining: 2m 35s\n",
      "270:\tlearn: 0.1162965\ttotal: 57.8s\tremaining: 2m 35s\n",
      "271:\tlearn: 0.1162287\ttotal: 58s\tremaining: 2m 35s\n",
      "272:\tlearn: 0.1161766\ttotal: 58.2s\tremaining: 2m 34s\n",
      "273:\tlearn: 0.1161174\ttotal: 58.4s\tremaining: 2m 34s\n",
      "274:\tlearn: 0.1160251\ttotal: 58.6s\tremaining: 2m 34s\n",
      "275:\tlearn: 0.1159460\ttotal: 58.8s\tremaining: 2m 34s\n",
      "276:\tlearn: 0.1158605\ttotal: 59.1s\tremaining: 2m 34s\n",
      "277:\tlearn: 0.1157729\ttotal: 59.3s\tremaining: 2m 33s\n",
      "278:\tlearn: 0.1157042\ttotal: 59.5s\tremaining: 2m 33s\n",
      "279:\tlearn: 0.1156536\ttotal: 59.7s\tremaining: 2m 33s\n",
      "280:\tlearn: 0.1155926\ttotal: 59.9s\tremaining: 2m 33s\n",
      "281:\tlearn: 0.1155369\ttotal: 1m\tremaining: 2m 33s\n",
      "282:\tlearn: 0.1154787\ttotal: 1m\tremaining: 2m 32s\n",
      "283:\tlearn: 0.1153878\ttotal: 1m\tremaining: 2m 32s\n",
      "284:\tlearn: 0.1152783\ttotal: 1m\tremaining: 2m 32s\n",
      "285:\tlearn: 0.1151582\ttotal: 1m\tremaining: 2m 32s\n",
      "286:\tlearn: 0.1150786\ttotal: 1m 1s\tremaining: 2m 31s\n",
      "287:\tlearn: 0.1149710\ttotal: 1m 1s\tremaining: 2m 31s\n",
      "288:\tlearn: 0.1149078\ttotal: 1m 1s\tremaining: 2m 31s\n",
      "289:\tlearn: 0.1147923\ttotal: 1m 1s\tremaining: 2m 31s\n",
      "290:\tlearn: 0.1147260\ttotal: 1m 2s\tremaining: 2m 31s\n",
      "291:\tlearn: 0.1146474\ttotal: 1m 2s\tremaining: 2m 30s\n",
      "292:\tlearn: 0.1145992\ttotal: 1m 2s\tremaining: 2m 30s\n",
      "293:\tlearn: 0.1145318\ttotal: 1m 2s\tremaining: 2m 30s\n",
      "294:\tlearn: 0.1144548\ttotal: 1m 2s\tremaining: 2m 30s\n",
      "295:\tlearn: 0.1143010\ttotal: 1m 3s\tremaining: 2m 30s\n",
      "296:\tlearn: 0.1142269\ttotal: 1m 3s\tremaining: 2m 29s\n",
      "297:\tlearn: 0.1141418\ttotal: 1m 3s\tremaining: 2m 29s\n",
      "298:\tlearn: 0.1140613\ttotal: 1m 3s\tremaining: 2m 29s\n",
      "299:\tlearn: 0.1140018\ttotal: 1m 3s\tremaining: 2m 29s\n",
      "300:\tlearn: 0.1139363\ttotal: 1m 4s\tremaining: 2m 29s\n",
      "301:\tlearn: 0.1138755\ttotal: 1m 4s\tremaining: 2m 28s\n",
      "302:\tlearn: 0.1138246\ttotal: 1m 4s\tremaining: 2m 28s\n",
      "303:\tlearn: 0.1137732\ttotal: 1m 4s\tremaining: 2m 28s\n",
      "304:\tlearn: 0.1136974\ttotal: 1m 5s\tremaining: 2m 28s\n",
      "305:\tlearn: 0.1136035\ttotal: 1m 5s\tremaining: 2m 27s\n",
      "306:\tlearn: 0.1135560\ttotal: 1m 5s\tremaining: 2m 27s\n",
      "307:\tlearn: 0.1134000\ttotal: 1m 5s\tremaining: 2m 27s\n",
      "308:\tlearn: 0.1133349\ttotal: 1m 5s\tremaining: 2m 27s\n",
      "309:\tlearn: 0.1132515\ttotal: 1m 6s\tremaining: 2m 27s\n",
      "310:\tlearn: 0.1131804\ttotal: 1m 6s\tremaining: 2m 27s\n",
      "311:\tlearn: 0.1131110\ttotal: 1m 6s\tremaining: 2m 26s\n",
      "312:\tlearn: 0.1130331\ttotal: 1m 6s\tremaining: 2m 26s\n",
      "313:\tlearn: 0.1129558\ttotal: 1m 7s\tremaining: 2m 26s\n",
      "314:\tlearn: 0.1129154\ttotal: 1m 7s\tremaining: 2m 26s\n",
      "315:\tlearn: 0.1128378\ttotal: 1m 7s\tremaining: 2m 26s\n",
      "316:\tlearn: 0.1127946\ttotal: 1m 7s\tremaining: 2m 25s\n",
      "317:\tlearn: 0.1127591\ttotal: 1m 7s\tremaining: 2m 25s\n",
      "318:\tlearn: 0.1127046\ttotal: 1m 8s\tremaining: 2m 25s\n",
      "319:\tlearn: 0.1126556\ttotal: 1m 8s\tremaining: 2m 25s\n",
      "320:\tlearn: 0.1125289\ttotal: 1m 8s\tremaining: 2m 25s\n",
      "321:\tlearn: 0.1124892\ttotal: 1m 8s\tremaining: 2m 24s\n",
      "322:\tlearn: 0.1124250\ttotal: 1m 9s\tremaining: 2m 24s\n",
      "323:\tlearn: 0.1123265\ttotal: 1m 9s\tremaining: 2m 24s\n",
      "324:\tlearn: 0.1122483\ttotal: 1m 9s\tremaining: 2m 24s\n",
      "325:\tlearn: 0.1121542\ttotal: 1m 9s\tremaining: 2m 24s\n",
      "326:\tlearn: 0.1120938\ttotal: 1m 9s\tremaining: 2m 23s\n",
      "327:\tlearn: 0.1120305\ttotal: 1m 10s\tremaining: 2m 23s\n",
      "328:\tlearn: 0.1119511\ttotal: 1m 10s\tremaining: 2m 23s\n",
      "329:\tlearn: 0.1118584\ttotal: 1m 10s\tremaining: 2m 23s\n",
      "330:\tlearn: 0.1117737\ttotal: 1m 10s\tremaining: 2m 22s\n",
      "331:\tlearn: 0.1117220\ttotal: 1m 10s\tremaining: 2m 22s\n",
      "332:\tlearn: 0.1116728\ttotal: 1m 11s\tremaining: 2m 22s\n",
      "333:\tlearn: 0.1115853\ttotal: 1m 11s\tremaining: 2m 22s\n",
      "334:\tlearn: 0.1115253\ttotal: 1m 11s\tremaining: 2m 22s\n",
      "335:\tlearn: 0.1114466\ttotal: 1m 11s\tremaining: 2m 21s\n",
      "336:\tlearn: 0.1113592\ttotal: 1m 12s\tremaining: 2m 21s\n",
      "337:\tlearn: 0.1113044\ttotal: 1m 12s\tremaining: 2m 21s\n",
      "338:\tlearn: 0.1112176\ttotal: 1m 12s\tremaining: 2m 21s\n",
      "339:\tlearn: 0.1111751\ttotal: 1m 12s\tremaining: 2m 21s\n",
      "340:\tlearn: 0.1111277\ttotal: 1m 12s\tremaining: 2m 20s\n",
      "341:\tlearn: 0.1110922\ttotal: 1m 13s\tremaining: 2m 20s\n",
      "342:\tlearn: 0.1110398\ttotal: 1m 13s\tremaining: 2m 20s\n",
      "343:\tlearn: 0.1109696\ttotal: 1m 13s\tremaining: 2m 20s\n",
      "344:\tlearn: 0.1108705\ttotal: 1m 13s\tremaining: 2m 19s\n",
      "345:\tlearn: 0.1107653\ttotal: 1m 13s\tremaining: 2m 19s\n",
      "346:\tlearn: 0.1106982\ttotal: 1m 14s\tremaining: 2m 19s\n",
      "347:\tlearn: 0.1106213\ttotal: 1m 14s\tremaining: 2m 19s\n",
      "348:\tlearn: 0.1105622\ttotal: 1m 14s\tremaining: 2m 19s\n",
      "349:\tlearn: 0.1104817\ttotal: 1m 14s\tremaining: 2m 18s\n",
      "350:\tlearn: 0.1103987\ttotal: 1m 14s\tremaining: 2m 18s\n",
      "351:\tlearn: 0.1103312\ttotal: 1m 15s\tremaining: 2m 18s\n",
      "352:\tlearn: 0.1102555\ttotal: 1m 15s\tremaining: 2m 18s\n",
      "353:\tlearn: 0.1102120\ttotal: 1m 15s\tremaining: 2m 17s\n",
      "354:\tlearn: 0.1101482\ttotal: 1m 15s\tremaining: 2m 17s\n",
      "355:\tlearn: 0.1100270\ttotal: 1m 16s\tremaining: 2m 17s\n",
      "356:\tlearn: 0.1100062\ttotal: 1m 16s\tremaining: 2m 17s\n",
      "357:\tlearn: 0.1099886\ttotal: 1m 16s\tremaining: 2m 17s\n",
      "358:\tlearn: 0.1099294\ttotal: 1m 16s\tremaining: 2m 16s\n",
      "359:\tlearn: 0.1098529\ttotal: 1m 16s\tremaining: 2m 16s\n",
      "360:\tlearn: 0.1098021\ttotal: 1m 17s\tremaining: 2m 16s\n",
      "361:\tlearn: 0.1097132\ttotal: 1m 17s\tremaining: 2m 16s\n",
      "362:\tlearn: 0.1095940\ttotal: 1m 17s\tremaining: 2m 15s\n",
      "363:\tlearn: 0.1095032\ttotal: 1m 17s\tremaining: 2m 15s\n",
      "364:\tlearn: 0.1094433\ttotal: 1m 17s\tremaining: 2m 15s\n",
      "365:\tlearn: 0.1093446\ttotal: 1m 18s\tremaining: 2m 15s\n",
      "366:\tlearn: 0.1092996\ttotal: 1m 18s\tremaining: 2m 15s\n",
      "367:\tlearn: 0.1092468\ttotal: 1m 18s\tremaining: 2m 14s\n",
      "368:\tlearn: 0.1091993\ttotal: 1m 18s\tremaining: 2m 14s\n",
      "369:\tlearn: 0.1091387\ttotal: 1m 18s\tremaining: 2m 14s\n",
      "370:\tlearn: 0.1090706\ttotal: 1m 19s\tremaining: 2m 14s\n",
      "371:\tlearn: 0.1090198\ttotal: 1m 19s\tremaining: 2m 14s\n",
      "372:\tlearn: 0.1089687\ttotal: 1m 19s\tremaining: 2m 13s\n",
      "373:\tlearn: 0.1089001\ttotal: 1m 19s\tremaining: 2m 13s\n",
      "374:\tlearn: 0.1088058\ttotal: 1m 20s\tremaining: 2m 13s\n",
      "375:\tlearn: 0.1087368\ttotal: 1m 20s\tremaining: 2m 13s\n",
      "376:\tlearn: 0.1086894\ttotal: 1m 20s\tremaining: 2m 12s\n",
      "377:\tlearn: 0.1086217\ttotal: 1m 20s\tremaining: 2m 12s\n",
      "378:\tlearn: 0.1085855\ttotal: 1m 20s\tremaining: 2m 12s\n",
      "379:\tlearn: 0.1085277\ttotal: 1m 21s\tremaining: 2m 12s\n",
      "380:\tlearn: 0.1084878\ttotal: 1m 21s\tremaining: 2m 12s\n",
      "381:\tlearn: 0.1084056\ttotal: 1m 21s\tremaining: 2m 11s\n",
      "382:\tlearn: 0.1083639\ttotal: 1m 21s\tremaining: 2m 11s\n",
      "383:\tlearn: 0.1083302\ttotal: 1m 21s\tremaining: 2m 11s\n",
      "384:\tlearn: 0.1082589\ttotal: 1m 22s\tremaining: 2m 11s\n",
      "385:\tlearn: 0.1082020\ttotal: 1m 22s\tremaining: 2m 10s\n",
      "386:\tlearn: 0.1081473\ttotal: 1m 22s\tremaining: 2m 10s\n",
      "387:\tlearn: 0.1080548\ttotal: 1m 22s\tremaining: 2m 10s\n",
      "388:\tlearn: 0.1080182\ttotal: 1m 22s\tremaining: 2m 10s\n",
      "389:\tlearn: 0.1079258\ttotal: 1m 23s\tremaining: 2m 10s\n",
      "390:\tlearn: 0.1078708\ttotal: 1m 23s\tremaining: 2m 9s\n",
      "391:\tlearn: 0.1078216\ttotal: 1m 23s\tremaining: 2m 9s\n",
      "392:\tlearn: 0.1077112\ttotal: 1m 23s\tremaining: 2m 9s\n",
      "393:\tlearn: 0.1076699\ttotal: 1m 24s\tremaining: 2m 9s\n",
      "394:\tlearn: 0.1076160\ttotal: 1m 24s\tremaining: 2m 9s\n",
      "395:\tlearn: 0.1075542\ttotal: 1m 24s\tremaining: 2m 8s\n",
      "396:\tlearn: 0.1074734\ttotal: 1m 24s\tremaining: 2m 8s\n",
      "397:\tlearn: 0.1074038\ttotal: 1m 24s\tremaining: 2m 8s\n",
      "398:\tlearn: 0.1073507\ttotal: 1m 25s\tremaining: 2m 8s\n",
      "399:\tlearn: 0.1072768\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "400:\tlearn: 0.1071663\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "401:\tlearn: 0.1071220\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "402:\tlearn: 0.1070480\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "403:\tlearn: 0.1070075\ttotal: 1m 26s\tremaining: 2m 6s\n",
      "404:\tlearn: 0.1069713\ttotal: 1m 26s\tremaining: 2m 6s\n",
      "405:\tlearn: 0.1069506\ttotal: 1m 26s\tremaining: 2m 6s\n",
      "406:\tlearn: 0.1068827\ttotal: 1m 26s\tremaining: 2m 6s\n",
      "407:\tlearn: 0.1068122\ttotal: 1m 26s\tremaining: 2m 6s\n",
      "408:\tlearn: 0.1067221\ttotal: 1m 27s\tremaining: 2m 5s\n",
      "409:\tlearn: 0.1066507\ttotal: 1m 27s\tremaining: 2m 5s\n",
      "410:\tlearn: 0.1065960\ttotal: 1m 27s\tremaining: 2m 5s\n",
      "411:\tlearn: 0.1065402\ttotal: 1m 27s\tremaining: 2m 5s\n",
      "412:\tlearn: 0.1065026\ttotal: 1m 28s\tremaining: 2m 5s\n",
      "413:\tlearn: 0.1064374\ttotal: 1m 28s\tremaining: 2m 4s\n",
      "414:\tlearn: 0.1063850\ttotal: 1m 28s\tremaining: 2m 4s\n",
      "415:\tlearn: 0.1062969\ttotal: 1m 28s\tremaining: 2m 4s\n",
      "416:\tlearn: 0.1062460\ttotal: 1m 28s\tremaining: 2m 4s\n",
      "417:\tlearn: 0.1061948\ttotal: 1m 29s\tremaining: 2m 4s\n",
      "418:\tlearn: 0.1061270\ttotal: 1m 29s\tremaining: 2m 3s\n",
      "419:\tlearn: 0.1060229\ttotal: 1m 29s\tremaining: 2m 3s\n",
      "420:\tlearn: 0.1059847\ttotal: 1m 29s\tremaining: 2m 3s\n",
      "421:\tlearn: 0.1059220\ttotal: 1m 29s\tremaining: 2m 3s\n",
      "422:\tlearn: 0.1058751\ttotal: 1m 30s\tremaining: 2m 2s\n",
      "423:\tlearn: 0.1057943\ttotal: 1m 30s\tremaining: 2m 2s\n",
      "424:\tlearn: 0.1056697\ttotal: 1m 30s\tremaining: 2m 2s\n",
      "425:\tlearn: 0.1056116\ttotal: 1m 30s\tremaining: 2m 2s\n",
      "426:\tlearn: 0.1055591\ttotal: 1m 30s\tremaining: 2m 2s\n",
      "427:\tlearn: 0.1055150\ttotal: 1m 31s\tremaining: 2m 1s\n",
      "428:\tlearn: 0.1054606\ttotal: 1m 31s\tremaining: 2m 1s\n",
      "429:\tlearn: 0.1054178\ttotal: 1m 31s\tremaining: 2m 1s\n",
      "430:\tlearn: 0.1053699\ttotal: 1m 31s\tremaining: 2m 1s\n",
      "431:\tlearn: 0.1052877\ttotal: 1m 32s\tremaining: 2m 1s\n",
      "432:\tlearn: 0.1052532\ttotal: 1m 32s\tremaining: 2m\n",
      "433:\tlearn: 0.1051582\ttotal: 1m 32s\tremaining: 2m\n",
      "434:\tlearn: 0.1051050\ttotal: 1m 32s\tremaining: 2m\n",
      "435:\tlearn: 0.1050566\ttotal: 1m 33s\tremaining: 2m\n",
      "436:\tlearn: 0.1050016\ttotal: 1m 33s\tremaining: 2m\n",
      "437:\tlearn: 0.1049399\ttotal: 1m 33s\tremaining: 1m 59s\n",
      "438:\tlearn: 0.1048934\ttotal: 1m 33s\tremaining: 1m 59s\n",
      "439:\tlearn: 0.1048434\ttotal: 1m 33s\tremaining: 1m 59s\n",
      "440:\tlearn: 0.1048243\ttotal: 1m 34s\tremaining: 1m 59s\n",
      "441:\tlearn: 0.1047827\ttotal: 1m 34s\tremaining: 1m 59s\n",
      "442:\tlearn: 0.1047407\ttotal: 1m 34s\tremaining: 1m 58s\n",
      "443:\tlearn: 0.1046648\ttotal: 1m 34s\tremaining: 1m 58s\n",
      "444:\tlearn: 0.1046133\ttotal: 1m 34s\tremaining: 1m 58s\n",
      "445:\tlearn: 0.1045721\ttotal: 1m 35s\tremaining: 1m 58s\n",
      "446:\tlearn: 0.1044903\ttotal: 1m 35s\tremaining: 1m 58s\n",
      "447:\tlearn: 0.1044278\ttotal: 1m 35s\tremaining: 1m 57s\n",
      "448:\tlearn: 0.1043907\ttotal: 1m 35s\tremaining: 1m 57s\n",
      "449:\tlearn: 0.1043450\ttotal: 1m 36s\tremaining: 1m 57s\n",
      "450:\tlearn: 0.1042636\ttotal: 1m 36s\tremaining: 1m 57s\n",
      "451:\tlearn: 0.1042065\ttotal: 1m 36s\tremaining: 1m 56s\n",
      "452:\tlearn: 0.1041707\ttotal: 1m 36s\tremaining: 1m 56s\n",
      "453:\tlearn: 0.1041380\ttotal: 1m 36s\tremaining: 1m 56s\n",
      "454:\tlearn: 0.1040744\ttotal: 1m 37s\tremaining: 1m 56s\n",
      "455:\tlearn: 0.1040428\ttotal: 1m 37s\tremaining: 1m 56s\n",
      "456:\tlearn: 0.1039900\ttotal: 1m 37s\tremaining: 1m 55s\n",
      "457:\tlearn: 0.1039470\ttotal: 1m 37s\tremaining: 1m 55s\n",
      "458:\tlearn: 0.1039029\ttotal: 1m 37s\tremaining: 1m 55s\n",
      "459:\tlearn: 0.1038415\ttotal: 1m 38s\tremaining: 1m 55s\n",
      "460:\tlearn: 0.1037808\ttotal: 1m 38s\tremaining: 1m 54s\n",
      "461:\tlearn: 0.1037062\ttotal: 1m 38s\tremaining: 1m 54s\n",
      "462:\tlearn: 0.1036390\ttotal: 1m 38s\tremaining: 1m 54s\n",
      "463:\tlearn: 0.1035920\ttotal: 1m 38s\tremaining: 1m 54s\n",
      "464:\tlearn: 0.1035584\ttotal: 1m 39s\tremaining: 1m 54s\n",
      "465:\tlearn: 0.1034972\ttotal: 1m 39s\tremaining: 1m 53s\n",
      "466:\tlearn: 0.1034709\ttotal: 1m 39s\tremaining: 1m 53s\n",
      "467:\tlearn: 0.1034301\ttotal: 1m 39s\tremaining: 1m 53s\n",
      "468:\tlearn: 0.1034032\ttotal: 1m 40s\tremaining: 1m 53s\n",
      "469:\tlearn: 0.1032885\ttotal: 1m 40s\tremaining: 1m 53s\n",
      "470:\tlearn: 0.1032320\ttotal: 1m 40s\tremaining: 1m 52s\n",
      "471:\tlearn: 0.1031768\ttotal: 1m 40s\tremaining: 1m 52s\n",
      "472:\tlearn: 0.1031143\ttotal: 1m 40s\tremaining: 1m 52s\n",
      "473:\tlearn: 0.1030406\ttotal: 1m 41s\tremaining: 1m 52s\n",
      "474:\tlearn: 0.1029944\ttotal: 1m 41s\tremaining: 1m 51s\n",
      "475:\tlearn: 0.1029465\ttotal: 1m 41s\tremaining: 1m 51s\n",
      "476:\tlearn: 0.1029135\ttotal: 1m 41s\tremaining: 1m 51s\n",
      "477:\tlearn: 0.1028445\ttotal: 1m 41s\tremaining: 1m 51s\n",
      "478:\tlearn: 0.1027910\ttotal: 1m 42s\tremaining: 1m 51s\n",
      "479:\tlearn: 0.1027337\ttotal: 1m 42s\tremaining: 1m 50s\n",
      "480:\tlearn: 0.1026998\ttotal: 1m 42s\tremaining: 1m 50s\n",
      "481:\tlearn: 0.1026682\ttotal: 1m 42s\tremaining: 1m 50s\n",
      "482:\tlearn: 0.1026155\ttotal: 1m 43s\tremaining: 1m 50s\n",
      "483:\tlearn: 0.1025528\ttotal: 1m 43s\tremaining: 1m 50s\n",
      "484:\tlearn: 0.1025194\ttotal: 1m 43s\tremaining: 1m 49s\n",
      "485:\tlearn: 0.1024645\ttotal: 1m 43s\tremaining: 1m 49s\n",
      "486:\tlearn: 0.1024109\ttotal: 1m 43s\tremaining: 1m 49s\n",
      "487:\tlearn: 0.1023713\ttotal: 1m 44s\tremaining: 1m 49s\n",
      "488:\tlearn: 0.1023224\ttotal: 1m 44s\tremaining: 1m 49s\n",
      "489:\tlearn: 0.1022719\ttotal: 1m 44s\tremaining: 1m 48s\n",
      "490:\tlearn: 0.1022208\ttotal: 1m 44s\tremaining: 1m 48s\n",
      "491:\tlearn: 0.1021441\ttotal: 1m 45s\tremaining: 1m 48s\n",
      "492:\tlearn: 0.1020885\ttotal: 1m 45s\tremaining: 1m 48s\n",
      "493:\tlearn: 0.1020461\ttotal: 1m 45s\tremaining: 1m 48s\n",
      "494:\tlearn: 0.1020052\ttotal: 1m 45s\tremaining: 1m 47s\n",
      "495:\tlearn: 0.1019676\ttotal: 1m 45s\tremaining: 1m 47s\n",
      "496:\tlearn: 0.1019230\ttotal: 1m 46s\tremaining: 1m 47s\n",
      "497:\tlearn: 0.1018505\ttotal: 1m 46s\tremaining: 1m 47s\n",
      "498:\tlearn: 0.1018004\ttotal: 1m 46s\tremaining: 1m 47s\n",
      "499:\tlearn: 0.1017659\ttotal: 1m 46s\tremaining: 1m 46s\n",
      "500:\tlearn: 0.1016843\ttotal: 1m 47s\tremaining: 1m 46s\n",
      "501:\tlearn: 0.1016511\ttotal: 1m 47s\tremaining: 1m 46s\n",
      "502:\tlearn: 0.1015997\ttotal: 1m 47s\tremaining: 1m 46s\n",
      "503:\tlearn: 0.1015649\ttotal: 1m 47s\tremaining: 1m 45s\n",
      "504:\tlearn: 0.1014701\ttotal: 1m 47s\tremaining: 1m 45s\n",
      "505:\tlearn: 0.1014271\ttotal: 1m 48s\tremaining: 1m 45s\n",
      "506:\tlearn: 0.1013740\ttotal: 1m 48s\tremaining: 1m 45s\n",
      "507:\tlearn: 0.1013378\ttotal: 1m 48s\tremaining: 1m 45s\n",
      "508:\tlearn: 0.1013012\ttotal: 1m 48s\tremaining: 1m 44s\n",
      "509:\tlearn: 0.1012063\ttotal: 1m 48s\tremaining: 1m 44s\n",
      "510:\tlearn: 0.1011425\ttotal: 1m 49s\tremaining: 1m 44s\n",
      "511:\tlearn: 0.1010962\ttotal: 1m 49s\tremaining: 1m 44s\n",
      "512:\tlearn: 0.1010088\ttotal: 1m 49s\tremaining: 1m 43s\n",
      "513:\tlearn: 0.1009575\ttotal: 1m 49s\tremaining: 1m 43s\n",
      "514:\tlearn: 0.1009256\ttotal: 1m 49s\tremaining: 1m 43s\n",
      "515:\tlearn: 0.1008317\ttotal: 1m 50s\tremaining: 1m 43s\n",
      "516:\tlearn: 0.1007746\ttotal: 1m 50s\tremaining: 1m 43s\n",
      "517:\tlearn: 0.1007080\ttotal: 1m 50s\tremaining: 1m 42s\n",
      "518:\tlearn: 0.1006484\ttotal: 1m 50s\tremaining: 1m 42s\n",
      "519:\tlearn: 0.1006304\ttotal: 1m 50s\tremaining: 1m 42s\n",
      "520:\tlearn: 0.1005852\ttotal: 1m 51s\tremaining: 1m 42s\n",
      "521:\tlearn: 0.1005445\ttotal: 1m 51s\tremaining: 1m 42s\n",
      "522:\tlearn: 0.1004954\ttotal: 1m 51s\tremaining: 1m 41s\n",
      "523:\tlearn: 0.1004245\ttotal: 1m 51s\tremaining: 1m 41s\n",
      "524:\tlearn: 0.1003895\ttotal: 1m 52s\tremaining: 1m 41s\n",
      "525:\tlearn: 0.1003417\ttotal: 1m 52s\tremaining: 1m 41s\n",
      "526:\tlearn: 0.1002670\ttotal: 1m 52s\tremaining: 1m 40s\n",
      "527:\tlearn: 0.1002130\ttotal: 1m 52s\tremaining: 1m 40s\n",
      "528:\tlearn: 0.1001493\ttotal: 1m 52s\tremaining: 1m 40s\n",
      "529:\tlearn: 0.1000951\ttotal: 1m 53s\tremaining: 1m 40s\n",
      "530:\tlearn: 0.1000637\ttotal: 1m 53s\tremaining: 1m 40s\n",
      "531:\tlearn: 0.1000091\ttotal: 1m 53s\tremaining: 1m 39s\n",
      "532:\tlearn: 0.0999343\ttotal: 1m 53s\tremaining: 1m 39s\n",
      "533:\tlearn: 0.0998963\ttotal: 1m 53s\tremaining: 1m 39s\n",
      "534:\tlearn: 0.0998392\ttotal: 1m 54s\tremaining: 1m 39s\n",
      "535:\tlearn: 0.0997972\ttotal: 1m 54s\tremaining: 1m 39s\n",
      "536:\tlearn: 0.0997618\ttotal: 1m 54s\tremaining: 1m 38s\n",
      "537:\tlearn: 0.0997227\ttotal: 1m 54s\tremaining: 1m 38s\n",
      "538:\tlearn: 0.0996637\ttotal: 1m 54s\tremaining: 1m 38s\n",
      "539:\tlearn: 0.0996424\ttotal: 1m 55s\tremaining: 1m 38s\n",
      "540:\tlearn: 0.0996039\ttotal: 1m 55s\tremaining: 1m 37s\n",
      "541:\tlearn: 0.0995496\ttotal: 1m 55s\tremaining: 1m 37s\n",
      "542:\tlearn: 0.0994898\ttotal: 1m 55s\tremaining: 1m 37s\n",
      "543:\tlearn: 0.0994529\ttotal: 1m 56s\tremaining: 1m 37s\n",
      "544:\tlearn: 0.0993621\ttotal: 1m 56s\tremaining: 1m 37s\n",
      "545:\tlearn: 0.0992837\ttotal: 1m 56s\tremaining: 1m 36s\n",
      "546:\tlearn: 0.0992390\ttotal: 1m 56s\tremaining: 1m 36s\n",
      "547:\tlearn: 0.0992183\ttotal: 1m 57s\tremaining: 1m 36s\n",
      "548:\tlearn: 0.0991682\ttotal: 1m 57s\tremaining: 1m 36s\n",
      "549:\tlearn: 0.0990897\ttotal: 1m 57s\tremaining: 1m 36s\n",
      "550:\tlearn: 0.0990584\ttotal: 1m 57s\tremaining: 1m 35s\n",
      "551:\tlearn: 0.0990246\ttotal: 1m 57s\tremaining: 1m 35s\n",
      "552:\tlearn: 0.0989873\ttotal: 1m 58s\tremaining: 1m 35s\n",
      "553:\tlearn: 0.0989215\ttotal: 1m 58s\tremaining: 1m 35s\n",
      "554:\tlearn: 0.0988734\ttotal: 1m 58s\tremaining: 1m 35s\n",
      "555:\tlearn: 0.0988449\ttotal: 1m 58s\tremaining: 1m 34s\n",
      "556:\tlearn: 0.0988022\ttotal: 1m 58s\tremaining: 1m 34s\n",
      "557:\tlearn: 0.0987657\ttotal: 1m 59s\tremaining: 1m 34s\n",
      "558:\tlearn: 0.0986825\ttotal: 1m 59s\tremaining: 1m 34s\n",
      "559:\tlearn: 0.0986137\ttotal: 1m 59s\tremaining: 1m 33s\n",
      "560:\tlearn: 0.0985540\ttotal: 1m 59s\tremaining: 1m 33s\n",
      "561:\tlearn: 0.0985145\ttotal: 1m 59s\tremaining: 1m 33s\n",
      "562:\tlearn: 0.0984522\ttotal: 2m\tremaining: 1m 33s\n",
      "563:\tlearn: 0.0984041\ttotal: 2m\tremaining: 1m 33s\n",
      "564:\tlearn: 0.0983712\ttotal: 2m\tremaining: 1m 32s\n",
      "565:\tlearn: 0.0983090\ttotal: 2m\tremaining: 1m 32s\n",
      "566:\tlearn: 0.0982702\ttotal: 2m 1s\tremaining: 1m 32s\n",
      "567:\tlearn: 0.0982241\ttotal: 2m 1s\tremaining: 1m 32s\n",
      "568:\tlearn: 0.0981868\ttotal: 2m 1s\tremaining: 1m 32s\n",
      "569:\tlearn: 0.0981464\ttotal: 2m 1s\tremaining: 1m 31s\n",
      "570:\tlearn: 0.0980657\ttotal: 2m 2s\tremaining: 1m 31s\n",
      "571:\tlearn: 0.0980119\ttotal: 2m 2s\tremaining: 1m 31s\n",
      "572:\tlearn: 0.0979884\ttotal: 2m 2s\tremaining: 1m 31s\n",
      "573:\tlearn: 0.0979340\ttotal: 2m 2s\tremaining: 1m 31s\n",
      "574:\tlearn: 0.0978544\ttotal: 2m 2s\tremaining: 1m 30s\n",
      "575:\tlearn: 0.0978354\ttotal: 2m 3s\tremaining: 1m 30s\n",
      "576:\tlearn: 0.0977736\ttotal: 2m 3s\tremaining: 1m 30s\n",
      "577:\tlearn: 0.0977671\ttotal: 2m 3s\tremaining: 1m 30s\n",
      "578:\tlearn: 0.0977189\ttotal: 2m 3s\tremaining: 1m 30s\n",
      "579:\tlearn: 0.0976717\ttotal: 2m 3s\tremaining: 1m 29s\n",
      "580:\tlearn: 0.0976322\ttotal: 2m 4s\tremaining: 1m 29s\n",
      "581:\tlearn: 0.0975758\ttotal: 2m 4s\tremaining: 1m 29s\n",
      "582:\tlearn: 0.0975144\ttotal: 2m 4s\tremaining: 1m 29s\n",
      "583:\tlearn: 0.0974651\ttotal: 2m 4s\tremaining: 1m 28s\n",
      "584:\tlearn: 0.0974117\ttotal: 2m 5s\tremaining: 1m 28s\n",
      "585:\tlearn: 0.0973614\ttotal: 2m 5s\tremaining: 1m 28s\n",
      "586:\tlearn: 0.0973159\ttotal: 2m 5s\tremaining: 1m 28s\n",
      "587:\tlearn: 0.0972856\ttotal: 2m 5s\tremaining: 1m 28s\n",
      "588:\tlearn: 0.0972684\ttotal: 2m 6s\tremaining: 1m 27s\n",
      "589:\tlearn: 0.0972272\ttotal: 2m 6s\tremaining: 1m 27s\n",
      "590:\tlearn: 0.0971984\ttotal: 2m 6s\tremaining: 1m 27s\n",
      "591:\tlearn: 0.0971749\ttotal: 2m 6s\tremaining: 1m 27s\n",
      "592:\tlearn: 0.0971327\ttotal: 2m 6s\tremaining: 1m 27s\n",
      "593:\tlearn: 0.0970701\ttotal: 2m 7s\tremaining: 1m 26s\n",
      "594:\tlearn: 0.0970302\ttotal: 2m 7s\tremaining: 1m 26s\n",
      "595:\tlearn: 0.0969825\ttotal: 2m 7s\tremaining: 1m 26s\n",
      "596:\tlearn: 0.0969518\ttotal: 2m 7s\tremaining: 1m 26s\n",
      "597:\tlearn: 0.0969212\ttotal: 2m 8s\tremaining: 1m 26s\n",
      "598:\tlearn: 0.0968791\ttotal: 2m 8s\tremaining: 1m 25s\n",
      "599:\tlearn: 0.0968487\ttotal: 2m 8s\tremaining: 1m 25s\n",
      "600:\tlearn: 0.0967985\ttotal: 2m 8s\tremaining: 1m 25s\n",
      "601:\tlearn: 0.0967593\ttotal: 2m 8s\tremaining: 1m 25s\n",
      "602:\tlearn: 0.0967263\ttotal: 2m 9s\tremaining: 1m 25s\n",
      "603:\tlearn: 0.0966986\ttotal: 2m 9s\tremaining: 1m 24s\n",
      "604:\tlearn: 0.0966469\ttotal: 2m 9s\tremaining: 1m 24s\n",
      "605:\tlearn: 0.0965844\ttotal: 2m 9s\tremaining: 1m 24s\n",
      "606:\tlearn: 0.0965659\ttotal: 2m 9s\tremaining: 1m 24s\n",
      "607:\tlearn: 0.0965347\ttotal: 2m 10s\tremaining: 1m 23s\n",
      "608:\tlearn: 0.0964390\ttotal: 2m 10s\tremaining: 1m 23s\n",
      "609:\tlearn: 0.0963327\ttotal: 2m 10s\tremaining: 1m 23s\n",
      "610:\tlearn: 0.0962879\ttotal: 2m 10s\tremaining: 1m 23s\n",
      "611:\tlearn: 0.0962663\ttotal: 2m 11s\tremaining: 1m 23s\n",
      "612:\tlearn: 0.0962100\ttotal: 2m 11s\tremaining: 1m 22s\n",
      "613:\tlearn: 0.0961907\ttotal: 2m 11s\tremaining: 1m 22s\n",
      "614:\tlearn: 0.0961627\ttotal: 2m 11s\tremaining: 1m 22s\n",
      "615:\tlearn: 0.0961242\ttotal: 2m 11s\tremaining: 1m 22s\n",
      "616:\tlearn: 0.0960833\ttotal: 2m 12s\tremaining: 1m 22s\n",
      "617:\tlearn: 0.0960386\ttotal: 2m 12s\tremaining: 1m 21s\n",
      "618:\tlearn: 0.0960116\ttotal: 2m 12s\tremaining: 1m 21s\n",
      "619:\tlearn: 0.0959072\ttotal: 2m 12s\tremaining: 1m 21s\n",
      "620:\tlearn: 0.0958662\ttotal: 2m 13s\tremaining: 1m 21s\n",
      "621:\tlearn: 0.0958343\ttotal: 2m 13s\tremaining: 1m 20s\n",
      "622:\tlearn: 0.0957749\ttotal: 2m 13s\tremaining: 1m 20s\n",
      "623:\tlearn: 0.0957193\ttotal: 2m 13s\tremaining: 1m 20s\n",
      "624:\tlearn: 0.0956686\ttotal: 2m 13s\tremaining: 1m 20s\n",
      "625:\tlearn: 0.0955907\ttotal: 2m 14s\tremaining: 1m 20s\n",
      "626:\tlearn: 0.0955525\ttotal: 2m 14s\tremaining: 1m 19s\n",
      "627:\tlearn: 0.0954483\ttotal: 2m 14s\tremaining: 1m 19s\n",
      "628:\tlearn: 0.0953940\ttotal: 2m 14s\tremaining: 1m 19s\n",
      "629:\tlearn: 0.0953614\ttotal: 2m 14s\tremaining: 1m 19s\n",
      "630:\tlearn: 0.0953202\ttotal: 2m 15s\tremaining: 1m 19s\n",
      "631:\tlearn: 0.0952865\ttotal: 2m 15s\tremaining: 1m 18s\n",
      "632:\tlearn: 0.0952318\ttotal: 2m 15s\tremaining: 1m 18s\n",
      "633:\tlearn: 0.0951989\ttotal: 2m 15s\tremaining: 1m 18s\n",
      "634:\tlearn: 0.0951109\ttotal: 2m 16s\tremaining: 1m 18s\n",
      "635:\tlearn: 0.0950372\ttotal: 2m 16s\tremaining: 1m 17s\n",
      "636:\tlearn: 0.0949858\ttotal: 2m 16s\tremaining: 1m 17s\n",
      "637:\tlearn: 0.0949449\ttotal: 2m 16s\tremaining: 1m 17s\n",
      "638:\tlearn: 0.0949068\ttotal: 2m 16s\tremaining: 1m 17s\n",
      "639:\tlearn: 0.0948408\ttotal: 2m 17s\tremaining: 1m 17s\n",
      "640:\tlearn: 0.0948130\ttotal: 2m 17s\tremaining: 1m 16s\n",
      "641:\tlearn: 0.0947880\ttotal: 2m 17s\tremaining: 1m 16s\n",
      "642:\tlearn: 0.0947385\ttotal: 2m 17s\tremaining: 1m 16s\n",
      "643:\tlearn: 0.0946676\ttotal: 2m 17s\tremaining: 1m 16s\n",
      "644:\tlearn: 0.0946321\ttotal: 2m 18s\tremaining: 1m 16s\n",
      "645:\tlearn: 0.0945800\ttotal: 2m 18s\tremaining: 1m 15s\n",
      "646:\tlearn: 0.0944688\ttotal: 2m 18s\tremaining: 1m 15s\n",
      "647:\tlearn: 0.0944234\ttotal: 2m 18s\tremaining: 1m 15s\n",
      "648:\tlearn: 0.0943517\ttotal: 2m 19s\tremaining: 1m 15s\n",
      "649:\tlearn: 0.0943028\ttotal: 2m 19s\tremaining: 1m 14s\n",
      "650:\tlearn: 0.0942338\ttotal: 2m 19s\tremaining: 1m 14s\n",
      "651:\tlearn: 0.0941870\ttotal: 2m 19s\tremaining: 1m 14s\n",
      "652:\tlearn: 0.0941515\ttotal: 2m 19s\tremaining: 1m 14s\n",
      "653:\tlearn: 0.0941178\ttotal: 2m 20s\tremaining: 1m 14s\n",
      "654:\tlearn: 0.0940630\ttotal: 2m 20s\tremaining: 1m 13s\n",
      "655:\tlearn: 0.0940069\ttotal: 2m 20s\tremaining: 1m 13s\n",
      "656:\tlearn: 0.0939659\ttotal: 2m 20s\tremaining: 1m 13s\n",
      "657:\tlearn: 0.0939426\ttotal: 2m 20s\tremaining: 1m 13s\n",
      "658:\tlearn: 0.0938998\ttotal: 2m 21s\tremaining: 1m 13s\n",
      "659:\tlearn: 0.0938642\ttotal: 2m 21s\tremaining: 1m 12s\n",
      "660:\tlearn: 0.0938408\ttotal: 2m 21s\tremaining: 1m 12s\n",
      "661:\tlearn: 0.0938180\ttotal: 2m 21s\tremaining: 1m 12s\n",
      "662:\tlearn: 0.0938002\ttotal: 2m 21s\tremaining: 1m 12s\n",
      "663:\tlearn: 0.0937803\ttotal: 2m 22s\tremaining: 1m 11s\n",
      "664:\tlearn: 0.0937180\ttotal: 2m 22s\tremaining: 1m 11s\n",
      "665:\tlearn: 0.0936681\ttotal: 2m 22s\tremaining: 1m 11s\n",
      "666:\tlearn: 0.0936217\ttotal: 2m 23s\tremaining: 1m 11s\n",
      "667:\tlearn: 0.0935696\ttotal: 2m 23s\tremaining: 1m 11s\n",
      "668:\tlearn: 0.0935402\ttotal: 2m 23s\tremaining: 1m 11s\n",
      "669:\tlearn: 0.0935234\ttotal: 2m 23s\tremaining: 1m 10s\n",
      "670:\tlearn: 0.0934758\ttotal: 2m 23s\tremaining: 1m 10s\n",
      "671:\tlearn: 0.0934351\ttotal: 2m 24s\tremaining: 1m 10s\n",
      "672:\tlearn: 0.0934021\ttotal: 2m 24s\tremaining: 1m 10s\n",
      "673:\tlearn: 0.0933809\ttotal: 2m 24s\tremaining: 1m 9s\n",
      "674:\tlearn: 0.0932947\ttotal: 2m 24s\tremaining: 1m 9s\n",
      "675:\tlearn: 0.0932602\ttotal: 2m 25s\tremaining: 1m 9s\n",
      "676:\tlearn: 0.0932326\ttotal: 2m 25s\tremaining: 1m 9s\n",
      "677:\tlearn: 0.0932018\ttotal: 2m 25s\tremaining: 1m 9s\n",
      "678:\tlearn: 0.0931667\ttotal: 2m 25s\tremaining: 1m 8s\n",
      "679:\tlearn: 0.0931245\ttotal: 2m 25s\tremaining: 1m 8s\n",
      "680:\tlearn: 0.0930746\ttotal: 2m 26s\tremaining: 1m 8s\n",
      "681:\tlearn: 0.0930396\ttotal: 2m 26s\tremaining: 1m 8s\n",
      "682:\tlearn: 0.0930009\ttotal: 2m 26s\tremaining: 1m 8s\n",
      "683:\tlearn: 0.0929668\ttotal: 2m 26s\tremaining: 1m 7s\n",
      "684:\tlearn: 0.0929169\ttotal: 2m 27s\tremaining: 1m 7s\n",
      "685:\tlearn: 0.0928750\ttotal: 2m 27s\tremaining: 1m 7s\n",
      "686:\tlearn: 0.0927919\ttotal: 2m 27s\tremaining: 1m 7s\n",
      "687:\tlearn: 0.0927804\ttotal: 2m 27s\tremaining: 1m 6s\n",
      "688:\tlearn: 0.0927395\ttotal: 2m 27s\tremaining: 1m 6s\n",
      "689:\tlearn: 0.0927061\ttotal: 2m 28s\tremaining: 1m 6s\n",
      "690:\tlearn: 0.0926651\ttotal: 2m 28s\tremaining: 1m 6s\n",
      "691:\tlearn: 0.0926034\ttotal: 2m 28s\tremaining: 1m 6s\n",
      "692:\tlearn: 0.0925689\ttotal: 2m 28s\tremaining: 1m 5s\n",
      "693:\tlearn: 0.0925238\ttotal: 2m 28s\tremaining: 1m 5s\n",
      "694:\tlearn: 0.0924604\ttotal: 2m 29s\tremaining: 1m 5s\n",
      "695:\tlearn: 0.0924181\ttotal: 2m 29s\tremaining: 1m 5s\n",
      "696:\tlearn: 0.0923627\ttotal: 2m 29s\tremaining: 1m 5s\n",
      "697:\tlearn: 0.0923250\ttotal: 2m 29s\tremaining: 1m 4s\n",
      "698:\tlearn: 0.0922854\ttotal: 2m 30s\tremaining: 1m 4s\n",
      "699:\tlearn: 0.0922607\ttotal: 2m 30s\tremaining: 1m 4s\n",
      "700:\tlearn: 0.0922169\ttotal: 2m 30s\tremaining: 1m 4s\n",
      "701:\tlearn: 0.0921560\ttotal: 2m 30s\tremaining: 1m 3s\n",
      "702:\tlearn: 0.0921060\ttotal: 2m 30s\tremaining: 1m 3s\n",
      "703:\tlearn: 0.0920819\ttotal: 2m 31s\tremaining: 1m 3s\n",
      "704:\tlearn: 0.0920096\ttotal: 2m 31s\tremaining: 1m 3s\n",
      "705:\tlearn: 0.0919667\ttotal: 2m 31s\tremaining: 1m 3s\n",
      "706:\tlearn: 0.0919369\ttotal: 2m 31s\tremaining: 1m 2s\n",
      "707:\tlearn: 0.0918835\ttotal: 2m 32s\tremaining: 1m 2s\n",
      "708:\tlearn: 0.0918535\ttotal: 2m 32s\tremaining: 1m 2s\n",
      "709:\tlearn: 0.0917921\ttotal: 2m 32s\tremaining: 1m 2s\n",
      "710:\tlearn: 0.0917442\ttotal: 2m 32s\tremaining: 1m 2s\n",
      "711:\tlearn: 0.0917145\ttotal: 2m 32s\tremaining: 1m 1s\n",
      "712:\tlearn: 0.0916936\ttotal: 2m 33s\tremaining: 1m 1s\n",
      "713:\tlearn: 0.0916612\ttotal: 2m 33s\tremaining: 1m 1s\n",
      "714:\tlearn: 0.0916073\ttotal: 2m 33s\tremaining: 1m 1s\n",
      "715:\tlearn: 0.0915910\ttotal: 2m 33s\tremaining: 1m 1s\n",
      "716:\tlearn: 0.0915475\ttotal: 2m 34s\tremaining: 1m\n",
      "717:\tlearn: 0.0915149\ttotal: 2m 34s\tremaining: 1m\n",
      "718:\tlearn: 0.0914751\ttotal: 2m 34s\tremaining: 1m\n",
      "719:\tlearn: 0.0914398\ttotal: 2m 34s\tremaining: 1m\n",
      "720:\tlearn: 0.0914002\ttotal: 2m 34s\tremaining: 60s\n",
      "721:\tlearn: 0.0913041\ttotal: 2m 35s\tremaining: 59.7s\n",
      "722:\tlearn: 0.0912773\ttotal: 2m 35s\tremaining: 59.5s\n",
      "723:\tlearn: 0.0912726\ttotal: 2m 35s\tremaining: 59.3s\n",
      "724:\tlearn: 0.0912452\ttotal: 2m 35s\tremaining: 59.1s\n",
      "725:\tlearn: 0.0912345\ttotal: 2m 36s\tremaining: 58.9s\n",
      "726:\tlearn: 0.0911699\ttotal: 2m 36s\tremaining: 58.7s\n",
      "727:\tlearn: 0.0911362\ttotal: 2m 36s\tremaining: 58.5s\n",
      "728:\tlearn: 0.0911051\ttotal: 2m 36s\tremaining: 58.2s\n",
      "729:\tlearn: 0.0910733\ttotal: 2m 36s\tremaining: 58s\n",
      "730:\tlearn: 0.0910349\ttotal: 2m 37s\tremaining: 57.8s\n",
      "731:\tlearn: 0.0909775\ttotal: 2m 37s\tremaining: 57.6s\n",
      "732:\tlearn: 0.0908846\ttotal: 2m 37s\tremaining: 57.4s\n",
      "733:\tlearn: 0.0908440\ttotal: 2m 37s\tremaining: 57.1s\n",
      "734:\tlearn: 0.0907840\ttotal: 2m 37s\tremaining: 56.9s\n",
      "735:\tlearn: 0.0907555\ttotal: 2m 38s\tremaining: 56.7s\n",
      "736:\tlearn: 0.0906972\ttotal: 2m 38s\tremaining: 56.5s\n",
      "737:\tlearn: 0.0906428\ttotal: 2m 38s\tremaining: 56.3s\n",
      "738:\tlearn: 0.0905736\ttotal: 2m 38s\tremaining: 56.1s\n",
      "739:\tlearn: 0.0905340\ttotal: 2m 38s\tremaining: 55.9s\n",
      "740:\tlearn: 0.0904876\ttotal: 2m 39s\tremaining: 55.6s\n",
      "741:\tlearn: 0.0904205\ttotal: 2m 39s\tremaining: 55.4s\n",
      "742:\tlearn: 0.0903944\ttotal: 2m 39s\tremaining: 55.2s\n",
      "743:\tlearn: 0.0903464\ttotal: 2m 39s\tremaining: 55s\n",
      "744:\tlearn: 0.0903363\ttotal: 2m 40s\tremaining: 54.8s\n",
      "745:\tlearn: 0.0902987\ttotal: 2m 40s\tremaining: 54.6s\n",
      "746:\tlearn: 0.0902901\ttotal: 2m 40s\tremaining: 54.4s\n",
      "747:\tlearn: 0.0902532\ttotal: 2m 40s\tremaining: 54.2s\n",
      "748:\tlearn: 0.0902090\ttotal: 2m 40s\tremaining: 53.9s\n",
      "749:\tlearn: 0.0901704\ttotal: 2m 41s\tremaining: 53.7s\n",
      "750:\tlearn: 0.0901339\ttotal: 2m 41s\tremaining: 53.5s\n",
      "751:\tlearn: 0.0901061\ttotal: 2m 41s\tremaining: 53.3s\n",
      "752:\tlearn: 0.0900665\ttotal: 2m 41s\tremaining: 53.1s\n",
      "753:\tlearn: 0.0900573\ttotal: 2m 42s\tremaining: 52.9s\n",
      "754:\tlearn: 0.0900001\ttotal: 2m 42s\tremaining: 52.7s\n",
      "755:\tlearn: 0.0899664\ttotal: 2m 42s\tremaining: 52.5s\n",
      "756:\tlearn: 0.0899434\ttotal: 2m 42s\tremaining: 52.2s\n",
      "757:\tlearn: 0.0898969\ttotal: 2m 42s\tremaining: 52s\n",
      "758:\tlearn: 0.0898719\ttotal: 2m 43s\tremaining: 51.8s\n",
      "759:\tlearn: 0.0898144\ttotal: 2m 43s\tremaining: 51.6s\n",
      "760:\tlearn: 0.0897595\ttotal: 2m 43s\tremaining: 51.4s\n",
      "761:\tlearn: 0.0897060\ttotal: 2m 43s\tremaining: 51.1s\n",
      "762:\tlearn: 0.0896602\ttotal: 2m 43s\tremaining: 50.9s\n",
      "763:\tlearn: 0.0896319\ttotal: 2m 44s\tremaining: 50.7s\n",
      "764:\tlearn: 0.0896028\ttotal: 2m 44s\tremaining: 50.6s\n",
      "765:\tlearn: 0.0895676\ttotal: 2m 44s\tremaining: 50.3s\n",
      "766:\tlearn: 0.0895298\ttotal: 2m 45s\tremaining: 50.1s\n",
      "767:\tlearn: 0.0894540\ttotal: 2m 45s\tremaining: 49.9s\n",
      "768:\tlearn: 0.0894295\ttotal: 2m 45s\tremaining: 49.7s\n",
      "769:\tlearn: 0.0894164\ttotal: 2m 45s\tremaining: 49.5s\n",
      "770:\tlearn: 0.0893563\ttotal: 2m 45s\tremaining: 49.3s\n",
      "771:\tlearn: 0.0892990\ttotal: 2m 46s\tremaining: 49.1s\n",
      "772:\tlearn: 0.0892575\ttotal: 2m 46s\tremaining: 48.9s\n",
      "773:\tlearn: 0.0891957\ttotal: 2m 46s\tremaining: 48.7s\n",
      "774:\tlearn: 0.0891735\ttotal: 2m 46s\tremaining: 48.5s\n",
      "775:\tlearn: 0.0891527\ttotal: 2m 47s\tremaining: 48.2s\n",
      "776:\tlearn: 0.0890698\ttotal: 2m 47s\tremaining: 48s\n",
      "777:\tlearn: 0.0890148\ttotal: 2m 47s\tremaining: 47.8s\n",
      "778:\tlearn: 0.0889924\ttotal: 2m 47s\tremaining: 47.6s\n",
      "779:\tlearn: 0.0889741\ttotal: 2m 47s\tremaining: 47.4s\n",
      "780:\tlearn: 0.0889061\ttotal: 2m 48s\tremaining: 47.1s\n",
      "781:\tlearn: 0.0888662\ttotal: 2m 48s\tremaining: 46.9s\n",
      "782:\tlearn: 0.0888367\ttotal: 2m 48s\tremaining: 46.7s\n",
      "783:\tlearn: 0.0887976\ttotal: 2m 48s\tremaining: 46.5s\n",
      "784:\tlearn: 0.0887665\ttotal: 2m 48s\tremaining: 46.3s\n",
      "785:\tlearn: 0.0887414\ttotal: 2m 49s\tremaining: 46s\n",
      "786:\tlearn: 0.0887128\ttotal: 2m 49s\tremaining: 45.8s\n",
      "787:\tlearn: 0.0886822\ttotal: 2m 49s\tremaining: 45.6s\n",
      "788:\tlearn: 0.0886441\ttotal: 2m 49s\tremaining: 45.4s\n",
      "789:\tlearn: 0.0886123\ttotal: 2m 49s\tremaining: 45.2s\n",
      "790:\tlearn: 0.0885310\ttotal: 2m 50s\tremaining: 45s\n",
      "791:\tlearn: 0.0884663\ttotal: 2m 50s\tremaining: 44.7s\n",
      "792:\tlearn: 0.0884302\ttotal: 2m 50s\tremaining: 44.5s\n",
      "793:\tlearn: 0.0884001\ttotal: 2m 50s\tremaining: 44.3s\n",
      "794:\tlearn: 0.0883512\ttotal: 2m 50s\tremaining: 44.1s\n",
      "795:\tlearn: 0.0883014\ttotal: 2m 51s\tremaining: 43.9s\n",
      "796:\tlearn: 0.0882982\ttotal: 2m 51s\tremaining: 43.6s\n",
      "797:\tlearn: 0.0882928\ttotal: 2m 51s\tremaining: 43.4s\n",
      "798:\tlearn: 0.0882725\ttotal: 2m 51s\tremaining: 43.2s\n",
      "799:\tlearn: 0.0882234\ttotal: 2m 51s\tremaining: 43s\n",
      "800:\tlearn: 0.0881920\ttotal: 2m 52s\tremaining: 42.8s\n",
      "801:\tlearn: 0.0881564\ttotal: 2m 52s\tremaining: 42.6s\n",
      "802:\tlearn: 0.0880703\ttotal: 2m 52s\tremaining: 42.3s\n",
      "803:\tlearn: 0.0880016\ttotal: 2m 52s\tremaining: 42.1s\n",
      "804:\tlearn: 0.0879730\ttotal: 2m 53s\tremaining: 41.9s\n",
      "805:\tlearn: 0.0879412\ttotal: 2m 53s\tremaining: 41.7s\n",
      "806:\tlearn: 0.0879198\ttotal: 2m 53s\tremaining: 41.5s\n",
      "807:\tlearn: 0.0879031\ttotal: 2m 53s\tremaining: 41.3s\n",
      "808:\tlearn: 0.0878641\ttotal: 2m 53s\tremaining: 41s\n",
      "809:\tlearn: 0.0878217\ttotal: 2m 54s\tremaining: 40.8s\n",
      "810:\tlearn: 0.0877846\ttotal: 2m 54s\tremaining: 40.6s\n",
      "811:\tlearn: 0.0877214\ttotal: 2m 54s\tremaining: 40.4s\n",
      "812:\tlearn: 0.0876379\ttotal: 2m 54s\tremaining: 40.2s\n",
      "813:\tlearn: 0.0875764\ttotal: 2m 54s\tremaining: 40s\n",
      "814:\tlearn: 0.0875493\ttotal: 2m 55s\tremaining: 39.7s\n",
      "815:\tlearn: 0.0875207\ttotal: 2m 55s\tremaining: 39.5s\n",
      "816:\tlearn: 0.0874982\ttotal: 2m 55s\tremaining: 39.3s\n",
      "817:\tlearn: 0.0874606\ttotal: 2m 55s\tremaining: 39.1s\n",
      "818:\tlearn: 0.0874168\ttotal: 2m 55s\tremaining: 38.9s\n",
      "819:\tlearn: 0.0873852\ttotal: 2m 56s\tremaining: 38.7s\n",
      "820:\tlearn: 0.0873713\ttotal: 2m 56s\tremaining: 38.4s\n",
      "821:\tlearn: 0.0873362\ttotal: 2m 56s\tremaining: 38.2s\n",
      "822:\tlearn: 0.0873055\ttotal: 2m 56s\tremaining: 38s\n",
      "823:\tlearn: 0.0872620\ttotal: 2m 56s\tremaining: 37.8s\n",
      "824:\tlearn: 0.0872380\ttotal: 2m 57s\tremaining: 37.6s\n",
      "825:\tlearn: 0.0872112\ttotal: 2m 57s\tremaining: 37.4s\n",
      "826:\tlearn: 0.0871563\ttotal: 2m 57s\tremaining: 37.2s\n",
      "827:\tlearn: 0.0871038\ttotal: 2m 57s\tremaining: 36.9s\n",
      "828:\tlearn: 0.0870658\ttotal: 2m 58s\tremaining: 36.7s\n",
      "829:\tlearn: 0.0870491\ttotal: 2m 58s\tremaining: 36.5s\n",
      "830:\tlearn: 0.0870023\ttotal: 2m 58s\tremaining: 36.3s\n",
      "831:\tlearn: 0.0869498\ttotal: 2m 58s\tremaining: 36.1s\n",
      "832:\tlearn: 0.0869411\ttotal: 2m 58s\tremaining: 35.9s\n",
      "833:\tlearn: 0.0868981\ttotal: 2m 59s\tremaining: 35.6s\n",
      "834:\tlearn: 0.0868691\ttotal: 2m 59s\tremaining: 35.4s\n",
      "835:\tlearn: 0.0868417\ttotal: 2m 59s\tremaining: 35.2s\n",
      "836:\tlearn: 0.0868176\ttotal: 2m 59s\tremaining: 35s\n",
      "837:\tlearn: 0.0867902\ttotal: 2m 59s\tremaining: 34.8s\n",
      "838:\tlearn: 0.0867017\ttotal: 3m\tremaining: 34.6s\n",
      "839:\tlearn: 0.0866769\ttotal: 3m\tremaining: 34.3s\n",
      "840:\tlearn: 0.0866218\ttotal: 3m\tremaining: 34.1s\n",
      "841:\tlearn: 0.0865760\ttotal: 3m\tremaining: 33.9s\n",
      "842:\tlearn: 0.0865408\ttotal: 3m\tremaining: 33.7s\n",
      "843:\tlearn: 0.0865129\ttotal: 3m 1s\tremaining: 33.5s\n",
      "844:\tlearn: 0.0864540\ttotal: 3m 1s\tremaining: 33.3s\n",
      "845:\tlearn: 0.0864273\ttotal: 3m 1s\tremaining: 33.1s\n",
      "846:\tlearn: 0.0863590\ttotal: 3m 1s\tremaining: 32.8s\n",
      "847:\tlearn: 0.0863300\ttotal: 3m 2s\tremaining: 32.6s\n",
      "848:\tlearn: 0.0862787\ttotal: 3m 2s\tremaining: 32.4s\n",
      "849:\tlearn: 0.0862250\ttotal: 3m 2s\tremaining: 32.2s\n",
      "850:\tlearn: 0.0861749\ttotal: 3m 2s\tremaining: 32s\n",
      "851:\tlearn: 0.0861454\ttotal: 3m 2s\tremaining: 31.8s\n",
      "852:\tlearn: 0.0860834\ttotal: 3m 3s\tremaining: 31.6s\n",
      "853:\tlearn: 0.0860320\ttotal: 3m 3s\tremaining: 31.3s\n",
      "854:\tlearn: 0.0859884\ttotal: 3m 3s\tremaining: 31.1s\n",
      "855:\tlearn: 0.0859230\ttotal: 3m 3s\tremaining: 30.9s\n",
      "856:\tlearn: 0.0858785\ttotal: 3m 3s\tremaining: 30.7s\n",
      "857:\tlearn: 0.0858350\ttotal: 3m 4s\tremaining: 30.5s\n",
      "858:\tlearn: 0.0857916\ttotal: 3m 4s\tremaining: 30.3s\n",
      "859:\tlearn: 0.0857667\ttotal: 3m 4s\tremaining: 30.1s\n",
      "860:\tlearn: 0.0857345\ttotal: 3m 4s\tremaining: 29.8s\n",
      "861:\tlearn: 0.0857037\ttotal: 3m 5s\tremaining: 29.6s\n",
      "862:\tlearn: 0.0856609\ttotal: 3m 5s\tremaining: 29.4s\n",
      "863:\tlearn: 0.0856200\ttotal: 3m 5s\tremaining: 29.2s\n",
      "864:\tlearn: 0.0855939\ttotal: 3m 5s\tremaining: 29s\n",
      "865:\tlearn: 0.0855779\ttotal: 3m 5s\tremaining: 28.8s\n",
      "866:\tlearn: 0.0855753\ttotal: 3m 6s\tremaining: 28.6s\n",
      "867:\tlearn: 0.0855458\ttotal: 3m 6s\tremaining: 28.3s\n",
      "868:\tlearn: 0.0855069\ttotal: 3m 6s\tremaining: 28.1s\n",
      "869:\tlearn: 0.0854991\ttotal: 3m 6s\tremaining: 27.9s\n",
      "870:\tlearn: 0.0854709\ttotal: 3m 7s\tremaining: 27.7s\n",
      "871:\tlearn: 0.0854553\ttotal: 3m 7s\tremaining: 27.5s\n",
      "872:\tlearn: 0.0854251\ttotal: 3m 7s\tremaining: 27.3s\n",
      "873:\tlearn: 0.0854084\ttotal: 3m 7s\tremaining: 27.1s\n",
      "874:\tlearn: 0.0853740\ttotal: 3m 7s\tremaining: 26.9s\n",
      "875:\tlearn: 0.0853476\ttotal: 3m 8s\tremaining: 26.6s\n",
      "876:\tlearn: 0.0853068\ttotal: 3m 8s\tremaining: 26.4s\n",
      "877:\tlearn: 0.0852773\ttotal: 3m 8s\tremaining: 26.2s\n",
      "878:\tlearn: 0.0852605\ttotal: 3m 8s\tremaining: 26s\n",
      "879:\tlearn: 0.0852264\ttotal: 3m 9s\tremaining: 25.8s\n",
      "880:\tlearn: 0.0851951\ttotal: 3m 9s\tremaining: 25.6s\n",
      "881:\tlearn: 0.0851658\ttotal: 3m 9s\tremaining: 25.4s\n",
      "882:\tlearn: 0.0851346\ttotal: 3m 9s\tremaining: 25.2s\n",
      "883:\tlearn: 0.0851113\ttotal: 3m 10s\tremaining: 24.9s\n",
      "884:\tlearn: 0.0850756\ttotal: 3m 10s\tremaining: 24.7s\n",
      "885:\tlearn: 0.0850305\ttotal: 3m 10s\tremaining: 24.5s\n",
      "886:\tlearn: 0.0849680\ttotal: 3m 10s\tremaining: 24.3s\n",
      "887:\tlearn: 0.0849386\ttotal: 3m 11s\tremaining: 24.1s\n",
      "888:\tlearn: 0.0849325\ttotal: 3m 11s\tremaining: 23.9s\n",
      "889:\tlearn: 0.0848847\ttotal: 3m 11s\tremaining: 23.7s\n",
      "890:\tlearn: 0.0848730\ttotal: 3m 11s\tremaining: 23.5s\n",
      "891:\tlearn: 0.0848374\ttotal: 3m 12s\tremaining: 23.3s\n",
      "892:\tlearn: 0.0847988\ttotal: 3m 12s\tremaining: 23.1s\n",
      "893:\tlearn: 0.0847645\ttotal: 3m 12s\tremaining: 22.8s\n",
      "894:\tlearn: 0.0847461\ttotal: 3m 12s\tremaining: 22.6s\n",
      "895:\tlearn: 0.0847058\ttotal: 3m 13s\tremaining: 22.4s\n",
      "896:\tlearn: 0.0846685\ttotal: 3m 13s\tremaining: 22.2s\n",
      "897:\tlearn: 0.0846147\ttotal: 3m 13s\tremaining: 22s\n",
      "898:\tlearn: 0.0845680\ttotal: 3m 14s\tremaining: 21.8s\n",
      "899:\tlearn: 0.0845407\ttotal: 3m 14s\tremaining: 21.6s\n",
      "900:\tlearn: 0.0845070\ttotal: 3m 14s\tremaining: 21.4s\n",
      "901:\tlearn: 0.0844641\ttotal: 3m 14s\tremaining: 21.1s\n",
      "902:\tlearn: 0.0844468\ttotal: 3m 14s\tremaining: 20.9s\n",
      "903:\tlearn: 0.0843689\ttotal: 3m 15s\tremaining: 20.7s\n",
      "904:\tlearn: 0.0843101\ttotal: 3m 15s\tremaining: 20.5s\n",
      "905:\tlearn: 0.0842701\ttotal: 3m 15s\tremaining: 20.3s\n",
      "906:\tlearn: 0.0842328\ttotal: 3m 15s\tremaining: 20.1s\n",
      "907:\tlearn: 0.0842115\ttotal: 3m 15s\tremaining: 19.9s\n",
      "908:\tlearn: 0.0841757\ttotal: 3m 16s\tremaining: 19.6s\n",
      "909:\tlearn: 0.0841587\ttotal: 3m 16s\tremaining: 19.4s\n",
      "910:\tlearn: 0.0841252\ttotal: 3m 16s\tremaining: 19.2s\n",
      "911:\tlearn: 0.0840742\ttotal: 3m 16s\tremaining: 19s\n",
      "912:\tlearn: 0.0840249\ttotal: 3m 17s\tremaining: 18.8s\n",
      "913:\tlearn: 0.0839970\ttotal: 3m 17s\tremaining: 18.6s\n",
      "914:\tlearn: 0.0839336\ttotal: 3m 17s\tremaining: 18.3s\n",
      "915:\tlearn: 0.0839003\ttotal: 3m 17s\tremaining: 18.1s\n",
      "916:\tlearn: 0.0838621\ttotal: 3m 17s\tremaining: 17.9s\n",
      "917:\tlearn: 0.0838095\ttotal: 3m 18s\tremaining: 17.7s\n",
      "918:\tlearn: 0.0837582\ttotal: 3m 18s\tremaining: 17.5s\n",
      "919:\tlearn: 0.0837422\ttotal: 3m 18s\tremaining: 17.3s\n",
      "920:\tlearn: 0.0837179\ttotal: 3m 18s\tremaining: 17s\n",
      "921:\tlearn: 0.0836824\ttotal: 3m 18s\tremaining: 16.8s\n",
      "922:\tlearn: 0.0836500\ttotal: 3m 19s\tremaining: 16.6s\n",
      "923:\tlearn: 0.0836073\ttotal: 3m 19s\tremaining: 16.4s\n",
      "924:\tlearn: 0.0835801\ttotal: 3m 19s\tremaining: 16.2s\n",
      "925:\tlearn: 0.0835565\ttotal: 3m 19s\tremaining: 16s\n",
      "926:\tlearn: 0.0835167\ttotal: 3m 20s\tremaining: 15.8s\n",
      "927:\tlearn: 0.0835128\ttotal: 3m 20s\tremaining: 15.5s\n",
      "928:\tlearn: 0.0834346\ttotal: 3m 20s\tremaining: 15.3s\n",
      "929:\tlearn: 0.0834150\ttotal: 3m 20s\tremaining: 15.1s\n",
      "930:\tlearn: 0.0833641\ttotal: 3m 20s\tremaining: 14.9s\n",
      "931:\tlearn: 0.0833021\ttotal: 3m 21s\tremaining: 14.7s\n",
      "932:\tlearn: 0.0832964\ttotal: 3m 21s\tremaining: 14.5s\n",
      "933:\tlearn: 0.0832611\ttotal: 3m 21s\tremaining: 14.2s\n",
      "934:\tlearn: 0.0832246\ttotal: 3m 21s\tremaining: 14s\n",
      "935:\tlearn: 0.0832015\ttotal: 3m 21s\tremaining: 13.8s\n",
      "936:\tlearn: 0.0831694\ttotal: 3m 22s\tremaining: 13.6s\n",
      "937:\tlearn: 0.0831536\ttotal: 3m 22s\tremaining: 13.4s\n",
      "938:\tlearn: 0.0831236\ttotal: 3m 22s\tremaining: 13.2s\n",
      "939:\tlearn: 0.0830744\ttotal: 3m 22s\tremaining: 12.9s\n",
      "940:\tlearn: 0.0830510\ttotal: 3m 22s\tremaining: 12.7s\n",
      "941:\tlearn: 0.0830015\ttotal: 3m 23s\tremaining: 12.5s\n",
      "942:\tlearn: 0.0829641\ttotal: 3m 23s\tremaining: 12.3s\n",
      "943:\tlearn: 0.0829501\ttotal: 3m 23s\tremaining: 12.1s\n",
      "944:\tlearn: 0.0828892\ttotal: 3m 23s\tremaining: 11.9s\n",
      "945:\tlearn: 0.0828737\ttotal: 3m 24s\tremaining: 11.6s\n",
      "946:\tlearn: 0.0828078\ttotal: 3m 24s\tremaining: 11.4s\n",
      "947:\tlearn: 0.0827612\ttotal: 3m 24s\tremaining: 11.2s\n",
      "948:\tlearn: 0.0827380\ttotal: 3m 24s\tremaining: 11s\n",
      "949:\tlearn: 0.0826949\ttotal: 3m 24s\tremaining: 10.8s\n",
      "950:\tlearn: 0.0826511\ttotal: 3m 25s\tremaining: 10.6s\n",
      "951:\tlearn: 0.0826238\ttotal: 3m 25s\tremaining: 10.4s\n",
      "952:\tlearn: 0.0825973\ttotal: 3m 25s\tremaining: 10.1s\n",
      "953:\tlearn: 0.0825725\ttotal: 3m 25s\tremaining: 9.92s\n",
      "954:\tlearn: 0.0825430\ttotal: 3m 25s\tremaining: 9.7s\n",
      "955:\tlearn: 0.0825068\ttotal: 3m 26s\tremaining: 9.48s\n",
      "956:\tlearn: 0.0824903\ttotal: 3m 26s\tremaining: 9.27s\n",
      "957:\tlearn: 0.0824042\ttotal: 3m 26s\tremaining: 9.05s\n",
      "958:\tlearn: 0.0823914\ttotal: 3m 26s\tremaining: 8.84s\n",
      "959:\tlearn: 0.0823713\ttotal: 3m 26s\tremaining: 8.62s\n",
      "960:\tlearn: 0.0823364\ttotal: 3m 27s\tremaining: 8.4s\n",
      "961:\tlearn: 0.0823022\ttotal: 3m 27s\tremaining: 8.19s\n",
      "962:\tlearn: 0.0822819\ttotal: 3m 27s\tremaining: 7.97s\n",
      "963:\tlearn: 0.0822208\ttotal: 3m 27s\tremaining: 7.76s\n",
      "964:\tlearn: 0.0821676\ttotal: 3m 27s\tremaining: 7.54s\n",
      "965:\tlearn: 0.0821491\ttotal: 3m 28s\tremaining: 7.33s\n",
      "966:\tlearn: 0.0821187\ttotal: 3m 28s\tremaining: 7.11s\n",
      "967:\tlearn: 0.0820907\ttotal: 3m 28s\tremaining: 6.89s\n",
      "968:\tlearn: 0.0820313\ttotal: 3m 28s\tremaining: 6.68s\n",
      "969:\tlearn: 0.0820051\ttotal: 3m 28s\tremaining: 6.46s\n",
      "970:\tlearn: 0.0819640\ttotal: 3m 29s\tremaining: 6.25s\n",
      "971:\tlearn: 0.0819340\ttotal: 3m 29s\tremaining: 6.03s\n",
      "972:\tlearn: 0.0818938\ttotal: 3m 29s\tremaining: 5.82s\n",
      "973:\tlearn: 0.0818398\ttotal: 3m 29s\tremaining: 5.6s\n",
      "974:\tlearn: 0.0817979\ttotal: 3m 30s\tremaining: 5.39s\n",
      "975:\tlearn: 0.0817619\ttotal: 3m 30s\tremaining: 5.17s\n",
      "976:\tlearn: 0.0817443\ttotal: 3m 30s\tremaining: 4.95s\n",
      "977:\tlearn: 0.0816932\ttotal: 3m 30s\tremaining: 4.74s\n",
      "978:\tlearn: 0.0816802\ttotal: 3m 30s\tremaining: 4.52s\n",
      "979:\tlearn: 0.0816361\ttotal: 3m 31s\tremaining: 4.31s\n",
      "980:\tlearn: 0.0816132\ttotal: 3m 31s\tremaining: 4.09s\n",
      "981:\tlearn: 0.0815945\ttotal: 3m 31s\tremaining: 3.88s\n",
      "982:\tlearn: 0.0815565\ttotal: 3m 31s\tremaining: 3.66s\n",
      "983:\tlearn: 0.0815321\ttotal: 3m 31s\tremaining: 3.44s\n",
      "984:\tlearn: 0.0814837\ttotal: 3m 32s\tremaining: 3.23s\n",
      "985:\tlearn: 0.0814656\ttotal: 3m 32s\tremaining: 3.01s\n",
      "986:\tlearn: 0.0814018\ttotal: 3m 32s\tremaining: 2.8s\n",
      "987:\tlearn: 0.0813413\ttotal: 3m 32s\tremaining: 2.58s\n",
      "988:\tlearn: 0.0813131\ttotal: 3m 32s\tremaining: 2.37s\n",
      "989:\tlearn: 0.0812847\ttotal: 3m 33s\tremaining: 2.15s\n",
      "990:\tlearn: 0.0812819\ttotal: 3m 33s\tremaining: 1.94s\n",
      "991:\tlearn: 0.0812301\ttotal: 3m 33s\tremaining: 1.72s\n",
      "992:\tlearn: 0.0811867\ttotal: 3m 33s\tremaining: 1.51s\n",
      "993:\tlearn: 0.0811214\ttotal: 3m 34s\tremaining: 1.29s\n",
      "994:\tlearn: 0.0810955\ttotal: 3m 34s\tremaining: 1.08s\n",
      "995:\tlearn: 0.0810455\ttotal: 3m 34s\tremaining: 861ms\n",
      "996:\tlearn: 0.0809863\ttotal: 3m 34s\tremaining: 646ms\n",
      "997:\tlearn: 0.0809627\ttotal: 3m 34s\tremaining: 430ms\n",
      "998:\tlearn: 0.0808893\ttotal: 3m 35s\tremaining: 215ms\n",
      "999:\tlearn: 0.0808457\ttotal: 3m 35s\tremaining: 0us\n",
      "Learning rate set to 0.149008\n",
      "0:\tlearn: 0.5305762\ttotal: 193ms\tremaining: 3m 12s\n",
      "1:\tlearn: 0.4293356\ttotal: 414ms\tremaining: 3m 26s\n",
      "2:\tlearn: 0.3652938\ttotal: 671ms\tremaining: 3m 43s\n",
      "3:\tlearn: 0.3277958\ttotal: 877ms\tremaining: 3m 38s\n",
      "4:\tlearn: 0.2987051\ttotal: 1.12s\tremaining: 3m 42s\n",
      "5:\tlearn: 0.2717319\ttotal: 1.36s\tremaining: 3m 45s\n",
      "6:\tlearn: 0.2588725\ttotal: 1.57s\tremaining: 3m 43s\n",
      "7:\tlearn: 0.2504893\ttotal: 1.76s\tremaining: 3m 38s\n",
      "8:\tlearn: 0.2388444\ttotal: 1.96s\tremaining: 3m 35s\n",
      "9:\tlearn: 0.2281880\ttotal: 2.13s\tremaining: 3m 31s\n",
      "10:\tlearn: 0.2218681\ttotal: 2.34s\tremaining: 3m 30s\n",
      "11:\tlearn: 0.2146351\ttotal: 2.52s\tremaining: 3m 27s\n",
      "12:\tlearn: 0.2107557\ttotal: 2.73s\tremaining: 3m 27s\n",
      "13:\tlearn: 0.2066373\ttotal: 2.9s\tremaining: 3m 24s\n",
      "14:\tlearn: 0.2038142\ttotal: 3.1s\tremaining: 3m 23s\n",
      "15:\tlearn: 0.1998826\ttotal: 3.35s\tremaining: 3m 26s\n",
      "16:\tlearn: 0.1976878\ttotal: 3.55s\tremaining: 3m 25s\n",
      "17:\tlearn: 0.1955689\ttotal: 3.78s\tremaining: 3m 26s\n",
      "18:\tlearn: 0.1930813\ttotal: 3.96s\tremaining: 3m 24s\n",
      "19:\tlearn: 0.1904410\ttotal: 4.16s\tremaining: 3m 23s\n",
      "20:\tlearn: 0.1890243\ttotal: 4.36s\tremaining: 3m 23s\n",
      "21:\tlearn: 0.1868505\ttotal: 4.58s\tremaining: 3m 23s\n",
      "22:\tlearn: 0.1846804\ttotal: 4.76s\tremaining: 3m 22s\n",
      "23:\tlearn: 0.1832019\ttotal: 4.97s\tremaining: 3m 22s\n",
      "24:\tlearn: 0.1817306\ttotal: 5.17s\tremaining: 3m 21s\n",
      "25:\tlearn: 0.1806415\ttotal: 5.38s\tremaining: 3m 21s\n",
      "26:\tlearn: 0.1795164\ttotal: 5.6s\tremaining: 3m 21s\n",
      "27:\tlearn: 0.1768966\ttotal: 5.79s\tremaining: 3m 21s\n",
      "28:\tlearn: 0.1754739\ttotal: 6s\tremaining: 3m 20s\n",
      "29:\tlearn: 0.1745268\ttotal: 6.2s\tremaining: 3m 20s\n",
      "30:\tlearn: 0.1734738\ttotal: 6.41s\tremaining: 3m 20s\n",
      "31:\tlearn: 0.1721848\ttotal: 6.63s\tremaining: 3m 20s\n",
      "32:\tlearn: 0.1714360\ttotal: 6.82s\tremaining: 3m 19s\n",
      "33:\tlearn: 0.1708028\ttotal: 7.03s\tremaining: 3m 19s\n",
      "34:\tlearn: 0.1699489\ttotal: 7.2s\tremaining: 3m 18s\n",
      "35:\tlearn: 0.1690373\ttotal: 7.39s\tremaining: 3m 17s\n",
      "36:\tlearn: 0.1682619\ttotal: 7.58s\tremaining: 3m 17s\n",
      "37:\tlearn: 0.1675239\ttotal: 7.79s\tremaining: 3m 17s\n",
      "38:\tlearn: 0.1667151\ttotal: 7.94s\tremaining: 3m 15s\n",
      "39:\tlearn: 0.1652728\ttotal: 8.16s\tremaining: 3m 15s\n",
      "40:\tlearn: 0.1643115\ttotal: 8.35s\tremaining: 3m 15s\n",
      "41:\tlearn: 0.1633742\ttotal: 8.54s\tremaining: 3m 14s\n",
      "42:\tlearn: 0.1625477\ttotal: 8.72s\tremaining: 3m 13s\n",
      "43:\tlearn: 0.1618784\ttotal: 8.93s\tremaining: 3m 14s\n",
      "44:\tlearn: 0.1611844\ttotal: 9.16s\tremaining: 3m 14s\n",
      "45:\tlearn: 0.1607323\ttotal: 9.39s\tremaining: 3m 14s\n",
      "46:\tlearn: 0.1600790\ttotal: 9.62s\tremaining: 3m 15s\n",
      "47:\tlearn: 0.1595710\ttotal: 9.82s\tremaining: 3m 14s\n",
      "48:\tlearn: 0.1590894\ttotal: 10s\tremaining: 3m 14s\n",
      "49:\tlearn: 0.1584576\ttotal: 10.3s\tremaining: 3m 15s\n",
      "50:\tlearn: 0.1579191\ttotal: 10.5s\tremaining: 3m 14s\n",
      "51:\tlearn: 0.1571936\ttotal: 10.7s\tremaining: 3m 14s\n",
      "52:\tlearn: 0.1563031\ttotal: 10.9s\tremaining: 3m 14s\n",
      "53:\tlearn: 0.1556238\ttotal: 11.1s\tremaining: 3m 14s\n",
      "54:\tlearn: 0.1552890\ttotal: 11.3s\tremaining: 3m 13s\n",
      "55:\tlearn: 0.1547427\ttotal: 11.5s\tremaining: 3m 13s\n",
      "56:\tlearn: 0.1542372\ttotal: 11.7s\tremaining: 3m 12s\n",
      "57:\tlearn: 0.1537021\ttotal: 11.9s\tremaining: 3m 12s\n",
      "58:\tlearn: 0.1532293\ttotal: 12.1s\tremaining: 3m 12s\n",
      "59:\tlearn: 0.1528140\ttotal: 12.3s\tremaining: 3m 12s\n",
      "60:\tlearn: 0.1525261\ttotal: 12.5s\tremaining: 3m 12s\n",
      "61:\tlearn: 0.1521899\ttotal: 12.7s\tremaining: 3m 12s\n",
      "62:\tlearn: 0.1517096\ttotal: 12.9s\tremaining: 3m 11s\n",
      "63:\tlearn: 0.1512353\ttotal: 13.1s\tremaining: 3m 11s\n",
      "64:\tlearn: 0.1508463\ttotal: 13.3s\tremaining: 3m 11s\n",
      "65:\tlearn: 0.1504604\ttotal: 13.5s\tremaining: 3m 11s\n",
      "66:\tlearn: 0.1500362\ttotal: 13.7s\tremaining: 3m 11s\n",
      "67:\tlearn: 0.1497446\ttotal: 13.9s\tremaining: 3m 10s\n",
      "68:\tlearn: 0.1493916\ttotal: 14.1s\tremaining: 3m 10s\n",
      "69:\tlearn: 0.1491489\ttotal: 14.4s\tremaining: 3m 11s\n",
      "70:\tlearn: 0.1488185\ttotal: 14.6s\tremaining: 3m 10s\n",
      "71:\tlearn: 0.1484230\ttotal: 14.8s\tremaining: 3m 11s\n",
      "72:\tlearn: 0.1481558\ttotal: 15s\tremaining: 3m 10s\n",
      "73:\tlearn: 0.1478935\ttotal: 15.2s\tremaining: 3m 10s\n",
      "74:\tlearn: 0.1474376\ttotal: 15.4s\tremaining: 3m 10s\n",
      "75:\tlearn: 0.1472089\ttotal: 15.7s\tremaining: 3m 10s\n",
      "76:\tlearn: 0.1468519\ttotal: 15.9s\tremaining: 3m 10s\n",
      "77:\tlearn: 0.1466160\ttotal: 16.1s\tremaining: 3m 10s\n",
      "78:\tlearn: 0.1462653\ttotal: 16.4s\tremaining: 3m 10s\n",
      "79:\tlearn: 0.1457765\ttotal: 16.6s\tremaining: 3m 10s\n",
      "80:\tlearn: 0.1455110\ttotal: 16.8s\tremaining: 3m 10s\n",
      "81:\tlearn: 0.1450832\ttotal: 17s\tremaining: 3m 10s\n",
      "82:\tlearn: 0.1448756\ttotal: 17.2s\tremaining: 3m 10s\n",
      "83:\tlearn: 0.1444840\ttotal: 17.5s\tremaining: 3m 10s\n",
      "84:\tlearn: 0.1442031\ttotal: 17.7s\tremaining: 3m 10s\n",
      "85:\tlearn: 0.1438991\ttotal: 17.9s\tremaining: 3m 10s\n",
      "86:\tlearn: 0.1436538\ttotal: 18.1s\tremaining: 3m 10s\n",
      "87:\tlearn: 0.1433876\ttotal: 18.4s\tremaining: 3m 10s\n",
      "88:\tlearn: 0.1431531\ttotal: 18.6s\tremaining: 3m 10s\n",
      "89:\tlearn: 0.1429437\ttotal: 18.8s\tremaining: 3m 9s\n",
      "90:\tlearn: 0.1427057\ttotal: 19s\tremaining: 3m 9s\n",
      "91:\tlearn: 0.1422875\ttotal: 19.2s\tremaining: 3m 9s\n",
      "92:\tlearn: 0.1420685\ttotal: 19.4s\tremaining: 3m 9s\n",
      "93:\tlearn: 0.1418807\ttotal: 19.6s\tremaining: 3m 9s\n",
      "94:\tlearn: 0.1415853\ttotal: 19.8s\tremaining: 3m 8s\n",
      "95:\tlearn: 0.1413487\ttotal: 20s\tremaining: 3m 8s\n",
      "96:\tlearn: 0.1409134\ttotal: 20.2s\tremaining: 3m 8s\n",
      "97:\tlearn: 0.1406326\ttotal: 20.5s\tremaining: 3m 8s\n",
      "98:\tlearn: 0.1404476\ttotal: 20.7s\tremaining: 3m 8s\n",
      "99:\tlearn: 0.1401304\ttotal: 20.9s\tremaining: 3m 8s\n",
      "100:\tlearn: 0.1398910\ttotal: 21.1s\tremaining: 3m 7s\n",
      "101:\tlearn: 0.1396906\ttotal: 21.3s\tremaining: 3m 7s\n",
      "102:\tlearn: 0.1395307\ttotal: 21.5s\tremaining: 3m 7s\n",
      "103:\tlearn: 0.1392149\ttotal: 21.7s\tremaining: 3m 7s\n",
      "104:\tlearn: 0.1389120\ttotal: 22s\tremaining: 3m 7s\n",
      "105:\tlearn: 0.1387186\ttotal: 22.2s\tremaining: 3m 7s\n",
      "106:\tlearn: 0.1384249\ttotal: 22.4s\tremaining: 3m 6s\n",
      "107:\tlearn: 0.1381963\ttotal: 22.6s\tremaining: 3m 6s\n",
      "108:\tlearn: 0.1380313\ttotal: 22.8s\tremaining: 3m 6s\n",
      "109:\tlearn: 0.1378899\ttotal: 23s\tremaining: 3m 6s\n",
      "110:\tlearn: 0.1376158\ttotal: 23.2s\tremaining: 3m 6s\n",
      "111:\tlearn: 0.1373726\ttotal: 23.4s\tremaining: 3m 5s\n",
      "112:\tlearn: 0.1372035\ttotal: 23.7s\tremaining: 3m 5s\n",
      "113:\tlearn: 0.1369422\ttotal: 23.9s\tremaining: 3m 5s\n",
      "114:\tlearn: 0.1367021\ttotal: 24.1s\tremaining: 3m 5s\n",
      "115:\tlearn: 0.1364107\ttotal: 24.3s\tremaining: 3m 5s\n",
      "116:\tlearn: 0.1362575\ttotal: 24.5s\tremaining: 3m 5s\n",
      "117:\tlearn: 0.1361350\ttotal: 24.7s\tremaining: 3m 4s\n",
      "118:\tlearn: 0.1359221\ttotal: 25s\tremaining: 3m 4s\n",
      "119:\tlearn: 0.1356985\ttotal: 25.1s\tremaining: 3m 4s\n",
      "120:\tlearn: 0.1354963\ttotal: 25.4s\tremaining: 3m 4s\n",
      "121:\tlearn: 0.1353770\ttotal: 25.6s\tremaining: 3m 3s\n",
      "122:\tlearn: 0.1351883\ttotal: 25.8s\tremaining: 3m 3s\n",
      "123:\tlearn: 0.1349447\ttotal: 26s\tremaining: 3m 3s\n",
      "124:\tlearn: 0.1347370\ttotal: 26.1s\tremaining: 3m 2s\n",
      "125:\tlearn: 0.1345486\ttotal: 26.4s\tremaining: 3m 2s\n",
      "126:\tlearn: 0.1343124\ttotal: 26.6s\tremaining: 3m 2s\n",
      "127:\tlearn: 0.1340833\ttotal: 26.8s\tremaining: 3m 2s\n",
      "128:\tlearn: 0.1337506\ttotal: 27s\tremaining: 3m 2s\n",
      "129:\tlearn: 0.1335951\ttotal: 27.2s\tremaining: 3m 2s\n",
      "130:\tlearn: 0.1334114\ttotal: 27.4s\tremaining: 3m 1s\n",
      "131:\tlearn: 0.1333086\ttotal: 27.6s\tremaining: 3m 1s\n",
      "132:\tlearn: 0.1331742\ttotal: 27.8s\tremaining: 3m 1s\n",
      "133:\tlearn: 0.1329542\ttotal: 28.1s\tremaining: 3m 1s\n",
      "134:\tlearn: 0.1327824\ttotal: 28.3s\tremaining: 3m 1s\n",
      "135:\tlearn: 0.1325145\ttotal: 28.5s\tremaining: 3m\n",
      "136:\tlearn: 0.1323484\ttotal: 28.7s\tremaining: 3m\n",
      "137:\tlearn: 0.1322115\ttotal: 28.9s\tremaining: 3m\n",
      "138:\tlearn: 0.1320655\ttotal: 29.1s\tremaining: 3m\n",
      "139:\tlearn: 0.1319157\ttotal: 29.3s\tremaining: 3m\n",
      "140:\tlearn: 0.1317200\ttotal: 29.5s\tremaining: 2m 59s\n",
      "141:\tlearn: 0.1315927\ttotal: 29.7s\tremaining: 2m 59s\n",
      "142:\tlearn: 0.1314538\ttotal: 29.9s\tremaining: 2m 59s\n",
      "143:\tlearn: 0.1313394\ttotal: 30.2s\tremaining: 2m 59s\n",
      "144:\tlearn: 0.1312510\ttotal: 30.4s\tremaining: 2m 59s\n",
      "145:\tlearn: 0.1310855\ttotal: 30.6s\tremaining: 2m 58s\n",
      "146:\tlearn: 0.1308356\ttotal: 30.8s\tremaining: 2m 58s\n",
      "147:\tlearn: 0.1306543\ttotal: 31s\tremaining: 2m 58s\n",
      "148:\tlearn: 0.1304933\ttotal: 31.2s\tremaining: 2m 58s\n",
      "149:\tlearn: 0.1303730\ttotal: 31.5s\tremaining: 2m 58s\n",
      "150:\tlearn: 0.1302521\ttotal: 31.7s\tremaining: 2m 58s\n",
      "151:\tlearn: 0.1301749\ttotal: 31.9s\tremaining: 2m 58s\n",
      "152:\tlearn: 0.1300378\ttotal: 32.2s\tremaining: 2m 58s\n",
      "153:\tlearn: 0.1299395\ttotal: 32.3s\tremaining: 2m 57s\n",
      "154:\tlearn: 0.1298772\ttotal: 32.6s\tremaining: 2m 57s\n",
      "155:\tlearn: 0.1297220\ttotal: 32.8s\tremaining: 2m 57s\n",
      "156:\tlearn: 0.1295544\ttotal: 33s\tremaining: 2m 57s\n",
      "157:\tlearn: 0.1294368\ttotal: 33.2s\tremaining: 2m 56s\n",
      "158:\tlearn: 0.1292247\ttotal: 33.4s\tremaining: 2m 56s\n",
      "159:\tlearn: 0.1291210\ttotal: 33.5s\tremaining: 2m 56s\n",
      "160:\tlearn: 0.1289400\ttotal: 33.7s\tremaining: 2m 55s\n",
      "161:\tlearn: 0.1287430\ttotal: 33.9s\tremaining: 2m 55s\n",
      "162:\tlearn: 0.1286097\ttotal: 34.1s\tremaining: 2m 55s\n",
      "163:\tlearn: 0.1284374\ttotal: 34.4s\tremaining: 2m 55s\n",
      "164:\tlearn: 0.1282765\ttotal: 34.6s\tremaining: 2m 54s\n",
      "165:\tlearn: 0.1281959\ttotal: 34.8s\tremaining: 2m 54s\n",
      "166:\tlearn: 0.1280552\ttotal: 35s\tremaining: 2m 54s\n",
      "167:\tlearn: 0.1278115\ttotal: 35.2s\tremaining: 2m 54s\n",
      "168:\tlearn: 0.1277176\ttotal: 35.4s\tremaining: 2m 54s\n",
      "169:\tlearn: 0.1276417\ttotal: 35.6s\tremaining: 2m 53s\n",
      "170:\tlearn: 0.1274819\ttotal: 35.8s\tremaining: 2m 53s\n",
      "171:\tlearn: 0.1273845\ttotal: 36.1s\tremaining: 2m 53s\n",
      "172:\tlearn: 0.1272227\ttotal: 36.3s\tremaining: 2m 53s\n",
      "173:\tlearn: 0.1271295\ttotal: 36.5s\tremaining: 2m 53s\n",
      "174:\tlearn: 0.1270040\ttotal: 36.7s\tremaining: 2m 53s\n",
      "175:\tlearn: 0.1268498\ttotal: 36.9s\tremaining: 2m 52s\n",
      "176:\tlearn: 0.1267255\ttotal: 37.1s\tremaining: 2m 52s\n",
      "177:\tlearn: 0.1266267\ttotal: 37.3s\tremaining: 2m 52s\n",
      "178:\tlearn: 0.1264909\ttotal: 37.5s\tremaining: 2m 51s\n",
      "179:\tlearn: 0.1264058\ttotal: 37.7s\tremaining: 2m 51s\n",
      "180:\tlearn: 0.1262913\ttotal: 37.9s\tremaining: 2m 51s\n",
      "181:\tlearn: 0.1261680\ttotal: 38.1s\tremaining: 2m 51s\n",
      "182:\tlearn: 0.1260807\ttotal: 38.3s\tremaining: 2m 50s\n",
      "183:\tlearn: 0.1259751\ttotal: 38.5s\tremaining: 2m 50s\n",
      "184:\tlearn: 0.1258862\ttotal: 38.7s\tremaining: 2m 50s\n",
      "185:\tlearn: 0.1257645\ttotal: 39s\tremaining: 2m 50s\n",
      "186:\tlearn: 0.1256504\ttotal: 39.2s\tremaining: 2m 50s\n",
      "187:\tlearn: 0.1255015\ttotal: 39.4s\tremaining: 2m 50s\n",
      "188:\tlearn: 0.1253826\ttotal: 39.7s\tremaining: 2m 50s\n",
      "189:\tlearn: 0.1252607\ttotal: 39.8s\tremaining: 2m 49s\n",
      "190:\tlearn: 0.1251154\ttotal: 40s\tremaining: 2m 49s\n",
      "191:\tlearn: 0.1249417\ttotal: 40.3s\tremaining: 2m 49s\n",
      "192:\tlearn: 0.1248210\ttotal: 40.5s\tremaining: 2m 49s\n",
      "193:\tlearn: 0.1247065\ttotal: 40.7s\tremaining: 2m 49s\n",
      "194:\tlearn: 0.1245856\ttotal: 41s\tremaining: 2m 49s\n",
      "195:\tlearn: 0.1244920\ttotal: 41.2s\tremaining: 2m 48s\n",
      "196:\tlearn: 0.1244169\ttotal: 41.4s\tremaining: 2m 48s\n",
      "197:\tlearn: 0.1242633\ttotal: 41.6s\tremaining: 2m 48s\n",
      "198:\tlearn: 0.1241016\ttotal: 41.8s\tremaining: 2m 48s\n",
      "199:\tlearn: 0.1240386\ttotal: 42s\tremaining: 2m 48s\n",
      "200:\tlearn: 0.1239127\ttotal: 42.2s\tremaining: 2m 47s\n",
      "201:\tlearn: 0.1238349\ttotal: 42.5s\tremaining: 2m 47s\n",
      "202:\tlearn: 0.1236436\ttotal: 42.7s\tremaining: 2m 47s\n",
      "203:\tlearn: 0.1235644\ttotal: 42.9s\tremaining: 2m 47s\n",
      "204:\tlearn: 0.1234299\ttotal: 43.1s\tremaining: 2m 47s\n",
      "205:\tlearn: 0.1233440\ttotal: 43.4s\tremaining: 2m 47s\n",
      "206:\tlearn: 0.1232334\ttotal: 43.6s\tremaining: 2m 47s\n",
      "207:\tlearn: 0.1231517\ttotal: 43.8s\tremaining: 2m 46s\n",
      "208:\tlearn: 0.1230659\ttotal: 44s\tremaining: 2m 46s\n",
      "209:\tlearn: 0.1229696\ttotal: 44.2s\tremaining: 2m 46s\n",
      "210:\tlearn: 0.1228954\ttotal: 44.4s\tremaining: 2m 46s\n",
      "211:\tlearn: 0.1228391\ttotal: 44.6s\tremaining: 2m 45s\n",
      "212:\tlearn: 0.1227071\ttotal: 44.8s\tremaining: 2m 45s\n",
      "213:\tlearn: 0.1226017\ttotal: 45s\tremaining: 2m 45s\n",
      "214:\tlearn: 0.1224937\ttotal: 45.2s\tremaining: 2m 45s\n",
      "215:\tlearn: 0.1224113\ttotal: 45.4s\tremaining: 2m 44s\n",
      "216:\tlearn: 0.1222952\ttotal: 45.6s\tremaining: 2m 44s\n",
      "217:\tlearn: 0.1222497\ttotal: 45.8s\tremaining: 2m 44s\n",
      "218:\tlearn: 0.1221840\ttotal: 46s\tremaining: 2m 44s\n",
      "219:\tlearn: 0.1220923\ttotal: 46.2s\tremaining: 2m 43s\n",
      "220:\tlearn: 0.1219303\ttotal: 46.5s\tremaining: 2m 43s\n",
      "221:\tlearn: 0.1218188\ttotal: 46.7s\tremaining: 2m 43s\n",
      "222:\tlearn: 0.1217566\ttotal: 46.9s\tremaining: 2m 43s\n",
      "223:\tlearn: 0.1216065\ttotal: 47.1s\tremaining: 2m 43s\n",
      "224:\tlearn: 0.1215036\ttotal: 47.3s\tremaining: 2m 42s\n",
      "225:\tlearn: 0.1214038\ttotal: 47.5s\tremaining: 2m 42s\n",
      "226:\tlearn: 0.1212880\ttotal: 47.7s\tremaining: 2m 42s\n",
      "227:\tlearn: 0.1211464\ttotal: 48s\tremaining: 2m 42s\n",
      "228:\tlearn: 0.1210562\ttotal: 48.2s\tremaining: 2m 42s\n",
      "229:\tlearn: 0.1210073\ttotal: 48.4s\tremaining: 2m 42s\n",
      "230:\tlearn: 0.1208741\ttotal: 48.6s\tremaining: 2m 41s\n",
      "231:\tlearn: 0.1207862\ttotal: 48.8s\tremaining: 2m 41s\n",
      "232:\tlearn: 0.1207205\ttotal: 49s\tremaining: 2m 41s\n",
      "233:\tlearn: 0.1206333\ttotal: 49.2s\tremaining: 2m 41s\n",
      "234:\tlearn: 0.1205616\ttotal: 49.4s\tremaining: 2m 40s\n",
      "235:\tlearn: 0.1204375\ttotal: 49.7s\tremaining: 2m 40s\n",
      "236:\tlearn: 0.1203464\ttotal: 49.9s\tremaining: 2m 40s\n",
      "237:\tlearn: 0.1202890\ttotal: 50.1s\tremaining: 2m 40s\n",
      "238:\tlearn: 0.1201638\ttotal: 50.3s\tremaining: 2m 40s\n",
      "239:\tlearn: 0.1200312\ttotal: 50.5s\tremaining: 2m 39s\n",
      "240:\tlearn: 0.1199726\ttotal: 50.7s\tremaining: 2m 39s\n",
      "241:\tlearn: 0.1198717\ttotal: 50.9s\tremaining: 2m 39s\n",
      "242:\tlearn: 0.1197687\ttotal: 51.1s\tremaining: 2m 39s\n",
      "243:\tlearn: 0.1196940\ttotal: 51.3s\tremaining: 2m 39s\n",
      "244:\tlearn: 0.1196444\ttotal: 51.5s\tremaining: 2m 38s\n",
      "245:\tlearn: 0.1195388\ttotal: 51.7s\tremaining: 2m 38s\n",
      "246:\tlearn: 0.1194765\ttotal: 52s\tremaining: 2m 38s\n",
      "247:\tlearn: 0.1193930\ttotal: 52.2s\tremaining: 2m 38s\n",
      "248:\tlearn: 0.1193130\ttotal: 52.4s\tremaining: 2m 38s\n",
      "249:\tlearn: 0.1192266\ttotal: 52.6s\tremaining: 2m 37s\n",
      "250:\tlearn: 0.1190789\ttotal: 52.8s\tremaining: 2m 37s\n",
      "251:\tlearn: 0.1190050\ttotal: 53s\tremaining: 2m 37s\n",
      "252:\tlearn: 0.1189500\ttotal: 53.2s\tremaining: 2m 37s\n",
      "253:\tlearn: 0.1188329\ttotal: 53.4s\tremaining: 2m 36s\n",
      "254:\tlearn: 0.1187322\ttotal: 53.6s\tremaining: 2m 36s\n",
      "255:\tlearn: 0.1186435\ttotal: 53.8s\tremaining: 2m 36s\n",
      "256:\tlearn: 0.1185747\ttotal: 54s\tremaining: 2m 36s\n",
      "257:\tlearn: 0.1184648\ttotal: 54.3s\tremaining: 2m 36s\n",
      "258:\tlearn: 0.1184133\ttotal: 54.5s\tremaining: 2m 35s\n",
      "259:\tlearn: 0.1183645\ttotal: 54.7s\tremaining: 2m 35s\n",
      "260:\tlearn: 0.1182858\ttotal: 54.9s\tremaining: 2m 35s\n",
      "261:\tlearn: 0.1181895\ttotal: 55.1s\tremaining: 2m 35s\n",
      "262:\tlearn: 0.1181282\ttotal: 55.3s\tremaining: 2m 35s\n",
      "263:\tlearn: 0.1179697\ttotal: 55.5s\tremaining: 2m 34s\n",
      "264:\tlearn: 0.1179227\ttotal: 55.7s\tremaining: 2m 34s\n",
      "265:\tlearn: 0.1178654\ttotal: 55.9s\tremaining: 2m 34s\n",
      "266:\tlearn: 0.1177635\ttotal: 56.1s\tremaining: 2m 34s\n",
      "267:\tlearn: 0.1177070\ttotal: 56.4s\tremaining: 2m 33s\n",
      "268:\tlearn: 0.1175471\ttotal: 56.6s\tremaining: 2m 33s\n",
      "269:\tlearn: 0.1174344\ttotal: 56.8s\tremaining: 2m 33s\n",
      "270:\tlearn: 0.1173267\ttotal: 57s\tremaining: 2m 33s\n",
      "271:\tlearn: 0.1172651\ttotal: 57.2s\tremaining: 2m 33s\n",
      "272:\tlearn: 0.1172085\ttotal: 57.4s\tremaining: 2m 32s\n",
      "273:\tlearn: 0.1171121\ttotal: 57.6s\tremaining: 2m 32s\n",
      "274:\tlearn: 0.1170284\ttotal: 57.8s\tremaining: 2m 32s\n",
      "275:\tlearn: 0.1169197\ttotal: 58.1s\tremaining: 2m 32s\n",
      "276:\tlearn: 0.1168589\ttotal: 58.3s\tremaining: 2m 32s\n",
      "277:\tlearn: 0.1168186\ttotal: 58.5s\tremaining: 2m 31s\n",
      "278:\tlearn: 0.1167376\ttotal: 58.7s\tremaining: 2m 31s\n",
      "279:\tlearn: 0.1166706\ttotal: 58.9s\tremaining: 2m 31s\n",
      "280:\tlearn: 0.1166133\ttotal: 59.1s\tremaining: 2m 31s\n",
      "281:\tlearn: 0.1165601\ttotal: 59.4s\tremaining: 2m 31s\n",
      "282:\tlearn: 0.1165022\ttotal: 59.5s\tremaining: 2m 30s\n",
      "283:\tlearn: 0.1164613\ttotal: 59.7s\tremaining: 2m 30s\n",
      "284:\tlearn: 0.1164030\ttotal: 59.9s\tremaining: 2m 30s\n",
      "285:\tlearn: 0.1163406\ttotal: 1m\tremaining: 2m 30s\n",
      "286:\tlearn: 0.1162783\ttotal: 1m\tremaining: 2m 29s\n",
      "287:\tlearn: 0.1162023\ttotal: 1m\tremaining: 2m 29s\n",
      "288:\tlearn: 0.1161088\ttotal: 1m\tremaining: 2m 29s\n",
      "289:\tlearn: 0.1160539\ttotal: 1m 1s\tremaining: 2m 29s\n",
      "290:\tlearn: 0.1159569\ttotal: 1m 1s\tremaining: 2m 29s\n",
      "291:\tlearn: 0.1158550\ttotal: 1m 1s\tremaining: 2m 29s\n",
      "292:\tlearn: 0.1157949\ttotal: 1m 1s\tremaining: 2m 28s\n",
      "293:\tlearn: 0.1156997\ttotal: 1m 1s\tremaining: 2m 28s\n",
      "294:\tlearn: 0.1156310\ttotal: 1m 2s\tremaining: 2m 28s\n",
      "295:\tlearn: 0.1155695\ttotal: 1m 2s\tremaining: 2m 28s\n",
      "296:\tlearn: 0.1154912\ttotal: 1m 2s\tremaining: 2m 28s\n",
      "297:\tlearn: 0.1153976\ttotal: 1m 2s\tremaining: 2m 28s\n",
      "298:\tlearn: 0.1152755\ttotal: 1m 3s\tremaining: 2m 27s\n",
      "299:\tlearn: 0.1151922\ttotal: 1m 3s\tremaining: 2m 27s\n",
      "300:\tlearn: 0.1151617\ttotal: 1m 3s\tremaining: 2m 27s\n",
      "301:\tlearn: 0.1150831\ttotal: 1m 3s\tremaining: 2m 27s\n",
      "302:\tlearn: 0.1150348\ttotal: 1m 4s\tremaining: 2m 27s\n",
      "303:\tlearn: 0.1149708\ttotal: 1m 4s\tremaining: 2m 27s\n",
      "304:\tlearn: 0.1149076\ttotal: 1m 4s\tremaining: 2m 26s\n",
      "305:\tlearn: 0.1148120\ttotal: 1m 4s\tremaining: 2m 26s\n",
      "306:\tlearn: 0.1147225\ttotal: 1m 4s\tremaining: 2m 26s\n",
      "307:\tlearn: 0.1146298\ttotal: 1m 5s\tremaining: 2m 26s\n",
      "308:\tlearn: 0.1145869\ttotal: 1m 5s\tremaining: 2m 25s\n",
      "309:\tlearn: 0.1145352\ttotal: 1m 5s\tremaining: 2m 25s\n",
      "310:\tlearn: 0.1144832\ttotal: 1m 5s\tremaining: 2m 25s\n",
      "311:\tlearn: 0.1143670\ttotal: 1m 5s\tremaining: 2m 25s\n",
      "312:\tlearn: 0.1142407\ttotal: 1m 6s\tremaining: 2m 25s\n",
      "313:\tlearn: 0.1141618\ttotal: 1m 6s\tremaining: 2m 24s\n",
      "314:\tlearn: 0.1141173\ttotal: 1m 6s\tremaining: 2m 24s\n",
      "315:\tlearn: 0.1140223\ttotal: 1m 6s\tremaining: 2m 24s\n",
      "316:\tlearn: 0.1139202\ttotal: 1m 6s\tremaining: 2m 24s\n",
      "317:\tlearn: 0.1138912\ttotal: 1m 7s\tremaining: 2m 24s\n",
      "318:\tlearn: 0.1138236\ttotal: 1m 7s\tremaining: 2m 23s\n",
      "319:\tlearn: 0.1137450\ttotal: 1m 7s\tremaining: 2m 23s\n",
      "320:\tlearn: 0.1136517\ttotal: 1m 7s\tremaining: 2m 23s\n",
      "321:\tlearn: 0.1135843\ttotal: 1m 8s\tremaining: 2m 23s\n",
      "322:\tlearn: 0.1135229\ttotal: 1m 8s\tremaining: 2m 23s\n",
      "323:\tlearn: 0.1134471\ttotal: 1m 8s\tremaining: 2m 22s\n",
      "324:\tlearn: 0.1133555\ttotal: 1m 8s\tremaining: 2m 22s\n",
      "325:\tlearn: 0.1132664\ttotal: 1m 8s\tremaining: 2m 22s\n",
      "326:\tlearn: 0.1131560\ttotal: 1m 9s\tremaining: 2m 22s\n",
      "327:\tlearn: 0.1130178\ttotal: 1m 9s\tremaining: 2m 22s\n",
      "328:\tlearn: 0.1129434\ttotal: 1m 9s\tremaining: 2m 21s\n",
      "329:\tlearn: 0.1128731\ttotal: 1m 9s\tremaining: 2m 21s\n",
      "330:\tlearn: 0.1128008\ttotal: 1m 9s\tremaining: 2m 21s\n",
      "331:\tlearn: 0.1127634\ttotal: 1m 10s\tremaining: 2m 21s\n",
      "332:\tlearn: 0.1127111\ttotal: 1m 10s\tremaining: 2m 20s\n",
      "333:\tlearn: 0.1126663\ttotal: 1m 10s\tremaining: 2m 20s\n",
      "334:\tlearn: 0.1126019\ttotal: 1m 10s\tremaining: 2m 20s\n",
      "335:\tlearn: 0.1125569\ttotal: 1m 11s\tremaining: 2m 20s\n",
      "336:\tlearn: 0.1124706\ttotal: 1m 11s\tremaining: 2m 20s\n",
      "337:\tlearn: 0.1124083\ttotal: 1m 11s\tremaining: 2m 19s\n",
      "338:\tlearn: 0.1123356\ttotal: 1m 11s\tremaining: 2m 19s\n",
      "339:\tlearn: 0.1122860\ttotal: 1m 11s\tremaining: 2m 19s\n",
      "340:\tlearn: 0.1121928\ttotal: 1m 12s\tremaining: 2m 19s\n",
      "341:\tlearn: 0.1121420\ttotal: 1m 12s\tremaining: 2m 18s\n",
      "342:\tlearn: 0.1120804\ttotal: 1m 12s\tremaining: 2m 18s\n",
      "343:\tlearn: 0.1119986\ttotal: 1m 12s\tremaining: 2m 18s\n",
      "344:\tlearn: 0.1119493\ttotal: 1m 12s\tremaining: 2m 18s\n",
      "345:\tlearn: 0.1118775\ttotal: 1m 13s\tremaining: 2m 18s\n",
      "346:\tlearn: 0.1118452\ttotal: 1m 13s\tremaining: 2m 17s\n",
      "347:\tlearn: 0.1117846\ttotal: 1m 13s\tremaining: 2m 17s\n",
      "348:\tlearn: 0.1117354\ttotal: 1m 13s\tremaining: 2m 17s\n",
      "349:\tlearn: 0.1116909\ttotal: 1m 13s\tremaining: 2m 17s\n",
      "350:\tlearn: 0.1116540\ttotal: 1m 14s\tremaining: 2m 16s\n",
      "351:\tlearn: 0.1114963\ttotal: 1m 14s\tremaining: 2m 16s\n",
      "352:\tlearn: 0.1114452\ttotal: 1m 14s\tremaining: 2m 16s\n",
      "353:\tlearn: 0.1113693\ttotal: 1m 14s\tremaining: 2m 16s\n",
      "354:\tlearn: 0.1113029\ttotal: 1m 14s\tremaining: 2m 16s\n",
      "355:\tlearn: 0.1112020\ttotal: 1m 15s\tremaining: 2m 15s\n",
      "356:\tlearn: 0.1111354\ttotal: 1m 15s\tremaining: 2m 15s\n",
      "357:\tlearn: 0.1110735\ttotal: 1m 15s\tremaining: 2m 15s\n",
      "358:\tlearn: 0.1110213\ttotal: 1m 15s\tremaining: 2m 15s\n",
      "359:\tlearn: 0.1109395\ttotal: 1m 15s\tremaining: 2m 14s\n",
      "360:\tlearn: 0.1108700\ttotal: 1m 16s\tremaining: 2m 14s\n",
      "361:\tlearn: 0.1108240\ttotal: 1m 16s\tremaining: 2m 14s\n",
      "362:\tlearn: 0.1107521\ttotal: 1m 16s\tremaining: 2m 14s\n",
      "363:\tlearn: 0.1106918\ttotal: 1m 16s\tremaining: 2m 14s\n",
      "364:\tlearn: 0.1106107\ttotal: 1m 17s\tremaining: 2m 14s\n",
      "365:\tlearn: 0.1105369\ttotal: 1m 17s\tremaining: 2m 13s\n",
      "366:\tlearn: 0.1104439\ttotal: 1m 17s\tremaining: 2m 13s\n",
      "367:\tlearn: 0.1103505\ttotal: 1m 17s\tremaining: 2m 13s\n",
      "368:\tlearn: 0.1102591\ttotal: 1m 17s\tremaining: 2m 13s\n",
      "369:\tlearn: 0.1102011\ttotal: 1m 18s\tremaining: 2m 12s\n",
      "370:\tlearn: 0.1101003\ttotal: 1m 18s\tremaining: 2m 12s\n",
      "371:\tlearn: 0.1100613\ttotal: 1m 18s\tremaining: 2m 12s\n",
      "372:\tlearn: 0.1099762\ttotal: 1m 18s\tremaining: 2m 12s\n",
      "373:\tlearn: 0.1099145\ttotal: 1m 18s\tremaining: 2m 11s\n",
      "374:\tlearn: 0.1098762\ttotal: 1m 19s\tremaining: 2m 11s\n",
      "375:\tlearn: 0.1098148\ttotal: 1m 19s\tremaining: 2m 11s\n",
      "376:\tlearn: 0.1097677\ttotal: 1m 19s\tremaining: 2m 11s\n",
      "377:\tlearn: 0.1097246\ttotal: 1m 19s\tremaining: 2m 11s\n",
      "378:\tlearn: 0.1096174\ttotal: 1m 19s\tremaining: 2m 10s\n",
      "379:\tlearn: 0.1095625\ttotal: 1m 20s\tremaining: 2m 10s\n",
      "380:\tlearn: 0.1095273\ttotal: 1m 20s\tremaining: 2m 10s\n",
      "381:\tlearn: 0.1094614\ttotal: 1m 20s\tremaining: 2m 10s\n",
      "382:\tlearn: 0.1094101\ttotal: 1m 20s\tremaining: 2m 10s\n",
      "383:\tlearn: 0.1093052\ttotal: 1m 20s\tremaining: 2m 9s\n",
      "384:\tlearn: 0.1092428\ttotal: 1m 21s\tremaining: 2m 9s\n",
      "385:\tlearn: 0.1091713\ttotal: 1m 21s\tremaining: 2m 9s\n",
      "386:\tlearn: 0.1091144\ttotal: 1m 21s\tremaining: 2m 9s\n",
      "387:\tlearn: 0.1090291\ttotal: 1m 21s\tremaining: 2m 8s\n",
      "388:\tlearn: 0.1089801\ttotal: 1m 22s\tremaining: 2m 8s\n",
      "389:\tlearn: 0.1089360\ttotal: 1m 22s\tremaining: 2m 8s\n",
      "390:\tlearn: 0.1088305\ttotal: 1m 22s\tremaining: 2m 8s\n",
      "391:\tlearn: 0.1087495\ttotal: 1m 22s\tremaining: 2m 8s\n",
      "392:\tlearn: 0.1086133\ttotal: 1m 22s\tremaining: 2m 7s\n",
      "393:\tlearn: 0.1085764\ttotal: 1m 23s\tremaining: 2m 7s\n",
      "394:\tlearn: 0.1085302\ttotal: 1m 23s\tremaining: 2m 7s\n",
      "395:\tlearn: 0.1084902\ttotal: 1m 23s\tremaining: 2m 7s\n",
      "396:\tlearn: 0.1083863\ttotal: 1m 23s\tremaining: 2m 7s\n",
      "397:\tlearn: 0.1082871\ttotal: 1m 23s\tremaining: 2m 6s\n",
      "398:\tlearn: 0.1082399\ttotal: 1m 24s\tremaining: 2m 6s\n",
      "399:\tlearn: 0.1081401\ttotal: 1m 24s\tremaining: 2m 6s\n",
      "400:\tlearn: 0.1080324\ttotal: 1m 24s\tremaining: 2m 6s\n",
      "401:\tlearn: 0.1079691\ttotal: 1m 24s\tremaining: 2m 6s\n",
      "402:\tlearn: 0.1079126\ttotal: 1m 24s\tremaining: 2m 5s\n",
      "403:\tlearn: 0.1078374\ttotal: 1m 25s\tremaining: 2m 5s\n",
      "404:\tlearn: 0.1077750\ttotal: 1m 25s\tremaining: 2m 5s\n",
      "405:\tlearn: 0.1077149\ttotal: 1m 25s\tremaining: 2m 5s\n",
      "406:\tlearn: 0.1076696\ttotal: 1m 25s\tremaining: 2m 5s\n",
      "407:\tlearn: 0.1075950\ttotal: 1m 26s\tremaining: 2m 4s\n",
      "408:\tlearn: 0.1075188\ttotal: 1m 26s\tremaining: 2m 4s\n",
      "409:\tlearn: 0.1074226\ttotal: 1m 26s\tremaining: 2m 4s\n",
      "410:\tlearn: 0.1073464\ttotal: 1m 26s\tremaining: 2m 4s\n",
      "411:\tlearn: 0.1073003\ttotal: 1m 26s\tremaining: 2m 3s\n",
      "412:\tlearn: 0.1072434\ttotal: 1m 27s\tremaining: 2m 3s\n",
      "413:\tlearn: 0.1071282\ttotal: 1m 27s\tremaining: 2m 3s\n",
      "414:\tlearn: 0.1071003\ttotal: 1m 27s\tremaining: 2m 3s\n",
      "415:\tlearn: 0.1070516\ttotal: 1m 27s\tremaining: 2m 3s\n",
      "416:\tlearn: 0.1070185\ttotal: 1m 27s\tremaining: 2m 2s\n",
      "417:\tlearn: 0.1069505\ttotal: 1m 28s\tremaining: 2m 2s\n",
      "418:\tlearn: 0.1068278\ttotal: 1m 28s\tremaining: 2m 2s\n",
      "419:\tlearn: 0.1068053\ttotal: 1m 28s\tremaining: 2m 2s\n",
      "420:\tlearn: 0.1067427\ttotal: 1m 28s\tremaining: 2m 1s\n",
      "421:\tlearn: 0.1066641\ttotal: 1m 28s\tremaining: 2m 1s\n",
      "422:\tlearn: 0.1065951\ttotal: 1m 28s\tremaining: 2m 1s\n",
      "423:\tlearn: 0.1064944\ttotal: 1m 29s\tremaining: 2m 1s\n",
      "424:\tlearn: 0.1064504\ttotal: 1m 29s\tremaining: 2m\n",
      "425:\tlearn: 0.1064062\ttotal: 1m 29s\tremaining: 2m\n",
      "426:\tlearn: 0.1063317\ttotal: 1m 29s\tremaining: 2m\n",
      "427:\tlearn: 0.1062838\ttotal: 1m 30s\tremaining: 2m\n",
      "428:\tlearn: 0.1062461\ttotal: 1m 30s\tremaining: 2m\n",
      "429:\tlearn: 0.1061558\ttotal: 1m 30s\tremaining: 1m 59s\n",
      "430:\tlearn: 0.1060828\ttotal: 1m 30s\tremaining: 1m 59s\n",
      "431:\tlearn: 0.1060286\ttotal: 1m 30s\tremaining: 1m 59s\n",
      "432:\tlearn: 0.1059804\ttotal: 1m 31s\tremaining: 1m 59s\n",
      "433:\tlearn: 0.1059250\ttotal: 1m 31s\tremaining: 1m 59s\n",
      "434:\tlearn: 0.1058217\ttotal: 1m 31s\tremaining: 1m 58s\n",
      "435:\tlearn: 0.1057284\ttotal: 1m 31s\tremaining: 1m 58s\n",
      "436:\tlearn: 0.1056752\ttotal: 1m 31s\tremaining: 1m 58s\n",
      "437:\tlearn: 0.1056139\ttotal: 1m 32s\tremaining: 1m 58s\n",
      "438:\tlearn: 0.1055649\ttotal: 1m 32s\tremaining: 1m 58s\n",
      "439:\tlearn: 0.1055149\ttotal: 1m 32s\tremaining: 1m 57s\n",
      "440:\tlearn: 0.1054636\ttotal: 1m 32s\tremaining: 1m 57s\n",
      "441:\tlearn: 0.1054089\ttotal: 1m 33s\tremaining: 1m 57s\n",
      "442:\tlearn: 0.1053369\ttotal: 1m 33s\tremaining: 1m 57s\n",
      "443:\tlearn: 0.1052204\ttotal: 1m 33s\tremaining: 1m 57s\n",
      "444:\tlearn: 0.1051566\ttotal: 1m 33s\tremaining: 1m 56s\n",
      "445:\tlearn: 0.1050736\ttotal: 1m 33s\tremaining: 1m 56s\n",
      "446:\tlearn: 0.1050295\ttotal: 1m 34s\tremaining: 1m 56s\n",
      "447:\tlearn: 0.1049854\ttotal: 1m 34s\tremaining: 1m 56s\n",
      "448:\tlearn: 0.1049749\ttotal: 1m 34s\tremaining: 1m 55s\n",
      "449:\tlearn: 0.1048955\ttotal: 1m 34s\tremaining: 1m 55s\n",
      "450:\tlearn: 0.1048662\ttotal: 1m 34s\tremaining: 1m 55s\n",
      "451:\tlearn: 0.1048262\ttotal: 1m 35s\tremaining: 1m 55s\n",
      "452:\tlearn: 0.1047660\ttotal: 1m 35s\tremaining: 1m 55s\n",
      "453:\tlearn: 0.1047245\ttotal: 1m 35s\tremaining: 1m 54s\n",
      "454:\tlearn: 0.1046554\ttotal: 1m 35s\tremaining: 1m 54s\n",
      "455:\tlearn: 0.1045327\ttotal: 1m 35s\tremaining: 1m 54s\n",
      "456:\tlearn: 0.1044988\ttotal: 1m 36s\tremaining: 1m 54s\n",
      "457:\tlearn: 0.1044299\ttotal: 1m 36s\tremaining: 1m 54s\n",
      "458:\tlearn: 0.1043627\ttotal: 1m 36s\tremaining: 1m 53s\n",
      "459:\tlearn: 0.1042728\ttotal: 1m 36s\tremaining: 1m 53s\n",
      "460:\tlearn: 0.1042291\ttotal: 1m 37s\tremaining: 1m 53s\n",
      "461:\tlearn: 0.1042118\ttotal: 1m 37s\tremaining: 1m 53s\n",
      "462:\tlearn: 0.1040953\ttotal: 1m 37s\tremaining: 1m 53s\n",
      "463:\tlearn: 0.1040525\ttotal: 1m 37s\tremaining: 1m 52s\n",
      "464:\tlearn: 0.1040010\ttotal: 1m 37s\tremaining: 1m 52s\n",
      "465:\tlearn: 0.1039403\ttotal: 1m 38s\tremaining: 1m 52s\n",
      "466:\tlearn: 0.1039100\ttotal: 1m 38s\tremaining: 1m 52s\n",
      "467:\tlearn: 0.1038543\ttotal: 1m 38s\tremaining: 1m 51s\n",
      "468:\tlearn: 0.1037837\ttotal: 1m 38s\tremaining: 1m 51s\n",
      "469:\tlearn: 0.1037388\ttotal: 1m 38s\tremaining: 1m 51s\n",
      "470:\tlearn: 0.1036491\ttotal: 1m 39s\tremaining: 1m 51s\n",
      "471:\tlearn: 0.1035580\ttotal: 1m 39s\tremaining: 1m 51s\n",
      "472:\tlearn: 0.1034854\ttotal: 1m 39s\tremaining: 1m 50s\n",
      "473:\tlearn: 0.1034767\ttotal: 1m 39s\tremaining: 1m 50s\n",
      "474:\tlearn: 0.1034297\ttotal: 1m 39s\tremaining: 1m 50s\n",
      "475:\tlearn: 0.1033708\ttotal: 1m 40s\tremaining: 1m 50s\n",
      "476:\tlearn: 0.1032923\ttotal: 1m 40s\tremaining: 1m 50s\n",
      "477:\tlearn: 0.1032282\ttotal: 1m 40s\tremaining: 1m 49s\n",
      "478:\tlearn: 0.1032027\ttotal: 1m 40s\tremaining: 1m 49s\n",
      "479:\tlearn: 0.1031959\ttotal: 1m 41s\tremaining: 1m 49s\n",
      "480:\tlearn: 0.1031186\ttotal: 1m 41s\tremaining: 1m 49s\n",
      "481:\tlearn: 0.1030522\ttotal: 1m 41s\tremaining: 1m 49s\n",
      "482:\tlearn: 0.1030083\ttotal: 1m 41s\tremaining: 1m 48s\n",
      "483:\tlearn: 0.1029627\ttotal: 1m 41s\tremaining: 1m 48s\n",
      "484:\tlearn: 0.1029161\ttotal: 1m 42s\tremaining: 1m 48s\n",
      "485:\tlearn: 0.1028841\ttotal: 1m 42s\tremaining: 1m 48s\n",
      "486:\tlearn: 0.1028268\ttotal: 1m 42s\tremaining: 1m 47s\n",
      "487:\tlearn: 0.1027890\ttotal: 1m 42s\tremaining: 1m 47s\n",
      "488:\tlearn: 0.1027273\ttotal: 1m 42s\tremaining: 1m 47s\n",
      "489:\tlearn: 0.1027206\ttotal: 1m 43s\tremaining: 1m 47s\n",
      "490:\tlearn: 0.1026200\ttotal: 1m 43s\tremaining: 1m 47s\n",
      "491:\tlearn: 0.1025597\ttotal: 1m 43s\tremaining: 1m 46s\n",
      "492:\tlearn: 0.1025341\ttotal: 1m 43s\tremaining: 1m 46s\n",
      "493:\tlearn: 0.1024242\ttotal: 1m 43s\tremaining: 1m 46s\n",
      "494:\tlearn: 0.1023897\ttotal: 1m 44s\tremaining: 1m 46s\n",
      "495:\tlearn: 0.1023339\ttotal: 1m 44s\tremaining: 1m 46s\n",
      "496:\tlearn: 0.1022858\ttotal: 1m 44s\tremaining: 1m 45s\n",
      "497:\tlearn: 0.1022194\ttotal: 1m 44s\tremaining: 1m 45s\n",
      "498:\tlearn: 0.1021656\ttotal: 1m 44s\tremaining: 1m 45s\n",
      "499:\tlearn: 0.1021272\ttotal: 1m 45s\tremaining: 1m 45s\n",
      "500:\tlearn: 0.1020985\ttotal: 1m 45s\tremaining: 1m 44s\n",
      "501:\tlearn: 0.1020794\ttotal: 1m 45s\tremaining: 1m 44s\n",
      "502:\tlearn: 0.1020231\ttotal: 1m 45s\tremaining: 1m 44s\n",
      "503:\tlearn: 0.1019823\ttotal: 1m 45s\tremaining: 1m 44s\n",
      "504:\tlearn: 0.1019521\ttotal: 1m 46s\tremaining: 1m 44s\n",
      "505:\tlearn: 0.1019151\ttotal: 1m 46s\tremaining: 1m 43s\n",
      "506:\tlearn: 0.1018894\ttotal: 1m 46s\tremaining: 1m 43s\n",
      "507:\tlearn: 0.1018444\ttotal: 1m 46s\tremaining: 1m 43s\n",
      "508:\tlearn: 0.1017638\ttotal: 1m 47s\tremaining: 1m 43s\n",
      "509:\tlearn: 0.1016894\ttotal: 1m 47s\tremaining: 1m 43s\n",
      "510:\tlearn: 0.1016106\ttotal: 1m 47s\tremaining: 1m 42s\n",
      "511:\tlearn: 0.1015793\ttotal: 1m 47s\tremaining: 1m 42s\n",
      "512:\tlearn: 0.1015351\ttotal: 1m 47s\tremaining: 1m 42s\n",
      "513:\tlearn: 0.1014736\ttotal: 1m 48s\tremaining: 1m 42s\n",
      "514:\tlearn: 0.1014262\ttotal: 1m 48s\tremaining: 1m 42s\n",
      "515:\tlearn: 0.1013491\ttotal: 1m 48s\tremaining: 1m 41s\n",
      "516:\tlearn: 0.1012642\ttotal: 1m 48s\tremaining: 1m 41s\n",
      "517:\tlearn: 0.1012073\ttotal: 1m 48s\tremaining: 1m 41s\n",
      "518:\tlearn: 0.1011405\ttotal: 1m 49s\tremaining: 1m 41s\n",
      "519:\tlearn: 0.1010773\ttotal: 1m 49s\tremaining: 1m 40s\n",
      "520:\tlearn: 0.1010328\ttotal: 1m 49s\tremaining: 1m 40s\n",
      "521:\tlearn: 0.1009864\ttotal: 1m 49s\tremaining: 1m 40s\n",
      "522:\tlearn: 0.1009574\ttotal: 1m 50s\tremaining: 1m 40s\n",
      "523:\tlearn: 0.1008626\ttotal: 1m 50s\tremaining: 1m 40s\n",
      "524:\tlearn: 0.1008337\ttotal: 1m 50s\tremaining: 1m 39s\n",
      "525:\tlearn: 0.1007404\ttotal: 1m 50s\tremaining: 1m 39s\n",
      "526:\tlearn: 0.1006816\ttotal: 1m 50s\tremaining: 1m 39s\n",
      "527:\tlearn: 0.1006052\ttotal: 1m 51s\tremaining: 1m 39s\n",
      "528:\tlearn: 0.1005303\ttotal: 1m 51s\tremaining: 1m 39s\n",
      "529:\tlearn: 0.1004987\ttotal: 1m 51s\tremaining: 1m 38s\n",
      "530:\tlearn: 0.1004222\ttotal: 1m 51s\tremaining: 1m 38s\n",
      "531:\tlearn: 0.1003359\ttotal: 1m 51s\tremaining: 1m 38s\n",
      "532:\tlearn: 0.1002849\ttotal: 1m 52s\tremaining: 1m 38s\n",
      "533:\tlearn: 0.1002442\ttotal: 1m 52s\tremaining: 1m 37s\n",
      "534:\tlearn: 0.1001974\ttotal: 1m 52s\tremaining: 1m 37s\n",
      "535:\tlearn: 0.1001644\ttotal: 1m 52s\tremaining: 1m 37s\n",
      "536:\tlearn: 0.1001100\ttotal: 1m 52s\tremaining: 1m 37s\n",
      "537:\tlearn: 0.1000710\ttotal: 1m 53s\tremaining: 1m 37s\n",
      "538:\tlearn: 0.1000111\ttotal: 1m 53s\tremaining: 1m 36s\n",
      "539:\tlearn: 0.0999607\ttotal: 1m 53s\tremaining: 1m 36s\n",
      "540:\tlearn: 0.0998740\ttotal: 1m 53s\tremaining: 1m 36s\n",
      "541:\tlearn: 0.0998376\ttotal: 1m 53s\tremaining: 1m 36s\n",
      "542:\tlearn: 0.0998056\ttotal: 1m 54s\tremaining: 1m 36s\n",
      "543:\tlearn: 0.0997649\ttotal: 1m 54s\tremaining: 1m 35s\n",
      "544:\tlearn: 0.0997037\ttotal: 1m 54s\tremaining: 1m 35s\n",
      "545:\tlearn: 0.0996502\ttotal: 1m 54s\tremaining: 1m 35s\n",
      "546:\tlearn: 0.0995962\ttotal: 1m 54s\tremaining: 1m 35s\n",
      "547:\tlearn: 0.0995663\ttotal: 1m 55s\tremaining: 1m 35s\n",
      "548:\tlearn: 0.0994633\ttotal: 1m 55s\tremaining: 1m 34s\n",
      "549:\tlearn: 0.0994057\ttotal: 1m 55s\tremaining: 1m 34s\n",
      "550:\tlearn: 0.0993479\ttotal: 1m 55s\tremaining: 1m 34s\n",
      "551:\tlearn: 0.0993076\ttotal: 1m 56s\tremaining: 1m 34s\n",
      "552:\tlearn: 0.0992605\ttotal: 1m 56s\tremaining: 1m 34s\n",
      "553:\tlearn: 0.0991861\ttotal: 1m 56s\tremaining: 1m 33s\n",
      "554:\tlearn: 0.0991565\ttotal: 1m 56s\tremaining: 1m 33s\n",
      "555:\tlearn: 0.0991095\ttotal: 1m 56s\tremaining: 1m 33s\n",
      "556:\tlearn: 0.0990794\ttotal: 1m 57s\tremaining: 1m 33s\n",
      "557:\tlearn: 0.0990455\ttotal: 1m 57s\tremaining: 1m 33s\n",
      "558:\tlearn: 0.0989691\ttotal: 1m 57s\tremaining: 1m 32s\n",
      "559:\tlearn: 0.0989288\ttotal: 1m 57s\tremaining: 1m 32s\n",
      "560:\tlearn: 0.0988422\ttotal: 1m 57s\tremaining: 1m 32s\n",
      "561:\tlearn: 0.0988097\ttotal: 1m 58s\tremaining: 1m 32s\n",
      "562:\tlearn: 0.0987549\ttotal: 1m 58s\tremaining: 1m 31s\n",
      "563:\tlearn: 0.0986558\ttotal: 1m 58s\tremaining: 1m 31s\n",
      "564:\tlearn: 0.0986075\ttotal: 1m 58s\tremaining: 1m 31s\n",
      "565:\tlearn: 0.0984958\ttotal: 1m 59s\tremaining: 1m 31s\n",
      "566:\tlearn: 0.0984500\ttotal: 1m 59s\tremaining: 1m 31s\n",
      "567:\tlearn: 0.0984264\ttotal: 1m 59s\tremaining: 1m 30s\n",
      "568:\tlearn: 0.0984029\ttotal: 1m 59s\tremaining: 1m 30s\n",
      "569:\tlearn: 0.0982970\ttotal: 1m 59s\tremaining: 1m 30s\n",
      "570:\tlearn: 0.0982481\ttotal: 2m\tremaining: 1m 30s\n",
      "571:\tlearn: 0.0982204\ttotal: 2m\tremaining: 1m 30s\n",
      "572:\tlearn: 0.0981650\ttotal: 2m\tremaining: 1m 29s\n",
      "573:\tlearn: 0.0981010\ttotal: 2m\tremaining: 1m 29s\n",
      "574:\tlearn: 0.0980394\ttotal: 2m 1s\tremaining: 1m 29s\n",
      "575:\tlearn: 0.0980071\ttotal: 2m 1s\tremaining: 1m 29s\n",
      "576:\tlearn: 0.0979778\ttotal: 2m 1s\tremaining: 1m 29s\n",
      "577:\tlearn: 0.0978939\ttotal: 2m 1s\tremaining: 1m 28s\n",
      "578:\tlearn: 0.0978556\ttotal: 2m 1s\tremaining: 1m 28s\n",
      "579:\tlearn: 0.0977954\ttotal: 2m 2s\tremaining: 1m 28s\n",
      "580:\tlearn: 0.0977683\ttotal: 2m 2s\tremaining: 1m 28s\n",
      "581:\tlearn: 0.0977194\ttotal: 2m 2s\tremaining: 1m 28s\n",
      "582:\tlearn: 0.0976748\ttotal: 2m 2s\tremaining: 1m 27s\n",
      "583:\tlearn: 0.0976157\ttotal: 2m 3s\tremaining: 1m 27s\n",
      "584:\tlearn: 0.0974878\ttotal: 2m 3s\tremaining: 1m 27s\n",
      "585:\tlearn: 0.0974502\ttotal: 2m 3s\tremaining: 1m 27s\n",
      "586:\tlearn: 0.0974232\ttotal: 2m 3s\tremaining: 1m 27s\n",
      "587:\tlearn: 0.0973783\ttotal: 2m 3s\tremaining: 1m 26s\n",
      "588:\tlearn: 0.0973550\ttotal: 2m 4s\tremaining: 1m 26s\n",
      "589:\tlearn: 0.0972901\ttotal: 2m 4s\tremaining: 1m 26s\n",
      "590:\tlearn: 0.0972556\ttotal: 2m 4s\tremaining: 1m 26s\n",
      "591:\tlearn: 0.0972095\ttotal: 2m 4s\tremaining: 1m 26s\n",
      "592:\tlearn: 0.0971757\ttotal: 2m 5s\tremaining: 1m 25s\n",
      "593:\tlearn: 0.0971423\ttotal: 2m 5s\tremaining: 1m 25s\n",
      "594:\tlearn: 0.0971140\ttotal: 2m 5s\tremaining: 1m 25s\n",
      "595:\tlearn: 0.0970877\ttotal: 2m 5s\tremaining: 1m 25s\n",
      "596:\tlearn: 0.0969896\ttotal: 2m 5s\tremaining: 1m 24s\n",
      "597:\tlearn: 0.0969400\ttotal: 2m 6s\tremaining: 1m 24s\n",
      "598:\tlearn: 0.0969059\ttotal: 2m 6s\tremaining: 1m 24s\n",
      "599:\tlearn: 0.0968669\ttotal: 2m 6s\tremaining: 1m 24s\n",
      "600:\tlearn: 0.0968301\ttotal: 2m 6s\tremaining: 1m 24s\n",
      "601:\tlearn: 0.0967783\ttotal: 2m 6s\tremaining: 1m 23s\n",
      "602:\tlearn: 0.0967215\ttotal: 2m 7s\tremaining: 1m 23s\n",
      "603:\tlearn: 0.0966804\ttotal: 2m 7s\tremaining: 1m 23s\n",
      "604:\tlearn: 0.0966547\ttotal: 2m 7s\tremaining: 1m 23s\n",
      "605:\tlearn: 0.0966120\ttotal: 2m 7s\tremaining: 1m 22s\n",
      "606:\tlearn: 0.0965769\ttotal: 2m 7s\tremaining: 1m 22s\n",
      "607:\tlearn: 0.0965145\ttotal: 2m 8s\tremaining: 1m 22s\n",
      "608:\tlearn: 0.0964772\ttotal: 2m 8s\tremaining: 1m 22s\n",
      "609:\tlearn: 0.0964422\ttotal: 2m 8s\tremaining: 1m 22s\n",
      "610:\tlearn: 0.0964081\ttotal: 2m 8s\tremaining: 1m 21s\n",
      "611:\tlearn: 0.0963832\ttotal: 2m 8s\tremaining: 1m 21s\n",
      "612:\tlearn: 0.0963282\ttotal: 2m 9s\tremaining: 1m 21s\n",
      "613:\tlearn: 0.0962739\ttotal: 2m 9s\tremaining: 1m 21s\n",
      "614:\tlearn: 0.0962228\ttotal: 2m 9s\tremaining: 1m 21s\n",
      "615:\tlearn: 0.0961758\ttotal: 2m 9s\tremaining: 1m 20s\n",
      "616:\tlearn: 0.0961361\ttotal: 2m 9s\tremaining: 1m 20s\n",
      "617:\tlearn: 0.0960625\ttotal: 2m 10s\tremaining: 1m 20s\n",
      "618:\tlearn: 0.0960145\ttotal: 2m 10s\tremaining: 1m 20s\n",
      "619:\tlearn: 0.0959887\ttotal: 2m 10s\tremaining: 1m 19s\n",
      "620:\tlearn: 0.0959409\ttotal: 2m 10s\tremaining: 1m 19s\n",
      "621:\tlearn: 0.0958971\ttotal: 2m 10s\tremaining: 1m 19s\n",
      "622:\tlearn: 0.0958453\ttotal: 2m 11s\tremaining: 1m 19s\n",
      "623:\tlearn: 0.0958016\ttotal: 2m 11s\tremaining: 1m 19s\n",
      "624:\tlearn: 0.0957838\ttotal: 2m 11s\tremaining: 1m 18s\n",
      "625:\tlearn: 0.0957385\ttotal: 2m 11s\tremaining: 1m 18s\n",
      "626:\tlearn: 0.0956860\ttotal: 2m 11s\tremaining: 1m 18s\n",
      "627:\tlearn: 0.0956387\ttotal: 2m 12s\tremaining: 1m 18s\n",
      "628:\tlearn: 0.0956080\ttotal: 2m 12s\tremaining: 1m 18s\n",
      "629:\tlearn: 0.0955813\ttotal: 2m 12s\tremaining: 1m 17s\n",
      "630:\tlearn: 0.0954935\ttotal: 2m 12s\tremaining: 1m 17s\n",
      "631:\tlearn: 0.0954707\ttotal: 2m 12s\tremaining: 1m 17s\n",
      "632:\tlearn: 0.0954179\ttotal: 2m 13s\tremaining: 1m 17s\n",
      "633:\tlearn: 0.0953444\ttotal: 2m 13s\tremaining: 1m 16s\n",
      "634:\tlearn: 0.0952679\ttotal: 2m 13s\tremaining: 1m 16s\n",
      "635:\tlearn: 0.0952435\ttotal: 2m 13s\tremaining: 1m 16s\n",
      "636:\tlearn: 0.0951812\ttotal: 2m 13s\tremaining: 1m 16s\n",
      "637:\tlearn: 0.0951345\ttotal: 2m 14s\tremaining: 1m 16s\n",
      "638:\tlearn: 0.0951018\ttotal: 2m 14s\tremaining: 1m 15s\n",
      "639:\tlearn: 0.0950488\ttotal: 2m 14s\tremaining: 1m 15s\n",
      "640:\tlearn: 0.0950181\ttotal: 2m 14s\tremaining: 1m 15s\n",
      "641:\tlearn: 0.0949496\ttotal: 2m 15s\tremaining: 1m 15s\n",
      "642:\tlearn: 0.0948527\ttotal: 2m 15s\tremaining: 1m 15s\n",
      "643:\tlearn: 0.0948265\ttotal: 2m 15s\tremaining: 1m 14s\n",
      "644:\tlearn: 0.0947958\ttotal: 2m 15s\tremaining: 1m 14s\n",
      "645:\tlearn: 0.0947487\ttotal: 2m 15s\tremaining: 1m 14s\n",
      "646:\tlearn: 0.0947344\ttotal: 2m 16s\tremaining: 1m 14s\n",
      "647:\tlearn: 0.0946797\ttotal: 2m 16s\tremaining: 1m 14s\n",
      "648:\tlearn: 0.0946152\ttotal: 2m 16s\tremaining: 1m 13s\n",
      "649:\tlearn: 0.0945794\ttotal: 2m 16s\tremaining: 1m 13s\n",
      "650:\tlearn: 0.0945500\ttotal: 2m 16s\tremaining: 1m 13s\n",
      "651:\tlearn: 0.0945191\ttotal: 2m 17s\tremaining: 1m 13s\n",
      "652:\tlearn: 0.0944774\ttotal: 2m 17s\tremaining: 1m 12s\n",
      "653:\tlearn: 0.0944433\ttotal: 2m 17s\tremaining: 1m 12s\n",
      "654:\tlearn: 0.0944005\ttotal: 2m 17s\tremaining: 1m 12s\n",
      "655:\tlearn: 0.0943539\ttotal: 2m 17s\tremaining: 1m 12s\n",
      "656:\tlearn: 0.0943425\ttotal: 2m 18s\tremaining: 1m 12s\n",
      "657:\tlearn: 0.0943027\ttotal: 2m 18s\tremaining: 1m 11s\n",
      "658:\tlearn: 0.0942314\ttotal: 2m 18s\tremaining: 1m 11s\n",
      "659:\tlearn: 0.0941976\ttotal: 2m 18s\tremaining: 1m 11s\n",
      "660:\tlearn: 0.0941410\ttotal: 2m 18s\tremaining: 1m 11s\n",
      "661:\tlearn: 0.0940905\ttotal: 2m 19s\tremaining: 1m 11s\n",
      "662:\tlearn: 0.0939999\ttotal: 2m 19s\tremaining: 1m 10s\n",
      "663:\tlearn: 0.0939355\ttotal: 2m 19s\tremaining: 1m 10s\n",
      "664:\tlearn: 0.0939070\ttotal: 2m 19s\tremaining: 1m 10s\n",
      "665:\tlearn: 0.0938710\ttotal: 2m 19s\tremaining: 1m 10s\n",
      "666:\tlearn: 0.0938399\ttotal: 2m 20s\tremaining: 1m 9s\n",
      "667:\tlearn: 0.0937874\ttotal: 2m 20s\tremaining: 1m 9s\n",
      "668:\tlearn: 0.0937481\ttotal: 2m 20s\tremaining: 1m 9s\n",
      "669:\tlearn: 0.0937011\ttotal: 2m 20s\tremaining: 1m 9s\n",
      "670:\tlearn: 0.0936704\ttotal: 2m 20s\tremaining: 1m 9s\n",
      "671:\tlearn: 0.0936141\ttotal: 2m 21s\tremaining: 1m 8s\n",
      "672:\tlearn: 0.0935395\ttotal: 2m 21s\tremaining: 1m 8s\n",
      "673:\tlearn: 0.0934618\ttotal: 2m 21s\tremaining: 1m 8s\n",
      "674:\tlearn: 0.0934423\ttotal: 2m 21s\tremaining: 1m 8s\n",
      "675:\tlearn: 0.0933993\ttotal: 2m 21s\tremaining: 1m 8s\n",
      "676:\tlearn: 0.0933695\ttotal: 2m 22s\tremaining: 1m 7s\n",
      "677:\tlearn: 0.0933182\ttotal: 2m 22s\tremaining: 1m 7s\n",
      "678:\tlearn: 0.0932794\ttotal: 2m 22s\tremaining: 1m 7s\n",
      "679:\tlearn: 0.0932536\ttotal: 2m 22s\tremaining: 1m 7s\n",
      "680:\tlearn: 0.0932321\ttotal: 2m 22s\tremaining: 1m 6s\n",
      "681:\tlearn: 0.0932227\ttotal: 2m 23s\tremaining: 1m 6s\n",
      "682:\tlearn: 0.0931736\ttotal: 2m 23s\tremaining: 1m 6s\n",
      "683:\tlearn: 0.0931201\ttotal: 2m 23s\tremaining: 1m 6s\n",
      "684:\tlearn: 0.0930771\ttotal: 2m 23s\tremaining: 1m 6s\n",
      "685:\tlearn: 0.0930517\ttotal: 2m 23s\tremaining: 1m 5s\n",
      "686:\tlearn: 0.0929855\ttotal: 2m 24s\tremaining: 1m 5s\n",
      "687:\tlearn: 0.0929595\ttotal: 2m 24s\tremaining: 1m 5s\n",
      "688:\tlearn: 0.0929122\ttotal: 2m 24s\tremaining: 1m 5s\n",
      "689:\tlearn: 0.0928180\ttotal: 2m 24s\tremaining: 1m 5s\n",
      "690:\tlearn: 0.0927516\ttotal: 2m 25s\tremaining: 1m 4s\n",
      "691:\tlearn: 0.0926364\ttotal: 2m 25s\tremaining: 1m 4s\n",
      "692:\tlearn: 0.0925878\ttotal: 2m 25s\tremaining: 1m 4s\n",
      "693:\tlearn: 0.0925506\ttotal: 2m 25s\tremaining: 1m 4s\n",
      "694:\tlearn: 0.0925154\ttotal: 2m 25s\tremaining: 1m 4s\n",
      "695:\tlearn: 0.0924762\ttotal: 2m 26s\tremaining: 1m 3s\n",
      "696:\tlearn: 0.0924506\ttotal: 2m 26s\tremaining: 1m 3s\n",
      "697:\tlearn: 0.0924213\ttotal: 2m 26s\tremaining: 1m 3s\n",
      "698:\tlearn: 0.0923954\ttotal: 2m 26s\tremaining: 1m 3s\n",
      "699:\tlearn: 0.0923341\ttotal: 2m 26s\tremaining: 1m 2s\n",
      "700:\tlearn: 0.0922913\ttotal: 2m 27s\tremaining: 1m 2s\n",
      "701:\tlearn: 0.0922789\ttotal: 2m 27s\tremaining: 1m 2s\n",
      "702:\tlearn: 0.0922601\ttotal: 2m 27s\tremaining: 1m 2s\n",
      "703:\tlearn: 0.0922298\ttotal: 2m 27s\tremaining: 1m 2s\n",
      "704:\tlearn: 0.0921634\ttotal: 2m 28s\tremaining: 1m 1s\n",
      "705:\tlearn: 0.0921149\ttotal: 2m 28s\tremaining: 1m 1s\n",
      "706:\tlearn: 0.0920687\ttotal: 2m 28s\tremaining: 1m 1s\n",
      "707:\tlearn: 0.0920230\ttotal: 2m 28s\tremaining: 1m 1s\n",
      "708:\tlearn: 0.0919941\ttotal: 2m 28s\tremaining: 1m 1s\n",
      "709:\tlearn: 0.0919835\ttotal: 2m 29s\tremaining: 1m\n",
      "710:\tlearn: 0.0919523\ttotal: 2m 29s\tremaining: 1m\n",
      "711:\tlearn: 0.0919187\ttotal: 2m 29s\tremaining: 1m\n",
      "712:\tlearn: 0.0918817\ttotal: 2m 29s\tremaining: 1m\n",
      "713:\tlearn: 0.0918200\ttotal: 2m 29s\tremaining: 1m\n",
      "714:\tlearn: 0.0917708\ttotal: 2m 30s\tremaining: 59.9s\n",
      "715:\tlearn: 0.0917429\ttotal: 2m 30s\tremaining: 59.7s\n",
      "716:\tlearn: 0.0916842\ttotal: 2m 30s\tremaining: 59.5s\n",
      "717:\tlearn: 0.0916088\ttotal: 2m 30s\tremaining: 59.3s\n",
      "718:\tlearn: 0.0915842\ttotal: 2m 31s\tremaining: 59s\n",
      "719:\tlearn: 0.0915282\ttotal: 2m 31s\tremaining: 58.8s\n",
      "720:\tlearn: 0.0914867\ttotal: 2m 31s\tremaining: 58.6s\n",
      "721:\tlearn: 0.0914445\ttotal: 2m 31s\tremaining: 58.4s\n",
      "722:\tlearn: 0.0914078\ttotal: 2m 31s\tremaining: 58.2s\n",
      "723:\tlearn: 0.0913773\ttotal: 2m 32s\tremaining: 58s\n",
      "724:\tlearn: 0.0913501\ttotal: 2m 32s\tremaining: 57.8s\n",
      "725:\tlearn: 0.0913074\ttotal: 2m 32s\tremaining: 57.6s\n",
      "726:\tlearn: 0.0912490\ttotal: 2m 32s\tremaining: 57.4s\n",
      "727:\tlearn: 0.0911930\ttotal: 2m 32s\tremaining: 57.1s\n",
      "728:\tlearn: 0.0911512\ttotal: 2m 33s\tremaining: 56.9s\n",
      "729:\tlearn: 0.0911266\ttotal: 2m 33s\tremaining: 56.7s\n",
      "730:\tlearn: 0.0910951\ttotal: 2m 33s\tremaining: 56.5s\n",
      "731:\tlearn: 0.0910816\ttotal: 2m 33s\tremaining: 56.3s\n",
      "732:\tlearn: 0.0910147\ttotal: 2m 33s\tremaining: 56.1s\n",
      "733:\tlearn: 0.0909843\ttotal: 2m 34s\tremaining: 55.8s\n",
      "734:\tlearn: 0.0909329\ttotal: 2m 34s\tremaining: 55.6s\n",
      "735:\tlearn: 0.0909055\ttotal: 2m 34s\tremaining: 55.4s\n",
      "736:\tlearn: 0.0908707\ttotal: 2m 34s\tremaining: 55.2s\n",
      "737:\tlearn: 0.0908465\ttotal: 2m 34s\tremaining: 55s\n",
      "738:\tlearn: 0.0907871\ttotal: 2m 35s\tremaining: 54.8s\n",
      "739:\tlearn: 0.0907430\ttotal: 2m 35s\tremaining: 54.6s\n",
      "740:\tlearn: 0.0907184\ttotal: 2m 35s\tremaining: 54.4s\n",
      "741:\tlearn: 0.0906700\ttotal: 2m 35s\tremaining: 54.2s\n",
      "742:\tlearn: 0.0906366\ttotal: 2m 36s\tremaining: 54s\n",
      "743:\tlearn: 0.0905955\ttotal: 2m 36s\tremaining: 53.8s\n",
      "744:\tlearn: 0.0905593\ttotal: 2m 36s\tremaining: 53.6s\n",
      "745:\tlearn: 0.0905334\ttotal: 2m 36s\tremaining: 53.4s\n",
      "746:\tlearn: 0.0905034\ttotal: 2m 36s\tremaining: 53.2s\n",
      "747:\tlearn: 0.0904768\ttotal: 2m 37s\tremaining: 53s\n",
      "748:\tlearn: 0.0904609\ttotal: 2m 37s\tremaining: 52.8s\n",
      "749:\tlearn: 0.0903972\ttotal: 2m 37s\tremaining: 52.6s\n",
      "750:\tlearn: 0.0903625\ttotal: 2m 37s\tremaining: 52.4s\n",
      "751:\tlearn: 0.0903338\ttotal: 2m 38s\tremaining: 52.1s\n",
      "752:\tlearn: 0.0902893\ttotal: 2m 38s\tremaining: 51.9s\n",
      "753:\tlearn: 0.0902626\ttotal: 2m 38s\tremaining: 51.7s\n",
      "754:\tlearn: 0.0902414\ttotal: 2m 38s\tremaining: 51.5s\n",
      "755:\tlearn: 0.0901815\ttotal: 2m 39s\tremaining: 51.3s\n",
      "756:\tlearn: 0.0901395\ttotal: 2m 39s\tremaining: 51.1s\n",
      "757:\tlearn: 0.0901126\ttotal: 2m 39s\tremaining: 50.9s\n",
      "758:\tlearn: 0.0900907\ttotal: 2m 39s\tremaining: 50.7s\n",
      "759:\tlearn: 0.0900584\ttotal: 2m 39s\tremaining: 50.5s\n",
      "760:\tlearn: 0.0900195\ttotal: 2m 40s\tremaining: 50.3s\n",
      "761:\tlearn: 0.0899808\ttotal: 2m 40s\tremaining: 50.1s\n",
      "762:\tlearn: 0.0899062\ttotal: 2m 40s\tremaining: 49.9s\n",
      "763:\tlearn: 0.0898678\ttotal: 2m 40s\tremaining: 49.7s\n",
      "764:\tlearn: 0.0898137\ttotal: 2m 40s\tremaining: 49.4s\n",
      "765:\tlearn: 0.0897452\ttotal: 2m 41s\tremaining: 49.2s\n",
      "766:\tlearn: 0.0897044\ttotal: 2m 41s\tremaining: 49s\n",
      "767:\tlearn: 0.0896606\ttotal: 2m 41s\tremaining: 48.8s\n",
      "768:\tlearn: 0.0896035\ttotal: 2m 41s\tremaining: 48.6s\n",
      "769:\tlearn: 0.0895884\ttotal: 2m 41s\tremaining: 48.4s\n",
      "770:\tlearn: 0.0895599\ttotal: 2m 42s\tremaining: 48.2s\n",
      "771:\tlearn: 0.0895237\ttotal: 2m 42s\tremaining: 48s\n",
      "772:\tlearn: 0.0894863\ttotal: 2m 42s\tremaining: 47.8s\n",
      "773:\tlearn: 0.0894325\ttotal: 2m 42s\tremaining: 47.5s\n",
      "774:\tlearn: 0.0893970\ttotal: 2m 43s\tremaining: 47.3s\n",
      "775:\tlearn: 0.0893288\ttotal: 2m 43s\tremaining: 47.1s\n",
      "776:\tlearn: 0.0892935\ttotal: 2m 43s\tremaining: 46.9s\n",
      "777:\tlearn: 0.0892614\ttotal: 2m 43s\tremaining: 46.7s\n",
      "778:\tlearn: 0.0892270\ttotal: 2m 43s\tremaining: 46.5s\n",
      "779:\tlearn: 0.0891627\ttotal: 2m 44s\tremaining: 46.3s\n",
      "780:\tlearn: 0.0891410\ttotal: 2m 44s\tremaining: 46.1s\n",
      "781:\tlearn: 0.0891358\ttotal: 2m 44s\tremaining: 45.9s\n",
      "782:\tlearn: 0.0890854\ttotal: 2m 44s\tremaining: 45.7s\n",
      "783:\tlearn: 0.0890543\ttotal: 2m 44s\tremaining: 45.4s\n",
      "784:\tlearn: 0.0890164\ttotal: 2m 45s\tremaining: 45.2s\n",
      "785:\tlearn: 0.0889811\ttotal: 2m 45s\tremaining: 45s\n",
      "786:\tlearn: 0.0889252\ttotal: 2m 45s\tremaining: 44.8s\n",
      "787:\tlearn: 0.0889189\ttotal: 2m 45s\tremaining: 44.6s\n",
      "788:\tlearn: 0.0888547\ttotal: 2m 46s\tremaining: 44.4s\n",
      "789:\tlearn: 0.0888137\ttotal: 2m 46s\tremaining: 44.2s\n",
      "790:\tlearn: 0.0887406\ttotal: 2m 46s\tremaining: 44s\n",
      "791:\tlearn: 0.0887069\ttotal: 2m 46s\tremaining: 43.8s\n",
      "792:\tlearn: 0.0886510\ttotal: 2m 46s\tremaining: 43.6s\n",
      "793:\tlearn: 0.0886325\ttotal: 2m 47s\tremaining: 43.4s\n",
      "794:\tlearn: 0.0885710\ttotal: 2m 47s\tremaining: 43.1s\n",
      "795:\tlearn: 0.0885239\ttotal: 2m 47s\tremaining: 42.9s\n",
      "796:\tlearn: 0.0884722\ttotal: 2m 47s\tremaining: 42.7s\n",
      "797:\tlearn: 0.0884497\ttotal: 2m 47s\tremaining: 42.5s\n",
      "798:\tlearn: 0.0883848\ttotal: 2m 48s\tremaining: 42.3s\n",
      "799:\tlearn: 0.0883521\ttotal: 2m 48s\tremaining: 42.1s\n",
      "800:\tlearn: 0.0883267\ttotal: 2m 48s\tremaining: 41.9s\n",
      "801:\tlearn: 0.0882974\ttotal: 2m 48s\tremaining: 41.7s\n",
      "802:\tlearn: 0.0882725\ttotal: 2m 49s\tremaining: 41.5s\n",
      "803:\tlearn: 0.0881787\ttotal: 2m 49s\tremaining: 41.3s\n",
      "804:\tlearn: 0.0881206\ttotal: 2m 49s\tremaining: 41s\n",
      "805:\tlearn: 0.0880434\ttotal: 2m 49s\tremaining: 40.8s\n",
      "806:\tlearn: 0.0880063\ttotal: 2m 49s\tremaining: 40.6s\n",
      "807:\tlearn: 0.0879735\ttotal: 2m 50s\tremaining: 40.4s\n",
      "808:\tlearn: 0.0879314\ttotal: 2m 50s\tremaining: 40.2s\n",
      "809:\tlearn: 0.0878980\ttotal: 2m 50s\tremaining: 40s\n",
      "810:\tlearn: 0.0878931\ttotal: 2m 50s\tremaining: 39.8s\n",
      "811:\tlearn: 0.0878491\ttotal: 2m 51s\tremaining: 39.6s\n",
      "812:\tlearn: 0.0878186\ttotal: 2m 51s\tremaining: 39.4s\n",
      "813:\tlearn: 0.0877875\ttotal: 2m 51s\tremaining: 39.2s\n",
      "814:\tlearn: 0.0877478\ttotal: 2m 51s\tremaining: 38.9s\n",
      "815:\tlearn: 0.0877238\ttotal: 2m 51s\tremaining: 38.7s\n",
      "816:\tlearn: 0.0877017\ttotal: 2m 52s\tremaining: 38.5s\n",
      "817:\tlearn: 0.0876482\ttotal: 2m 52s\tremaining: 38.3s\n",
      "818:\tlearn: 0.0876413\ttotal: 2m 52s\tremaining: 38.1s\n",
      "819:\tlearn: 0.0876081\ttotal: 2m 52s\tremaining: 37.9s\n",
      "820:\tlearn: 0.0875569\ttotal: 2m 52s\tremaining: 37.7s\n",
      "821:\tlearn: 0.0875220\ttotal: 2m 53s\tremaining: 37.5s\n",
      "822:\tlearn: 0.0874960\ttotal: 2m 53s\tremaining: 37.3s\n",
      "823:\tlearn: 0.0874616\ttotal: 2m 53s\tremaining: 37.1s\n",
      "824:\tlearn: 0.0874475\ttotal: 2m 53s\tremaining: 36.9s\n",
      "825:\tlearn: 0.0873833\ttotal: 2m 53s\tremaining: 36.7s\n",
      "826:\tlearn: 0.0873687\ttotal: 2m 54s\tremaining: 36.4s\n",
      "827:\tlearn: 0.0873007\ttotal: 2m 54s\tremaining: 36.2s\n",
      "828:\tlearn: 0.0872394\ttotal: 2m 54s\tremaining: 36s\n",
      "829:\tlearn: 0.0872320\ttotal: 2m 54s\tremaining: 35.8s\n",
      "830:\tlearn: 0.0872200\ttotal: 2m 55s\tremaining: 35.6s\n",
      "831:\tlearn: 0.0871551\ttotal: 2m 55s\tremaining: 35.4s\n",
      "832:\tlearn: 0.0871206\ttotal: 2m 55s\tremaining: 35.2s\n",
      "833:\tlearn: 0.0870698\ttotal: 2m 55s\tremaining: 35s\n",
      "834:\tlearn: 0.0870276\ttotal: 2m 55s\tremaining: 34.8s\n",
      "835:\tlearn: 0.0869828\ttotal: 2m 56s\tremaining: 34.6s\n",
      "836:\tlearn: 0.0869571\ttotal: 2m 56s\tremaining: 34.3s\n",
      "837:\tlearn: 0.0869150\ttotal: 2m 56s\tremaining: 34.1s\n",
      "838:\tlearn: 0.0868921\ttotal: 2m 56s\tremaining: 33.9s\n",
      "839:\tlearn: 0.0868426\ttotal: 2m 57s\tremaining: 33.7s\n",
      "840:\tlearn: 0.0867961\ttotal: 2m 57s\tremaining: 33.5s\n",
      "841:\tlearn: 0.0867665\ttotal: 2m 57s\tremaining: 33.3s\n",
      "842:\tlearn: 0.0867353\ttotal: 2m 57s\tremaining: 33.1s\n",
      "843:\tlearn: 0.0866860\ttotal: 2m 57s\tremaining: 32.9s\n",
      "844:\tlearn: 0.0866470\ttotal: 2m 58s\tremaining: 32.7s\n",
      "845:\tlearn: 0.0866213\ttotal: 2m 58s\tremaining: 32.5s\n",
      "846:\tlearn: 0.0865796\ttotal: 2m 58s\tremaining: 32.2s\n",
      "847:\tlearn: 0.0865276\ttotal: 2m 58s\tremaining: 32s\n",
      "848:\tlearn: 0.0864904\ttotal: 2m 58s\tremaining: 31.8s\n",
      "849:\tlearn: 0.0864477\ttotal: 2m 59s\tremaining: 31.6s\n",
      "850:\tlearn: 0.0864089\ttotal: 2m 59s\tremaining: 31.4s\n",
      "851:\tlearn: 0.0863767\ttotal: 2m 59s\tremaining: 31.2s\n",
      "852:\tlearn: 0.0863717\ttotal: 2m 59s\tremaining: 31s\n",
      "853:\tlearn: 0.0863286\ttotal: 2m 59s\tremaining: 30.8s\n",
      "854:\tlearn: 0.0863140\ttotal: 3m\tremaining: 30.6s\n",
      "855:\tlearn: 0.0862870\ttotal: 3m\tremaining: 30.4s\n",
      "856:\tlearn: 0.0862825\ttotal: 3m\tremaining: 30.1s\n",
      "857:\tlearn: 0.0862249\ttotal: 3m\tremaining: 29.9s\n",
      "858:\tlearn: 0.0861807\ttotal: 3m 1s\tremaining: 29.7s\n",
      "859:\tlearn: 0.0861414\ttotal: 3m 1s\tremaining: 29.5s\n",
      "860:\tlearn: 0.0861061\ttotal: 3m 1s\tremaining: 29.3s\n",
      "861:\tlearn: 0.0860634\ttotal: 3m 1s\tremaining: 29.1s\n",
      "862:\tlearn: 0.0860285\ttotal: 3m 1s\tremaining: 28.9s\n",
      "863:\tlearn: 0.0859729\ttotal: 3m 2s\tremaining: 28.7s\n",
      "864:\tlearn: 0.0859533\ttotal: 3m 2s\tremaining: 28.5s\n",
      "865:\tlearn: 0.0858918\ttotal: 3m 2s\tremaining: 28.2s\n",
      "866:\tlearn: 0.0858325\ttotal: 3m 2s\tremaining: 28s\n",
      "867:\tlearn: 0.0857919\ttotal: 3m 2s\tremaining: 27.8s\n",
      "868:\tlearn: 0.0856988\ttotal: 3m 3s\tremaining: 27.6s\n",
      "869:\tlearn: 0.0856540\ttotal: 3m 3s\tremaining: 27.4s\n",
      "870:\tlearn: 0.0856218\ttotal: 3m 3s\tremaining: 27.2s\n",
      "871:\tlearn: 0.0855877\ttotal: 3m 3s\tremaining: 27s\n",
      "872:\tlearn: 0.0855492\ttotal: 3m 3s\tremaining: 26.8s\n",
      "873:\tlearn: 0.0855331\ttotal: 3m 4s\tremaining: 26.6s\n",
      "874:\tlearn: 0.0854940\ttotal: 3m 4s\tremaining: 26.3s\n",
      "875:\tlearn: 0.0854876\ttotal: 3m 4s\tremaining: 26.1s\n",
      "876:\tlearn: 0.0854674\ttotal: 3m 4s\tremaining: 25.9s\n",
      "877:\tlearn: 0.0854419\ttotal: 3m 5s\tremaining: 25.7s\n",
      "878:\tlearn: 0.0854147\ttotal: 3m 5s\tremaining: 25.5s\n",
      "879:\tlearn: 0.0853686\ttotal: 3m 5s\tremaining: 25.3s\n",
      "880:\tlearn: 0.0853280\ttotal: 3m 5s\tremaining: 25.1s\n",
      "881:\tlearn: 0.0852962\ttotal: 3m 5s\tremaining: 24.9s\n",
      "882:\tlearn: 0.0852565\ttotal: 3m 6s\tremaining: 24.7s\n",
      "883:\tlearn: 0.0852260\ttotal: 3m 6s\tremaining: 24.4s\n",
      "884:\tlearn: 0.0851847\ttotal: 3m 6s\tremaining: 24.2s\n",
      "885:\tlearn: 0.0851320\ttotal: 3m 6s\tremaining: 24s\n",
      "886:\tlearn: 0.0851027\ttotal: 3m 6s\tremaining: 23.8s\n",
      "887:\tlearn: 0.0850764\ttotal: 3m 7s\tremaining: 23.6s\n",
      "888:\tlearn: 0.0850457\ttotal: 3m 7s\tremaining: 23.4s\n",
      "889:\tlearn: 0.0850389\ttotal: 3m 7s\tremaining: 23.2s\n",
      "890:\tlearn: 0.0849714\ttotal: 3m 7s\tremaining: 23s\n",
      "891:\tlearn: 0.0849499\ttotal: 3m 7s\tremaining: 22.8s\n",
      "892:\tlearn: 0.0849062\ttotal: 3m 8s\tremaining: 22.6s\n",
      "893:\tlearn: 0.0848599\ttotal: 3m 8s\tremaining: 22.3s\n",
      "894:\tlearn: 0.0848389\ttotal: 3m 8s\tremaining: 22.1s\n",
      "895:\tlearn: 0.0847946\ttotal: 3m 8s\tremaining: 21.9s\n",
      "896:\tlearn: 0.0847367\ttotal: 3m 8s\tremaining: 21.7s\n",
      "897:\tlearn: 0.0846964\ttotal: 3m 9s\tremaining: 21.5s\n",
      "898:\tlearn: 0.0846709\ttotal: 3m 9s\tremaining: 21.3s\n",
      "899:\tlearn: 0.0846219\ttotal: 3m 9s\tremaining: 21.1s\n",
      "900:\tlearn: 0.0845653\ttotal: 3m 9s\tremaining: 20.9s\n",
      "901:\tlearn: 0.0845454\ttotal: 3m 10s\tremaining: 20.7s\n",
      "902:\tlearn: 0.0845207\ttotal: 3m 10s\tremaining: 20.4s\n",
      "903:\tlearn: 0.0845017\ttotal: 3m 10s\tremaining: 20.2s\n",
      "904:\tlearn: 0.0844941\ttotal: 3m 10s\tremaining: 20s\n",
      "905:\tlearn: 0.0844244\ttotal: 3m 10s\tremaining: 19.8s\n",
      "906:\tlearn: 0.0843944\ttotal: 3m 11s\tremaining: 19.6s\n",
      "907:\tlearn: 0.0843625\ttotal: 3m 11s\tremaining: 19.4s\n",
      "908:\tlearn: 0.0843391\ttotal: 3m 11s\tremaining: 19.2s\n",
      "909:\tlearn: 0.0842959\ttotal: 3m 11s\tremaining: 19s\n",
      "910:\tlearn: 0.0842525\ttotal: 3m 11s\tremaining: 18.8s\n",
      "911:\tlearn: 0.0841919\ttotal: 3m 12s\tremaining: 18.5s\n",
      "912:\tlearn: 0.0841677\ttotal: 3m 12s\tremaining: 18.3s\n",
      "913:\tlearn: 0.0841375\ttotal: 3m 12s\tremaining: 18.1s\n",
      "914:\tlearn: 0.0841217\ttotal: 3m 12s\tremaining: 17.9s\n",
      "915:\tlearn: 0.0840669\ttotal: 3m 13s\tremaining: 17.7s\n",
      "916:\tlearn: 0.0840145\ttotal: 3m 13s\tremaining: 17.5s\n",
      "917:\tlearn: 0.0839656\ttotal: 3m 13s\tremaining: 17.3s\n",
      "918:\tlearn: 0.0839447\ttotal: 3m 13s\tremaining: 17.1s\n",
      "919:\tlearn: 0.0838942\ttotal: 3m 14s\tremaining: 16.9s\n",
      "920:\tlearn: 0.0838698\ttotal: 3m 14s\tremaining: 16.7s\n",
      "921:\tlearn: 0.0838076\ttotal: 3m 14s\tremaining: 16.5s\n",
      "922:\tlearn: 0.0837658\ttotal: 3m 14s\tremaining: 16.2s\n",
      "923:\tlearn: 0.0837121\ttotal: 3m 14s\tremaining: 16s\n",
      "924:\tlearn: 0.0836857\ttotal: 3m 15s\tremaining: 15.8s\n",
      "925:\tlearn: 0.0836628\ttotal: 3m 15s\tremaining: 15.6s\n",
      "926:\tlearn: 0.0835940\ttotal: 3m 15s\tremaining: 15.4s\n",
      "927:\tlearn: 0.0835544\ttotal: 3m 15s\tremaining: 15.2s\n",
      "928:\tlearn: 0.0835180\ttotal: 3m 15s\tremaining: 15s\n",
      "929:\tlearn: 0.0834843\ttotal: 3m 16s\tremaining: 14.8s\n",
      "930:\tlearn: 0.0834211\ttotal: 3m 16s\tremaining: 14.6s\n",
      "931:\tlearn: 0.0834060\ttotal: 3m 16s\tremaining: 14.3s\n",
      "932:\tlearn: 0.0833696\ttotal: 3m 16s\tremaining: 14.1s\n",
      "933:\tlearn: 0.0833310\ttotal: 3m 17s\tremaining: 13.9s\n",
      "934:\tlearn: 0.0832753\ttotal: 3m 17s\tremaining: 13.7s\n",
      "935:\tlearn: 0.0832417\ttotal: 3m 17s\tremaining: 13.5s\n",
      "936:\tlearn: 0.0831898\ttotal: 3m 17s\tremaining: 13.3s\n",
      "937:\tlearn: 0.0831646\ttotal: 3m 18s\tremaining: 13.1s\n",
      "938:\tlearn: 0.0831525\ttotal: 3m 18s\tremaining: 12.9s\n",
      "939:\tlearn: 0.0831119\ttotal: 3m 18s\tremaining: 12.7s\n",
      "940:\tlearn: 0.0830741\ttotal: 3m 18s\tremaining: 12.5s\n",
      "941:\tlearn: 0.0830361\ttotal: 3m 18s\tremaining: 12.2s\n",
      "942:\tlearn: 0.0830021\ttotal: 3m 19s\tremaining: 12s\n",
      "943:\tlearn: 0.0829724\ttotal: 3m 19s\tremaining: 11.8s\n",
      "944:\tlearn: 0.0829420\ttotal: 3m 19s\tremaining: 11.6s\n",
      "945:\tlearn: 0.0829183\ttotal: 3m 19s\tremaining: 11.4s\n",
      "946:\tlearn: 0.0828491\ttotal: 3m 19s\tremaining: 11.2s\n",
      "947:\tlearn: 0.0828169\ttotal: 3m 20s\tremaining: 11s\n",
      "948:\tlearn: 0.0827872\ttotal: 3m 20s\tremaining: 10.8s\n",
      "949:\tlearn: 0.0827577\ttotal: 3m 20s\tremaining: 10.6s\n",
      "950:\tlearn: 0.0827411\ttotal: 3m 20s\tremaining: 10.3s\n",
      "951:\tlearn: 0.0826985\ttotal: 3m 21s\tremaining: 10.1s\n",
      "952:\tlearn: 0.0826345\ttotal: 3m 21s\tremaining: 9.92s\n",
      "953:\tlearn: 0.0826111\ttotal: 3m 21s\tremaining: 9.71s\n",
      "954:\tlearn: 0.0825712\ttotal: 3m 21s\tremaining: 9.5s\n",
      "955:\tlearn: 0.0825154\ttotal: 3m 21s\tremaining: 9.29s\n",
      "956:\tlearn: 0.0824640\ttotal: 3m 22s\tremaining: 9.08s\n",
      "957:\tlearn: 0.0824215\ttotal: 3m 22s\tremaining: 8.87s\n",
      "958:\tlearn: 0.0823642\ttotal: 3m 22s\tremaining: 8.65s\n",
      "959:\tlearn: 0.0823232\ttotal: 3m 22s\tremaining: 8.45s\n",
      "960:\tlearn: 0.0822996\ttotal: 3m 22s\tremaining: 8.23s\n",
      "961:\tlearn: 0.0822548\ttotal: 3m 23s\tremaining: 8.02s\n",
      "962:\tlearn: 0.0822415\ttotal: 3m 23s\tremaining: 7.81s\n",
      "963:\tlearn: 0.0822068\ttotal: 3m 23s\tremaining: 7.6s\n",
      "964:\tlearn: 0.0821568\ttotal: 3m 23s\tremaining: 7.39s\n",
      "965:\tlearn: 0.0821063\ttotal: 3m 24s\tremaining: 7.18s\n",
      "966:\tlearn: 0.0820761\ttotal: 3m 24s\tremaining: 6.97s\n",
      "967:\tlearn: 0.0820248\ttotal: 3m 24s\tremaining: 6.76s\n",
      "968:\tlearn: 0.0819894\ttotal: 3m 24s\tremaining: 6.55s\n",
      "969:\tlearn: 0.0819633\ttotal: 3m 24s\tremaining: 6.34s\n",
      "970:\tlearn: 0.0819294\ttotal: 3m 25s\tremaining: 6.13s\n",
      "971:\tlearn: 0.0818981\ttotal: 3m 25s\tremaining: 5.92s\n",
      "972:\tlearn: 0.0818541\ttotal: 3m 25s\tremaining: 5.7s\n",
      "973:\tlearn: 0.0818411\ttotal: 3m 25s\tremaining: 5.49s\n",
      "974:\tlearn: 0.0817788\ttotal: 3m 25s\tremaining: 5.28s\n",
      "975:\tlearn: 0.0817157\ttotal: 3m 26s\tremaining: 5.07s\n",
      "976:\tlearn: 0.0816744\ttotal: 3m 26s\tremaining: 4.86s\n",
      "977:\tlearn: 0.0816629\ttotal: 3m 26s\tremaining: 4.65s\n",
      "978:\tlearn: 0.0816348\ttotal: 3m 26s\tremaining: 4.43s\n",
      "979:\tlearn: 0.0816116\ttotal: 3m 27s\tremaining: 4.22s\n",
      "980:\tlearn: 0.0815854\ttotal: 3m 27s\tremaining: 4.01s\n",
      "981:\tlearn: 0.0815802\ttotal: 3m 27s\tremaining: 3.8s\n",
      "982:\tlearn: 0.0815536\ttotal: 3m 27s\tremaining: 3.59s\n",
      "983:\tlearn: 0.0815206\ttotal: 3m 27s\tremaining: 3.38s\n",
      "984:\tlearn: 0.0814474\ttotal: 3m 28s\tremaining: 3.17s\n",
      "985:\tlearn: 0.0814154\ttotal: 3m 28s\tremaining: 2.96s\n",
      "986:\tlearn: 0.0813841\ttotal: 3m 28s\tremaining: 2.75s\n",
      "987:\tlearn: 0.0813289\ttotal: 3m 28s\tremaining: 2.54s\n",
      "988:\tlearn: 0.0813091\ttotal: 3m 28s\tremaining: 2.32s\n",
      "989:\tlearn: 0.0812968\ttotal: 3m 29s\tremaining: 2.11s\n",
      "990:\tlearn: 0.0812716\ttotal: 3m 29s\tremaining: 1.9s\n",
      "991:\tlearn: 0.0812433\ttotal: 3m 29s\tremaining: 1.69s\n",
      "992:\tlearn: 0.0812198\ttotal: 3m 29s\tremaining: 1.48s\n",
      "993:\tlearn: 0.0811939\ttotal: 3m 30s\tremaining: 1.27s\n",
      "994:\tlearn: 0.0811558\ttotal: 3m 30s\tremaining: 1.06s\n",
      "995:\tlearn: 0.0811072\ttotal: 3m 30s\tremaining: 845ms\n",
      "996:\tlearn: 0.0810892\ttotal: 3m 30s\tremaining: 634ms\n",
      "997:\tlearn: 0.0810789\ttotal: 3m 30s\tremaining: 423ms\n",
      "998:\tlearn: 0.0810704\ttotal: 3m 31s\tremaining: 211ms\n",
      "999:\tlearn: 0.0809816\ttotal: 3m 31s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99806331, 0.00193669],\n",
       "       [0.99578068, 0.00421932],\n",
       "       [0.98460837, 0.01539163],\n",
       "       ...,\n",
       "       [0.98592463, 0.01407537],\n",
       "       [0.99464787, 0.00535213],\n",
       "       [0.02397172, 0.97602828]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "\n",
    "# Au lieu d'obtenir les prédictions, on récupere les scores de probabilités pour chaque observations\n",
    "y_probas_cat = cross_val_predict(catboost_pipeline, X_train, y_train, method=\"predict_proba\")\n",
    "display(y_probas_cat)\n",
    "\n",
    "# On récupere uniquement les probabilités pour la classe positive\n",
    "y_scores_cat = y_probas_cat[:, 1]\n",
    "\n",
    "# A partir des probabilités on calcule les combinaisons de scores pour recall et precision en fonction du seuil\n",
    "precisions_cat, recalls_cat, thresholds_cat = precision_recall_curve(y_train, y_scores_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(thresholds_cat, precisions_cat[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds_cat, recalls_cat[:-1], \"g--\", label=\"Recall\")\n",
    "plt.title(\"Evolution du score precision/recall en fonction du seuil\")\n",
    "plt.xlabel(\"Seuil\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9405133913391643"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "plt.plot(recalls_cat[:-1], precisions_cat[:-1])\n",
    "\n",
    "plt.title(\"Evolution de la precision en fonction du recall\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "auc_pr = average_precision_score(y_train, y_scores_cat)\n",
    "display(auc_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# La fonction roc_curve retourne le FPR, TPR et les seuils \n",
    "fpr_cat, tpr_cat, tresholds_cat = roc_curve(y_train, y_scores_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour représenter la courbe ROC\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.title(\"Courbe ROC\")\n",
    "    plt.xlabel(\"Taux de Faux Positif\")\n",
    "    plt.ylabel(\"Taux de Vrai Positif\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(fpr_cat, tpr_cat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9837052441317594"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "display(roc_auc_score(y_train, y_scores_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.25      0.88      0.38      6950\n",
      "       P I F       0.91      0.31      0.47     27361\n",
      "\n",
      "    accuracy                           0.43     34311\n",
      "   macro avg       0.58      0.60      0.43     34311\n",
      "weighted avg       0.78      0.43      0.45     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.37      0.62      0.46      6950\n",
      "       P I F       0.88      0.73      0.80     27361\n",
      "\n",
      "    accuracy                           0.71     34311\n",
      "   macro avg       0.63      0.67      0.63     34311\n",
      "weighted avg       0.78      0.71      0.73     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', BernoulliNB())\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.73      0.42      0.53      6950\n",
      "       P I F       0.87      0.96      0.91     27361\n",
      "\n",
      "    accuracy                           0.85     34311\n",
      "   macro avg       0.80      0.69      0.72     34311\n",
      "weighted avg       0.84      0.85      0.83     34311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "df = pd.read_csv('dataset_test2.csv')\n",
    "\n",
    "# Sélectionner les variables indépendantes et la variable cible\n",
    "X = df.drop('MIS_Status', axis=1)\n",
    "y = df['MIS_Status']\n",
    "\n",
    "# Encoder la variable cible\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identifier les variables catégorielles et numériques\n",
    "cat_vars = X.select_dtypes(include=['object']).columns.tolist() + ['NewExist'] + ['UrbanRural'] + ['FranchiseBinary'] \n",
    "num_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_vars.remove('NewExist')  \n",
    "num_vars.remove('UrbanRural') \n",
    "num_vars.remove('FranchiseBinary') \n",
    "\n",
    "# Créer les transformateurs pour les pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformateurs dans un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_vars),\n",
    "        ('cat', categorical_transformer, cat_vars)\n",
    "    ])\n",
    "\n",
    "# Créer la pipeline de traitement et de modélisation\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test, stratifier sur y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, shuffle=True, test_size=0.05, random_state=42, stratify=y_encoded)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
